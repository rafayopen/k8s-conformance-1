Mar  2 22:29:12.669: INFO: Overriding default scale value of zero to 1
Mar  2 22:29:12.669: INFO: Overriding default milliseconds value of zero to 5000
I0302 22:29:13.145584   27430 e2e.go:304] Starting e2e run "9acb83be-3d3a-11e9-9008-0a58ac10f353" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551565752 - Will randomize all specs
Will run 189 of 2011 specs

I0302 22:29:13.216664   27430 e2e.go:59] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
Mar  2 22:29:13.216: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 22:29:13.221: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Mar  2 22:29:13.305: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 22:29:13.375: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 22:29:13.375: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Mar  2 22:29:13.375: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 22:29:13.397: INFO: e2e test version: v1.12.7-beta.0.5+b17cdb9d0b95c4
Mar  2 22:29:13.411: INFO: kube-apiserver version: v1.12.4+761b685
I0302 22:29:13.411965   27430 e2e.go:59] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:29:13.412: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
Mar  2 22:29:14.429: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-tc5b9
Mar  2 22:29:24.528: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-tc5b9
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:29:24.557: INFO: Initial restart count of pod liveness-exec is 0
Mar  2 22:30:13.014: INFO: Restart count of pod e2e-tests-container-probe-tc5b9/liveness-exec is now 1 (48.457245115s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:30:13.045: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-tc5b9" for this suite.
Mar  2 22:30:19.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:30:21.007: INFO: namespace: e2e-tests-container-probe-tc5b9, resource: bindings, ignored listing per whitelist
Mar  2 22:30:21.234: INFO: namespace: e2e-tests-container-probe-tc5b9, resource: packagemanifests, items remaining: 1
Mar  2 22:30:21.837: INFO: namespace: e2e-tests-container-probe-tc5b9 no longer exists
Mar  2 22:30:21.856: INFO: namespace: e2e-tests-container-probe-tc5b9, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:30:21.873: INFO: namespace e2e-tests-container-probe-tc5b9 deletion completed in 8.79597548s

• [SLOW TEST:68.461 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:30:21.873: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 22:30:22.960: INFO: Waiting up to 5m0s for pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-8c9q9" to be "success or failure"
Mar  2 22:30:22.981: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.927904ms
Mar  2 22:30:24.999: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039087831s
Mar  2 22:30:27.017: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05719552s
Mar  2 22:30:29.036: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075445712s
Mar  2 22:30:31.054: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09413673s
Mar  2 22:30:33.073: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.11253955s
Mar  2 22:30:35.095: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.135243254s
STEP: Saw pod success
Mar  2 22:30:35.095: INFO: Pod "pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:30:35.113: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 22:30:35.207: INFO: Waiting for pod pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:30:35.225: INFO: Pod pod-c4b64da5-3d3a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:30:35.225: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8c9q9" for this suite.
Mar  2 22:30:41.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:30:42.212: INFO: namespace: e2e-tests-emptydir-8c9q9, resource: packagemanifests, items remaining: 1
Mar  2 22:30:42.572: INFO: namespace: e2e-tests-emptydir-8c9q9, resource: bindings, ignored listing per whitelist
Mar  2 22:30:43.747: INFO: namespace: e2e-tests-emptydir-8c9q9 no longer exists
Mar  2 22:30:43.766: INFO: namespace: e2e-tests-emptydir-8c9q9, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:30:43.783: INFO: namespace e2e-tests-emptydir-8c9q9 deletion completed in 8.525816195s

• [SLOW TEST:21.910 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:30:43.783: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-d1c73504-3d3a-11e9-9008-0a58ac10f353
STEP: Creating secret with name secret-projected-all-test-volume-d1c734f0-3d3a-11e9-9008-0a58ac10f353
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  2 22:30:44.915: INFO: Waiting up to 5m0s for pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-dx2m7" to be "success or failure"
Mar  2 22:30:44.933: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.725602ms
Mar  2 22:30:46.952: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036017978s
Mar  2 22:30:48.969: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053852825s
Mar  2 22:30:50.989: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073699933s
Mar  2 22:30:53.007: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091900151s
Mar  2 22:30:55.031: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.115944611s
STEP: Saw pod success
Mar  2 22:30:55.032: INFO: Pod "projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:30:55.049: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  2 22:30:55.144: INFO: Waiting for pod projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:30:55.162: INFO: Pod projected-volume-d1c734bc-3d3a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:30:55.162: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dx2m7" for this suite.
Mar  2 22:31:03.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:31:05.077: INFO: namespace: e2e-tests-projected-dx2m7, resource: packagemanifests, items remaining: 1
Mar  2 22:31:05.095: INFO: namespace: e2e-tests-projected-dx2m7, resource: bindings, ignored listing per whitelist
Mar  2 22:31:05.698: INFO: namespace: e2e-tests-projected-dx2m7 no longer exists
Mar  2 22:31:05.716: INFO: namespace: e2e-tests-projected-dx2m7, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:31:05.733: INFO: namespace e2e-tests-projected-dx2m7 deletion completed in 10.535129705s

• [SLOW TEST:21.950 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:31:05.734: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar  2 22:31:06.787: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig api-versions'
Mar  2 22:31:06.994: INFO: stderr: ""
Mar  2 22:31:06.994: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1alpha1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1beta1\nconfig.openshift.io/v1\ncoordination.k8s.io/v1beta1\ndns.openshift.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.openshift.io/v1alpha1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmarketplace.redhat.com/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\nnetworkoperator.openshift.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperatorstatus.openshift.io/v1\npackages.apps.redhat.com/v1alpha1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nservicecertsigner.config.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:31:06.994: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hphlk" for this suite.
Mar  2 22:31:13.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:31:14.599: INFO: namespace: e2e-tests-kubectl-hphlk, resource: packagemanifests, items remaining: 1
Mar  2 22:31:15.222: INFO: namespace: e2e-tests-kubectl-hphlk, resource: bindings, ignored listing per whitelist
Mar  2 22:31:15.524: INFO: namespace: e2e-tests-kubectl-hphlk no longer exists
Mar  2 22:31:15.543: INFO: namespace: e2e-tests-kubectl-hphlk, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:31:15.560: INFO: namespace e2e-tests-kubectl-hphlk deletion completed in 8.534976574s

• [SLOW TEST:9.827 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:31:15.560: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e4b67fbb-3d3a-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 22:31:16.713: INFO: Waiting up to 5m0s for pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-468r6" to be "success or failure"
Mar  2 22:31:16.732: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.704965ms
Mar  2 22:31:18.750: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037617072s
Mar  2 22:31:20.769: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056113465s
Mar  2 22:31:22.787: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074778922s
Mar  2 22:31:24.807: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094373765s
Mar  2 22:31:26.825: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112463237s
STEP: Saw pod success
Mar  2 22:31:26.825: INFO: Pod "pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:31:26.843: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:31:26.895: INFO: Waiting for pod pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:31:26.913: INFO: Pod pod-secrets-e4bf41aa-3d3a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:31:26.913: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-468r6" for this suite.
Mar  2 22:31:33.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:31:34.452: INFO: namespace: e2e-tests-secrets-468r6, resource: bindings, ignored listing per whitelist
Mar  2 22:31:35.429: INFO: namespace: e2e-tests-secrets-468r6, resource: packagemanifests, items remaining: 1
Mar  2 22:31:35.448: INFO: namespace: e2e-tests-secrets-468r6 no longer exists
Mar  2 22:31:35.466: INFO: namespace: e2e-tests-secrets-468r6, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:31:35.483: INFO: namespace e2e-tests-secrets-468r6 deletion completed in 8.536365627s

• [SLOW TEST:19.923 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:31:35.483: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  2 22:31:36.574: INFO: Waiting up to 5m0s for pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-4dbv6" to be "success or failure"
Mar  2 22:31:36.592: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.491894ms
Mar  2 22:31:38.611: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036445696s
Mar  2 22:31:40.628: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054395114s
Mar  2 22:31:42.647: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073094768s
Mar  2 22:31:44.665: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091108025s
Mar  2 22:31:46.705: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.130496331s
STEP: Saw pod success
Mar  2 22:31:46.705: INFO: Pod "downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:31:46.722: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:31:46.811: INFO: Waiting for pod downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:31:46.829: INFO: Pod downward-api-f0979527-3d3a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:31:46.829: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4dbv6" for this suite.
Mar  2 22:31:52.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:31:53.987: INFO: namespace: e2e-tests-downward-api-4dbv6, resource: packagemanifests, items remaining: 1
Mar  2 22:31:54.039: INFO: namespace: e2e-tests-downward-api-4dbv6, resource: bindings, ignored listing per whitelist
Mar  2 22:31:55.352: INFO: namespace: e2e-tests-downward-api-4dbv6 no longer exists
Mar  2 22:31:55.370: INFO: namespace: e2e-tests-downward-api-4dbv6, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:31:55.387: INFO: namespace e2e-tests-downward-api-4dbv6 deletion completed in 8.526750599s

• [SLOW TEST:19.904 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:31:55.388: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  2 22:31:56.435: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-8jmpt'
Mar  2 22:31:57.868: INFO: stderr: ""
Mar  2 22:31:57.868: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  2 22:31:58.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:31:58.886: INFO: Found 0 / 1
Mar  2 22:31:59.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:31:59.886: INFO: Found 0 / 1
Mar  2 22:32:00.888: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:00.888: INFO: Found 0 / 1
Mar  2 22:32:01.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:01.887: INFO: Found 0 / 1
Mar  2 22:32:02.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:02.886: INFO: Found 0 / 1
Mar  2 22:32:03.887: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:03.887: INFO: Found 0 / 1
Mar  2 22:32:04.887: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:04.887: INFO: Found 0 / 1
Mar  2 22:32:05.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:05.886: INFO: Found 0 / 1
Mar  2 22:32:06.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:06.887: INFO: Found 0 / 1
Mar  2 22:32:07.887: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:07.888: INFO: Found 0 / 1
Mar  2 22:32:08.887: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:08.887: INFO: Found 1 / 1
Mar  2 22:32:08.887: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  2 22:32:08.906: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:08.906: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:32:08.906: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig patch pod redis-master-xrc9z --namespace=e2e-tests-kubectl-8jmpt -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 22:32:09.172: INFO: stderr: ""
Mar  2 22:32:09.172: INFO: stdout: "pod/redis-master-xrc9z patched\n"
STEP: checking annotations
Mar  2 22:32:09.190: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:32:09.190: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:32:09.190: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8jmpt" for this suite.
Mar  2 22:32:33.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:32:34.511: INFO: namespace: e2e-tests-kubectl-8jmpt, resource: packagemanifests, items remaining: 1
Mar  2 22:32:35.204: INFO: namespace: e2e-tests-kubectl-8jmpt, resource: bindings, ignored listing per whitelist
Mar  2 22:32:35.712: INFO: namespace: e2e-tests-kubectl-8jmpt no longer exists
Mar  2 22:32:35.730: INFO: namespace: e2e-tests-kubectl-8jmpt, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:32:35.747: INFO: namespace e2e-tests-kubectl-8jmpt deletion completed in 26.524126152s

• [SLOW TEST:40.360 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:32:35.748: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 22:32:49.499: INFO: Successfully updated pod "pod-update-1488c01e-3d3b-11e9-9008-0a58ac10f353"
STEP: verifying the updated pod is in kubernetes
Mar  2 22:32:49.536: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:32:49.536: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gqsxc" for this suite.
Mar  2 22:33:13.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:33:14.567: INFO: namespace: e2e-tests-pods-gqsxc, resource: packagemanifests, items remaining: 1
Mar  2 22:33:15.445: INFO: namespace: e2e-tests-pods-gqsxc, resource: bindings, ignored listing per whitelist
Mar  2 22:33:16.059: INFO: namespace: e2e-tests-pods-gqsxc no longer exists
Mar  2 22:33:16.077: INFO: namespace: e2e-tests-pods-gqsxc, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:33:16.094: INFO: namespace e2e-tests-pods-gqsxc deletion completed in 26.526477643s

• [SLOW TEST:40.347 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:33:16.094: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:33:17.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-szfs8" to be "success or failure"
Mar  2 22:33:17.231: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 26.495986ms
Mar  2 22:33:19.249: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0447299s
Mar  2 22:33:21.268: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063555306s
Mar  2 22:33:23.287: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082292449s
Mar  2 22:33:25.306: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100850216s
Mar  2 22:33:27.324: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119249553s
STEP: Saw pod success
Mar  2 22:33:27.324: INFO: Pod "downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:33:27.341: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:33:27.396: INFO: Waiting for pod downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:33:27.414: INFO: Pod downwardapi-volume-2c925d4a-3d3b-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:33:27.414: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-szfs8" for this suite.
Mar  2 22:33:33.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:33:35.107: INFO: namespace: e2e-tests-projected-szfs8, resource: bindings, ignored listing per whitelist
Mar  2 22:33:35.250: INFO: namespace: e2e-tests-projected-szfs8, resource: packagemanifests, items remaining: 1
Mar  2 22:33:35.938: INFO: namespace: e2e-tests-projected-szfs8 no longer exists
Mar  2 22:33:35.957: INFO: namespace: e2e-tests-projected-szfs8, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:33:35.974: INFO: namespace e2e-tests-projected-szfs8 deletion completed in 8.525209519s

• [SLOW TEST:19.879 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:33:35.974: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-x74ts
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-x74ts
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-x74ts
Mar  2 22:33:37.077: INFO: Found 0 stateful pods, waiting for 1
Mar  2 22:33:47.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 22:33:47.125: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 22:33:47.511: INFO: stderr: ""
Mar  2 22:33:47.511: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 22:33:47.511: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 22:33:47.529: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 22:33:57.548: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:33:57.548: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:33:57.621: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:33:57.621: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:33:57.621: INFO: 
Mar  2 22:33:57.621: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 22:33:58.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982090333s
Mar  2 22:33:59.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963831042s
Mar  2 22:34:00.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.945567385s
Mar  2 22:34:01.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.924781242s
Mar  2 22:34:02.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.906231206s
Mar  2 22:34:03.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.887457523s
Mar  2 22:34:04.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.869178214s
Mar  2 22:34:05.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.850814579s
Mar  2 22:34:06.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 832.5056ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-x74ts
Mar  2 22:34:07.809: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:08.157: INFO: stderr: ""
Mar  2 22:34:08.157: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 22:34:08.157: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 22:34:08.157: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:08.858: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  2 22:34:08.858: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 22:34:08.859: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 22:34:08.859: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:09.259: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  2 22:34:09.259: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 22:34:09.259: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 22:34:09.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:34:09.278: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:34:09.278: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  2 22:34:09.296: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 22:34:09.678: INFO: stderr: ""
Mar  2 22:34:09.678: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 22:34:09.678: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 22:34:09.678: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 22:34:10.053: INFO: stderr: ""
Mar  2 22:34:10.053: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 22:34:10.053: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 22:34:10.053: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 22:34:10.402: INFO: stderr: ""
Mar  2 22:34:10.402: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 22:34:10.402: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 22:34:10.402: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:34:10.421: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 22:34:20.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:34:20.462: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:34:20.462: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:34:20.522: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:20.522: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:20.522: INFO: ss-1  ip-10-0-148-56.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:20.522: INFO: ss-2  ip-10-0-143-63.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:20.522: INFO: 
Mar  2 22:34:20.522: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 22:34:21.540: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:21.540: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:21.540: INFO: ss-1  ip-10-0-148-56.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:21.540: INFO: ss-2  ip-10-0-143-63.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:21.540: INFO: 
Mar  2 22:34:21.540: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 22:34:22.558: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:22.558: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:22.559: INFO: ss-1  ip-10-0-148-56.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:22.559: INFO: ss-2  ip-10-0-143-63.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:22.559: INFO: 
Mar  2 22:34:22.559: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 22:34:23.578: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:23.578: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:23.578: INFO: ss-1  ip-10-0-148-56.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:23.578: INFO: ss-2  ip-10-0-143-63.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:23.578: INFO: 
Mar  2 22:34:23.578: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 22:34:24.598: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:24.598: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:24.598: INFO: ss-1  ip-10-0-148-56.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:24.598: INFO: ss-2  ip-10-0-143-63.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:24.598: INFO: 
Mar  2 22:34:24.598: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 22:34:25.617: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 22:34:25.618: INFO: ss-0  ip-10-0-160-190.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:37 +0000 UTC  }]
Mar  2 22:34:25.618: INFO: ss-2  ip-10-0-143-63.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:25.618: INFO: 
Mar  2 22:34:25.618: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  2 22:34:26.637: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 22:34:26.637: INFO: ss-2  ip-10-0-143-63.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:26.638: INFO: 
Mar  2 22:34:26.638: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 22:34:27.656: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 22:34:27.656: INFO: ss-2  ip-10-0-143-63.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:27.656: INFO: 
Mar  2 22:34:27.656: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 22:34:28.674: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 22:34:28.674: INFO: ss-2  ip-10-0-143-63.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:28.674: INFO: 
Mar  2 22:34:28.674: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 22:34:29.693: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 22:34:29.693: INFO: ss-2  ip-10-0-143-63.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:34:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:33:57 +0000 UTC  }]
Mar  2 22:34:29.693: INFO: 
Mar  2 22:34:29.693: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-x74ts
Mar  2 22:34:30.713: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:30.986: INFO: rc: 1
Mar  2 22:34:30.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc421f9c360 exit status 1 <nil> <nil> true [0xc42000e580 0xc42000e5a0 0xc42000e5c0] [0xc42000e580 0xc42000e5a0 0xc42000e5c0] [0xc42000e590 0xc42000e5b8] [0x8fea40 0x8fea40] 0xc421f64d80 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Mar  2 22:34:40.987: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:41.139: INFO: rc: 1
Mar  2 22:34:41.139: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f9ca20 exit status 1 <nil> <nil> true [0xc42000e5c8 0xc42000e5e0 0xc42000e5f8] [0xc42000e5c8 0xc42000e5e0 0xc42000e5f8] [0xc42000e5d8 0xc42000e5f0] [0x8fea40 0x8fea40] 0xc421f64e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:34:51.140: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:34:51.331: INFO: rc: 1
Mar  2 22:34:51.331: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc422477230 exit status 1 <nil> <nil> true [0xc4207e68d0 0xc4207e6938 0xc4207e69a0] [0xc4207e68d0 0xc4207e6938 0xc4207e69a0] [0xc4207e6918 0xc4207e6958] [0x8fea40 0x8fea40] 0xc4220b6240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:01.331: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:01.491: INFO: rc: 1
Mar  2 22:35:01.491: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210ae40 exit status 1 <nil> <nil> true [0xc421ba2038 0xc421ba2050 0xc421ba2068] [0xc421ba2038 0xc421ba2050 0xc421ba2068] [0xc421ba2048 0xc421ba2060] [0x8fea40 0x8fea40] 0xc42115c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:11.492: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:11.667: INFO: rc: 1
Mar  2 22:35:11.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f9d110 exit status 1 <nil> <nil> true [0xc42000e600 0xc42000e618 0xc42000e630] [0xc42000e600 0xc42000e618 0xc42000e630] [0xc42000e610 0xc42000e628] [0x8fea40 0x8fea40] 0xc421f64f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:21.667: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:21.828: INFO: rc: 1
Mar  2 22:35:21.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210b500 exit status 1 <nil> <nil> true [0xc421ba2070 0xc421ba2088 0xc421ba20a0] [0xc421ba2070 0xc421ba2088 0xc421ba20a0] [0xc421ba2080 0xc421ba2098] [0x8fea40 0x8fea40] 0xc42115c7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:31.828: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:31.986: INFO: rc: 1
Mar  2 22:35:31.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc422477950 exit status 1 <nil> <nil> true [0xc4207e69b0 0xc4207e6a18 0xc4207e6a70] [0xc4207e69b0 0xc4207e6a18 0xc4207e6a70] [0xc4207e69f8 0xc4207e6a40] [0x8fea40 0x8fea40] 0xc4220b6300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:41.986: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:42.170: INFO: rc: 1
Mar  2 22:35:42.170: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210bd10 exit status 1 <nil> <nil> true [0xc421ba20a8 0xc421ba20c0 0xc421ba20d8] [0xc421ba20a8 0xc421ba20c0 0xc421ba20d8] [0xc421ba20b8 0xc421ba20d0] [0x8fea40 0x8fea40] 0xc42115c900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:35:52.171: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:35:52.377: INFO: rc: 1
Mar  2 22:35:52.377: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42083e2a0 exit status 1 <nil> <nil> true [0xc4207e6a78 0xc4207e6b80 0xc4207e6c98] [0xc4207e6a78 0xc4207e6b80 0xc4207e6c98] [0xc4207e6b38 0xc4207e6c70] [0x8fea40 0x8fea40] 0xc4220b63c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:02.377: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:02.542: INFO: rc: 1
Mar  2 22:36:02.542: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc4202b1c50 exit status 1 <nil> <nil> true [0xc421ba20e0 0xc421ba20f8 0xc421ba2110] [0xc421ba20e0 0xc421ba20f8 0xc421ba2110] [0xc421ba20f0 0xc421ba2108] [0x8fea40 0x8fea40] 0xc42115c9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:12.542: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:12.705: INFO: rc: 1
Mar  2 22:36:12.705: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f926c0 exit status 1 <nil> <nil> true [0xc42000e008 0xc42000e048 0xc42000e078] [0xc42000e008 0xc42000e048 0xc42000e078] [0xc42000e040 0xc42000e070] [0x8fea40 0x8fea40] 0xc421eee000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:22.706: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:22.866: INFO: rc: 1
Mar  2 22:36:22.866: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210a690 exit status 1 <nil> <nil> true [0xc4213c0008 0xc4213c0020 0xc4213c0038] [0xc4213c0008 0xc4213c0020 0xc4213c0038] [0xc4213c0018 0xc4213c0030] [0x8fea40 0x8fea40] 0xc421f64000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:32.866: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:33.017: INFO: rc: 1
Mar  2 22:36:33.017: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210ade0 exit status 1 <nil> <nil> true [0xc4213c0040 0xc4213c0058 0xc4213c0070] [0xc4213c0040 0xc4213c0058 0xc4213c0070] [0xc4213c0050 0xc4213c0068] [0x8fea40 0x8fea40] 0xc421f640c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:43.017: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:43.181: INFO: rc: 1
Mar  2 22:36:43.181: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210b470 exit status 1 <nil> <nil> true [0xc4213c0078 0xc4213c0090 0xc4213c00a8] [0xc4213c0078 0xc4213c0090 0xc4213c00a8] [0xc4213c0088 0xc4213c00a0] [0x8fea40 0x8fea40] 0xc421f64180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:36:53.181: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:36:53.374: INFO: rc: 1
Mar  2 22:36:53.375: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210bce0 exit status 1 <nil> <nil> true [0xc4213c00b0 0xc4213c00c8 0xc4213c00e0] [0xc4213c00b0 0xc4213c00c8 0xc4213c00e0] [0xc4213c00c0 0xc4213c00d8] [0x8fea40 0x8fea40] 0xc421f64240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:03.375: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:03.550: INFO: rc: 1
Mar  2 22:37:03.550: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42216c3c0 exit status 1 <nil> <nil> true [0xc4213c00e8 0xc4213c0100 0xc4213c0118] [0xc4213c00e8 0xc4213c0100 0xc4213c0118] [0xc4213c00f8 0xc4213c0110] [0x8fea40 0x8fea40] 0xc421f64300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:13.551: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:13.701: INFO: rc: 1
Mar  2 22:37:13.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc422476660 exit status 1 <nil> <nil> true [0xc4207e60e0 0xc4207e64f0 0xc4207e6530] [0xc4207e60e0 0xc4207e64f0 0xc4207e6530] [0xc4207e6470 0xc4207e6528] [0x8fea40 0x8fea40] 0xc4220b6000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:23.701: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:23.850: INFO: rc: 1
Mar  2 22:37:23.850: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42216cae0 exit status 1 <nil> <nil> true [0xc4213c0120 0xc4213c0140 0xc4213c0158] [0xc4213c0120 0xc4213c0140 0xc4213c0158] [0xc4213c0130 0xc4213c0150] [0x8fea40 0x8fea40] 0xc421f643c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:33.850: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:34.011: INFO: rc: 1
Mar  2 22:37:34.011: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f92f30 exit status 1 <nil> <nil> true [0xc42000e088 0xc42000e108 0xc42000e190] [0xc42000e088 0xc42000e108 0xc42000e190] [0xc42000e0c8 0xc42000e168] [0x8fea40 0x8fea40] 0xc421eee0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:44.011: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:44.173: INFO: rc: 1
Mar  2 22:37:44.173: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f93590 exit status 1 <nil> <nil> true [0xc42000e1a0 0xc42000e248 0xc42000e288] [0xc42000e1a0 0xc42000e248 0xc42000e288] [0xc42000e228 0xc42000e278] [0x8fea40 0x8fea40] 0xc421eee180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:37:54.173: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:37:54.384: INFO: rc: 1
Mar  2 22:37:54.384: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f93e30 exit status 1 <nil> <nil> true [0xc42000e2b8 0xc42000e308 0xc42000e3c8] [0xc42000e2b8 0xc42000e308 0xc42000e3c8] [0xc42000e2e8 0xc42000e3a8] [0x8fea40 0x8fea40] 0xc421eee240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:04.385: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:04.545: INFO: rc: 1
Mar  2 22:38:04.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc422476d80 exit status 1 <nil> <nil> true [0xc4207e6588 0xc4207e66e8 0xc4207e67b0] [0xc4207e6588 0xc4207e66e8 0xc4207e67b0] [0xc4207e6628 0xc4207e6750] [0x8fea40 0x8fea40] 0xc4220b60c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:14.545: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:14.703: INFO: rc: 1
Mar  2 22:38:14.703: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210a6f0 exit status 1 <nil> <nil> true [0xc42000e040 0xc42000e070 0xc42000e090] [0xc42000e040 0xc42000e070 0xc42000e090] [0xc42000e050 0xc42000e088] [0x8fea40 0x8fea40] 0xc421eee000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:24.703: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:24.867: INFO: rc: 1
Mar  2 22:38:24.867: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f92690 exit status 1 <nil> <nil> true [0xc4207e60e0 0xc4207e64f0 0xc4207e6530] [0xc4207e60e0 0xc4207e64f0 0xc4207e6530] [0xc4207e6470 0xc4207e6528] [0x8fea40 0x8fea40] 0xc4220b6000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:34.868: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:35.026: INFO: rc: 1
Mar  2 22:38:35.026: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210ae70 exit status 1 <nil> <nil> true [0xc42000e0c8 0xc42000e168 0xc42000e1c8] [0xc42000e0c8 0xc42000e168 0xc42000e1c8] [0xc42000e138 0xc42000e1a0] [0x8fea40 0x8fea40] 0xc421eee120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:45.027: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:45.182: INFO: rc: 1
Mar  2 22:38:45.182: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210b500 exit status 1 <nil> <nil> true [0xc42000e228 0xc42000e278 0xc42000e2d8] [0xc42000e228 0xc42000e278 0xc42000e2d8] [0xc42000e258 0xc42000e2b8] [0x8fea40 0x8fea40] 0xc421eee1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:38:55.182: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:38:55.364: INFO: rc: 1
Mar  2 22:38:55.364: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42210bd40 exit status 1 <nil> <nil> true [0xc42000e2e8 0xc42000e3a8 0xc42000e3e0] [0xc42000e2e8 0xc42000e3a8 0xc42000e3e0] [0xc42000e3a0 0xc42000e3d8] [0x8fea40 0x8fea40] 0xc421eee2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:39:05.365: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:39:05.516: INFO: rc: 1
Mar  2 22:39:05.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421f92f60 exit status 1 <nil> <nil> true [0xc4207e6588 0xc4207e66e8 0xc4207e67b0] [0xc4207e6588 0xc4207e66e8 0xc4207e67b0] [0xc4207e6628 0xc4207e6750] [0x8fea40 0x8fea40] 0xc4220b60c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:39:15.516: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:39:15.672: INFO: rc: 1
Mar  2 22:39:15.672: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc422476690 exit status 1 <nil> <nil> true [0xc4213c0000 0xc4213c0018 0xc4213c0030] [0xc4213c0000 0xc4213c0018 0xc4213c0030] [0xc4213c0010 0xc4213c0028] [0x8fea40 0x8fea40] 0xc421f64000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:39:25.672: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:39:25.824: INFO: rc: 1
Mar  2 22:39:25.824: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42216c450 exit status 1 <nil> <nil> true [0xc42000e3f0 0xc42000e408 0xc42000e468] [0xc42000e3f0 0xc42000e408 0xc42000e468] [0xc42000e400 0xc42000e440] [0x8fea40 0x8fea40] 0xc421eee3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Mar  2 22:39:35.825: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-x74ts ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 22:39:35.978: INFO: rc: 1
Mar  2 22:39:35.978: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Mar  2 22:39:35.978: INFO: Scaling statefulset ss to 0
Mar  2 22:39:36.032: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  2 22:39:36.051: INFO: Deleting all statefulset in ns e2e-tests-statefulset-x74ts
Mar  2 22:39:36.069: INFO: Scaling statefulset ss to 0
Mar  2 22:39:36.122: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:39:36.140: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:39:36.203: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-x74ts" for this suite.
Mar  2 22:39:44.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:39:45.171: INFO: namespace: e2e-tests-statefulset-x74ts, resource: bindings, ignored listing per whitelist
Mar  2 22:39:46.195: INFO: namespace: e2e-tests-statefulset-x74ts, resource: packagemanifests, items remaining: 1
Mar  2 22:39:46.726: INFO: namespace: e2e-tests-statefulset-x74ts no longer exists
Mar  2 22:39:46.745: INFO: namespace: e2e-tests-statefulset-x74ts, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:39:46.762: INFO: namespace e2e-tests-statefulset-x74ts deletion completed in 10.527489989s

• [SLOW TEST:370.789 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:39:46.763: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 22:39:47.813: INFO: Creating deployment "test-recreate-deployment"
Mar  2 22:39:47.841: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 22:39:47.880: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 22:39:49.918: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 22:39:49.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:39:51.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:39:53.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:39:55.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163187, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:39:57.968: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 22:39:58.006: INFO: Updating deployment test-recreate-deployment
Mar  2 22:39:58.006: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  2 22:39:58.138: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-tghhn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tghhn/deployments/test-recreate-deployment,UID:156b8887-3d3c-11e9-b620-0a843850fdce,ResourceVersion:32796,Generation:2,CreationTimestamp:2019-03-02 22:39:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-02 22:39:58 +0000 UTC 2019-03-02 22:39:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-02 22:39:58 +0000 UTC 2019-03-02 22:39:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 22:39:58.156: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-tghhn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tghhn/replicasets/test-recreate-deployment-7cf749666b,UID:1b85ecde-3d3c-11e9-8060-0e81de5d2554,ResourceVersion:32795,Generation:1,CreationTimestamp:2019-03-02 22:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 156b8887-3d3c-11e9-b620-0a843850fdce 0xc422270b67 0xc422270b68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  2 22:39:58.156: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 22:39:58.156: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-tghhn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tghhn/replicasets/test-recreate-deployment-79f694ff59,UID:156e99fe-3d3c-11e9-8060-0e81de5d2554,ResourceVersion:32785,Generation:2,CreationTimestamp:2019-03-02 22:39:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 156b8887-3d3c-11e9-b620-0a843850fdce 0xc422270aa7 0xc422270aa8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  2 22:39:58.175: INFO: Pod "test-recreate-deployment-7cf749666b-kzsgj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-kzsgj,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-tghhn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tghhn/pods/test-recreate-deployment-7cf749666b-kzsgj,UID:1b877b7a-3d3c-11e9-8060-0e81de5d2554,ResourceVersion:32797,Generation:0,CreationTimestamp:2019-03-02 22:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 1b85ecde-3d3c-11e9-8060-0e81de5d2554 0xc422271367 0xc422271368}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-48whv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-48whv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-48whv true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-7gsqb}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222713d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222713f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:39:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:39:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 22:39:58 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 22:39:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:39:58.175: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tghhn" for this suite.
Mar  2 22:40:04.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:40:05.294: INFO: namespace: e2e-tests-deployment-tghhn, resource: packagemanifests, items remaining: 1
Mar  2 22:40:05.913: INFO: namespace: e2e-tests-deployment-tghhn, resource: bindings, ignored listing per whitelist
Mar  2 22:40:06.704: INFO: namespace: e2e-tests-deployment-tghhn no longer exists
Mar  2 22:40:06.722: INFO: namespace: e2e-tests-deployment-tghhn, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:40:06.740: INFO: namespace e2e-tests-deployment-tghhn deletion completed in 8.533254259s

• [SLOW TEST:19.978 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:40:06.740: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-21679f96-3d3c-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 22:40:08.271: INFO: Waiting up to 5m0s for pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-g8v9w" to be "success or failure"
Mar  2 22:40:08.292: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.793537ms
Mar  2 22:40:10.310: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038950195s
Mar  2 22:40:12.332: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060639578s
Mar  2 22:40:14.350: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078719385s
Mar  2 22:40:16.368: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096856068s
Mar  2 22:40:18.386: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.115305339s
STEP: Saw pod success
Mar  2 22:40:18.386: INFO: Pod "pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:40:18.404: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:40:18.463: INFO: Waiting for pod pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:40:18.481: INFO: Pod pod-secrets-21971b9a-3d3c-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:40:18.481: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-g8v9w" for this suite.
Mar  2 22:40:24.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:40:26.335: INFO: namespace: e2e-tests-secrets-g8v9w, resource: packagemanifests, items remaining: 1
Mar  2 22:40:26.653: INFO: namespace: e2e-tests-secrets-g8v9w, resource: bindings, ignored listing per whitelist
Mar  2 22:40:27.013: INFO: namespace: e2e-tests-secrets-g8v9w no longer exists
Mar  2 22:40:27.032: INFO: namespace: e2e-tests-secrets-g8v9w, total namespaces: 48, active: 48, terminating: 0
Mar  2 22:40:27.050: INFO: namespace e2e-tests-secrets-g8v9w deletion completed in 8.536899434s
STEP: Destroying namespace "e2e-tests-secret-namespace-8r4kk" for this suite.
Mar  2 22:40:33.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:40:33.962: INFO: namespace: e2e-tests-secret-namespace-8r4kk, resource: packagemanifests, items remaining: 1
Mar  2 22:40:35.148: INFO: namespace: e2e-tests-secret-namespace-8r4kk, resource: bindings, ignored listing per whitelist
Mar  2 22:40:35.540: INFO: namespace: e2e-tests-secret-namespace-8r4kk no longer exists
Mar  2 22:40:35.558: INFO: namespace: e2e-tests-secret-namespace-8r4kk, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:40:35.575: INFO: namespace e2e-tests-secret-namespace-8r4kk deletion completed in 8.52560037s

• [SLOW TEST:28.835 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:40:35.575: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-3283f9d5-3d3c-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 22:40:36.693: INFO: Waiting up to 5m0s for pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-2q7mc" to be "success or failure"
Mar  2 22:40:36.713: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.438251ms
Mar  2 22:40:38.731: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037541279s
Mar  2 22:40:40.749: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055478559s
Mar  2 22:40:42.767: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073525349s
Mar  2 22:40:44.792: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099262071s
Mar  2 22:40:46.812: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.118671263s
Mar  2 22:40:48.831: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.138008127s
STEP: Saw pod success
Mar  2 22:40:48.831: INFO: Pod "pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:40:48.852: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:40:48.912: INFO: Waiting for pod pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:40:48.929: INFO: Pod pod-configmaps-3287de0c-3d3c-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:40:48.929: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2q7mc" for this suite.
Mar  2 22:40:55.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:40:56.610: INFO: namespace: e2e-tests-configmap-2q7mc, resource: packagemanifests, items remaining: 1
Mar  2 22:40:57.086: INFO: namespace: e2e-tests-configmap-2q7mc, resource: bindings, ignored listing per whitelist
Mar  2 22:40:57.458: INFO: namespace: e2e-tests-configmap-2q7mc no longer exists
Mar  2 22:40:57.477: INFO: namespace: e2e-tests-configmap-2q7mc, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:40:57.494: INFO: namespace e2e-tests-configmap-2q7mc deletion completed in 8.532749117s

• [SLOW TEST:21.919 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:40:57.494: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  2 22:40:58.569: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 22:41:51.744: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3f96ceac-3d3c-11e9-9008-0a58ac10f353", GenerateName:"", Namespace:"e2e-tests-init-container-hdknn", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-hdknn/pods/pod-init-3f96ceac-3d3c-11e9-9008-0a58ac10f353", UID:"3f99697a-3d3c-11e9-b620-0a843850fdce", ResourceVersion:"34327", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687163258, loc:(*time.Location)(0x6c3af00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"569085628"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"ips\": [\n        \"10.129.2.14\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-fzqzw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc4221cb080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-fzqzw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4221aca00), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-fzqzw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4221aca50), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-fzqzw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4221ac9b0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc422044978), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-160-190.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc421f64b40), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-tzd7m"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422044a30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422044a50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc422044a58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163258, loc:(*time.Location)(0x6c3af00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163258, loc:(*time.Location)(0x6c3af00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163258, loc:(*time.Location)(0x6c3af00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163258, loc:(*time.Location)(0x6c3af00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.160.190", PodIP:"10.129.2.14", StartTime:(*v1.Time)(0xc421d05940), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc421e89420)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc421e89490)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://04f4190b8f333d403e46bfbde16db3c61844e28f9108eca4efb026a35b8614d3"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421d05980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421d05960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:41:51.744: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-hdknn" for this suite.
Mar  2 22:42:15.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:42:16.756: INFO: namespace: e2e-tests-init-container-hdknn, resource: bindings, ignored listing per whitelist
Mar  2 22:42:17.346: INFO: namespace: e2e-tests-init-container-hdknn, resource: packagemanifests, items remaining: 1
Mar  2 22:42:18.272: INFO: namespace: e2e-tests-init-container-hdknn no longer exists
Mar  2 22:42:18.291: INFO: namespace: e2e-tests-init-container-hdknn, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:42:18.308: INFO: namespace e2e-tests-init-container-hdknn deletion completed in 26.532286805s

• [SLOW TEST:80.814 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:42:18.308: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-6fc374f6-3d3c-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 22:42:19.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-pjlk8" to be "success or failure"
Mar  2 22:42:19.561: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 27.659038ms
Mar  2 22:42:21.579: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045640523s
Mar  2 22:42:23.597: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063557879s
Mar  2 22:42:25.616: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081682242s
Mar  2 22:42:27.634: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099707655s
Mar  2 22:42:29.652: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.117952164s
STEP: Saw pod success
Mar  2 22:42:29.652: INFO: Pod "pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:42:29.669: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:42:29.723: INFO: Waiting for pod pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:42:29.744: INFO: Pod pod-projected-configmaps-6fd1c9c4-3d3c-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:42:29.744: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pjlk8" for this suite.
Mar  2 22:42:35.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:42:37.404: INFO: namespace: e2e-tests-projected-pjlk8, resource: packagemanifests, items remaining: 1
Mar  2 22:42:37.792: INFO: namespace: e2e-tests-projected-pjlk8, resource: bindings, ignored listing per whitelist
Mar  2 22:42:38.278: INFO: namespace: e2e-tests-projected-pjlk8 no longer exists
Mar  2 22:42:38.297: INFO: namespace: e2e-tests-projected-pjlk8, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:42:38.315: INFO: namespace e2e-tests-projected-pjlk8 deletion completed in 8.538591844s

• [SLOW TEST:20.007 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:42:38.316: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0302 22:43:09.539340   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 22:43:09.539: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:43:09.539: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jsjsq" for this suite.
Mar  2 22:43:15.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:43:16.830: INFO: namespace: e2e-tests-gc-jsjsq, resource: bindings, ignored listing per whitelist
Mar  2 22:43:17.758: INFO: namespace: e2e-tests-gc-jsjsq, resource: packagemanifests, items remaining: 1
Mar  2 22:43:18.048: INFO: namespace: e2e-tests-gc-jsjsq no longer exists
Mar  2 22:43:18.067: INFO: namespace: e2e-tests-gc-jsjsq, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:43:18.084: INFO: namespace e2e-tests-gc-jsjsq deletion completed in 8.526422793s

• [SLOW TEST:39.768 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:43:18.084: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar  2 22:43:19.157: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 22:43:19.208: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 22:43:19.229: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-63.ec2.internal before test
Mar  2 22:43:19.300: INFO: node-ca-d9zfb from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:43:19.300: INFO: router-default-74d6bbbf54-dsl6h from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container router ready: true, restart count 0
Mar  2 22:43:19.300: INFO: router-default-74d6bbbf54-6fv8g from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container router ready: true, restart count 0
Mar  2 22:43:19.300: INFO: telemeter-client-6db8c847b7-grhl9 from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container reload ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 22:43:19.300: INFO: certified-operators-76b9f9bdfb-9nrwb from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container certified-operators ready: true, restart count 0
Mar  2 22:43:19.300: INFO: community-operators-768f567b68-sqssn from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container community-operators ready: true, restart count 0
Mar  2 22:43:19.300: INFO: ovs-hwm6x from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 22:43:19.300: INFO: tuned-4jp7z from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:23 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:43:19.300: INFO: sdn-p29xm from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container sdn ready: true, restart count 0
Mar  2 22:43:19.300: INFO: dns-default-hfm86 from openshift-dns started at 2019-03-02 22:17:23 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:43:19.300: INFO: machine-config-daemon-4snhh from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 22:43:19.300: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-03-02 22:20:07 +0000 UTC (3 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:43:19.300: INFO: redhat-operators-8484568578-xdtzk from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container redhat-operators ready: true, restart count 0
Mar  2 22:43:19.300: INFO: kube-state-metrics-67bf67646d-c76bn from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 22:43:19.300: INFO: multus-7z6tf from openshift-multus started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:43:19.300: INFO: node-exporter-68xtl from openshift-monitoring started at 2019-03-02 22:17:22 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.300: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:43:19.300: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-148-56.ec2.internal before test
Mar  2 22:43:19.337: INFO: prometheus-adapter-7b89fc5f5-cb2hj from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 22:43:19.337: INFO: sdn-5tbhh from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container sdn ready: true, restart count 0
Mar  2 22:43:19.337: INFO: multus-kpz5d from openshift-multus started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:43:19.337: INFO: dns-default-blbmc from openshift-dns started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:43:19.337: INFO: tuned-j48rm from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:43:19.337: INFO: node-ca-54mqp from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:43:19.337: INFO: grafana-5c65588698-xtfnz from openshift-monitoring started at 2019-03-02 22:18:45 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container grafana ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 22:43:19.337: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 22:43:19.337: INFO: ovs-2njbk from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 22:43:19.337: INFO: node-exporter-b5gbl from openshift-monitoring started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:43:19.337: INFO: machine-config-daemon-kqwc8 from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 22:43:19.337: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-03-02 22:19:55 +0000 UTC (3 container statuses recorded)
Mar  2 22:43:19.337: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:43:19.337: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-160-190.ec2.internal before test
Mar  2 22:43:19.376: INFO: multus-9466n from openshift-multus started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 22:43:19.376: INFO: dns-default-2f5st from openshift-dns started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container dns ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 22:43:19.376: INFO: ovs-xtgl4 from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 22:43:19.376: INFO: tuned-vptgs from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container tuned ready: true, restart count 0
Mar  2 22:43:19.376: INFO: machine-config-daemon-mlhcn from openshift-machine-config-operator started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 22:43:19.376: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 22:43:19.376: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 22:43:19.376: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (3 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 22:43:19.376: INFO: sdn-n87hs from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container sdn ready: true, restart count 0
Mar  2 22:43:19.376: INFO: node-exporter-n6h9d from openshift-monitoring started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 22:43:19.376: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 22:43:19.376: INFO: node-ca-brndt from openshift-image-registry started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 22:43:19.376: INFO: prometheus-operator-76f5fb67cd-2kb84 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 22:43:19.376: INFO: prometheus-adapter-7b89fc5f5-v7747 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 22:43:19.376: INFO: 	Container prometheus-adapter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-0-143-63.ec2.internal
STEP: verifying the node has the label node ip-10-0-148-56.ec2.internal
STEP: verifying the node has the label node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod tuned-4jp7z requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod tuned-j48rm requesting resource cpu=0m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod tuned-vptgs requesting resource cpu=0m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod dns-default-2f5st requesting resource cpu=110m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod dns-default-blbmc requesting resource cpu=110m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod dns-default-hfm86 requesting resource cpu=110m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-ca-54mqp requesting resource cpu=0m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-ca-brndt requesting resource cpu=0m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-ca-d9zfb requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod router-default-74d6bbbf54-6fv8g requesting resource cpu=100m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod router-default-74d6bbbf54-dsl6h requesting resource cpu=100m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod machine-config-daemon-4snhh requesting resource cpu=20m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod machine-config-daemon-kqwc8 requesting resource cpu=20m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod machine-config-daemon-mlhcn requesting resource cpu=20m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod certified-operators-76b9f9bdfb-9nrwb requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod community-operators-768f567b68-sqssn requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod redhat-operators-8484568578-xdtzk requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod alertmanager-main-0 requesting resource cpu=50m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod alertmanager-main-1 requesting resource cpu=50m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod alertmanager-main-2 requesting resource cpu=50m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod grafana-5c65588698-xtfnz requesting resource cpu=100m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod kube-state-metrics-67bf67646d-c76bn requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-exporter-68xtl requesting resource cpu=10m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-exporter-b5gbl requesting resource cpu=10m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod node-exporter-n6h9d requesting resource cpu=10m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod prometheus-adapter-7b89fc5f5-cb2hj requesting resource cpu=0m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod prometheus-adapter-7b89fc5f5-v7747 requesting resource cpu=0m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod prometheus-operator-76f5fb67cd-2kb84 requesting resource cpu=0m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod telemeter-client-6db8c847b7-grhl9 requesting resource cpu=10m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod multus-7z6tf requesting resource cpu=0m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod multus-9466n requesting resource cpu=0m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod multus-kpz5d requesting resource cpu=0m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod ovs-2njbk requesting resource cpu=200m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod ovs-hwm6x requesting resource cpu=200m on Node ip-10-0-143-63.ec2.internal
Mar  2 22:43:19.720: INFO: Pod ovs-xtgl4 requesting resource cpu=200m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod sdn-5tbhh requesting resource cpu=100m on Node ip-10-0-148-56.ec2.internal
Mar  2 22:43:19.720: INFO: Pod sdn-n87hs requesting resource cpu=100m on Node ip-10-0-160-190.ec2.internal
Mar  2 22:43:19.720: INFO: Pod sdn-p29xm requesting resource cpu=100m on Node ip-10-0-143-63.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353.1588459a178df570], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpdqm/filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353 to ip-10-0-143-63.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353.1588459bfc0911a1], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353.1588459c46434b09], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353.1588459c52245526], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b8c60c-3d3c-11e9-9008-0a58ac10f353.1588459c534d33b3], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353.1588459a194553f7], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpdqm/filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353 to ip-10-0-148-56.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353.1588459bf39aeb91], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353.1588459c47c80dc2], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353.1588459c53004c8f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bdece1-3d3c-11e9-9008-0a58ac10f353.1588459c5430bf68], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353.1588459a1b018ca4], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpdqm/filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353 to ip-10-0-160-190.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353.1588459bea9b59fb], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353.1588459c3e3c720d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353.1588459c49592672], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c263f8-3d3c-11e9-9008-0a58ac10f353.1588459c4a94a5f9], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1588459cedde4a73], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-143-63.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-148-56.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-160-190.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:43:33.128: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-gpdqm" for this suite.
Mar  2 22:43:41.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:43:42.122: INFO: namespace: e2e-tests-sched-pred-gpdqm, resource: bindings, ignored listing per whitelist
Mar  2 22:43:42.979: INFO: namespace: e2e-tests-sched-pred-gpdqm, resource: packagemanifests, items remaining: 1
Mar  2 22:43:43.651: INFO: namespace: e2e-tests-sched-pred-gpdqm no longer exists
Mar  2 22:43:43.669: INFO: namespace: e2e-tests-sched-pred-gpdqm, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:43:43.686: INFO: namespace e2e-tests-sched-pred-gpdqm deletion completed in 10.527277631s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:25.602 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:43:43.687: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar  2 22:43:45.325: INFO: Waiting up to 5m0s for pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx" in namespace "e2e-tests-svcaccounts-nwftv" to be "success or failure"
Mar  2 22:43:45.345: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 19.948045ms
Mar  2 22:43:47.363: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038012058s
Mar  2 22:43:49.381: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056027957s
Mar  2 22:43:51.401: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076719047s
Mar  2 22:43:53.420: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094924821s
Mar  2 22:43:55.438: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.112965196s
Mar  2 22:43:57.456: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.131525957s
STEP: Saw pod success
Mar  2 22:43:57.456: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx" satisfied condition "success or failure"
Mar  2 22:43:57.474: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx container token-test: <nil>
STEP: delete the pod
Mar  2 22:43:57.529: INFO: Waiting for pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx to disappear
Mar  2 22:43:57.546: INFO: Pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-qcpcx no longer exists
STEP: Creating a pod to test consume service account root CA
Mar  2 22:43:57.574: INFO: Waiting up to 5m0s for pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j" in namespace "e2e-tests-svcaccounts-nwftv" to be "success or failure"
Mar  2 22:43:57.591: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Pending", Reason="", readiness=false. Elapsed: 17.424599ms
Mar  2 22:43:59.610: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036181008s
Mar  2 22:44:01.628: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0542026s
Mar  2 22:44:03.647: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073207806s
Mar  2 22:44:05.672: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097637675s
Mar  2 22:44:07.690: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.116261079s
STEP: Saw pod success
Mar  2 22:44:07.690: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j" satisfied condition "success or failure"
Mar  2 22:44:07.708: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j container root-ca-test: <nil>
STEP: delete the pod
Mar  2 22:44:07.769: INFO: Waiting for pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j to disappear
Mar  2 22:44:07.787: INFO: Pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-fbz8j no longer exists
STEP: Creating a pod to test consume service account namespace
Mar  2 22:44:07.815: INFO: Waiting up to 5m0s for pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w" in namespace "e2e-tests-svcaccounts-nwftv" to be "success or failure"
Mar  2 22:44:07.834: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Pending", Reason="", readiness=false. Elapsed: 19.043608ms
Mar  2 22:44:09.852: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036837715s
Mar  2 22:44:11.870: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054260927s
Mar  2 22:44:13.888: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072982305s
Mar  2 22:44:15.910: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094774399s
Mar  2 22:44:17.928: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.11290658s
STEP: Saw pod success
Mar  2 22:44:17.928: INFO: Pod "pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w" satisfied condition "success or failure"
Mar  2 22:44:17.945: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w container namespace-test: <nil>
STEP: delete the pod
Mar  2 22:44:17.998: INFO: Waiting for pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w to disappear
Mar  2 22:44:18.015: INFO: Pod pod-service-account-a2f65a9f-3d3c-11e9-9008-0a58ac10f353-lch2w no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:44:18.015: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-nwftv" for this suite.
Mar  2 22:44:26.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:44:27.741: INFO: namespace: e2e-tests-svcaccounts-nwftv, resource: bindings, ignored listing per whitelist
Mar  2 22:44:28.020: INFO: namespace: e2e-tests-svcaccounts-nwftv, resource: packagemanifests, items remaining: 1
Mar  2 22:44:28.538: INFO: namespace: e2e-tests-svcaccounts-nwftv no longer exists
Mar  2 22:44:28.556: INFO: namespace: e2e-tests-svcaccounts-nwftv, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:44:28.573: INFO: namespace e2e-tests-svcaccounts-nwftv deletion completed in 10.526908282s

• [SLOW TEST:44.886 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:44:28.574: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 22:44:29.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-l56fn,SelfLink:/api/v1/namespaces/e2e-tests-watch-l56fn/configmaps/e2e-watch-test-resource-version,UID:bd644747-3d3c-11e9-b620-0a843850fdce,ResourceVersion:36466,Generation:0,CreationTimestamp:2019-03-02 22:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 22:44:29.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-l56fn,SelfLink:/api/v1/namespaces/e2e-tests-watch-l56fn/configmaps/e2e-watch-test-resource-version,UID:bd644747-3d3c-11e9-b620-0a843850fdce,ResourceVersion:36467,Generation:0,CreationTimestamp:2019-03-02 22:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:44:29.776: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-l56fn" for this suite.
Mar  2 22:44:35.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:44:37.062: INFO: namespace: e2e-tests-watch-l56fn, resource: packagemanifests, items remaining: 1
Mar  2 22:44:37.612: INFO: namespace: e2e-tests-watch-l56fn, resource: bindings, ignored listing per whitelist
Mar  2 22:44:38.286: INFO: namespace: e2e-tests-watch-l56fn no longer exists
Mar  2 22:44:38.305: INFO: namespace: e2e-tests-watch-l56fn, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:44:38.322: INFO: namespace e2e-tests-watch-l56fn deletion completed in 8.52672439s

• [SLOW TEST:9.749 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:44:38.322: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  2 22:44:48.068: INFO: Successfully updated pod "annotationupdatec330bdb1-3d3c-11e9-9008-0a58ac10f353"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:44:50.122: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hl7wn" for this suite.
Mar  2 22:45:14.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:45:15.218: INFO: namespace: e2e-tests-downward-api-hl7wn, resource: packagemanifests, items remaining: 1
Mar  2 22:45:16.477: INFO: namespace: e2e-tests-downward-api-hl7wn, resource: bindings, ignored listing per whitelist
Mar  2 22:45:16.645: INFO: namespace: e2e-tests-downward-api-hl7wn no longer exists
Mar  2 22:45:16.663: INFO: namespace: e2e-tests-downward-api-hl7wn, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:45:16.680: INFO: namespace e2e-tests-downward-api-hl7wn deletion completed in 26.526289485s

• [SLOW TEST:38.358 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:45:16.680: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  2 22:45:17.774: INFO: Waiting up to 5m0s for pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-zhbrm" to be "success or failure"
Mar  2 22:45:17.793: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.525559ms
Mar  2 22:45:19.812: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038660074s
Mar  2 22:45:21.831: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057656284s
Mar  2 22:45:23.852: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078043998s
Mar  2 22:45:25.874: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100847601s
Mar  2 22:45:27.892: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.118828557s
STEP: Saw pod success
Mar  2 22:45:27.892: INFO: Pod "pod-da107bda-3d3c-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:45:27.910: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-da107bda-3d3c-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 22:45:27.966: INFO: Waiting for pod pod-da107bda-3d3c-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:45:27.984: INFO: Pod pod-da107bda-3d3c-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:45:27.984: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zhbrm" for this suite.
Mar  2 22:45:34.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:45:34.895: INFO: namespace: e2e-tests-emptydir-zhbrm, resource: bindings, ignored listing per whitelist
Mar  2 22:45:35.831: INFO: namespace: e2e-tests-emptydir-zhbrm, resource: packagemanifests, items remaining: 1
Mar  2 22:45:36.506: INFO: namespace: e2e-tests-emptydir-zhbrm no longer exists
Mar  2 22:45:36.524: INFO: namespace: e2e-tests-emptydir-zhbrm, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:45:36.542: INFO: namespace e2e-tests-emptydir-zhbrm deletion completed in 8.525940627s

• [SLOW TEST:19.861 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:45:36.542: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e5e8edec-3d3c-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 22:45:37.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-mx9js" to be "success or failure"
Mar  2 22:45:37.690: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87968ms
Mar  2 22:45:39.708: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036994994s
Mar  2 22:45:41.727: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055971009s
Mar  2 22:45:43.746: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074492818s
Mar  2 22:45:45.765: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094049757s
Mar  2 22:45:47.785: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113833764s
STEP: Saw pod success
Mar  2 22:45:47.785: INFO: Pod "pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:45:47.803: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:45:47.857: INFO: Waiting for pod pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:45:47.875: INFO: Pod pod-configmaps-e5ecad7b-3d3c-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:45:47.875: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mx9js" for this suite.
Mar  2 22:45:55.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:45:56.731: INFO: namespace: e2e-tests-configmap-mx9js, resource: packagemanifests, items remaining: 1
Mar  2 22:45:57.885: INFO: namespace: e2e-tests-configmap-mx9js, resource: bindings, ignored listing per whitelist
Mar  2 22:45:58.399: INFO: namespace: e2e-tests-configmap-mx9js no longer exists
Mar  2 22:45:58.417: INFO: namespace: e2e-tests-configmap-mx9js, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:45:58.434: INFO: namespace e2e-tests-configmap-mx9js deletion completed in 10.528162049s

• [SLOW TEST:21.893 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:45:58.434: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  2 22:46:09.640: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-f2f22fae-3d3c-11e9-9008-0a58ac10f353", GenerateName:"", Namespace:"e2e-tests-pods-5dgms", SelfLink:"/api/v1/namespaces/e2e-tests-pods-5dgms/pods/pod-submit-remove-f2f22fae-3d3c-11e9-9008-0a58ac10f353", UID:"f2f9ef70-3d3c-11e9-b620-0a843850fdce", ResourceVersion:"37829", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687163559, loc:(*time.Location)(0x6c3af00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"480112159"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"ips\": [\n        \"10.128.2.20\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-g9nws", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc42245c500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g9nws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4214bb6d0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc421e602d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-148-56.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc422644240), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-z7ntx"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc421e60320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc421e60340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc421e60348), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163559, loc:(*time.Location)(0x6c3af00)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163567, loc:(*time.Location)(0x6c3af00)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163567, loc:(*time.Location)(0x6c3af00)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687163559, loc:(*time.Location)(0x6c3af00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.148.56", PodIP:"10.128.2.20", StartTime:(*v1.Time)(0xc4210f6d00), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc4210f6d20), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df", ContainerID:"cri-o://9608020d076bbb7d306bd79b5440fdc8d9b46c0ee0b746fbd6b12b5855c7ed94"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:46:14.795: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5dgms" for this suite.
Mar  2 22:46:20.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:46:21.725: INFO: namespace: e2e-tests-pods-5dgms, resource: packagemanifests, items remaining: 1
Mar  2 22:46:22.500: INFO: namespace: e2e-tests-pods-5dgms, resource: bindings, ignored listing per whitelist
Mar  2 22:46:23.304: INFO: namespace: e2e-tests-pods-5dgms no longer exists
Mar  2 22:46:23.323: INFO: namespace: e2e-tests-pods-5dgms, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:46:23.340: INFO: namespace e2e-tests-pods-5dgms deletion completed in 8.526234868s

• [SLOW TEST:24.906 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:46:23.340: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-01ca9af6-3d3d-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 22:46:24.450: INFO: Waiting up to 5m0s for pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-pjvcf" to be "success or failure"
Mar  2 22:46:24.468: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.852585ms
Mar  2 22:46:26.486: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035881469s
Mar  2 22:46:28.504: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054037511s
Mar  2 22:46:30.522: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072335776s
Mar  2 22:46:32.540: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090417219s
Mar  2 22:46:34.558: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108759557s
STEP: Saw pod success
Mar  2 22:46:34.559: INFO: Pod "pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:46:34.576: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:46:34.631: INFO: Waiting for pod pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:46:34.649: INFO: Pod pod-secrets-01cf1b60-3d3d-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:46:34.649: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pjvcf" for this suite.
Mar  2 22:46:40.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:46:41.992: INFO: namespace: e2e-tests-secrets-pjvcf, resource: bindings, ignored listing per whitelist
Mar  2 22:46:42.578: INFO: namespace: e2e-tests-secrets-pjvcf, resource: packagemanifests, items remaining: 1
Mar  2 22:46:43.172: INFO: namespace: e2e-tests-secrets-pjvcf no longer exists
Mar  2 22:46:43.193: INFO: namespace: e2e-tests-secrets-pjvcf, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:46:43.211: INFO: namespace e2e-tests-secrets-pjvcf deletion completed in 8.529030778s

• [SLOW TEST:19.871 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:46:43.211: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 22:46:44.312: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-w9drz'
Mar  2 22:46:45.485: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar  2 22:46:45.485: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Mar  2 22:46:45.503: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-w9drz'
Mar  2 22:46:45.710: INFO: stderr: ""
Mar  2 22:46:45.710: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:46:45.710: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w9drz" for this suite.
Mar  2 22:47:09.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:47:11.024: INFO: namespace: e2e-tests-kubectl-w9drz, resource: bindings, ignored listing per whitelist
Mar  2 22:47:11.795: INFO: namespace: e2e-tests-kubectl-w9drz, resource: packagemanifests, items remaining: 1
Mar  2 22:47:12.234: INFO: namespace: e2e-tests-kubectl-w9drz no longer exists
Mar  2 22:47:12.252: INFO: namespace: e2e-tests-kubectl-w9drz, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:47:12.269: INFO: namespace e2e-tests-kubectl-w9drz deletion completed in 26.526610832s

• [SLOW TEST:29.058 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:47:12.270: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 22:47:13.279: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:13.472: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar  2 22:47:13.472: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Mar  2 22:47:13.509: INFO: scanned /tmp/home for discovery docs: <nil>
Mar  2 22:47:13.509: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:34.856: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  2 22:47:34.856: INFO: stdout: "Created e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548\nScaling up e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  2 22:47:34.856: INFO: stdout: "Created e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548\nScaling up e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  2 22:47:34.857: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:35.030: INFO: stderr: ""
Mar  2 22:47:35.030: INFO: stdout: "e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548-l6k6c e2e-test-nginx-rc-wrs6z "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Mar  2 22:47:40.030: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:40.213: INFO: stderr: ""
Mar  2 22:47:40.213: INFO: stdout: "e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548-l6k6c "
Mar  2 22:47:40.213: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548-l6k6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:40.398: INFO: stderr: ""
Mar  2 22:47:40.398: INFO: stdout: "true"
Mar  2 22:47:40.398: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548-l6k6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:40.568: INFO: stderr: ""
Mar  2 22:47:40.569: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  2 22:47:40.569: INFO: e2e-test-nginx-rc-531ff0529b9f0c24f17d23073a1f2548-l6k6c is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Mar  2 22:47:40.569: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cwb6l'
Mar  2 22:47:40.774: INFO: stderr: ""
Mar  2 22:47:40.774: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:47:40.774: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cwb6l" for this suite.
Mar  2 22:47:46.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:47:47.669: INFO: namespace: e2e-tests-kubectl-cwb6l, resource: bindings, ignored listing per whitelist
Mar  2 22:47:48.798: INFO: namespace: e2e-tests-kubectl-cwb6l, resource: packagemanifests, items remaining: 1
Mar  2 22:47:49.300: INFO: namespace: e2e-tests-kubectl-cwb6l no longer exists
Mar  2 22:47:49.321: INFO: namespace: e2e-tests-kubectl-cwb6l, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:47:49.339: INFO: namespace e2e-tests-kubectl-cwb6l deletion completed in 8.532995304s

• [SLOW TEST:37.070 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:47:49.340: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-350c419e-3d3d-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 22:47:50.439: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-t6jq2" to be "success or failure"
Mar  2 22:47:50.457: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.880466ms
Mar  2 22:47:52.475: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035732133s
Mar  2 22:47:54.493: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05380881s
Mar  2 22:47:56.512: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072282029s
Mar  2 22:47:58.530: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090740025s
Mar  2 22:48:00.548: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108964771s
STEP: Saw pod success
Mar  2 22:48:00.548: INFO: Pod "pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:48:00.566: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:48:00.635: INFO: Waiting for pod pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:48:00.652: INFO: Pod pod-projected-configmaps-350fef28-3d3d-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:48:00.653: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t6jq2" for this suite.
Mar  2 22:48:06.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:48:09.053: INFO: namespace: e2e-tests-projected-t6jq2, resource: bindings, ignored listing per whitelist
Mar  2 22:48:09.446: INFO: namespace: e2e-tests-projected-t6jq2, resource: packagemanifests, items remaining: 1
Mar  2 22:48:09.571: INFO: namespace: e2e-tests-projected-t6jq2 no longer exists
Mar  2 22:48:09.589: INFO: namespace: e2e-tests-projected-t6jq2, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:48:09.607: INFO: namespace e2e-tests-projected-t6jq2 deletion completed in 8.9221202s

• [SLOW TEST:20.267 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:48:09.607: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  2 22:48:10.790: INFO: Waiting up to 5m0s for pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-n87rd" to be "success or failure"
Mar  2 22:48:10.809: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.051495ms
Mar  2 22:48:12.828: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037951218s
Mar  2 22:48:14.846: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05616086s
Mar  2 22:48:16.864: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074238591s
Mar  2 22:48:18.883: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092420433s
Mar  2 22:48:20.901: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110629067s
STEP: Saw pod success
Mar  2 22:48:20.901: INFO: Pod "downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:48:20.918: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:48:20.970: INFO: Waiting for pod downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:48:20.987: INFO: Pod downward-api-412fd062-3d3d-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:48:20.987: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n87rd" for this suite.
Mar  2 22:48:27.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:48:28.605: INFO: namespace: e2e-tests-downward-api-n87rd, resource: bindings, ignored listing per whitelist
Mar  2 22:48:28.750: INFO: namespace: e2e-tests-downward-api-n87rd, resource: packagemanifests, items remaining: 1
Mar  2 22:48:29.512: INFO: namespace: e2e-tests-downward-api-n87rd no longer exists
Mar  2 22:48:29.530: INFO: namespace: e2e-tests-downward-api-n87rd, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:48:29.551: INFO: namespace e2e-tests-downward-api-n87rd deletion completed in 8.531284866s

• [SLOW TEST:19.944 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:48:29.551: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 22:48:30.633: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-9czqw'
Mar  2 22:48:30.848: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar  2 22:48:30.849: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  2 22:48:30.903: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kzw9b]
Mar  2 22:48:30.903: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kzw9b" in namespace "e2e-tests-kubectl-9czqw" to be "running and ready"
Mar  2 22:48:30.928: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.861755ms
Mar  2 22:48:32.946: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042758338s
Mar  2 22:48:34.964: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060723881s
Mar  2 22:48:36.982: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078669922s
Mar  2 22:48:39.000: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097030658s
Mar  2 22:48:41.018: INFO: Pod "e2e-test-nginx-rc-kzw9b": Phase="Running", Reason="", readiness=true. Elapsed: 10.115003642s
Mar  2 22:48:41.018: INFO: Pod "e2e-test-nginx-rc-kzw9b" satisfied condition "running and ready"
Mar  2 22:48:41.018: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kzw9b]
Mar  2 22:48:41.019: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9czqw'
Mar  2 22:48:41.290: INFO: stderr: ""
Mar  2 22:48:41.290: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Mar  2 22:48:41.290: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9czqw'
Mar  2 22:48:41.492: INFO: stderr: ""
Mar  2 22:48:41.492: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:48:41.492: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9czqw" for this suite.
Mar  2 22:49:05.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:49:06.421: INFO: namespace: e2e-tests-kubectl-9czqw, resource: packagemanifests, items remaining: 1
Mar  2 22:49:07.034: INFO: namespace: e2e-tests-kubectl-9czqw, resource: bindings, ignored listing per whitelist
Mar  2 22:49:08.012: INFO: namespace: e2e-tests-kubectl-9czqw no longer exists
Mar  2 22:49:08.032: INFO: namespace: e2e-tests-kubectl-9czqw, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:49:08.049: INFO: namespace e2e-tests-kubectl-9czqw deletion completed in 26.525557959s

• [SLOW TEST:38.498 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:49:08.049: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  2 22:49:09.100: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:09.476: INFO: stderr: ""
Mar  2 22:49:09.477: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 22:49:09.477: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:09.656: INFO: stderr: ""
Mar  2 22:49:09.656: INFO: stdout: "update-demo-nautilus-kwb5t update-demo-nautilus-mkdws "
Mar  2 22:49:09.656: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-kwb5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:09.843: INFO: stderr: ""
Mar  2 22:49:09.843: INFO: stdout: ""
Mar  2 22:49:09.843: INFO: update-demo-nautilus-kwb5t is created but not running
Mar  2 22:49:14.843: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:15.021: INFO: stderr: ""
Mar  2 22:49:15.021: INFO: stdout: "update-demo-nautilus-kwb5t update-demo-nautilus-mkdws "
Mar  2 22:49:15.021: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-kwb5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:15.191: INFO: stderr: ""
Mar  2 22:49:15.191: INFO: stdout: ""
Mar  2 22:49:15.191: INFO: update-demo-nautilus-kwb5t is created but not running
Mar  2 22:49:20.192: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:20.372: INFO: stderr: ""
Mar  2 22:49:20.372: INFO: stdout: "update-demo-nautilus-kwb5t update-demo-nautilus-mkdws "
Mar  2 22:49:20.372: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-kwb5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:20.542: INFO: stderr: ""
Mar  2 22:49:20.542: INFO: stdout: "true"
Mar  2 22:49:20.542: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-kwb5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:20.715: INFO: stderr: ""
Mar  2 22:49:20.715: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:49:20.715: INFO: validating pod update-demo-nautilus-kwb5t
Mar  2 22:49:20.737: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:49:20.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:49:20.737: INFO: update-demo-nautilus-kwb5t is verified up and running
Mar  2 22:49:20.737: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-mkdws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:20.913: INFO: stderr: ""
Mar  2 22:49:20.914: INFO: stdout: "true"
Mar  2 22:49:20.914: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-mkdws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:21.089: INFO: stderr: ""
Mar  2 22:49:21.089: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:49:21.089: INFO: validating pod update-demo-nautilus-mkdws
Mar  2 22:49:21.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:49:21.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:49:21.114: INFO: update-demo-nautilus-mkdws is verified up and running
STEP: using delete to clean up resources
Mar  2 22:49:21.114: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:21.312: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:49:21.312: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 22:49:21.312: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-jqp2q'
Mar  2 22:49:21.521: INFO: stderr: "No resources found.\n"
Mar  2 22:49:21.521: INFO: stdout: ""
Mar  2 22:49:21.521: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-jqp2q -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 22:49:21.707: INFO: stderr: ""
Mar  2 22:49:21.707: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:49:21.707: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jqp2q" for this suite.
Mar  2 22:49:27.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:49:28.562: INFO: namespace: e2e-tests-kubectl-jqp2q, resource: bindings, ignored listing per whitelist
Mar  2 22:49:29.126: INFO: namespace: e2e-tests-kubectl-jqp2q, resource: packagemanifests, items remaining: 1
Mar  2 22:49:30.231: INFO: namespace: e2e-tests-kubectl-jqp2q no longer exists
Mar  2 22:49:30.249: INFO: namespace: e2e-tests-kubectl-jqp2q, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:49:30.268: INFO: namespace e2e-tests-kubectl-jqp2q deletion completed in 8.528212442s

• [SLOW TEST:22.219 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:49:30.268: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:49:31.349: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-n8rms" to be "success or failure"
Mar  2 22:49:31.367: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.093445ms
Mar  2 22:49:33.386: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036694294s
Mar  2 22:49:35.405: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055839584s
Mar  2 22:49:37.423: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073962151s
Mar  2 22:49:39.442: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092885855s
Mar  2 22:49:41.460: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111097501s
STEP: Saw pod success
Mar  2 22:49:41.460: INFO: Pod "downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:49:41.478: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:49:41.534: INFO: Waiting for pod downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:49:41.552: INFO: Pod downwardapi-volume-71351023-3d3d-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:49:41.552: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n8rms" for this suite.
Mar  2 22:49:47.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:49:48.550: INFO: namespace: e2e-tests-downward-api-n8rms, resource: packagemanifests, items remaining: 1
Mar  2 22:49:49.276: INFO: namespace: e2e-tests-downward-api-n8rms, resource: bindings, ignored listing per whitelist
Mar  2 22:49:50.074: INFO: namespace: e2e-tests-downward-api-n8rms no longer exists
Mar  2 22:49:50.092: INFO: namespace: e2e-tests-downward-api-n8rms, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:49:50.114: INFO: namespace e2e-tests-downward-api-n8rms deletion completed in 8.529680544s

• [SLOW TEST:19.845 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:49:50.114: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-dvsl
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:49:51.275: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dvsl" in namespace "e2e-tests-subpath-fvt25" to be "success or failure"
Mar  2 22:49:51.296: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 21.395398ms
Mar  2 22:49:53.314: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039529678s
Mar  2 22:49:55.333: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057743075s
Mar  2 22:49:57.351: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076272018s
Mar  2 22:49:59.369: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094364295s
Mar  2 22:50:01.388: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113241996s
Mar  2 22:50:03.406: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 12.131503597s
Mar  2 22:50:05.424: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 14.149464265s
Mar  2 22:50:07.444: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 16.169360854s
Mar  2 22:50:09.466: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 18.190749531s
Mar  2 22:50:11.484: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 20.208748688s
Mar  2 22:50:13.502: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 22.227040357s
Mar  2 22:50:15.520: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 24.24499231s
Mar  2 22:50:17.538: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 26.263611875s
Mar  2 22:50:19.556: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Running", Reason="", readiness=false. Elapsed: 28.281531346s
Mar  2 22:50:21.574: INFO: Pod "pod-subpath-test-configmap-dvsl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.299514456s
STEP: Saw pod success
Mar  2 22:50:21.574: INFO: Pod "pod-subpath-test-configmap-dvsl" satisfied condition "success or failure"
Mar  2 22:50:21.592: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-subpath-test-configmap-dvsl container test-container-subpath-configmap-dvsl: <nil>
STEP: delete the pod
Mar  2 22:50:21.648: INFO: Waiting for pod pod-subpath-test-configmap-dvsl to disappear
Mar  2 22:50:21.669: INFO: Pod pod-subpath-test-configmap-dvsl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dvsl
Mar  2 22:50:21.669: INFO: Deleting pod "pod-subpath-test-configmap-dvsl" in namespace "e2e-tests-subpath-fvt25"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:50:21.687: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fvt25" for this suite.
Mar  2 22:50:27.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:50:28.848: INFO: namespace: e2e-tests-subpath-fvt25, resource: packagemanifests, items remaining: 1
Mar  2 22:50:29.391: INFO: namespace: e2e-tests-subpath-fvt25, resource: bindings, ignored listing per whitelist
Mar  2 22:50:30.211: INFO: namespace: e2e-tests-subpath-fvt25 no longer exists
Mar  2 22:50:30.234: INFO: namespace: e2e-tests-subpath-fvt25, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:50:30.252: INFO: namespace e2e-tests-subpath-fvt25 deletion completed in 8.533204845s

• [SLOW TEST:40.138 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:50:30.252: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-94fa1855-3d3d-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 22:50:31.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-879dx" to be "success or failure"
Mar  2 22:50:31.403: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.388092ms
Mar  2 22:50:33.422: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037140973s
Mar  2 22:50:35.439: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054998359s
Mar  2 22:50:37.458: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07377152s
Mar  2 22:50:39.476: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091891721s
Mar  2 22:50:41.494: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109973481s
STEP: Saw pod success
Mar  2 22:50:41.495: INFO: Pod "pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:50:41.513: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:50:41.568: INFO: Waiting for pod pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:50:41.585: INFO: Pod pod-configmaps-94fe637a-3d3d-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:50:41.585: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-879dx" for this suite.
Mar  2 22:50:47.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:50:49.665: INFO: namespace: e2e-tests-configmap-879dx, resource: bindings, ignored listing per whitelist
Mar  2 22:50:50.046: INFO: namespace: e2e-tests-configmap-879dx, resource: packagemanifests, items remaining: 1
Mar  2 22:50:50.108: INFO: namespace: e2e-tests-configmap-879dx no longer exists
Mar  2 22:50:50.127: INFO: namespace: e2e-tests-configmap-879dx, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:50:50.150: INFO: namespace e2e-tests-configmap-879dx deletion completed in 8.532121523s

• [SLOW TEST:19.898 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:50:50.151: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-28qp
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:50:51.310: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-28qp" in namespace "e2e-tests-subpath-9nqtc" to be "success or failure"
Mar  2 22:50:51.330: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Pending", Reason="", readiness=false. Elapsed: 20.646674ms
Mar  2 22:50:53.350: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040340415s
Mar  2 22:50:55.368: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058254491s
Mar  2 22:50:57.386: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076347473s
Mar  2 22:50:59.404: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094364761s
Mar  2 22:51:01.423: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 10.113615062s
Mar  2 22:51:03.442: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 12.132681521s
Mar  2 22:51:05.461: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 14.151273902s
Mar  2 22:51:07.479: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 16.169171494s
Mar  2 22:51:09.497: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 18.187841373s
Mar  2 22:51:11.516: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 20.206262749s
Mar  2 22:51:13.534: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 22.22448583s
Mar  2 22:51:15.554: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 24.244471706s
Mar  2 22:51:17.572: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 26.262472537s
Mar  2 22:51:19.593: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Running", Reason="", readiness=false. Elapsed: 28.28307532s
Mar  2 22:51:21.611: INFO: Pod "pod-subpath-test-configmap-28qp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.301262756s
STEP: Saw pod success
Mar  2 22:51:21.611: INFO: Pod "pod-subpath-test-configmap-28qp" satisfied condition "success or failure"
Mar  2 22:51:21.628: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-subpath-test-configmap-28qp container test-container-subpath-configmap-28qp: <nil>
STEP: delete the pod
Mar  2 22:51:21.685: INFO: Waiting for pod pod-subpath-test-configmap-28qp to disappear
Mar  2 22:51:21.703: INFO: Pod pod-subpath-test-configmap-28qp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-28qp
Mar  2 22:51:21.703: INFO: Deleting pod "pod-subpath-test-configmap-28qp" in namespace "e2e-tests-subpath-9nqtc"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:51:21.720: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-9nqtc" for this suite.
Mar  2 22:51:27.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:51:28.695: INFO: namespace: e2e-tests-subpath-9nqtc, resource: bindings, ignored listing per whitelist
Mar  2 22:51:30.126: INFO: namespace: e2e-tests-subpath-9nqtc, resource: packagemanifests, items remaining: 1
Mar  2 22:51:30.242: INFO: namespace: e2e-tests-subpath-9nqtc no longer exists
Mar  2 22:51:30.260: INFO: namespace: e2e-tests-subpath-9nqtc, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:51:30.277: INFO: namespace e2e-tests-subpath-9nqtc deletion completed in 8.524480735s

• [SLOW TEST:40.126 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:51:30.278: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-xqljb
Mar  2 22:51:43.389: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-xqljb
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:51:43.407: INFO: Initial restart count of pod liveness-http is 0
Mar  2 22:52:01.587: INFO: Restart count of pod e2e-tests-container-probe-xqljb/liveness-http is now 1 (18.180306918s elapsed)
Mar  2 22:52:21.772: INFO: Restart count of pod e2e-tests-container-probe-xqljb/liveness-http is now 2 (38.36483204s elapsed)
Mar  2 22:52:42.002: INFO: Restart count of pod e2e-tests-container-probe-xqljb/liveness-http is now 3 (58.595245785s elapsed)
Mar  2 22:53:02.184: INFO: Restart count of pod e2e-tests-container-probe-xqljb/liveness-http is now 4 (1m18.777169148s elapsed)
Mar  2 22:54:14.882: INFO: Restart count of pod e2e-tests-container-probe-xqljb/liveness-http is now 5 (2m31.47499577s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:54:14.912: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-xqljb" for this suite.
Mar  2 22:54:23.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:54:24.110: INFO: namespace: e2e-tests-container-probe-xqljb, resource: packagemanifests, items remaining: 1
Mar  2 22:54:24.577: INFO: namespace: e2e-tests-container-probe-xqljb, resource: bindings, ignored listing per whitelist
Mar  2 22:54:25.436: INFO: namespace: e2e-tests-container-probe-xqljb no longer exists
Mar  2 22:54:25.455: INFO: namespace: e2e-tests-container-probe-xqljb, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:54:25.472: INFO: namespace e2e-tests-container-probe-xqljb deletion completed in 10.528527654s

• [SLOW TEST:175.195 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:54:25.473: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-212a6177-3d3e-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 22:54:26.584: INFO: Waiting up to 5m0s for pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-9bmwj" to be "success or failure"
Mar  2 22:54:26.604: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.673018ms
Mar  2 22:54:28.622: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03871026s
Mar  2 22:54:30.641: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056813942s
Mar  2 22:54:32.660: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075914869s
Mar  2 22:54:34.677: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09360585s
Mar  2 22:54:36.696: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112436201s
STEP: Saw pod success
Mar  2 22:54:36.696: INFO: Pod "pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:54:36.714: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:54:36.771: INFO: Waiting for pod pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:54:36.788: INFO: Pod pod-secrets-212f4cca-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:54:36.788: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-9bmwj" for this suite.
Mar  2 22:54:42.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:54:44.618: INFO: namespace: e2e-tests-secrets-9bmwj, resource: packagemanifests, items remaining: 1
Mar  2 22:54:44.947: INFO: namespace: e2e-tests-secrets-9bmwj, resource: bindings, ignored listing per whitelist
Mar  2 22:54:45.318: INFO: namespace: e2e-tests-secrets-9bmwj no longer exists
Mar  2 22:54:45.337: INFO: namespace: e2e-tests-secrets-9bmwj, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:54:45.355: INFO: namespace e2e-tests-secrets-9bmwj deletion completed in 8.526867752s

• [SLOW TEST:19.882 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:54:45.355: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:54:46.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-6j8pg" to be "success or failure"
Mar  2 22:54:46.467: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.433276ms
Mar  2 22:54:48.485: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036425484s
Mar  2 22:54:50.503: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054591604s
Mar  2 22:54:52.522: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072996592s
Mar  2 22:54:54.539: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090902306s
Mar  2 22:54:56.557: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108746792s
STEP: Saw pod success
Mar  2 22:54:56.557: INFO: Pod "downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:54:56.575: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:54:56.628: INFO: Waiting for pod downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:54:56.645: INFO: Pod downwardapi-volume-2d05097b-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:54:56.645: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6j8pg" for this suite.
Mar  2 22:55:02.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:55:03.940: INFO: namespace: e2e-tests-downward-api-6j8pg, resource: packagemanifests, items remaining: 1
Mar  2 22:55:04.011: INFO: namespace: e2e-tests-downward-api-6j8pg, resource: bindings, ignored listing per whitelist
Mar  2 22:55:05.167: INFO: namespace: e2e-tests-downward-api-6j8pg no longer exists
Mar  2 22:55:05.185: INFO: namespace: e2e-tests-downward-api-6j8pg, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:55:05.202: INFO: namespace e2e-tests-downward-api-6j8pg deletion completed in 8.525500079s

• [SLOW TEST:19.847 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:55:05.203: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:55:06.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-4ccdr" to be "success or failure"
Mar  2 22:55:06.309: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.818052ms
Mar  2 22:55:08.327: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035788269s
Mar  2 22:55:10.346: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05484149s
Mar  2 22:55:12.365: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073017502s
Mar  2 22:55:14.383: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091400537s
Mar  2 22:55:16.401: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109517112s
STEP: Saw pod success
Mar  2 22:55:16.401: INFO: Pod "downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:55:16.418: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:55:16.475: INFO: Waiting for pod downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:55:16.493: INFO: Pod downwardapi-volume-38d8c5f4-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:55:16.493: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4ccdr" for this suite.
Mar  2 22:55:22.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:55:23.558: INFO: namespace: e2e-tests-downward-api-4ccdr, resource: bindings, ignored listing per whitelist
Mar  2 22:55:24.034: INFO: namespace: e2e-tests-downward-api-4ccdr, resource: packagemanifests, items remaining: 1
Mar  2 22:55:25.015: INFO: namespace: e2e-tests-downward-api-4ccdr no longer exists
Mar  2 22:55:25.034: INFO: namespace: e2e-tests-downward-api-4ccdr, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:55:25.051: INFO: namespace e2e-tests-downward-api-4ccdr deletion completed in 8.526250975s

• [SLOW TEST:19.849 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:55:25.052: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 22:55:54.247: INFO: Container started at 2019-03-02 22:55:35 +0000 UTC, pod became ready at 2019-03-02 22:55:52 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:55:54.247: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-z2vrg" for this suite.
Mar  2 22:56:18.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:56:19.599: INFO: namespace: e2e-tests-container-probe-z2vrg, resource: packagemanifests, items remaining: 1
Mar  2 22:56:20.298: INFO: namespace: e2e-tests-container-probe-z2vrg, resource: bindings, ignored listing per whitelist
Mar  2 22:56:20.770: INFO: namespace: e2e-tests-container-probe-z2vrg no longer exists
Mar  2 22:56:20.788: INFO: namespace: e2e-tests-container-probe-z2vrg, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:56:20.806: INFO: namespace e2e-tests-container-probe-z2vrg deletion completed in 26.527051252s

• [SLOW TEST:55.754 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:56:20.806: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:56:21.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-j2z54" to be "success or failure"
Mar  2 22:56:21.944: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.818275ms
Mar  2 22:56:23.962: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037338694s
Mar  2 22:56:25.981: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056269312s
Mar  2 22:56:27.999: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074559206s
Mar  2 22:56:30.018: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09296896s
Mar  2 22:56:32.036: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111662919s
STEP: Saw pod success
Mar  2 22:56:32.036: INFO: Pod "downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:56:32.054: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:56:32.107: INFO: Waiting for pod downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:56:32.125: INFO: Pod downwardapi-volume-65ecac78-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:56:32.125: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j2z54" for this suite.
Mar  2 22:56:38.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:56:39.944: INFO: namespace: e2e-tests-projected-j2z54, resource: packagemanifests, items remaining: 1
Mar  2 22:56:40.050: INFO: namespace: e2e-tests-projected-j2z54, resource: bindings, ignored listing per whitelist
Mar  2 22:56:40.654: INFO: namespace: e2e-tests-projected-j2z54 no longer exists
Mar  2 22:56:40.673: INFO: namespace: e2e-tests-projected-j2z54, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:56:40.690: INFO: namespace e2e-tests-projected-j2z54 deletion completed in 8.531047587s

• [SLOW TEST:19.884 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:56:40.690: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-71c36d52-3d3e-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 22:56:41.798: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-vgskn" to be "success or failure"
Mar  2 22:56:41.818: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.291056ms
Mar  2 22:56:43.837: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038694917s
Mar  2 22:56:45.855: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056914044s
Mar  2 22:56:47.873: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074913464s
Mar  2 22:56:49.892: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094072548s
Mar  2 22:56:51.911: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.11230347s
STEP: Saw pod success
Mar  2 22:56:51.911: INFO: Pod "pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:56:51.928: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:56:51.984: INFO: Waiting for pod pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:56:52.003: INFO: Pod pod-projected-secrets-71c71451-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:56:52.003: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vgskn" for this suite.
Mar  2 22:56:58.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:56:59.438: INFO: namespace: e2e-tests-projected-vgskn, resource: bindings, ignored listing per whitelist
Mar  2 22:56:59.914: INFO: namespace: e2e-tests-projected-vgskn, resource: packagemanifests, items remaining: 1
Mar  2 22:57:00.523: INFO: namespace: e2e-tests-projected-vgskn no longer exists
Mar  2 22:57:00.541: INFO: namespace: e2e-tests-projected-vgskn, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:57:00.559: INFO: namespace e2e-tests-projected-vgskn deletion completed in 8.524477527s

• [SLOW TEST:19.868 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:57:00.559: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:57:01.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-9s2c2" to be "success or failure"
Mar  2 22:57:01.659: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.150146ms
Mar  2 22:57:03.678: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038564391s
Mar  2 22:57:05.696: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056900004s
Mar  2 22:57:07.714: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075043518s
Mar  2 22:57:09.732: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092952814s
Mar  2 22:57:11.750: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111070054s
STEP: Saw pod success
Mar  2 22:57:11.750: INFO: Pod "downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:57:11.768: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 22:57:11.825: INFO: Waiting for pod downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:57:11.842: INFO: Pod downwardapi-volume-7d99ae08-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:57:11.842: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9s2c2" for this suite.
Mar  2 22:57:17.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:57:18.960: INFO: namespace: e2e-tests-projected-9s2c2, resource: packagemanifests, items remaining: 1
Mar  2 22:57:19.835: INFO: namespace: e2e-tests-projected-9s2c2, resource: bindings, ignored listing per whitelist
Mar  2 22:57:20.366: INFO: namespace: e2e-tests-projected-9s2c2 no longer exists
Mar  2 22:57:20.384: INFO: namespace: e2e-tests-projected-9s2c2, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:57:20.401: INFO: namespace e2e-tests-projected-9s2c2 deletion completed in 8.526666566s

• [SLOW TEST:19.842 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:57:20.401: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar  2 22:57:22.107: INFO: created pod pod-service-account-defaultsa
Mar  2 22:57:22.107: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 22:57:22.147: INFO: created pod pod-service-account-mountsa
Mar  2 22:57:22.147: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 22:57:22.175: INFO: created pod pod-service-account-nomountsa
Mar  2 22:57:22.175: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 22:57:22.205: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 22:57:22.205: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 22:57:22.235: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 22:57:22.235: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 22:57:22.262: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 22:57:22.262: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 22:57:22.288: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 22:57:22.288: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 22:57:22.317: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 22:57:22.317: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 22:57:22.346: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 22:57:22.346: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:57:22.346: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-ml598" for this suite.
Mar  2 22:57:46.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:57:47.911: INFO: namespace: e2e-tests-svcaccounts-ml598, resource: bindings, ignored listing per whitelist
Mar  2 22:57:48.656: INFO: namespace: e2e-tests-svcaccounts-ml598, resource: packagemanifests, items remaining: 1
Mar  2 22:57:48.869: INFO: namespace: e2e-tests-svcaccounts-ml598 no longer exists
Mar  2 22:57:48.887: INFO: namespace: e2e-tests-svcaccounts-ml598, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:57:48.905: INFO: namespace e2e-tests-svcaccounts-ml598 deletion completed in 26.526774438s

• [SLOW TEST:28.504 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:57:48.905: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-n7jf9
[It] Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-n7jf9
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-n7jf9
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-n7jf9
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-n7jf9
Mar  2 22:58:00.134: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n7jf9, name: ss-0, uid: a077c99c-3d3e-11e9-8060-0e81de5d2554, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 22:58:01.073: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n7jf9, name: ss-0, uid: a077c99c-3d3e-11e9-8060-0e81de5d2554, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 22:58:01.083: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n7jf9, name: ss-0, uid: a077c99c-3d3e-11e9-8060-0e81de5d2554, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 22:58:01.092: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-n7jf9
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-n7jf9
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-n7jf9 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  2 22:58:13.260: INFO: Deleting all statefulset in ns e2e-tests-statefulset-n7jf9
Mar  2 22:58:13.279: INFO: Scaling statefulset ss to 0
Mar  2 22:58:23.353: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:58:23.371: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:58:23.437: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-n7jf9" for this suite.
Mar  2 22:58:31.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:58:33.846: INFO: namespace: e2e-tests-statefulset-n7jf9, resource: packagemanifests, items remaining: 1
Mar  2 22:58:33.892: INFO: namespace: e2e-tests-statefulset-n7jf9, resource: bindings, ignored listing per whitelist
Mar  2 22:58:33.957: INFO: namespace: e2e-tests-statefulset-n7jf9 no longer exists
Mar  2 22:58:33.976: INFO: namespace: e2e-tests-statefulset-n7jf9, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:58:33.995: INFO: namespace e2e-tests-statefulset-n7jf9 deletion completed in 10.525961038s

• [SLOW TEST:45.090 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:58:33.995: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar  2 22:58:35.104: INFO: Waiting up to 5m0s for pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353" in namespace "e2e-tests-var-expansion-jvfg8" to be "success or failure"
Mar  2 22:58:35.123: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.271834ms
Mar  2 22:58:37.142: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037402755s
Mar  2 22:58:39.159: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055040281s
Mar  2 22:58:41.177: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07311545s
Mar  2 22:58:43.197: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093001549s
Mar  2 22:58:45.216: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111767621s
STEP: Saw pod success
Mar  2 22:58:45.216: INFO: Pod "var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 22:58:45.233: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:58:45.283: INFO: Waiting for pod var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353 to disappear
Mar  2 22:58:45.301: INFO: Pod var-expansion-b54e05f7-3d3e-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:58:45.301: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jvfg8" for this suite.
Mar  2 22:58:51.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:58:52.151: INFO: namespace: e2e-tests-var-expansion-jvfg8, resource: bindings, ignored listing per whitelist
Mar  2 22:58:52.558: INFO: namespace: e2e-tests-var-expansion-jvfg8, resource: packagemanifests, items remaining: 1
Mar  2 22:58:53.839: INFO: namespace: e2e-tests-var-expansion-jvfg8 no longer exists
Mar  2 22:58:53.858: INFO: namespace: e2e-tests-var-expansion-jvfg8, total namespaces: 47, active: 47, terminating: 0
Mar  2 22:58:53.875: INFO: namespace e2e-tests-var-expansion-jvfg8 deletion completed in 8.530400163s

• [SLOW TEST:19.880 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 22:58:53.875: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-v95m2
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 22:58:54.949: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 22:59:33.400: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.129.2.32 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v95m2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:59:33.400: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 22:59:34.680: INFO: Found all expected endpoints: [netserver-0]
Mar  2 22:59:34.698: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.131.0.17 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v95m2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:59:34.698: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 22:59:35.900: INFO: Found all expected endpoints: [netserver-1]
Mar  2 22:59:35.917: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.128.2.36 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v95m2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:59:35.917: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 22:59:37.111: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 22:59:37.111: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-v95m2" for this suite.
Mar  2 23:00:01.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:00:02.517: INFO: namespace: e2e-tests-pod-network-test-v95m2, resource: packagemanifests, items remaining: 1
Mar  2 23:00:02.623: INFO: namespace: e2e-tests-pod-network-test-v95m2, resource: bindings, ignored listing per whitelist
Mar  2 23:00:03.650: INFO: namespace: e2e-tests-pod-network-test-v95m2 no longer exists
Mar  2 23:00:03.672: INFO: namespace: e2e-tests-pod-network-test-v95m2, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:00:03.689: INFO: namespace e2e-tests-pod-network-test-v95m2 deletion completed in 26.533174127s

• [SLOW TEST:69.814 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:00:03.690: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:00:04.829: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ead03dbe-3d3e-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ead03dbe-3d3e-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:01:21.858: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ljpx6" for this suite.
Mar  2 23:01:45.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:01:47.182: INFO: namespace: e2e-tests-projected-ljpx6, resource: packagemanifests, items remaining: 1
Mar  2 23:01:47.217: INFO: namespace: e2e-tests-projected-ljpx6, resource: bindings, ignored listing per whitelist
Mar  2 23:01:48.396: INFO: namespace: e2e-tests-projected-ljpx6 no longer exists
Mar  2 23:01:48.415: INFO: namespace: e2e-tests-projected-ljpx6, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:01:48.432: INFO: namespace e2e-tests-projected-ljpx6 deletion completed in 26.528521281s

• [SLOW TEST:104.743 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:01:48.433: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar  2 23:01:49.528: INFO: Waiting up to 5m0s for pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353" in namespace "e2e-tests-containers-59tcl" to be "success or failure"
Mar  2 23:01:49.548: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.846821ms
Mar  2 23:01:51.566: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038702249s
Mar  2 23:01:53.585: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056948621s
Mar  2 23:01:55.604: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076778676s
Mar  2 23:01:57.622: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094619606s
Mar  2 23:01:59.642: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.114165537s
Mar  2 23:02:01.660: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.13277523s
STEP: Saw pod success
Mar  2 23:02:01.660: INFO: Pod "client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:02:01.678: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:02:01.736: INFO: Waiting for pod client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:02:01.753: INFO: Pod client-containers-29323d30-3d3f-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:02:01.754: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-59tcl" for this suite.
Mar  2 23:02:07.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:02:08.691: INFO: namespace: e2e-tests-containers-59tcl, resource: bindings, ignored listing per whitelist
Mar  2 23:02:09.226: INFO: namespace: e2e-tests-containers-59tcl, resource: packagemanifests, items remaining: 1
Mar  2 23:02:10.292: INFO: namespace: e2e-tests-containers-59tcl no longer exists
Mar  2 23:02:10.311: INFO: namespace: e2e-tests-containers-59tcl, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:02:10.329: INFO: namespace e2e-tests-containers-59tcl deletion completed in 8.530002182s

• [SLOW TEST:21.896 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:02:10.329: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-8hj7z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-8hj7z to expose endpoints map[]
Mar  2 23:02:11.424: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-8hj7z exposes endpoints map[] (18.831785ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-8hj7z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-8hj7z to expose endpoints map[pod1:[80]]
Mar  2 23:02:15.650: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.184268824s elapsed, will retry)
Mar  2 23:02:19.794: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-8hj7z exposes endpoints map[pod1:[80]] (8.328066735s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-8hj7z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-8hj7z to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 23:02:24.087: INFO: Unexpected endpoints: found map[36487685-3d3f-11e9-b620-0a843850fdce:[80]], expected map[pod1:[80] pod2:[80]] (4.266644897s elapsed, will retry)
Mar  2 23:02:29.355: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-8hj7z exposes endpoints map[pod1:[80] pod2:[80]] (9.534454361s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-8hj7z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-8hj7z to expose endpoints map[pod2:[80]]
Mar  2 23:02:29.414: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-8hj7z exposes endpoints map[pod2:[80]] (37.187279ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-8hj7z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-8hj7z to expose endpoints map[]
Mar  2 23:02:29.456: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-8hj7z exposes endpoints map[] (18.044688ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:02:29.518: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-8hj7z" for this suite.
Mar  2 23:02:35.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:02:36.954: INFO: namespace: e2e-tests-services-8hj7z, resource: packagemanifests, items remaining: 1
Mar  2 23:02:37.352: INFO: namespace: e2e-tests-services-8hj7z, resource: bindings, ignored listing per whitelist
Mar  2 23:02:38.053: INFO: namespace: e2e-tests-services-8hj7z no longer exists
Mar  2 23:02:38.071: INFO: namespace: e2e-tests-services-8hj7z, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:02:38.090: INFO: namespace e2e-tests-services-8hj7z deletion completed in 8.526916618s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:27.761 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:02:38.090: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 23:02:39.158: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-qnlmx'
Mar  2 23:02:40.353: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar  2 23:02:40.353: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Mar  2 23:02:42.390: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qnlmx'
Mar  2 23:02:42.588: INFO: stderr: ""
Mar  2 23:02:42.588: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:02:42.589: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qnlmx" for this suite.
Mar  2 23:02:48.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:02:49.851: INFO: namespace: e2e-tests-kubectl-qnlmx, resource: bindings, ignored listing per whitelist
Mar  2 23:02:49.902: INFO: namespace: e2e-tests-kubectl-qnlmx, resource: packagemanifests, items remaining: 1
Mar  2 23:02:51.123: INFO: namespace: e2e-tests-kubectl-qnlmx no longer exists
Mar  2 23:02:51.141: INFO: namespace: e2e-tests-kubectl-qnlmx, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:02:51.159: INFO: namespace e2e-tests-kubectl-qnlmx deletion completed in 8.5254383s

• [SLOW TEST:13.068 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:02:51.159: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-4e968c8e-3d3f-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:02:52.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-rztdj" to be "success or failure"
Mar  2 23:02:52.391: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 28.24104ms
Mar  2 23:02:54.409: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046730009s
Mar  2 23:02:56.427: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064904801s
Mar  2 23:02:58.446: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083063857s
Mar  2 23:03:00.464: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101155569s
Mar  2 23:03:02.483: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.120746903s
STEP: Saw pod success
Mar  2 23:03:02.483: INFO: Pod "pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:03:02.501: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:03:02.554: INFO: Waiting for pod pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:03:02.574: INFO: Pod pod-configmaps-4e9e4f39-3d3f-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:03:02.574: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rztdj" for this suite.
Mar  2 23:03:08.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:03:09.779: INFO: namespace: e2e-tests-configmap-rztdj, resource: packagemanifests, items remaining: 1
Mar  2 23:03:10.363: INFO: namespace: e2e-tests-configmap-rztdj, resource: bindings, ignored listing per whitelist
Mar  2 23:03:11.108: INFO: namespace: e2e-tests-configmap-rztdj no longer exists
Mar  2 23:03:11.127: INFO: namespace: e2e-tests-configmap-rztdj, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:03:11.145: INFO: namespace e2e-tests-configmap-rztdj deletion completed in 8.526410897s

• [SLOW TEST:19.986 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:03:11.145: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 23:03:12.239: INFO: Waiting up to 5m0s for pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-dl5fw" to be "success or failure"
Mar  2 23:03:12.257: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.253545ms
Mar  2 23:03:14.275: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036155667s
Mar  2 23:03:16.293: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054286746s
Mar  2 23:03:18.311: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072328964s
Mar  2 23:03:20.329: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090273371s
Mar  2 23:03:22.347: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107915489s
STEP: Saw pod success
Mar  2 23:03:22.347: INFO: Pod "pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:03:22.364: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:03:22.420: INFO: Waiting for pod pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:03:22.438: INFO: Pod pod-5a7e8d2f-3d3f-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:03:22.438: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dl5fw" for this suite.
Mar  2 23:03:28.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:03:29.375: INFO: namespace: e2e-tests-emptydir-dl5fw, resource: packagemanifests, items remaining: 1
Mar  2 23:03:30.544: INFO: namespace: e2e-tests-emptydir-dl5fw, resource: bindings, ignored listing per whitelist
Mar  2 23:03:30.975: INFO: namespace: e2e-tests-emptydir-dl5fw no longer exists
Mar  2 23:03:30.993: INFO: namespace: e2e-tests-emptydir-dl5fw, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:03:31.011: INFO: namespace e2e-tests-emptydir-dl5fw deletion completed in 8.528398277s

• [SLOW TEST:19.866 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:03:31.011: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-665c631e-3d3f-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:03:32.164: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-gx6jk" to be "success or failure"
Mar  2 23:03:32.183: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.536847ms
Mar  2 23:03:34.201: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036569264s
Mar  2 23:03:36.219: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054582776s
Mar  2 23:03:38.237: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072700727s
Mar  2 23:03:40.255: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091076971s
Mar  2 23:03:42.274: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10989809s
STEP: Saw pod success
Mar  2 23:03:42.274: INFO: Pod "pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:03:42.291: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:03:42.346: INFO: Waiting for pod pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:03:42.366: INFO: Pod pod-projected-configmaps-665ff661-3d3f-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:03:42.366: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gx6jk" for this suite.
Mar  2 23:03:48.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:03:49.442: INFO: namespace: e2e-tests-projected-gx6jk, resource: bindings, ignored listing per whitelist
Mar  2 23:03:49.517: INFO: namespace: e2e-tests-projected-gx6jk, resource: packagemanifests, items remaining: 1
Mar  2 23:03:50.905: INFO: namespace: e2e-tests-projected-gx6jk no longer exists
Mar  2 23:03:50.928: INFO: namespace: e2e-tests-projected-gx6jk, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:03:50.945: INFO: namespace e2e-tests-projected-gx6jk deletion completed in 8.533846108s

• [SLOW TEST:19.934 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed  [Flaky] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:03:50.945: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed  [Flaky] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  2 23:04:02.194: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  2 23:04:07.453: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:04:07.471: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-99qdv" for this suite.
Mar  2 23:04:13.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:04:15.157: INFO: namespace: e2e-tests-pods-99qdv, resource: packagemanifests, items remaining: 1
Mar  2 23:04:15.977: INFO: namespace: e2e-tests-pods-99qdv, resource: bindings, ignored listing per whitelist
Mar  2 23:04:15.994: INFO: namespace: e2e-tests-pods-99qdv no longer exists
Mar  2 23:04:16.012: INFO: namespace: e2e-tests-pods-99qdv, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:04:16.029: INFO: namespace e2e-tests-pods-99qdv deletion completed in 8.526240556s

• [SLOW TEST:25.084 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Flaky] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:04:16.030: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  2 23:04:25.754: INFO: Successfully updated pod "annotationupdate81267ac3-3d3f-11e9-9008-0a58ac10f353"
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:04:27.817: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bbk5c" for this suite.
Mar  2 23:04:51.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:04:52.668: INFO: namespace: e2e-tests-projected-bbk5c, resource: bindings, ignored listing per whitelist
Mar  2 23:04:52.704: INFO: namespace: e2e-tests-projected-bbk5c, resource: packagemanifests, items remaining: 1
Mar  2 23:04:54.356: INFO: namespace: e2e-tests-projected-bbk5c no longer exists
Mar  2 23:04:54.374: INFO: namespace: e2e-tests-projected-bbk5c, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:04:54.391: INFO: namespace e2e-tests-projected-bbk5c deletion completed in 26.5284801s

• [SLOW TEST:38.361 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:04:54.391: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-cjjm
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:04:55.520: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cjjm" in namespace "e2e-tests-subpath-gzj6c" to be "success or failure"
Mar  2 23:04:55.539: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 18.624585ms
Mar  2 23:04:57.559: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039077649s
Mar  2 23:04:59.578: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058536484s
Mar  2 23:05:01.596: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076530521s
Mar  2 23:05:03.615: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094631347s
Mar  2 23:05:05.633: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 10.113472329s
Mar  2 23:05:07.652: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 12.131561438s
Mar  2 23:05:09.670: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 14.149998802s
Mar  2 23:05:11.689: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 16.168865726s
Mar  2 23:05:13.709: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 18.189075186s
Mar  2 23:05:15.728: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 20.208242738s
Mar  2 23:05:17.746: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 22.226545014s
Mar  2 23:05:19.764: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 24.244498413s
Mar  2 23:05:21.783: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 26.26266967s
Mar  2 23:05:23.802: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Running", Reason="", readiness=false. Elapsed: 28.282132393s
Mar  2 23:05:25.821: INFO: Pod "pod-subpath-test-secret-cjjm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.301303181s
STEP: Saw pod success
Mar  2 23:05:25.822: INFO: Pod "pod-subpath-test-secret-cjjm" satisfied condition "success or failure"
Mar  2 23:05:25.840: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-subpath-test-secret-cjjm container test-container-subpath-secret-cjjm: <nil>
STEP: delete the pod
Mar  2 23:05:25.904: INFO: Waiting for pod pod-subpath-test-secret-cjjm to disappear
Mar  2 23:05:25.923: INFO: Pod pod-subpath-test-secret-cjjm no longer exists
STEP: Deleting pod pod-subpath-test-secret-cjjm
Mar  2 23:05:25.923: INFO: Deleting pod "pod-subpath-test-secret-cjjm" in namespace "e2e-tests-subpath-gzj6c"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:05:25.941: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-gzj6c" for this suite.
Mar  2 23:05:32.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:05:33.449: INFO: namespace: e2e-tests-subpath-gzj6c, resource: packagemanifests, items remaining: 1
Mar  2 23:05:33.997: INFO: namespace: e2e-tests-subpath-gzj6c, resource: bindings, ignored listing per whitelist
Mar  2 23:05:34.478: INFO: namespace: e2e-tests-subpath-gzj6c no longer exists
Mar  2 23:05:34.496: INFO: namespace: e2e-tests-subpath-gzj6c, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:05:34.513: INFO: namespace e2e-tests-subpath-gzj6c deletion completed in 8.528021181s

• [SLOW TEST:40.122 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:05:34.514: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4mrbb
Mar  2 23:05:46.647: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4mrbb
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 23:05:46.665: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:09:46.893: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4mrbb" for this suite.
Mar  2 23:09:52.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:09:53.849: INFO: namespace: e2e-tests-container-probe-4mrbb, resource: bindings, ignored listing per whitelist
Mar  2 23:09:54.568: INFO: namespace: e2e-tests-container-probe-4mrbb, resource: packagemanifests, items remaining: 1
Mar  2 23:09:55.428: INFO: namespace: e2e-tests-container-probe-4mrbb no longer exists
Mar  2 23:09:55.448: INFO: namespace: e2e-tests-container-probe-4mrbb, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:09:55.465: INFO: namespace e2e-tests-container-probe-4mrbb deletion completed in 8.527039967s

• [SLOW TEST:260.952 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:09:55.465: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:09:56.557: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 23:10:01.575: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 23:10:05.611: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  2 23:10:17.757: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-g9lcq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g9lcq/deployments/test-cleanup-deployment,UID:50eec278-3d40-11e9-b620-0a843850fdce,ResourceVersion:56264,Generation:1,CreationTimestamp:2019-03-02 23:10:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-02 23:10:05 +0000 UTC 2019-03-02 23:10:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-02 23:10:15 +0000 UTC 2019-03-02 23:10:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-755f6b95cc" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 23:10:17.775: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-g9lcq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g9lcq/replicasets/test-cleanup-deployment-755f6b95cc,UID:50f21a29-3d40-11e9-8060-0e81de5d2554,ResourceVersion:56254,Generation:1,CreationTimestamp:2019-03-02 23:10:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 50eec278-3d40-11e9-b620-0a843850fdce 0xc4223ed127 0xc4223ed128}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  2 23:10:17.794: INFO: Pod "test-cleanup-deployment-755f6b95cc-jfpz5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-jfpz5,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-g9lcq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-g9lcq/pods/test-cleanup-deployment-755f6b95cc-jfpz5,UID:50f3a472-3d40-11e9-8060-0e81de5d2554,ResourceVersion:56253,Generation:0,CreationTimestamp:2019-03-02 23:10:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.40"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc 50f21a29-3d40-11e9-8060-0e81de5d2554 0xc4222da317 0xc4222da318}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s4n8f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s4n8f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-s4n8f true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-7wpw5}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222da3f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222da410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:10:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:10:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:10:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:10:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.40,StartTime:2019-03-02 23:10:05 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-02 23:10:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://7a5086dbe7f2401a4ac7d0452e9b4103b79168f36ba221750fe77f4a13ec2f9c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:10:17.794: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-g9lcq" for this suite.
Mar  2 23:10:25.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:10:26.686: INFO: namespace: e2e-tests-deployment-g9lcq, resource: packagemanifests, items remaining: 1
Mar  2 23:10:27.678: INFO: namespace: e2e-tests-deployment-g9lcq, resource: bindings, ignored listing per whitelist
Mar  2 23:10:28.335: INFO: namespace: e2e-tests-deployment-g9lcq no longer exists
Mar  2 23:10:28.354: INFO: namespace: e2e-tests-deployment-g9lcq, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:10:28.371: INFO: namespace e2e-tests-deployment-g9lcq deletion completed in 10.532587043s

• [SLOW TEST:32.906 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:10:28.371: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  2 23:10:29.440: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:10:40.947: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-vncnn" for this suite.
Mar  2 23:11:05.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:11:05.874: INFO: namespace: e2e-tests-init-container-vncnn, resource: packagemanifests, items remaining: 1
Mar  2 23:11:06.538: INFO: namespace: e2e-tests-init-container-vncnn, resource: bindings, ignored listing per whitelist
Mar  2 23:11:07.486: INFO: namespace: e2e-tests-init-container-vncnn no longer exists
Mar  2 23:11:07.505: INFO: namespace: e2e-tests-init-container-vncnn, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:11:07.522: INFO: namespace e2e-tests-init-container-vncnn deletion completed in 26.529686865s

• [SLOW TEST:39.150 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:11:07.522: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:11:08.617: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-76764bd1-3d40-11e9-9008-0a58ac10f353
STEP: Creating secret with name s-test-opt-upd-76764c51-3d40-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-76764bd1-3d40-11e9-9008-0a58ac10f353
STEP: Updating secret s-test-opt-upd-76764c51-3d40-11e9-9008-0a58ac10f353
STEP: Creating secret with name s-test-opt-create-76764c71-3d40-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:11:23.015: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tnr6x" for this suite.
Mar  2 23:11:47.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:11:48.331: INFO: namespace: e2e-tests-projected-tnr6x, resource: bindings, ignored listing per whitelist
Mar  2 23:11:48.997: INFO: namespace: e2e-tests-projected-tnr6x, resource: packagemanifests, items remaining: 1
Mar  2 23:11:49.550: INFO: namespace: e2e-tests-projected-tnr6x no longer exists
Mar  2 23:11:49.570: INFO: namespace: e2e-tests-projected-tnr6x, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:11:49.587: INFO: namespace e2e-tests-projected-tnr6x deletion completed in 26.527748664s

• [SLOW TEST:42.065 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:11:49.587: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Mar  2 23:11:50.648: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6lj9n'
Mar  2 23:11:51.069: INFO: stderr: ""
Mar  2 23:11:51.069: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar  2 23:11:52.091: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:52.091: INFO: Found 0 / 1
Mar  2 23:11:53.087: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:53.087: INFO: Found 0 / 1
Mar  2 23:11:54.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:54.088: INFO: Found 0 / 1
Mar  2 23:11:55.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:55.088: INFO: Found 0 / 1
Mar  2 23:11:56.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:56.088: INFO: Found 0 / 1
Mar  2 23:11:57.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:57.088: INFO: Found 0 / 1
Mar  2 23:11:58.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:58.088: INFO: Found 0 / 1
Mar  2 23:11:59.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:11:59.088: INFO: Found 0 / 1
Mar  2 23:12:00.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:12:00.088: INFO: Found 0 / 1
Mar  2 23:12:01.088: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:12:01.088: INFO: Found 1 / 1
Mar  2 23:12:01.088: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 23:12:01.106: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:12:01.106: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  2 23:12:01.106: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n'
Mar  2 23:12:01.329: INFO: stderr: ""
Mar  2 23:12:01.329: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Mar 23:11:59.546 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Mar 23:11:59.546 # Server started, Redis version 3.2.12\n1:M 02 Mar 23:11:59.546 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Mar 23:11:59.546 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  2 23:12:01.329: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n --tail=1'
Mar  2 23:12:01.545: INFO: stderr: ""
Mar  2 23:12:01.545: INFO: stdout: "1:M 02 Mar 23:11:59.546 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  2 23:12:01.545: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n --limit-bytes=1'
Mar  2 23:12:01.767: INFO: stderr: ""
Mar  2 23:12:01.768: INFO: stdout: " "
STEP: exposing timestamps
Mar  2 23:12:01.768: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n --tail=1 --timestamps'
Mar  2 23:12:02.020: INFO: stderr: ""
Mar  2 23:12:02.020: INFO: stdout: "2019-03-02T23:11:59.54762051Z 1:M 02 Mar 23:11:59.546 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  2 23:12:04.521: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n --since=1s'
Mar  2 23:12:04.732: INFO: stderr: ""
Mar  2 23:12:04.732: INFO: stdout: ""
Mar  2 23:12:04.732: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-mkb9d redis-master --namespace=e2e-tests-kubectl-6lj9n --since=24h'
Mar  2 23:12:04.934: INFO: stderr: ""
Mar  2 23:12:04.934: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Mar 23:11:59.546 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Mar 23:11:59.546 # Server started, Redis version 3.2.12\n1:M 02 Mar 23:11:59.546 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Mar 23:11:59.546 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Mar  2 23:12:04.934: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6lj9n'
Mar  2 23:12:05.132: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:12:05.132: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  2 23:12:05.132: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-6lj9n'
Mar  2 23:12:05.343: INFO: stderr: "No resources found.\n"
Mar  2 23:12:05.343: INFO: stdout: ""
Mar  2 23:12:05.343: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-6lj9n -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 23:12:05.514: INFO: stderr: ""
Mar  2 23:12:05.514: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:12:05.514: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6lj9n" for this suite.
Mar  2 23:12:11.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:12:12.791: INFO: namespace: e2e-tests-kubectl-6lj9n, resource: packagemanifests, items remaining: 1
Mar  2 23:12:13.453: INFO: namespace: e2e-tests-kubectl-6lj9n, resource: bindings, ignored listing per whitelist
Mar  2 23:12:14.048: INFO: namespace: e2e-tests-kubectl-6lj9n no longer exists
Mar  2 23:12:14.067: INFO: namespace: e2e-tests-kubectl-6lj9n, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:12:14.086: INFO: namespace e2e-tests-kubectl-6lj9n deletion completed in 8.52747012s

• [SLOW TEST:24.499 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:12:14.087: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9e1b2ab1-3d40-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:12:15.188: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-wbmjx" to be "success or failure"
Mar  2 23:12:15.206: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.706397ms
Mar  2 23:12:17.225: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036924312s
Mar  2 23:12:19.244: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056044463s
Mar  2 23:12:21.262: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074870031s
Mar  2 23:12:23.281: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09302988s
Mar  2 23:12:25.299: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110956408s
STEP: Saw pod success
Mar  2 23:12:25.299: INFO: Pod "pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:12:25.317: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:12:25.376: INFO: Waiting for pod pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:12:25.393: INFO: Pod pod-projected-configmaps-9e1efb4b-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:12:25.393: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wbmjx" for this suite.
Mar  2 23:12:31.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:12:32.794: INFO: namespace: e2e-tests-projected-wbmjx, resource: bindings, ignored listing per whitelist
Mar  2 23:12:33.457: INFO: namespace: e2e-tests-projected-wbmjx, resource: packagemanifests, items remaining: 1
Mar  2 23:12:33.928: INFO: namespace: e2e-tests-projected-wbmjx no longer exists
Mar  2 23:12:33.947: INFO: namespace: e2e-tests-projected-wbmjx, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:12:33.964: INFO: namespace e2e-tests-projected-wbmjx deletion completed in 8.525553561s

• [SLOW TEST:19.878 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:12:33.965: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a9f3f3d2-3d40-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:12:35.069: INFO: Waiting up to 5m0s for pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-lbdpj" to be "success or failure"
Mar  2 23:12:35.087: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.001899ms
Mar  2 23:12:37.105: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036228045s
Mar  2 23:12:39.123: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054578942s
Mar  2 23:12:41.142: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072892683s
Mar  2 23:12:43.160: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091184483s
Mar  2 23:12:45.179: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109985895s
STEP: Saw pod success
Mar  2 23:12:45.179: INFO: Pod "pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:12:45.197: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:12:45.248: INFO: Waiting for pod pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:12:45.265: INFO: Pod pod-secrets-a9f8631c-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:12:45.265: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lbdpj" for this suite.
Mar  2 23:12:51.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:12:53.362: INFO: namespace: e2e-tests-secrets-lbdpj, resource: bindings, ignored listing per whitelist
Mar  2 23:12:53.651: INFO: namespace: e2e-tests-secrets-lbdpj, resource: packagemanifests, items remaining: 1
Mar  2 23:12:53.816: INFO: namespace: e2e-tests-secrets-lbdpj no longer exists
Mar  2 23:12:53.834: INFO: namespace: e2e-tests-secrets-lbdpj, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:12:53.852: INFO: namespace e2e-tests-secrets-lbdpj deletion completed in 8.540757847s

• [SLOW TEST:19.887 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:12:53.852: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 23:12:54.947: INFO: Waiting up to 5m0s for pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-dk9ns" to be "success or failure"
Mar  2 23:12:54.968: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 21.278528ms
Mar  2 23:12:56.986: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039279619s
Mar  2 23:12:59.004: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057204701s
Mar  2 23:13:01.022: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075319205s
Mar  2 23:13:03.041: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093561497s
Mar  2 23:13:05.059: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111525854s
Mar  2 23:13:07.077: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.129799204s
STEP: Saw pod success
Mar  2 23:13:07.077: INFO: Pod "pod-b5d0338c-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:13:07.097: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-b5d0338c-3d40-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:13:07.149: INFO: Waiting for pod pod-b5d0338c-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:13:07.167: INFO: Pod pod-b5d0338c-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:13:07.167: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dk9ns" for this suite.
Mar  2 23:13:13.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:13:14.522: INFO: namespace: e2e-tests-emptydir-dk9ns, resource: packagemanifests, items remaining: 1
Mar  2 23:13:14.662: INFO: namespace: e2e-tests-emptydir-dk9ns, resource: bindings, ignored listing per whitelist
Mar  2 23:13:15.702: INFO: namespace: e2e-tests-emptydir-dk9ns no longer exists
Mar  2 23:13:15.720: INFO: namespace: e2e-tests-emptydir-dk9ns, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:13:15.738: INFO: namespace e2e-tests-emptydir-dk9ns deletion completed in 8.526139237s

• [SLOW TEST:21.886 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:13:15.738: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:13:16.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-fsfls" to be "success or failure"
Mar  2 23:13:16.864: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.217362ms
Mar  2 23:13:18.883: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03767439s
Mar  2 23:13:20.901: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055588836s
Mar  2 23:13:22.921: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076053339s
Mar  2 23:13:24.939: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094376283s
Mar  2 23:13:26.958: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112445125s
STEP: Saw pod success
Mar  2 23:13:26.958: INFO: Pod "downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:13:26.975: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:13:27.030: INFO: Waiting for pod downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:13:27.047: INFO: Pod downwardapi-volume-c2ded6a5-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:13:27.047: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fsfls" for this suite.
Mar  2 23:13:33.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:13:34.142: INFO: namespace: e2e-tests-downward-api-fsfls, resource: packagemanifests, items remaining: 1
Mar  2 23:13:34.231: INFO: namespace: e2e-tests-downward-api-fsfls, resource: bindings, ignored listing per whitelist
Mar  2 23:13:35.581: INFO: namespace: e2e-tests-downward-api-fsfls no longer exists
Mar  2 23:13:35.599: INFO: namespace: e2e-tests-downward-api-fsfls, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:13:35.616: INFO: namespace e2e-tests-downward-api-fsfls deletion completed in 8.524053734s

• [SLOW TEST:19.878 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:13:35.616: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-ceb2f238-3d40-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:13:36.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-wcm4p" to be "success or failure"
Mar  2 23:13:36.739: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 23.173592ms
Mar  2 23:13:38.757: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041910212s
Mar  2 23:13:40.775: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059828547s
Mar  2 23:13:42.793: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07778641s
Mar  2 23:13:44.812: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096556613s
Mar  2 23:13:46.831: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.115598268s
STEP: Saw pod success
Mar  2 23:13:46.831: INFO: Pod "pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:13:46.850: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:13:46.904: INFO: Waiting for pod pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:13:46.924: INFO: Pod pod-projected-secrets-ceb702aa-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:13:46.924: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wcm4p" for this suite.
Mar  2 23:13:53.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:13:53.791: INFO: namespace: e2e-tests-projected-wcm4p, resource: packagemanifests, items remaining: 1
Mar  2 23:13:54.498: INFO: namespace: e2e-tests-projected-wcm4p, resource: bindings, ignored listing per whitelist
Mar  2 23:13:55.461: INFO: namespace: e2e-tests-projected-wcm4p no longer exists
Mar  2 23:13:55.479: INFO: namespace: e2e-tests-projected-wcm4p, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:13:55.501: INFO: namespace e2e-tests-projected-wcm4p deletion completed in 8.532123565s

• [SLOW TEST:19.885 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:13:55.501: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:13:56.546: INFO: Creating deployment "nginx-deployment"
Mar  2 23:13:56.571: INFO: Waiting for observed generation 1
Mar  2 23:13:58.607: INFO: Waiting for all required pods to come up
Mar  2 23:13:58.639: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  2 23:14:06.691: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  2 23:14:06.730: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  2 23:14:06.784: INFO: Updating deployment nginx-deployment
Mar  2 23:14:06.784: INFO: Waiting for observed generation 2
Mar  2 23:14:08.821: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 23:14:08.839: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 23:14:08.856: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  2 23:14:08.908: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 23:14:08.908: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 23:14:08.927: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  2 23:14:08.961: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  2 23:14:08.961: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  2 23:14:08.998: INFO: Updating deployment nginx-deployment
Mar  2 23:14:08.998: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  2 23:14:09.035: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 23:14:11.075: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  2 23:14:11.110: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-kljn2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kljn2/deployments/nginx-deployment,UID:da8f64bd-3d40-11e9-b620-0a843850fdce,ResourceVersion:59746,Generation:3,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-03-02 23:14:09 +0000 UTC 2019-03-02 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-02 23:14:09 +0000 UTC 2019-03-02 23:13:56 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 23:14:11.127: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-kljn2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kljn2/replicasets/nginx-deployment-7dc8f79789,UID:e0a85150-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59745,Generation:3,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment da8f64bd-3d40-11e9-b620-0a843850fdce 0xc42224c917 0xc42224c918}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  2 23:14:11.127: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  2 23:14:11.127: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-kljn2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kljn2/replicasets/nginx-deployment-7f9675fb8b,UID:da9234a6-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59688,Generation:3,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment da8f64bd-3d40-11e9-b620-0a843850fdce 0xc42224c9d7 0xc42224c9d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-29zpn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-29zpn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-29zpn,UID:e217f189-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59759,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc422043ba7 0xc422043ba8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422043c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422043c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-5knsd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-5knsd,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-5knsd,UID:e0b66431-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59600,Generation:0,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc422043e00 0xc422043e01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422043f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422043f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-9czzt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-9czzt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-9czzt,UID:e0c5ec49-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59615,Generation:0,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc01c0 0xc421dc01c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc0490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc04b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-bxkbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-bxkbd,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-bxkbd,UID:e20b9de9-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59744,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc05a0 0xc421dc05a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc0650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc0670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-c4hq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-c4hq8,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-c4hq8,UID:e0bf4a5f-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59606,Generation:0,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc0ee0 0xc421dc0ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc1130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc1150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-gxl9v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gxl9v,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-gxl9v,UID:e2173e9a-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59758,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc12c0 0xc421dc12c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc1470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc1520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.174: INFO: Pod "nginx-deployment-7dc8f79789-hdrck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-hdrck,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-hdrck,UID:e229d9a1-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59766,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc1790 0xc421dc1791}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc1800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc1820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-q8nrt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-q8nrt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-q8nrt,UID:e2174202-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59728,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc1ad0 0xc421dc1ad1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc1b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc1bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-qlw9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qlw9h,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-qlw9h,UID:e0b55b30-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59601,Generation:0,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc1cd0 0xc421dc1cd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421dc1de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421dc1e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-rpcqt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-rpcqt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-rpcqt,UID:e20b9d52-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59742,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc421dc1f90 0xc421dc1f91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff23f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff2420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-s98bd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-s98bd,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-s98bd,UID:e218aab9-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59732,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc420ff2560 0xc420ff2561}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff26f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff2710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-sdpnk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-sdpnk,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-sdpnk,UID:e1feb737-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59683,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc420ff2a20 0xc420ff2a21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff2a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff2ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7dc8f79789-vnvcf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-vnvcf,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7dc8f79789-vnvcf,UID:e0ad5db2-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59589,Generation:0,CreationTimestamp:2019-03-02 23:14:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 e0a85150-3d40-11e9-8060-0e81de5d2554 0xc420ff2c70 0xc420ff2c71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff2ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff2d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7f9675fb8b-44z9k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-44z9k,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-44z9k,UID:e208b86c-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59704,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff2f30 0xc420ff2f31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff2f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff2fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.175: INFO: Pod "nginx-deployment-7f9675fb8b-55zpl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-55zpl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-55zpl,UID:e1fad81b-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59656,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff31a7 0xc420ff31a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff3320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff3450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-62n9w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-62n9w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-62n9w,UID:da9d7c2f-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59536,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.131.0.19"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3637 0xc420ff3638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff3740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff3760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:10.131.0.19,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://939d47576638379668bd0280fc531796779bc51e70aa1b277872c4438460e215}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-7gqvg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7gqvg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-7gqvg,UID:da994467-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59520,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.45"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3840 0xc420ff3841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff38a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff38c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.45,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://14da39d2e2feaea660dae986efa63e238a4ea7a91fedea88cd71dc05d4a62f82}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-8mk6r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8mk6r,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-8mk6r,UID:e2077a03-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59708,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3a20 0xc420ff3a21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff3ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff3bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-dgrdl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-dgrdl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-dgrdl,UID:e208b2c2-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59709,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3c90 0xc420ff3c91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff3cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff3d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-dwzpn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-dwzpn,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-dwzpn,UID:e2024ab9-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59681,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3e10 0xc420ff3e11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420ff3e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420ff3e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-dxfrr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-dxfrr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-dxfrr,UID:da9d4bbf-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59542,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.128.2.49"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420ff3ff0 0xc420ff3ff1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:10.128.2.49,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://9d46fda3a6359a5d04aead2fd1f93313db3eb93ebc72fad961c13db619a8f164}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-h6rrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-h6rrv,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-h6rrv,UID:e2020913-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59697,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6260 0xc421cd6261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd62d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd62f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-l5pwz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-l5pwz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-l5pwz,UID:e2025a52-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59698,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6470 0xc421cd6471}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd64e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-m55kf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-m55kf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-m55kf,UID:daa2081d-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59488,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.131.0.20"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6920 0xc421cd6921}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd69a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:10.131.0.20,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://2ff1af45c5ceaf55decccbf41537a3edeaa844bcf78e8d9afc61d701e0904ff6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.176: INFO: Pod "nginx-deployment-7f9675fb8b-nmdvh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-nmdvh,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-nmdvh,UID:da9d5479-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59540,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.128.2.48"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6a80 0xc421cd6a81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:10.128.2.48,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:05 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://c9a7203c138eb286a4b6aaa5ebdc73c749bf3809a15f4020ca4cd09dfd2c38f7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-qcmfh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-qcmfh,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-qcmfh,UID:e1fe1a21-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59659,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6c30 0xc421cd6c31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-qfd2f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-qfd2f,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-qfd2f,UID:da9cd671-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59556,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.46"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6d80 0xc421cd6d81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.46,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:05 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://b18631a3a9a043ad3239910b4426dab71ab4e23f1ffb73a4127271e9d257a0e0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-svgdh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-svgdh,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-svgdh,UID:e1feba82-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59670,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd6ee0 0xc421cd6ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd6f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd6f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-t9m7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-t9m7p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-t9m7p,UID:e208af6f-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59714,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd7107 0xc421cd7108}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd7170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd71a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-tc7jv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-tc7jv,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-tc7jv,UID:e20205e8-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59680,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd7320 0xc421cd7321}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd7380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd73a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-wtcmw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wtcmw,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-wtcmw,UID:da961f2a-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59506,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.128.2.47"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc421cd7767 0xc421cd7768}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-148-56.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cd7c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cd7c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.148.56,PodIP:10.128.2.47,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://5c7d4d50ddd9c45746c769e4734fac1ef53f0eefc26479ffd98039b5b7f7a320}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-wtmq6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wtmq6,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-wtmq6,UID:e2086571-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59712,Generation:0,CreationTimestamp:2019-03-02 23:14:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc4205001c0 0xc4205001c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4205005c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4205005e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:09 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:,StartTime:2019-03-02 23:14:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  2 23:14:11.177: INFO: Pod "nginx-deployment-7f9675fb8b-zj57z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-zj57z,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-kljn2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kljn2/pods/nginx-deployment-7f9675fb8b-zj57z,UID:da9971f6-3d40-11e9-8060-0e81de5d2554,ResourceVersion:59491,Generation:0,CreationTimestamp:2019-03-02 23:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.131.0.18"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b da9234a6-3d40-11e9-8060-0e81de5d2554 0xc420500b17 0xc420500b18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-486w5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-486w5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-486w5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-63.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-dr5dz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420501100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420501150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:14:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:13:56 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.63,PodIP:10.131.0.18,StartTime:2019-03-02 23:13:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-02 23:14:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df cri-o://98cba8fc02dd25611096ccffa85d03b819b62e04dd4bb4b48156654a89823a3b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:14:11.177: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-kljn2" for this suite.
Mar  2 23:14:19.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:14:20.246: INFO: namespace: e2e-tests-deployment-kljn2, resource: packagemanifests, items remaining: 1
Mar  2 23:14:20.600: INFO: namespace: e2e-tests-deployment-kljn2, resource: bindings, ignored listing per whitelist
Mar  2 23:14:21.698: INFO: namespace: e2e-tests-deployment-kljn2 no longer exists
Mar  2 23:14:21.717: INFO: namespace: e2e-tests-deployment-kljn2, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:14:21.734: INFO: namespace e2e-tests-deployment-kljn2 deletion completed in 10.524930731s

• [SLOW TEST:26.233 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:14:21.734: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  2 23:14:23.884: INFO: Waiting up to 5m0s for pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-jb6tp" to be "success or failure"
Mar  2 23:14:23.904: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.438271ms
Mar  2 23:14:25.922: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037229789s
Mar  2 23:14:27.941: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056679523s
Mar  2 23:14:29.960: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075103566s
Mar  2 23:14:31.978: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093158665s
Mar  2 23:14:33.996: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111834848s
STEP: Saw pod success
Mar  2 23:14:33.997: INFO: Pod "downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:14:34.015: INFO: Trying to get logs from node ip-10-0-143-63.ec2.internal pod downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 23:14:34.070: INFO: Waiting for pod downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:14:34.088: INFO: Pod downward-api-ea38bbb3-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:14:34.088: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jb6tp" for this suite.
Mar  2 23:14:40.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:14:41.400: INFO: namespace: e2e-tests-downward-api-jb6tp, resource: bindings, ignored listing per whitelist
Mar  2 23:14:41.584: INFO: namespace: e2e-tests-downward-api-jb6tp, resource: packagemanifests, items remaining: 1
Mar  2 23:14:42.623: INFO: namespace: e2e-tests-downward-api-jb6tp no longer exists
Mar  2 23:14:42.642: INFO: namespace: e2e-tests-downward-api-jb6tp, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:14:42.659: INFO: namespace e2e-tests-downward-api-jb6tp deletion completed in 8.526213697s

• [SLOW TEST:20.925 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:14:42.659: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:14:43.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-b9v2m" to be "success or failure"
Mar  2 23:14:43.755: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.378625ms
Mar  2 23:14:45.773: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038412657s
Mar  2 23:14:47.792: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056565026s
Mar  2 23:14:49.810: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07475781s
Mar  2 23:14:51.832: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096542299s
Mar  2 23:14:53.852: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.117314203s
STEP: Saw pod success
Mar  2 23:14:53.852: INFO: Pod "downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:14:53.870: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:14:53.924: INFO: Waiting for pod downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:14:53.941: INFO: Pod downwardapi-volume-f6a92c1e-3d40-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:14:53.941: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b9v2m" for this suite.
Mar  2 23:15:00.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:15:01.825: INFO: namespace: e2e-tests-projected-b9v2m, resource: bindings, ignored listing per whitelist
Mar  2 23:15:02.128: INFO: namespace: e2e-tests-projected-b9v2m, resource: packagemanifests, items remaining: 1
Mar  2 23:15:02.480: INFO: namespace: e2e-tests-projected-b9v2m no longer exists
Mar  2 23:15:02.498: INFO: namespace: e2e-tests-projected-b9v2m, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:15:02.516: INFO: namespace e2e-tests-projected-b9v2m deletion completed in 8.529166889s

• [SLOW TEST:19.857 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:15:02.516: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 23:15:03.581: INFO: Waiting up to 5m0s for pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-blqg6" to be "success or failure"
Mar  2 23:15:03.599: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.171036ms
Mar  2 23:15:05.618: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037426381s
Mar  2 23:15:07.637: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055477713s
Mar  2 23:15:09.654: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073365834s
Mar  2 23:15:11.672: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091370583s
Mar  2 23:15:13.691: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109627134s
STEP: Saw pod success
Mar  2 23:15:13.691: INFO: Pod "pod-027cd7ac-3d41-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:15:13.709: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-027cd7ac-3d41-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:15:13.765: INFO: Waiting for pod pod-027cd7ac-3d41-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:15:13.784: INFO: Pod pod-027cd7ac-3d41-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:15:13.784: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-blqg6" for this suite.
Mar  2 23:15:19.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:15:21.089: INFO: namespace: e2e-tests-emptydir-blqg6, resource: bindings, ignored listing per whitelist
Mar  2 23:15:21.653: INFO: namespace: e2e-tests-emptydir-blqg6, resource: packagemanifests, items remaining: 1
Mar  2 23:15:22.325: INFO: namespace: e2e-tests-emptydir-blqg6 no longer exists
Mar  2 23:15:22.343: INFO: namespace: e2e-tests-emptydir-blqg6, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:15:22.360: INFO: namespace e2e-tests-emptydir-blqg6 deletion completed in 8.531449927s

• [SLOW TEST:19.845 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:15:22.361: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar  2 23:15:23.444: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:15:23.644: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q5vhl" for this suite.
Mar  2 23:15:29.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:15:30.481: INFO: namespace: e2e-tests-kubectl-q5vhl, resource: bindings, ignored listing per whitelist
Mar  2 23:15:31.107: INFO: namespace: e2e-tests-kubectl-q5vhl, resource: packagemanifests, items remaining: 1
Mar  2 23:15:32.167: INFO: namespace: e2e-tests-kubectl-q5vhl no longer exists
Mar  2 23:15:32.185: INFO: namespace: e2e-tests-kubectl-q5vhl, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:15:32.203: INFO: namespace e2e-tests-kubectl-q5vhl deletion completed in 8.527263222s

• [SLOW TEST:9.842 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:15:32.203: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Mar  2 23:15:33.278: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:34.761: INFO: stderr: ""
Mar  2 23:15:34.761: INFO: stdout: "pod/pause created\n"
Mar  2 23:15:34.761: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 23:15:34.762: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-k5csr" to be "running and ready"
Mar  2 23:15:34.779: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.573838ms
Mar  2 23:15:36.797: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035372769s
Mar  2 23:15:38.816: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054137504s
Mar  2 23:15:40.834: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072228969s
Mar  2 23:15:42.851: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089942719s
Mar  2 23:15:44.870: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.108453035s
Mar  2 23:15:44.870: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 23:15:44.870: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  2 23:15:44.870: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:45.081: INFO: stderr: ""
Mar  2 23:15:45.081: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 23:15:45.081: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:45.252: INFO: stderr: ""
Mar  2 23:15:45.252: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  2 23:15:45.252: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:45.457: INFO: stderr: ""
Mar  2 23:15:45.457: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  2 23:15:45.457: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:45.631: INFO: stderr: ""
Mar  2 23:15:45.631: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Mar  2 23:15:45.631: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:45.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:15:45.827: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 23:15:45.827: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-k5csr'
Mar  2 23:15:46.021: INFO: stderr: "No resources found.\n"
Mar  2 23:15:46.021: INFO: stdout: ""
Mar  2 23:15:46.021: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-k5csr -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 23:15:46.191: INFO: stderr: ""
Mar  2 23:15:46.191: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:15:46.191: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k5csr" for this suite.
Mar  2 23:15:52.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:15:53.750: INFO: namespace: e2e-tests-kubectl-k5csr, resource: packagemanifests, items remaining: 1
Mar  2 23:15:53.973: INFO: namespace: e2e-tests-kubectl-k5csr, resource: bindings, ignored listing per whitelist
Mar  2 23:15:54.726: INFO: namespace: e2e-tests-kubectl-k5csr no longer exists
Mar  2 23:15:54.745: INFO: namespace: e2e-tests-kubectl-k5csr, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:15:54.762: INFO: namespace e2e-tests-kubectl-k5csr deletion completed in 8.526412804s

• [SLOW TEST:22.559 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:15:54.762: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:15:55.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-ghsnd" to be "success or failure"
Mar  2 23:15:55.873: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.735468ms
Mar  2 23:15:57.892: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036708327s
Mar  2 23:15:59.910: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055217033s
Mar  2 23:16:01.929: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074061658s
Mar  2 23:16:03.948: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092654661s
Mar  2 23:16:05.966: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110969071s
STEP: Saw pod success
Mar  2 23:16:05.966: INFO: Pod "downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:16:05.983: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:16:06.043: INFO: Waiting for pod downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:16:06.060: INFO: Pod downwardapi-volume-21a39c5d-3d41-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:16:06.060: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ghsnd" for this suite.
Mar  2 23:16:12.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:16:13.087: INFO: namespace: e2e-tests-downward-api-ghsnd, resource: packagemanifests, items remaining: 1
Mar  2 23:16:13.742: INFO: namespace: e2e-tests-downward-api-ghsnd, resource: bindings, ignored listing per whitelist
Mar  2 23:16:14.597: INFO: namespace: e2e-tests-downward-api-ghsnd no longer exists
Mar  2 23:16:14.615: INFO: namespace: e2e-tests-downward-api-ghsnd, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:16:14.632: INFO: namespace e2e-tests-downward-api-ghsnd deletion completed in 8.527299655s

• [SLOW TEST:19.870 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:16:14.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rzzzt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 23:16:15.661: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 23:16:54.129: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.60:8080/dial?request=hostName&protocol=http&host=10.131.0.29&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rzzzt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:16:54.129: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:16:54.395: INFO: Waiting for endpoints: map[]
Mar  2 23:16:54.413: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.60:8080/dial?request=hostName&protocol=http&host=10.129.2.60&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rzzzt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:16:54.413: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:16:54.632: INFO: Waiting for endpoints: map[]
Mar  2 23:16:54.650: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.60:8080/dial?request=hostName&protocol=http&host=10.128.2.59&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rzzzt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:16:54.650: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:16:54.847: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:16:54.847: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rzzzt" for this suite.
Mar  2 23:17:18.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:17:20.374: INFO: namespace: e2e-tests-pod-network-test-rzzzt, resource: bindings, ignored listing per whitelist
Mar  2 23:17:20.576: INFO: namespace: e2e-tests-pod-network-test-rzzzt, resource: packagemanifests, items remaining: 1
Mar  2 23:17:21.407: INFO: namespace: e2e-tests-pod-network-test-rzzzt no longer exists
Mar  2 23:17:21.436: INFO: namespace: e2e-tests-pod-network-test-rzzzt, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:17:21.465: INFO: namespace e2e-tests-pod-network-test-rzzzt deletion completed in 26.572962233s

• [SLOW TEST:66.832 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:17:21.465: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5smxn in namespace e2e-tests-proxy-kkgh6
I0302 23:17:22.551313   27430 runners.go:180] Created replication controller with name: proxy-service-5smxn, namespace: e2e-tests-proxy-kkgh6, replica count: 1
I0302 23:17:23.601893   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:24.602184   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:25.602466   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:26.602739   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:27.603071   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:28.603361   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:29.603708   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:30.603943   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:31.604213   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:32.604560   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 23:17:33.604853   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 23:17:34.605167   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 23:17:35.605519   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 23:17:36.605816   27430 runners.go:180] proxy-service-5smxn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 23:17:36.624: INFO: setup took 14.127050126s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 23:17:36.645: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.837005ms)
Mar  2 23:17:36.645: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.78785ms)
Mar  2 23:17:36.645: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.04815ms)
Mar  2 23:17:36.645: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.275279ms)
Mar  2 23:17:36.645: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 21.294744ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 33.993391ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 34.04527ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 34.164744ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 34.026152ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 34.21881ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 34.024694ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 34.009292ms)
Mar  2 23:17:36.658: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 34.200468ms)
Mar  2 23:17:36.663: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 38.931245ms)
Mar  2 23:17:36.663: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 38.844502ms)
Mar  2 23:17:36.664: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 39.834603ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.52874ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.57283ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 21.747432ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.577726ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 21.668123ms)
Mar  2 23:17:36.686: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.909753ms)
Mar  2 23:17:36.689: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 25.061212ms)
Mar  2 23:17:36.690: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 25.967318ms)
Mar  2 23:17:36.691: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 26.452792ms)
Mar  2 23:17:36.691: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 26.642435ms)
Mar  2 23:17:36.691: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 26.551309ms)
Mar  2 23:17:36.692: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 28.188809ms)
Mar  2 23:17:36.693: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 28.297624ms)
Mar  2 23:17:36.693: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 28.583659ms)
Mar  2 23:17:36.699: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 34.368125ms)
Mar  2 23:17:36.699: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 34.366423ms)
Mar  2 23:17:36.717: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 18.207971ms)
Mar  2 23:17:36.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.571496ms)
Mar  2 23:17:36.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 21.724556ms)
Mar  2 23:17:36.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.94553ms)
Mar  2 23:17:36.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.93699ms)
Mar  2 23:17:36.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 21.691219ms)
Mar  2 23:17:36.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 23.92821ms)
Mar  2 23:17:36.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 24.1058ms)
Mar  2 23:17:36.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 24.085616ms)
Mar  2 23:17:36.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 24.091752ms)
Mar  2 23:17:36.725: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 26.03082ms)
Mar  2 23:17:36.730: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.085342ms)
Mar  2 23:17:36.730: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.142965ms)
Mar  2 23:17:36.730: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.110583ms)
Mar  2 23:17:36.730: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.13097ms)
Mar  2 23:17:36.730: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.265182ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.325231ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 22.417418ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 22.604962ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.672581ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 22.757843ms)
Mar  2 23:17:36.753: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.743655ms)
Mar  2 23:17:36.784: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 53.802531ms)
Mar  2 23:17:36.784: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 54.035653ms)
Mar  2 23:17:36.784: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 53.812373ms)
Mar  2 23:17:36.784: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 53.87239ms)
Mar  2 23:17:36.784: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 53.97658ms)
Mar  2 23:17:36.785: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 54.038127ms)
Mar  2 23:17:36.785: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 54.005746ms)
Mar  2 23:17:36.785: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 54.568234ms)
Mar  2 23:17:36.785: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 54.440912ms)
Mar  2 23:17:36.785: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 54.490974ms)
Mar  2 23:17:36.804: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 19.059962ms)
Mar  2 23:17:36.805: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 19.691481ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 20.304554ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.27045ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.265849ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.289255ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 20.481588ms)
Mar  2 23:17:36.806: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.576212ms)
Mar  2 23:17:36.808: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.933458ms)
Mar  2 23:17:36.808: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 22.884939ms)
Mar  2 23:17:36.810: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 24.888949ms)
Mar  2 23:17:36.810: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 24.957262ms)
Mar  2 23:17:36.817: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.94119ms)
Mar  2 23:17:36.817: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.902596ms)
Mar  2 23:17:36.817: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.994652ms)
Mar  2 23:17:36.817: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 32.087627ms)
Mar  2 23:17:36.836: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 18.521653ms)
Mar  2 23:17:36.838: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.418198ms)
Mar  2 23:17:36.838: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.766921ms)
Mar  2 23:17:36.838: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.764466ms)
Mar  2 23:17:36.838: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 20.655934ms)
Mar  2 23:17:36.839: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.966175ms)
Mar  2 23:17:36.839: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 20.995756ms)
Mar  2 23:17:36.839: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 21.440895ms)
Mar  2 23:17:36.839: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.267188ms)
Mar  2 23:17:36.840: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 21.941576ms)
Mar  2 23:17:36.842: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 24.102652ms)
Mar  2 23:17:36.842: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 24.181591ms)
Mar  2 23:17:36.849: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.684093ms)
Mar  2 23:17:36.849: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.657076ms)
Mar  2 23:17:36.849: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.87396ms)
Mar  2 23:17:36.849: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 31.729332ms)
Mar  2 23:17:36.869: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 18.968166ms)
Mar  2 23:17:36.869: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 19.181659ms)
Mar  2 23:17:36.871: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.597044ms)
Mar  2 23:17:36.871: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.446609ms)
Mar  2 23:17:36.871: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.47774ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 21.928083ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 22.034188ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 22.464367ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.493689ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 22.488404ms)
Mar  2 23:17:36.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.766585ms)
Mar  2 23:17:36.874: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 24.050067ms)
Mar  2 23:17:36.882: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 32.05111ms)
Mar  2 23:17:36.882: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 32.63143ms)
Mar  2 23:17:36.883: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 32.993611ms)
Mar  2 23:17:36.883: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 32.919929ms)
Mar  2 23:17:36.901: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 18.446061ms)
Mar  2 23:17:36.904: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.163821ms)
Mar  2 23:17:36.904: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.722075ms)
Mar  2 23:17:36.904: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.365062ms)
Mar  2 23:17:36.904: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.328523ms)
Mar  2 23:17:36.904: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.68091ms)
Mar  2 23:17:36.905: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 22.078012ms)
Mar  2 23:17:36.906: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.147949ms)
Mar  2 23:17:36.906: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 22.510196ms)
Mar  2 23:17:36.906: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 22.598584ms)
Mar  2 23:17:36.907: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 23.615065ms)
Mar  2 23:17:36.907: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 23.953926ms)
Mar  2 23:17:36.914: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 30.523506ms)
Mar  2 23:17:36.914: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.25409ms)
Mar  2 23:17:36.914: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 30.518247ms)
Mar  2 23:17:36.914: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 30.867293ms)
Mar  2 23:17:36.935: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.987143ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.184239ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.035668ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 21.103769ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.0943ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.248483ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.226492ms)
Mar  2 23:17:36.936: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 21.253425ms)
Mar  2 23:17:36.937: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 22.356079ms)
Mar  2 23:17:36.937: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.289044ms)
Mar  2 23:17:36.937: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 22.360692ms)
Mar  2 23:17:36.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 34.464974ms)
Mar  2 23:17:36.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 34.445146ms)
Mar  2 23:17:36.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 34.498275ms)
Mar  2 23:17:36.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 34.502572ms)
Mar  2 23:17:36.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 34.611224ms)
Mar  2 23:17:36.968: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 18.353923ms)
Mar  2 23:17:36.969: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.163353ms)
Mar  2 23:17:36.970: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.221481ms)
Mar  2 23:17:36.970: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.516278ms)
Mar  2 23:17:36.970: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.578612ms)
Mar  2 23:17:36.971: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.974405ms)
Mar  2 23:17:36.971: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.027013ms)
Mar  2 23:17:36.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 22.384854ms)
Mar  2 23:17:36.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 22.435079ms)
Mar  2 23:17:36.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 23.79945ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 31.288993ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 31.41785ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.4659ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.421191ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.417633ms)
Mar  2 23:17:36.981: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.639012ms)
Mar  2 23:17:36.999: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 18.141306ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.135507ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 19.985895ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 20.295429ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.332537ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.329959ms)
Mar  2 23:17:37.001: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 20.387602ms)
Mar  2 23:17:37.003: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.894386ms)
Mar  2 23:17:37.003: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.830204ms)
Mar  2 23:17:37.003: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.844179ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.042757ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.209666ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.264262ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 31.180858ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.121472ms)
Mar  2 23:17:37.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.257735ms)
Mar  2 23:17:37.032: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 19.661896ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.261323ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.261535ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.397904ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 21.80807ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.68026ms)
Mar  2 23:17:37.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.784665ms)
Mar  2 23:17:37.035: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.883989ms)
Mar  2 23:17:37.038: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 24.977615ms)
Mar  2 23:17:37.038: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 24.946196ms)
Mar  2 23:17:37.040: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 26.906119ms)
Mar  2 23:17:37.045: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 32.470073ms)
Mar  2 23:17:37.045: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 32.531927ms)
Mar  2 23:17:37.045: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 32.686295ms)
Mar  2 23:17:37.045: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 32.466962ms)
Mar  2 23:17:37.045: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 32.581704ms)
Mar  2 23:17:37.064: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 19.191132ms)
Mar  2 23:17:37.066: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.992815ms)
Mar  2 23:17:37.067: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 21.446434ms)
Mar  2 23:17:37.067: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.444865ms)
Mar  2 23:17:37.067: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.616216ms)
Mar  2 23:17:37.067: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 22.017466ms)
Mar  2 23:17:37.067: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.959355ms)
Mar  2 23:17:37.068: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.262146ms)
Mar  2 23:17:37.068: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.214718ms)
Mar  2 23:17:37.068: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 22.252505ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.880358ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 32.090248ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 32.191028ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 32.065363ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.976166ms)
Mar  2 23:17:37.077: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 32.038532ms)
Mar  2 23:17:37.096: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 18.793072ms)
Mar  2 23:17:37.099: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.528926ms)
Mar  2 23:17:37.099: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.567018ms)
Mar  2 23:17:37.099: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.665825ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.91796ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 22.41277ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 22.261498ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.332785ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 22.452237ms)
Mar  2 23:17:37.100: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.610554ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.69861ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.617923ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.73611ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 31.873086ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.69398ms)
Mar  2 23:17:37.109: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.757488ms)
Mar  2 23:17:37.131: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 20.878004ms)
Mar  2 23:17:37.131: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 20.880623ms)
Mar  2 23:17:37.131: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 20.890463ms)
Mar  2 23:17:37.131: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.148931ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 24.125003ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 24.009134ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 24.050676ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 24.163354ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 24.701373ms)
Mar  2 23:17:37.134: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 24.770157ms)
Mar  2 23:17:37.135: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 25.012279ms)
Mar  2 23:17:37.144: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 34.079621ms)
Mar  2 23:17:37.144: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 34.029672ms)
Mar  2 23:17:37.144: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 34.077322ms)
Mar  2 23:17:37.144: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 34.208267ms)
Mar  2 23:17:37.144: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 34.186096ms)
Mar  2 23:17:37.163: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 19.373889ms)
Mar  2 23:17:37.166: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.74083ms)
Mar  2 23:17:37.166: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.45015ms)
Mar  2 23:17:37.168: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 24.08507ms)
Mar  2 23:17:37.168: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 24.130235ms)
Mar  2 23:17:37.168: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 24.178825ms)
Mar  2 23:17:37.169: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 24.911397ms)
Mar  2 23:17:37.169: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 24.955743ms)
Mar  2 23:17:37.169: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 25.253902ms)
Mar  2 23:17:37.169: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 25.355261ms)
Mar  2 23:17:37.170: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 25.981973ms)
Mar  2 23:17:37.187: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 42.643447ms)
Mar  2 23:17:37.187: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 42.622916ms)
Mar  2 23:17:37.187: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 42.682461ms)
Mar  2 23:17:37.187: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 42.580981ms)
Mar  2 23:17:37.187: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 42.688478ms)
Mar  2 23:17:37.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.056268ms)
Mar  2 23:17:37.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.852216ms)
Mar  2 23:17:37.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.90388ms)
Mar  2 23:17:37.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 20.920233ms)
Mar  2 23:17:37.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.118061ms)
Mar  2 23:17:37.212: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 25.357292ms)
Mar  2 23:17:37.212: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 25.441899ms)
Mar  2 23:17:37.213: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 25.851544ms)
Mar  2 23:17:37.213: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 25.761749ms)
Mar  2 23:17:37.213: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 25.95465ms)
Mar  2 23:17:37.213: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 26.001531ms)
Mar  2 23:17:37.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 33.866677ms)
Mar  2 23:17:37.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 33.927526ms)
Mar  2 23:17:37.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 33.888542ms)
Mar  2 23:17:37.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 33.905781ms)
Mar  2 23:17:37.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 33.941111ms)
Mar  2 23:17:37.239: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 18.340758ms)
Mar  2 23:17:37.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 20.379128ms)
Mar  2 23:17:37.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 20.405255ms)
Mar  2 23:17:37.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 20.501757ms)
Mar  2 23:17:37.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.726587ms)
Mar  2 23:17:37.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 20.778517ms)
Mar  2 23:17:37.244: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 23.122752ms)
Mar  2 23:17:37.244: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 23.109241ms)
Mar  2 23:17:37.244: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 22.959997ms)
Mar  2 23:17:37.245: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 23.66159ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 31.206394ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.218833ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.307106ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.38522ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.353909ms)
Mar  2 23:17:37.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.386454ms)
Mar  2 23:17:37.274: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.116929ms)
Mar  2 23:17:37.274: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 21.229258ms)
Mar  2 23:17:37.275: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 21.766171ms)
Mar  2 23:17:37.275: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.908358ms)
Mar  2 23:17:37.275: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.941275ms)
Mar  2 23:17:37.275: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 22.042754ms)
Mar  2 23:17:37.275: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 22.275708ms)
Mar  2 23:17:37.276: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 22.78071ms)
Mar  2 23:17:37.276: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 22.909721ms)
Mar  2 23:17:37.276: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 23.03154ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 34.35481ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 34.33431ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 34.152163ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 34.166136ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 34.194291ms)
Mar  2 23:17:37.287: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 34.133636ms)
Mar  2 23:17:37.306: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:443/proxy/... (200; 18.143055ms)
Mar  2 23:17:37.308: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:1080/proxy/rewri... (200; 20.701942ms)
Mar  2 23:17:37.308: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:460/proxy/: tls baz (200; 20.814208ms)
Mar  2 23:17:37.308: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 20.735644ms)
Mar  2 23:17:37.308: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:162/proxy/: bar (200; 21.128317ms)
Mar  2 23:17:37.309: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 21.498061ms)
Mar  2 23:17:37.309: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/proxy-service-5smxn-q7hrg/proxy/rewriteme"... (200; 21.497597ms)
Mar  2 23:17:37.309: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/https:proxy-service-5smxn-q7hrg:462/proxy/: tls qux (200; 21.64073ms)
Mar  2 23:17:37.310: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:160/proxy/: foo (200; 22.391807ms)
Mar  2 23:17:37.310: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-kkgh6/pods/http:proxy-service-5smxn-q7hrg:1080/proxy/... (200; 22.407367ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname2/proxy/: bar (200; 31.334872ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname2/proxy/: tls qux (200; 31.470876ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname1/proxy/: foo (200; 31.406782ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/proxy-service-5smxn:portname1/proxy/: foo (200; 31.329034ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/http:proxy-service-5smxn:portname2/proxy/: bar (200; 31.415401ms)
Mar  2 23:17:37.319: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-kkgh6/services/https:proxy-service-5smxn:tlsportname1/proxy/: tls baz (200; 31.430003ms)
STEP: deleting { ReplicationController} proxy-service-5smxn in namespace e2e-tests-proxy-kkgh6, will wait for the garbage collector to delete the pods
Mar  2 23:17:37.411: INFO: Deleting { ReplicationController} proxy-service-5smxn took: 24.202238ms
Mar  2 23:17:37.512: INFO: Terminating { ReplicationController} proxy-service-5smxn pods took: 100.370812ms
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:17:44.812: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-kkgh6" for this suite.
Mar  2 23:17:52.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:17:53.549: INFO: namespace: e2e-tests-proxy-kkgh6, resource: bindings, ignored listing per whitelist
Mar  2 23:17:54.321: INFO: namespace: e2e-tests-proxy-kkgh6, resource: packagemanifests, items remaining: 1
Mar  2 23:17:54.455: INFO: namespace: e2e-tests-proxy-kkgh6 no longer exists
Mar  2 23:17:54.473: INFO: namespace: e2e-tests-proxy-kkgh6, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:17:54.491: INFO: namespace e2e-tests-proxy-kkgh6 deletion completed in 9.63400252s

• [SLOW TEST:33.026 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:17:54.492: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:17:55.582: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 23:18:00.601: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 23:18:04.637: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 23:18:06.655: INFO: Creating deployment "test-rollover-deployment"
Mar  2 23:18:06.694: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 23:18:08.731: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 23:18:08.767: INFO: Ensure that both replica sets have 1 created replica
Mar  2 23:18:08.803: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 23:18:08.839: INFO: Updating deployment test-rollover-deployment
Mar  2 23:18:08.840: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 23:18:10.876: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 23:18:10.917: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 23:18:10.958: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:10.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165488, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:12.994: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:12.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165488, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:14.994: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:14.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165488, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:16.997: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:16.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165488, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:18.996: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:18.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165497, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:20.997: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:20.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165497, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:22.994: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:22.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165497, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:24.995: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:24.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165497, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:26.994: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 23:18:26.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165497, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687165486, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 23:18:28.997: INFO: 
Mar  2 23:18:28.997: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  2 23:18:29.049: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-xd556,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xd556/deployments/test-rollover-deployment,UID:6fa313af-3d41-11e9-b620-0a843850fdce,ResourceVersion:63455,Generation:2,CreationTimestamp:2019-03-02 23:18:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-02 23:18:06 +0000 UTC 2019-03-02 23:18:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-02 23:18:27 +0000 UTC 2019-03-02 23:18:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 23:18:29.067: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-xd556,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xd556/replicasets/test-rollover-deployment-5b76ff8c4,UID:70ede409-3d41-11e9-8060-0e81de5d2554,ResourceVersion:63446,Generation:2,CreationTimestamp:2019-03-02 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6fa313af-3d41-11e9-b620-0a843850fdce 0xc4220d1d37 0xc4220d1d38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  2 23:18:29.067: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 23:18:29.067: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-xd556,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xd556/replicasets/test-rollover-controller,UID:6901e597-3d41-11e9-b620-0a843850fdce,ResourceVersion:63454,Generation:2,CreationTimestamp:2019-03-02 23:17:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6fa313af-3d41-11e9-b620-0a843850fdce 0xc4220d1b8e 0xc4220d1b8f}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  2 23:18:29.068: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-xd556,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xd556/replicasets/test-rollover-deployment-6975f4fb87,UID:6fa68b42-3d41-11e9-8060-0e81de5d2554,ResourceVersion:63243,Generation:2,CreationTimestamp:2019-03-02 23:18:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6fa313af-3d41-11e9-b620-0a843850fdce 0xc4220d1e07 0xc4220d1e08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  2 23:18:29.086: INFO: Pod "test-rollover-deployment-5b76ff8c4-2vlff" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-2vlff,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-xd556,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xd556/pods/test-rollover-deployment-5b76ff8c4-2vlff,UID:70f3d2fa-3d41-11e9-8060-0e81de5d2554,ResourceVersion:63341,Generation:0,CreationTimestamp:2019-03-02 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.62"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 70ede409-3d41-11e9-8060-0e81de5d2554 0xc422335230 0xc422335231}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-ng5gk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ng5gk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ng5gk true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pcqfj}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422335290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422335340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:18:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:18:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:18:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-02 23:18:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.62,StartTime:2019-03-02 23:18:08 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-02 23:18:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://9a87ac876f8b89db92c09927ad129197f5a0660f996651dbc337c204a567acc6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:18:29.086: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xd556" for this suite.
Mar  2 23:18:37.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:18:38.009: INFO: namespace: e2e-tests-deployment-xd556, resource: packagemanifests, items remaining: 1
Mar  2 23:18:38.915: INFO: namespace: e2e-tests-deployment-xd556, resource: bindings, ignored listing per whitelist
Mar  2 23:18:39.623: INFO: namespace: e2e-tests-deployment-xd556 no longer exists
Mar  2 23:18:39.642: INFO: namespace: e2e-tests-deployment-xd556, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:18:39.660: INFO: namespace e2e-tests-deployment-xd556 deletion completed in 10.528617606s

• [SLOW TEST:45.168 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:18:39.660: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar  2 23:18:40.711: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 23:18:40.780: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 23:18:40.800: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-63.ec2.internal before test
Mar  2 23:18:40.852: INFO: redhat-operators-8484568578-xdtzk from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container redhat-operators ready: true, restart count 0
Mar  2 23:18:40.852: INFO: node-exporter-68xtl from openshift-monitoring started at 2019-03-02 22:17:22 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:18:40.852: INFO: kube-state-metrics-67bf67646d-c76bn from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 23:18:40.852: INFO: multus-7z6tf from openshift-multus started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:18:40.852: INFO: certified-operators-76b9f9bdfb-9nrwb from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container certified-operators ready: true, restart count 0
Mar  2 23:18:40.852: INFO: community-operators-768f567b68-sqssn from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container community-operators ready: true, restart count 0
Mar  2 23:18:40.852: INFO: ovs-hwm6x from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:18:40.852: INFO: tuned-4jp7z from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:18:40.852: INFO: node-ca-d9zfb from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:18:40.852: INFO: router-default-74d6bbbf54-dsl6h from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container router ready: true, restart count 0
Mar  2 23:18:40.852: INFO: router-default-74d6bbbf54-6fv8g from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container router ready: true, restart count 0
Mar  2 23:18:40.852: INFO: telemeter-client-6db8c847b7-grhl9 from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container reload ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 23:18:40.852: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-03-02 22:20:07 +0000 UTC (3 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:18:40.852: INFO: sdn-p29xm from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:18:40.852: INFO: dns-default-hfm86 from openshift-dns started at 2019-03-02 22:17:23 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:18:40.852: INFO: machine-config-daemon-4snhh from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.852: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:18:40.852: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-148-56.ec2.internal before test
Mar  2 23:18:40.907: INFO: sdn-5tbhh from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:18:40.907: INFO: tuned-j48rm from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:18:40.907: INFO: ovs-2njbk from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:18:40.907: INFO: node-exporter-b5gbl from openshift-monitoring started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:18:40.907: INFO: machine-config-daemon-kqwc8 from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:18:40.907: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-03-02 22:19:55 +0000 UTC (3 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:18:40.907: INFO: multus-kpz5d from openshift-multus started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:18:40.907: INFO: dns-default-blbmc from openshift-dns started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:18:40.907: INFO: node-ca-54mqp from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:18:40.907: INFO: grafana-5c65588698-xtfnz from openshift-monitoring started at 2019-03-02 22:18:45 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container grafana ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:18:40.907: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 23:18:40.907: INFO: prometheus-adapter-7b89fc5f5-cb2hj from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.907: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:18:40.907: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-160-190.ec2.internal before test
Mar  2 23:18:40.964: INFO: dns-default-2f5st from openshift-dns started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:18:40.965: INFO: tuned-vptgs from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:18:40.965: INFO: machine-config-daemon-mlhcn from openshift-machine-config-operator started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:18:40.965: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:18:40.965: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 23:18:40.965: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (3 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:18:40.965: INFO: sdn-n87hs from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:18:40.965: INFO: node-ca-brndt from openshift-image-registry started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:18:40.965: INFO: prometheus-operator-76f5fb67cd-2kb84 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 23:18:40.965: INFO: prometheus-adapter-7b89fc5f5-v7747 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:18:40.965: INFO: multus-9466n from openshift-multus started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:18:40.965: INFO: ovs-xtgl4 from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:18:40.965: INFO: node-exporter-n6h9d from openshift-monitoring started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:18:40.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:18:40.965: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-88ee5840-3d41-11e9-9008-0a58ac10f353 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-88ee5840-3d41-11e9-9008-0a58ac10f353 off the node ip-10-0-148-56.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-88ee5840-3d41-11e9-9008-0a58ac10f353
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:18:59.275: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-nsqj4" for this suite.
Mar  2 23:19:09.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:19:10.858: INFO: namespace: e2e-tests-sched-pred-nsqj4, resource: bindings, ignored listing per whitelist
Mar  2 23:19:10.915: INFO: namespace: e2e-tests-sched-pred-nsqj4, resource: packagemanifests, items remaining: 1
Mar  2 23:19:11.796: INFO: namespace: e2e-tests-sched-pred-nsqj4 no longer exists
Mar  2 23:19:11.815: INFO: namespace: e2e-tests-sched-pred-nsqj4, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:19:11.832: INFO: namespace e2e-tests-sched-pred-nsqj4 deletion completed in 12.525095253s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:32.172 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:19:11.832: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0302 23:19:23.029670   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 23:19:23.029: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:19:23.029: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fjm65" for this suite.
Mar  2 23:19:29.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:19:29.934: INFO: namespace: e2e-tests-gc-fjm65, resource: packagemanifests, items remaining: 1
Mar  2 23:19:30.564: INFO: namespace: e2e-tests-gc-fjm65, resource: bindings, ignored listing per whitelist
Mar  2 23:19:31.553: INFO: namespace: e2e-tests-gc-fjm65 no longer exists
Mar  2 23:19:31.571: INFO: namespace: e2e-tests-gc-fjm65, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:19:31.588: INFO: namespace e2e-tests-gc-fjm65 deletion completed in 8.526863266s

• [SLOW TEST:19.756 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:19:31.588: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:19:32.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-7ztr5" to be "success or failure"
Mar  2 23:19:32.709: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 26.168878ms
Mar  2 23:19:34.727: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044437872s
Mar  2 23:19:36.747: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064634045s
Mar  2 23:19:38.766: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083397384s
Mar  2 23:19:40.784: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10130204s
Mar  2 23:19:42.802: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119216633s
STEP: Saw pod success
Mar  2 23:19:42.802: INFO: Pod "downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:19:42.819: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:19:42.876: INFO: Waiting for pod downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:19:42.894: INFO: Pod downwardapi-volume-a2e2374d-3d41-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:19:42.895: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7ztr5" for this suite.
Mar  2 23:19:49.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:19:50.128: INFO: namespace: e2e-tests-projected-7ztr5, resource: packagemanifests, items remaining: 1
Mar  2 23:19:50.128: INFO: namespace: e2e-tests-projected-7ztr5, resource: bindings, ignored listing per whitelist
Mar  2 23:19:51.433: INFO: namespace: e2e-tests-projected-7ztr5 no longer exists
Mar  2 23:19:51.452: INFO: namespace: e2e-tests-projected-7ztr5, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:19:51.470: INFO: namespace e2e-tests-projected-7ztr5 deletion completed in 8.528625273s

• [SLOW TEST:19.881 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:19:51.470: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar  2 23:19:52.557: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=e2e-tests-kubectl-rnkxx run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  2 23:20:02.574: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  2 23:20:02.574: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:20:04.610: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rnkxx" for this suite.
Mar  2 23:20:16.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:20:17.925: INFO: namespace: e2e-tests-kubectl-rnkxx, resource: packagemanifests, items remaining: 1
Mar  2 23:20:18.568: INFO: namespace: e2e-tests-kubectl-rnkxx, resource: bindings, ignored listing per whitelist
Mar  2 23:20:19.165: INFO: namespace: e2e-tests-kubectl-rnkxx no longer exists
Mar  2 23:20:19.188: INFO: namespace: e2e-tests-kubectl-rnkxx, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:20:19.206: INFO: namespace e2e-tests-kubectl-rnkxx deletion completed in 14.551629823s

• [SLOW TEST:27.737 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:20:19.207: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:20:20.298: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-bf4a0fb1-3d41-11e9-9008-0a58ac10f353
STEP: Creating configMap with name cm-test-opt-upd-bf4a1006-3d41-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bf4a0fb1-3d41-11e9-9008-0a58ac10f353
STEP: Updating configmap cm-test-opt-upd-bf4a1006-3d41-11e9-9008-0a58ac10f353
STEP: Creating configMap with name cm-test-opt-create-bf4a101c-3d41-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:20:32.676: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6w5dk" for this suite.
Mar  2 23:20:56.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:20:57.735: INFO: namespace: e2e-tests-projected-6w5dk, resource: packagemanifests, items remaining: 1
Mar  2 23:20:57.753: INFO: namespace: e2e-tests-projected-6w5dk, resource: bindings, ignored listing per whitelist
Mar  2 23:20:59.214: INFO: namespace: e2e-tests-projected-6w5dk no longer exists
Mar  2 23:20:59.232: INFO: namespace: e2e-tests-projected-6w5dk, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:20:59.249: INFO: namespace e2e-tests-projected-6w5dk deletion completed in 26.527878255s

• [SLOW TEST:40.043 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:20:59.250: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:21:00.262: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version'
Mar  2 23:21:00.456: INFO: stderr: ""
Mar  2 23:21:00.456: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12+\", GitVersion:\"v1.12.7-beta.0.5+b17cdb9d0b95c4\", GitCommit:\"b17cdb9d0b95c4812325739f5fd40edd9e3daa4d\", GitTreeState:\"clean\", BuildDate:\"2019-03-02T22:24:57Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12+\", GitVersion:\"v1.12.4+761b685\", GitCommit:\"761b685\", GitTreeState:\"clean\", BuildDate:\"2019-03-02T21:28:13Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:21:00.456: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d9ckv" for this suite.
Mar  2 23:21:06.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:21:07.345: INFO: namespace: e2e-tests-kubectl-d9ckv, resource: bindings, ignored listing per whitelist
Mar  2 23:21:08.072: INFO: namespace: e2e-tests-kubectl-d9ckv, resource: packagemanifests, items remaining: 1
Mar  2 23:21:08.980: INFO: namespace: e2e-tests-kubectl-d9ckv no longer exists
Mar  2 23:21:08.999: INFO: namespace: e2e-tests-kubectl-d9ckv, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:21:09.016: INFO: namespace e2e-tests-kubectl-d9ckv deletion completed in 8.526474294s

• [SLOW TEST:9.767 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:21:09.017: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 23:21:10.118: INFO: Waiting up to 5m0s for pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-9gjlk" to be "success or failure"
Mar  2 23:21:10.138: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.853551ms
Mar  2 23:21:12.156: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038437729s
Mar  2 23:21:14.174: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056489472s
Mar  2 23:21:16.192: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074137507s
Mar  2 23:21:18.210: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092450233s
Mar  2 23:21:20.229: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111059395s
STEP: Saw pod success
Mar  2 23:21:20.229: INFO: Pod "pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:21:20.249: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:21:20.314: INFO: Waiting for pod pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:21:20.332: INFO: Pod pod-dcf5c5dd-3d41-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:21:20.332: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9gjlk" for this suite.
Mar  2 23:21:26.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:21:28.641: INFO: namespace: e2e-tests-emptydir-9gjlk, resource: bindings, ignored listing per whitelist
Mar  2 23:21:28.810: INFO: namespace: e2e-tests-emptydir-9gjlk, resource: packagemanifests, items remaining: 1
Mar  2 23:21:28.872: INFO: namespace: e2e-tests-emptydir-9gjlk no longer exists
Mar  2 23:21:28.890: INFO: namespace: e2e-tests-emptydir-9gjlk, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:21:28.907: INFO: namespace e2e-tests-emptydir-9gjlk deletion completed in 8.530598696s

• [SLOW TEST:19.890 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:21:28.908: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:21:29.987: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:21:30.661: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-bd9m4" for this suite.
Mar  2 23:21:36.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:21:37.869: INFO: namespace: e2e-tests-custom-resource-definition-bd9m4, resource: bindings, ignored listing per whitelist
Mar  2 23:21:38.156: INFO: namespace: e2e-tests-custom-resource-definition-bd9m4, resource: packagemanifests, items remaining: 1
Mar  2 23:21:39.197: INFO: namespace: e2e-tests-custom-resource-definition-bd9m4 no longer exists
Mar  2 23:21:39.215: INFO: namespace: e2e-tests-custom-resource-definition-bd9m4, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:21:39.232: INFO: namespace e2e-tests-custom-resource-definition-bd9m4 deletion completed in 8.523438872s

• [SLOW TEST:10.325 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:21:39.232: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  2 23:21:40.287: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:21:49.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-khhhw" for this suite.
Mar  2 23:21:56.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:21:56.866: INFO: namespace: e2e-tests-init-container-khhhw, resource: packagemanifests, items remaining: 1
Mar  2 23:21:57.915: INFO: namespace: e2e-tests-init-container-khhhw, resource: bindings, ignored listing per whitelist
Mar  2 23:21:58.471: INFO: namespace: e2e-tests-init-container-khhhw no longer exists
Mar  2 23:21:58.489: INFO: namespace: e2e-tests-init-container-khhhw, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:21:58.506: INFO: namespace e2e-tests-init-container-khhhw deletion completed in 8.524929036s

• [SLOW TEST:19.274 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:21:58.507: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 23:21:59.738: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:21:59.738: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:21:59.739: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:21:59.803: INFO: Number of nodes with available pods: 0
Mar  2 23:21:59.803: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:00.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:00.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:00.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:00.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:00.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:01.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:01.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:01.836: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:01.854: INFO: Number of nodes with available pods: 0
Mar  2 23:22:01.854: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:02.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:02.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:02.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:02.852: INFO: Number of nodes with available pods: 0
Mar  2 23:22:02.852: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:03.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:03.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:03.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:03.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:03.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:04.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:04.836: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:04.836: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:04.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:04.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:05.836: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:05.836: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:05.836: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:05.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:05.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:06.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:06.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:06.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:06.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:06.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:07.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:07.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:07.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:07.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:07.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:08.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:08.836: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:08.836: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:08.853: INFO: Number of nodes with available pods: 0
Mar  2 23:22:08.853: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:09.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:09.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:09.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:09.854: INFO: Number of nodes with available pods: 1
Mar  2 23:22:09.854: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:10.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:10.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:10.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:10.853: INFO: Number of nodes with available pods: 2
Mar  2 23:22:10.853: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:22:11.835: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:11.835: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:11.835: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:11.853: INFO: Number of nodes with available pods: 3
Mar  2 23:22:11.853: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 23:22:11.942: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:11.942: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:11.942: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:11.960: INFO: Number of nodes with available pods: 2
Mar  2 23:22:11.960: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:13.012: INFO: Number of nodes with available pods: 2
Mar  2 23:22:13.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:13.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:13.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:13.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:14.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:14.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:14.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:14.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:14.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:15.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:15.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:16.012: INFO: Number of nodes with available pods: 2
Mar  2 23:22:16.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:16.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:16.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:16.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:17.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:17.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:17.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:17.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:17.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:18.012: INFO: Number of nodes with available pods: 2
Mar  2 23:22:18.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:18.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:18.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:18.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:19.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:19.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:19.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:19.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:19.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:20.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:20.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:20.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:20.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:20.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:21.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:21.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:22.012: INFO: Number of nodes with available pods: 2
Mar  2 23:22:22.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:22.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:22.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:22.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:23.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:23.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:23.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:23.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:23.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:24.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:24.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:24.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:24.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:24.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:25.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:25.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:25.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:25.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:25.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:26.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:26.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:26.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:26.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:26.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:27.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:27.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:27.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:27.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:27.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:28.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:28.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:28.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:28.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:28.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:29.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:29.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:29.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:29.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:29.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:30.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:30.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:31.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:31.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:31.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:31.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:31.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:32.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:32.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:33.003: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:33.003: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:33.003: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:33.020: INFO: Number of nodes with available pods: 2
Mar  2 23:22:33.020: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:33.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:33.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:33.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:34.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:34.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:34.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:34.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:34.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:35.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:35.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:35.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:35.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:35.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:36.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:36.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:36.995: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:36.995: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:36.996: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:37.013: INFO: Number of nodes with available pods: 2
Mar  2 23:22:37.013: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:37.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:37.992: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:37.992: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:38.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:38.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:38.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:38.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:38.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:39.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:39.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:39.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:39.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:39.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:40.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:40.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:40.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:40.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:40.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:41.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:41.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:41.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:41.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:41.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:42.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:42.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:42.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:42.992: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:42.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:43.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:43.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:43.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:43.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:43.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:44.012: INFO: Number of nodes with available pods: 2
Mar  2 23:22:44.012: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:45.005: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:45.005: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:45.005: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:45.024: INFO: Number of nodes with available pods: 2
Mar  2 23:22:45.024: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:46.001: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:46.001: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:46.001: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:46.019: INFO: Number of nodes with available pods: 2
Mar  2 23:22:46.019: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:46.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:46.992: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:46.992: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:47.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:47.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:47.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:47.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:47.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:48.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:48.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:48.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:48.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:48.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:49.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:49.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:49.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:49.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:49.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:50.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:50.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:50.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:50.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:50.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:51.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:51.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:51.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:51.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:51.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:52.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:52.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:52.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:52.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:52.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:53.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:53.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:53.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:53.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:53.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:54.010: INFO: Number of nodes with available pods: 2
Mar  2 23:22:54.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:54.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:54.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:54.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:55.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:55.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:55.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:55.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:55.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:56.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:56.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:57.013: INFO: Number of nodes with available pods: 2
Mar  2 23:22:57.013: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:57.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:57.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:57.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:58.015: INFO: Number of nodes with available pods: 2
Mar  2 23:22:58.015: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:58.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:58.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:58.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:22:59.011: INFO: Number of nodes with available pods: 2
Mar  2 23:22:59.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:22:59.992: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:59.992: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:22:59.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:23:00.010: INFO: Number of nodes with available pods: 2
Mar  2 23:23:00.010: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:23:00.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:00.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:00.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:23:01.011: INFO: Number of nodes with available pods: 2
Mar  2 23:23:01.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:23:01.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:01.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:01.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:23:02.011: INFO: Number of nodes with available pods: 2
Mar  2 23:23:02.011: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:23:02.993: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:02.993: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:23:02.993: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:23:03.011: INFO: Number of nodes with available pods: 3
Mar  2 23:23:03.011: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-fgrgm, will wait for the garbage collector to delete the pods
Mar  2 23:23:03.122: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.744234ms
Mar  2 23:23:03.222: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.40836ms
Mar  2 23:23:46.540: INFO: Number of nodes with available pods: 0
Mar  2 23:23:46.541: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:23:46.562: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-fgrgm/daemonsets","resourceVersion":"67684"},"items":null}

Mar  2 23:23:46.580: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-fgrgm/pods","resourceVersion":"67684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:23:46.664: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-fgrgm" for this suite.
Mar  2 23:23:54.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:23:56.155: INFO: namespace: e2e-tests-daemonsets-fgrgm, resource: packagemanifests, items remaining: 1
Mar  2 23:23:56.897: INFO: namespace: e2e-tests-daemonsets-fgrgm, resource: bindings, ignored listing per whitelist
Mar  2 23:23:57.186: INFO: namespace: e2e-tests-daemonsets-fgrgm no longer exists
Mar  2 23:23:57.205: INFO: namespace: e2e-tests-daemonsets-fgrgm, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:23:57.222: INFO: namespace e2e-tests-daemonsets-fgrgm deletion completed in 10.525815162s

• [SLOW TEST:118.715 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:23:57.222: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-41422206-3d42-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:23:58.422: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-dqv6m" to be "success or failure"
Mar  2 23:23:58.446: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 23.570291ms
Mar  2 23:24:00.464: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041781381s
Mar  2 23:24:02.489: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066104149s
Mar  2 23:24:04.507: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084576822s
Mar  2 23:24:06.525: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.102525489s
Mar  2 23:24:08.545: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.122873604s
STEP: Saw pod success
Mar  2 23:24:08.545: INFO: Pod "pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:24:08.563: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:24:08.621: INFO: Waiting for pod pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:24:08.648: INFO: Pod pod-projected-configmaps-41471c60-3d42-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:24:08.648: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dqv6m" for this suite.
Mar  2 23:24:14.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:24:15.901: INFO: namespace: e2e-tests-projected-dqv6m, resource: packagemanifests, items remaining: 1
Mar  2 23:24:17.123: INFO: namespace: e2e-tests-projected-dqv6m, resource: bindings, ignored listing per whitelist
Mar  2 23:24:17.190: INFO: namespace: e2e-tests-projected-dqv6m no longer exists
Mar  2 23:24:17.208: INFO: namespace: e2e-tests-projected-dqv6m, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:24:17.225: INFO: namespace e2e-tests-projected-dqv6m deletion completed in 8.527434154s

• [SLOW TEST:20.003 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:24:17.225: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 23:24:18.430: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:18.430: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:18.430: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:18.451: INFO: Number of nodes with available pods: 0
Mar  2 23:24:18.451: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:19.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:19.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:19.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:19.501: INFO: Number of nodes with available pods: 0
Mar  2 23:24:19.501: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:20.495: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:20.495: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:20.495: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:20.513: INFO: Number of nodes with available pods: 0
Mar  2 23:24:20.513: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:21.496: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:21.496: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:21.496: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:21.513: INFO: Number of nodes with available pods: 0
Mar  2 23:24:21.513: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:22.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:22.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:22.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:22.501: INFO: Number of nodes with available pods: 0
Mar  2 23:24:22.501: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:23.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:23.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:23.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:23.500: INFO: Number of nodes with available pods: 0
Mar  2 23:24:23.500: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:24.485: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:24.485: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:24.485: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:24.505: INFO: Number of nodes with available pods: 0
Mar  2 23:24:24.505: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:25.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:25.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:25.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:25.500: INFO: Number of nodes with available pods: 0
Mar  2 23:24:25.501: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:24:26.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:26.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:26.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:26.501: INFO: Number of nodes with available pods: 1
Mar  2 23:24:26.501: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:27.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:27.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:27.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:27.502: INFO: Number of nodes with available pods: 2
Mar  2 23:24:27.502: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:24:28.483: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:28.483: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:28.483: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:28.501: INFO: Number of nodes with available pods: 3
Mar  2 23:24:28.501: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 23:24:28.595: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:28.595: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:28.595: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:28.613: INFO: Number of nodes with available pods: 2
Mar  2 23:24:28.613: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:29.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:29.646: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:29.646: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:29.663: INFO: Number of nodes with available pods: 2
Mar  2 23:24:29.663: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:30.646: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:30.646: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:30.646: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:30.664: INFO: Number of nodes with available pods: 2
Mar  2 23:24:30.664: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:31.658: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:31.658: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:31.658: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:31.676: INFO: Number of nodes with available pods: 2
Mar  2 23:24:31.676: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:32.659: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:32.659: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:32.659: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:32.677: INFO: Number of nodes with available pods: 2
Mar  2 23:24:32.677: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:33.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:33.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:33.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:33.665: INFO: Number of nodes with available pods: 2
Mar  2 23:24:33.665: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:34.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:34.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:34.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:34.663: INFO: Number of nodes with available pods: 2
Mar  2 23:24:34.663: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:35.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:35.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:35.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:35.663: INFO: Number of nodes with available pods: 2
Mar  2 23:24:35.663: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:36.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:36.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:36.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:36.663: INFO: Number of nodes with available pods: 2
Mar  2 23:24:36.663: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:37.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:37.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:37.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:37.663: INFO: Number of nodes with available pods: 2
Mar  2 23:24:37.663: INFO: Node ip-10-0-148-56.ec2.internal is running more than one daemon pod
Mar  2 23:24:38.645: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:38.645: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:24:38.645: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:24:38.665: INFO: Number of nodes with available pods: 3
Mar  2 23:24:38.665: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-w746s, will wait for the garbage collector to delete the pods
Mar  2 23:24:38.796: INFO: Deleting {extensions DaemonSet} daemon-set took: 26.412055ms
Mar  2 23:24:38.897: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.344107ms
Mar  2 23:25:14.814: INFO: Number of nodes with available pods: 0
Mar  2 23:25:14.814: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:25:14.832: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w746s/daemonsets","resourceVersion":"68844"},"items":null}

Mar  2 23:25:14.849: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w746s/pods","resourceVersion":"68844"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:25:14.932: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w746s" for this suite.
Mar  2 23:25:23.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:25:23.991: INFO: namespace: e2e-tests-daemonsets-w746s, resource: packagemanifests, items remaining: 1
Mar  2 23:25:24.325: INFO: namespace: e2e-tests-daemonsets-w746s, resource: bindings, ignored listing per whitelist
Mar  2 23:25:25.454: INFO: namespace: e2e-tests-daemonsets-w746s no longer exists
Mar  2 23:25:25.472: INFO: namespace: e2e-tests-daemonsets-w746s, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:25:25.489: INFO: namespace e2e-tests-daemonsets-w746s deletion completed in 10.526230339s

• [SLOW TEST:68.264 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:25:25.490: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:25:26.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-55lmm" to be "success or failure"
Mar  2 23:25:26.592: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.747737ms
Mar  2 23:25:28.610: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035898415s
Mar  2 23:25:30.633: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05892799s
Mar  2 23:25:32.651: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076959827s
Mar  2 23:25:34.670: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095796458s
Mar  2 23:25:36.688: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.114076802s
STEP: Saw pod success
Mar  2 23:25:36.689: INFO: Pod "downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:25:36.706: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:25:36.772: INFO: Waiting for pod downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:25:36.789: INFO: Pod downwardapi-volume-75d20918-3d42-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:25:36.790: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-55lmm" for this suite.
Mar  2 23:25:42.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:25:43.912: INFO: namespace: e2e-tests-downward-api-55lmm, resource: packagemanifests, items remaining: 1
Mar  2 23:25:44.986: INFO: namespace: e2e-tests-downward-api-55lmm, resource: bindings, ignored listing per whitelist
Mar  2 23:25:45.324: INFO: namespace: e2e-tests-downward-api-55lmm no longer exists
Mar  2 23:25:45.343: INFO: namespace: e2e-tests-downward-api-55lmm, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:25:45.360: INFO: namespace e2e-tests-downward-api-55lmm deletion completed in 8.525415096s

• [SLOW TEST:19.871 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:25:45.360: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-v54tv
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar  2 23:25:46.457: INFO: Found 0 stateful pods, waiting for 3
Mar  2 23:25:56.476: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:26:06.476: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:06.476: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:06.476: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 23:26:16.475: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:16.475: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:16.475: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  2 23:26:16.577: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  2 23:26:26.697: INFO: Updating stateful set ss2
Mar  2 23:26:26.734: INFO: Waiting for Pod e2e-tests-statefulset-v54tv/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar  2 23:26:36.855: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:26:46.874: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:46.874: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:46.874: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 23:26:56.874: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:56.874: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:26:56.874: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Mar  2 23:27:06.873: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:27:06.873: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:27:06.873: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  2 23:27:06.959: INFO: Updating stateful set ss2
Mar  2 23:27:06.995: INFO: Waiting for Pod e2e-tests-statefulset-v54tv/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:27:17.082: INFO: Updating stateful set ss2
Mar  2 23:27:17.118: INFO: Waiting for StatefulSet e2e-tests-statefulset-v54tv/ss2 to complete update
Mar  2 23:27:17.118: INFO: Waiting for Pod e2e-tests-statefulset-v54tv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:27:27.155: INFO: Waiting for StatefulSet e2e-tests-statefulset-v54tv/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  2 23:27:37.154: INFO: Deleting all statefulset in ns e2e-tests-statefulset-v54tv
Mar  2 23:27:37.172: INFO: Scaling statefulset ss2 to 0
Mar  2 23:27:57.253: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:27:57.273: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:27:57.336: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-v54tv" for this suite.
Mar  2 23:28:05.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:28:06.244: INFO: namespace: e2e-tests-statefulset-v54tv, resource: packagemanifests, items remaining: 1
Mar  2 23:28:07.806: INFO: namespace: e2e-tests-statefulset-v54tv, resource: bindings, ignored listing per whitelist
Mar  2 23:28:07.874: INFO: namespace: e2e-tests-statefulset-v54tv no longer exists
Mar  2 23:28:07.892: INFO: namespace: e2e-tests-statefulset-v54tv, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:28:07.909: INFO: namespace e2e-tests-statefulset-v54tv deletion completed in 10.527084694s

• [SLOW TEST:142.549 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:28:07.910: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  2 23:28:09.062: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71171,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 23:28:09.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71172,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  2 23:28:09.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71175,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 23:28:19.205: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71283,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 23:28:19.205: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71284,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  2 23:28:19.205: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kxw7f,SelfLink:/api/v1/namespaces/e2e-tests-watch-kxw7f/configmaps/e2e-watch-test-label-changed,UID:d6a48816-3d42-11e9-b620-0a843850fdce,ResourceVersion:71285,Generation:0,CreationTimestamp:2019-03-02 23:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:28:19.206: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kxw7f" for this suite.
Mar  2 23:28:25.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:28:26.125: INFO: namespace: e2e-tests-watch-kxw7f, resource: bindings, ignored listing per whitelist
Mar  2 23:28:26.944: INFO: namespace: e2e-tests-watch-kxw7f, resource: packagemanifests, items remaining: 1
Mar  2 23:28:27.743: INFO: namespace: e2e-tests-watch-kxw7f no longer exists
Mar  2 23:28:27.761: INFO: namespace: e2e-tests-watch-kxw7f, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:28:27.779: INFO: namespace e2e-tests-watch-kxw7f deletion completed in 8.527885782s

• [SLOW TEST:19.869 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:28:27.779: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 23:28:28.902: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71442,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 23:28:28.902: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71442,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 23:28:38.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71550,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  2 23:28:38.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71550,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 23:28:48.978: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71655,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 23:28:48.978: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71655,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 23:28:59.002: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71758,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 23:28:59.002: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-a,UID:e282d92d-3d42-11e9-b620-0a843850fdce,ResourceVersion:71758,Generation:0,CreationTimestamp:2019-03-02 23:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 23:29:09.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-b,UID:fa6d5414-3d42-11e9-b620-0a843850fdce,ResourceVersion:71862,Generation:0,CreationTimestamp:2019-03-02 23:29:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 23:29:09.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-b,UID:fa6d5414-3d42-11e9-b620-0a843850fdce,ResourceVersion:71862,Generation:0,CreationTimestamp:2019-03-02 23:29:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 23:29:19.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-b,UID:fa6d5414-3d42-11e9-b620-0a843850fdce,ResourceVersion:71968,Generation:0,CreationTimestamp:2019-03-02 23:29:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 23:29:19.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fzsf4,SelfLink:/api/v1/namespaces/e2e-tests-watch-fzsf4/configmaps/e2e-watch-test-configmap-b,UID:fa6d5414-3d42-11e9-b620-0a843850fdce,ResourceVersion:71968,Generation:0,CreationTimestamp:2019-03-02 23:29:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:29:29.053: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fzsf4" for this suite.
Mar  2 23:29:35.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:29:36.336: INFO: namespace: e2e-tests-watch-fzsf4, resource: packagemanifests, items remaining: 1
Mar  2 23:29:36.862: INFO: namespace: e2e-tests-watch-fzsf4, resource: bindings, ignored listing per whitelist
Mar  2 23:29:37.591: INFO: namespace: e2e-tests-watch-fzsf4 no longer exists
Mar  2 23:29:37.608: INFO: namespace: e2e-tests-watch-fzsf4, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:29:37.626: INFO: namespace e2e-tests-watch-fzsf4 deletion completed in 8.527713463s

• [SLOW TEST:69.847 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:29:37.626: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:29:38.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-5m892" to be "success or failure"
Mar  2 23:29:38.758: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.78837ms
Mar  2 23:29:40.776: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038956951s
Mar  2 23:29:42.794: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056852167s
Mar  2 23:29:44.813: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075690978s
Mar  2 23:29:46.831: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09375985s
Mar  2 23:29:48.849: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111949174s
Mar  2 23:29:50.867: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.129874413s
STEP: Saw pod success
Mar  2 23:29:50.867: INFO: Pod "downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:29:50.884: INFO: Trying to get logs from node ip-10-0-143-63.ec2.internal pod downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:29:50.941: INFO: Waiting for pod downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:29:50.959: INFO: Pod downwardapi-volume-0c1bd4d1-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:29:50.959: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5m892" for this suite.
Mar  2 23:29:57.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:29:57.931: INFO: namespace: e2e-tests-projected-5m892, resource: bindings, ignored listing per whitelist
Mar  2 23:29:58.098: INFO: namespace: e2e-tests-projected-5m892, resource: packagemanifests, items remaining: 1
Mar  2 23:29:59.498: INFO: namespace: e2e-tests-projected-5m892 no longer exists
Mar  2 23:29:59.516: INFO: namespace: e2e-tests-projected-5m892, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:29:59.534: INFO: namespace e2e-tests-projected-5m892 deletion completed in 8.528395795s

• [SLOW TEST:21.908 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:29:59.534: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-1928f3ff-3d43-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:30:00.644: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-6w7qb" to be "success or failure"
Mar  2 23:30:00.688: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 43.816052ms
Mar  2 23:30:02.706: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062009072s
Mar  2 23:30:04.724: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080036281s
Mar  2 23:30:06.743: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09845165s
Mar  2 23:30:08.761: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116743495s
Mar  2 23:30:10.778: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.13443656s
STEP: Saw pod success
Mar  2 23:30:10.779: INFO: Pod "pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:30:10.796: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:30:10.916: INFO: Waiting for pod pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:30:10.934: INFO: Pod pod-projected-configmaps-192cc60b-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:30:10.934: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6w7qb" for this suite.
Mar  2 23:30:17.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:30:18.555: INFO: namespace: e2e-tests-projected-6w7qb, resource: packagemanifests, items remaining: 1
Mar  2 23:30:18.798: INFO: namespace: e2e-tests-projected-6w7qb, resource: bindings, ignored listing per whitelist
Mar  2 23:30:19.469: INFO: namespace: e2e-tests-projected-6w7qb no longer exists
Mar  2 23:30:19.488: INFO: namespace: e2e-tests-projected-6w7qb, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:30:19.506: INFO: namespace e2e-tests-projected-6w7qb deletion completed in 8.526931906s

• [SLOW TEST:19.971 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:30:19.506: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar  2 23:30:20.562: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig cluster-info'
Mar  2 23:30:20.855: INFO: stderr: ""
Mar  2 23:30:20.855: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.ci-op-s3r522g9-1ac2a.origin-ci-int-aws.dev.rhcloud.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:30:20.855: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cw2zm" for this suite.
Mar  2 23:30:26.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:30:27.883: INFO: namespace: e2e-tests-kubectl-cw2zm, resource: bindings, ignored listing per whitelist
Mar  2 23:30:28.580: INFO: namespace: e2e-tests-kubectl-cw2zm, resource: packagemanifests, items remaining: 1
Mar  2 23:30:29.379: INFO: namespace: e2e-tests-kubectl-cw2zm no longer exists
Mar  2 23:30:29.397: INFO: namespace: e2e-tests-kubectl-cw2zm, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:30:29.414: INFO: namespace e2e-tests-kubectl-cw2zm deletion completed in 8.525852084s

• [SLOW TEST:9.909 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:30:29.415: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:30:31.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-cjpbt" to be "success or failure"
Mar  2 23:30:31.527: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.84352ms
Mar  2 23:30:33.546: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038496278s
Mar  2 23:30:35.564: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057003408s
Mar  2 23:30:37.583: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075401013s
Mar  2 23:30:39.601: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093327939s
STEP: Saw pod success
Mar  2 23:30:39.601: INFO: Pod "downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:30:39.618: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:30:39.677: INFO: Waiting for pod downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:30:39.694: INFO: Pod downwardapi-volume-2af8e02b-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:30:39.694: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cjpbt" for this suite.
Mar  2 23:30:45.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:30:46.787: INFO: namespace: e2e-tests-projected-cjpbt, resource: bindings, ignored listing per whitelist
Mar  2 23:30:47.163: INFO: namespace: e2e-tests-projected-cjpbt, resource: packagemanifests, items remaining: 1
Mar  2 23:30:48.230: INFO: namespace: e2e-tests-projected-cjpbt no longer exists
Mar  2 23:30:48.250: INFO: namespace: e2e-tests-projected-cjpbt, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:30:48.267: INFO: namespace e2e-tests-projected-cjpbt deletion completed in 8.526824914s

• [SLOW TEST:18.853 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:30:48.267: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-rkxdf/secret-test-363f26e3-3d43-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:30:49.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-rkxdf" to be "success or failure"
Mar  2 23:30:49.457: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.338188ms
Mar  2 23:30:51.474: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037118314s
Mar  2 23:30:53.502: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064344984s
Mar  2 23:30:55.519: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082232163s
Mar  2 23:30:57.539: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101498202s
Mar  2 23:30:59.559: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121330263s
STEP: Saw pod success
Mar  2 23:30:59.559: INFO: Pod "pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:30:59.576: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353 container env-test: <nil>
STEP: delete the pod
Mar  2 23:30:59.637: INFO: Waiting for pod pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:30:59.655: INFO: Pod pod-configmaps-36445688-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:30:59.655: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rkxdf" for this suite.
Mar  2 23:31:05.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:31:06.987: INFO: namespace: e2e-tests-secrets-rkxdf, resource: bindings, ignored listing per whitelist
Mar  2 23:31:07.354: INFO: namespace: e2e-tests-secrets-rkxdf, resource: packagemanifests, items remaining: 1
Mar  2 23:31:08.191: INFO: namespace: e2e-tests-secrets-rkxdf no longer exists
Mar  2 23:31:08.209: INFO: namespace: e2e-tests-secrets-rkxdf, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:31:08.226: INFO: namespace e2e-tests-secrets-rkxdf deletion completed in 8.526401412s

• [SLOW TEST:19.959 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:31:08.227: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar  2 23:31:09.303: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  2 23:31:09.303: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:09.779: INFO: stderr: ""
Mar  2 23:31:09.779: INFO: stdout: "service/redis-slave created\n"
Mar  2 23:31:09.779: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  2 23:31:09.779: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:10.257: INFO: stderr: ""
Mar  2 23:31:10.257: INFO: stdout: "service/redis-master created\n"
Mar  2 23:31:10.257: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 23:31:10.257: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:10.730: INFO: stderr: ""
Mar  2 23:31:10.730: INFO: stdout: "service/frontend created\n"
Mar  2 23:31:10.731: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  2 23:31:10.731: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:11.196: INFO: stderr: ""
Mar  2 23:31:11.196: INFO: stdout: "deployment.extensions/frontend created\n"
Mar  2 23:31:11.196: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 23:31:11.196: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:11.652: INFO: stderr: ""
Mar  2 23:31:11.652: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar  2 23:31:11.653: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  2 23:31:11.653: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:12.124: INFO: stderr: ""
Mar  2 23:31:12.124: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar  2 23:31:12.124: INFO: Waiting for all frontend pods to be Running.
Mar  2 23:31:47.176: INFO: Waiting for frontend to serve content.
Mar  2 23:31:47.219: INFO: Trying to add a new entry to the guestbook.
Mar  2 23:31:47.252: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  2 23:31:47.285: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:47.550: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:47.550: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 23:31:47.551: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:47.767: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:47.767: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 23:31:47.768: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:47.977: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:47.977: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 23:31:47.977: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:48.177: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:48.177: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 23:31:48.177: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:48.370: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:48.371: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 23:31:48.371: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jcbtf'
Mar  2 23:31:48.568: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:31:48.568: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:31:48.568: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jcbtf" for this suite.
Mar  2 23:32:28.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:32:30.555: INFO: namespace: e2e-tests-kubectl-jcbtf, resource: packagemanifests, items remaining: 1
Mar  2 23:32:30.788: INFO: namespace: e2e-tests-kubectl-jcbtf, resource: bindings, ignored listing per whitelist
Mar  2 23:32:31.112: INFO: namespace: e2e-tests-kubectl-jcbtf no longer exists
Mar  2 23:32:31.131: INFO: namespace: e2e-tests-kubectl-jcbtf, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:32:31.148: INFO: namespace e2e-tests-kubectl-jcbtf deletion completed in 42.527144039s

• [SLOW TEST:82.921 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:32:31.148: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 23:32:52.532: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:32:52.549: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:32:54.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:32:54.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:32:56.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:32:56.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:32:58.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:32:58.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:00.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:00.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:02.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:02.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:04.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:04.569: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:06.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:06.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:08.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:08.604: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:10.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:10.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:12.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:12.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:14.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:14.568: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 23:33:16.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 23:33:16.568: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:33:16.568: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-xhbhw" for this suite.
Mar  2 23:33:40.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:33:41.613: INFO: namespace: e2e-tests-container-lifecycle-hook-xhbhw, resource: bindings, ignored listing per whitelist
Mar  2 23:33:42.032: INFO: namespace: e2e-tests-container-lifecycle-hook-xhbhw, resource: packagemanifests, items remaining: 1
Mar  2 23:33:43.122: INFO: namespace: e2e-tests-container-lifecycle-hook-xhbhw no longer exists
Mar  2 23:33:43.140: INFO: namespace: e2e-tests-container-lifecycle-hook-xhbhw, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:33:43.157: INFO: namespace e2e-tests-container-lifecycle-hook-xhbhw deletion completed in 26.526983559s

• [SLOW TEST:72.009 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:33:43.157: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-9e803c61-3d43-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:33:44.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-q52jq" to be "success or failure"
Mar  2 23:33:44.356: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.825508ms
Mar  2 23:33:46.374: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038071557s
Mar  2 23:33:48.393: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056952552s
Mar  2 23:33:50.411: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075127541s
Mar  2 23:33:52.429: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093271309s
Mar  2 23:33:54.448: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111812618s
STEP: Saw pod success
Mar  2 23:33:54.448: INFO: Pod "pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:33:54.465: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:33:54.520: INFO: Waiting for pod pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:33:54.539: INFO: Pod pod-projected-secrets-9e83e94a-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:33:54.539: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q52jq" for this suite.
Mar  2 23:34:00.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:34:02.106: INFO: namespace: e2e-tests-projected-q52jq, resource: packagemanifests, items remaining: 1
Mar  2 23:34:03.008: INFO: namespace: e2e-tests-projected-q52jq, resource: bindings, ignored listing per whitelist
Mar  2 23:34:03.075: INFO: namespace: e2e-tests-projected-q52jq no longer exists
Mar  2 23:34:03.094: INFO: namespace: e2e-tests-projected-q52jq, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:34:03.115: INFO: namespace e2e-tests-projected-q52jq deletion completed in 8.531314838s

• [SLOW TEST:19.958 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:34:03.116: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 23:34:05.217: INFO: Waiting up to 5m0s for pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-nx4tr" to be "success or failure"
Mar  2 23:34:05.236: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.148188ms
Mar  2 23:34:07.255: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037487855s
Mar  2 23:34:09.272: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055181348s
Mar  2 23:34:11.290: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073118445s
Mar  2 23:34:13.309: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091497607s
Mar  2 23:34:15.330: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113161747s
STEP: Saw pod success
Mar  2 23:34:15.330: INFO: Pod "pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:34:15.349: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:34:15.404: INFO: Waiting for pod pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:34:15.421: INFO: Pod pod-aa5a0cef-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:34:15.421: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nx4tr" for this suite.
Mar  2 23:34:23.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:34:24.900: INFO: namespace: e2e-tests-emptydir-nx4tr, resource: bindings, ignored listing per whitelist
Mar  2 23:34:25.062: INFO: namespace: e2e-tests-emptydir-nx4tr, resource: packagemanifests, items remaining: 1
Mar  2 23:34:25.957: INFO: namespace: e2e-tests-emptydir-nx4tr no longer exists
Mar  2 23:34:25.975: INFO: namespace: e2e-tests-emptydir-nx4tr, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:34:25.992: INFO: namespace e2e-tests-emptydir-nx4tr deletion completed in 10.524740654s

• [SLOW TEST:22.877 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:34:25.992: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 23:34:39.292: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:39.292: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:39.558: INFO: Exec stderr: ""
Mar  2 23:34:39.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:39.558: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:39.791: INFO: Exec stderr: ""
Mar  2 23:34:39.791: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:39.791: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:39.970: INFO: Exec stderr: ""
Mar  2 23:34:39.970: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:39.970: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:40.153: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 23:34:40.153: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:40.153: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:40.328: INFO: Exec stderr: ""
Mar  2 23:34:40.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:40.328: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:40.505: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 23:34:40.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:40.506: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:40.868: INFO: Exec stderr: ""
Mar  2 23:34:40.868: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:40.868: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:41.113: INFO: Exec stderr: ""
Mar  2 23:34:41.113: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:41.113: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:41.311: INFO: Exec stderr: ""
Mar  2 23:34:41.311: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-95wjb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:34:41.311: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:34:41.517: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:34:41.517: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-95wjb" for this suite.
Mar  2 23:35:25.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:35:26.493: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-95wjb, resource: bindings, ignored listing per whitelist
Mar  2 23:35:27.534: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-95wjb, resource: packagemanifests, items remaining: 1
Mar  2 23:35:28.053: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-95wjb no longer exists
Mar  2 23:35:28.072: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-95wjb, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:35:28.089: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-95wjb deletion completed in 46.526590941s

• [SLOW TEST:62.096 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:35:28.089: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-dcff114a-3d43-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:35:29.189: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-fxcjd" to be "success or failure"
Mar  2 23:35:29.210: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.39476ms
Mar  2 23:35:31.229: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040053634s
Mar  2 23:35:33.247: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058230813s
Mar  2 23:35:35.265: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076229659s
Mar  2 23:35:37.284: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094356397s
Mar  2 23:35:39.302: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112595976s
STEP: Saw pod success
Mar  2 23:35:39.303: INFO: Pod "pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:35:39.320: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:35:39.382: INFO: Waiting for pod pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:35:39.402: INFO: Pod pod-projected-secrets-dd02c943-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:35:39.402: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fxcjd" for this suite.
Mar  2 23:35:45.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:35:46.632: INFO: namespace: e2e-tests-projected-fxcjd, resource: packagemanifests, items remaining: 1
Mar  2 23:35:47.877: INFO: namespace: e2e-tests-projected-fxcjd, resource: bindings, ignored listing per whitelist
Mar  2 23:35:47.943: INFO: namespace: e2e-tests-projected-fxcjd no longer exists
Mar  2 23:35:47.961: INFO: namespace: e2e-tests-projected-fxcjd, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:35:47.978: INFO: namespace e2e-tests-projected-fxcjd deletion completed in 8.531204051s

• [SLOW TEST:19.889 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:35:47.978: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e8dabe0d-3d43-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:35:49.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-bgkrn" to be "success or failure"
Mar  2 23:35:49.113: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 27.094853ms
Mar  2 23:35:51.131: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045069217s
Mar  2 23:35:53.149: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063061893s
Mar  2 23:35:55.167: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081002304s
Mar  2 23:35:57.185: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099010955s
Mar  2 23:35:59.203: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.11660142s
STEP: Saw pod success
Mar  2 23:35:59.203: INFO: Pod "pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:35:59.220: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 23:35:59.276: INFO: Waiting for pod pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:35:59.294: INFO: Pod pod-configmaps-e8de43fd-3d43-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:35:59.294: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bgkrn" for this suite.
Mar  2 23:36:05.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:36:06.694: INFO: namespace: e2e-tests-configmap-bgkrn, resource: bindings, ignored listing per whitelist
Mar  2 23:36:06.960: INFO: namespace: e2e-tests-configmap-bgkrn, resource: packagemanifests, items remaining: 1
Mar  2 23:36:07.831: INFO: namespace: e2e-tests-configmap-bgkrn no longer exists
Mar  2 23:36:07.849: INFO: namespace: e2e-tests-configmap-bgkrn, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:36:07.866: INFO: namespace e2e-tests-configmap-bgkrn deletion completed in 8.527777952s

• [SLOW TEST:19.888 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:36:07.867: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rvbmb
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 23:36:08.926: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 23:36:41.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.80:8080/dial?request=hostName&protocol=udp&host=10.129.2.79&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rvbmb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:36:41.378: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:36:41.628: INFO: Waiting for endpoints: map[]
Mar  2 23:36:41.647: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.80:8080/dial?request=hostName&protocol=udp&host=10.128.2.79&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rvbmb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:36:41.647: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:36:41.881: INFO: Waiting for endpoints: map[]
Mar  2 23:36:41.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.80:8080/dial?request=hostName&protocol=udp&host=10.131.0.38&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rvbmb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 23:36:41.899: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  2 23:36:42.113: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:36:42.113: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rvbmb" for this suite.
Mar  2 23:37:06.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:37:07.683: INFO: namespace: e2e-tests-pod-network-test-rvbmb, resource: bindings, ignored listing per whitelist
Mar  2 23:37:08.051: INFO: namespace: e2e-tests-pod-network-test-rvbmb, resource: packagemanifests, items remaining: 1
Mar  2 23:37:08.661: INFO: namespace: e2e-tests-pod-network-test-rvbmb no longer exists
Mar  2 23:37:08.679: INFO: namespace: e2e-tests-pod-network-test-rvbmb, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:37:08.698: INFO: namespace e2e-tests-pod-network-test-rvbmb deletion completed in 26.53291193s

• [SLOW TEST:60.831 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:37:08.698: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0302 23:37:10.487515   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 23:37:10.487: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:37:10.487: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-qv22t" for this suite.
Mar  2 23:37:16.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:37:17.815: INFO: namespace: e2e-tests-gc-qv22t, resource: packagemanifests, items remaining: 1
Mar  2 23:37:17.833: INFO: namespace: e2e-tests-gc-qv22t, resource: bindings, ignored listing per whitelist
Mar  2 23:37:19.010: INFO: namespace: e2e-tests-gc-qv22t no longer exists
Mar  2 23:37:19.028: INFO: namespace: e2e-tests-gc-qv22t, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:37:19.046: INFO: namespace e2e-tests-gc-qv22t deletion completed in 8.52568762s

• [SLOW TEST:10.348 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:37:19.046: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 23:37:20.054: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-bnmh2'
Mar  2 23:37:20.259: INFO: stderr: ""
Mar  2 23:37:20.259: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Mar  2 23:37:20.277: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-bnmh2'
Mar  2 23:37:24.770: INFO: stderr: ""
Mar  2 23:37:24.770: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:37:24.770: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bnmh2" for this suite.
Mar  2 23:37:30.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:37:32.403: INFO: namespace: e2e-tests-kubectl-bnmh2, resource: packagemanifests, items remaining: 1
Mar  2 23:37:32.650: INFO: namespace: e2e-tests-kubectl-bnmh2, resource: bindings, ignored listing per whitelist
Mar  2 23:37:33.307: INFO: namespace: e2e-tests-kubectl-bnmh2 no longer exists
Mar  2 23:37:33.325: INFO: namespace: e2e-tests-kubectl-bnmh2, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:37:33.342: INFO: namespace e2e-tests-kubectl-bnmh2 deletion completed in 8.525615078s

• [SLOW TEST:14.296 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:37:33.342: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-c2zp4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c2zp4 to expose endpoints map[]
Mar  2 23:37:34.436: INFO: Get endpoints failed (20.486531ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  2 23:37:35.454: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c2zp4 exposes endpoints map[] (1.038415382s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-c2zp4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c2zp4 to expose endpoints map[pod1:[100]]
Mar  2 23:37:39.668: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.181971518s elapsed, will retry)
Mar  2 23:37:44.846: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c2zp4 exposes endpoints map[pod1:[100]] (9.360041203s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-c2zp4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c2zp4 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 23:37:49.141: INFO: Unexpected endpoints: found map[284cd760-3d44-11e9-b620-0a843850fdce:[100]], expected map[pod1:[100] pod2:[101]] (4.267339623s elapsed, will retry)
Mar  2 23:37:54.428: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c2zp4 exposes endpoints map[pod1:[100] pod2:[101]] (9.554534218s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-c2zp4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c2zp4 to expose endpoints map[pod2:[101]]
Mar  2 23:37:54.486: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c2zp4 exposes endpoints map[pod2:[101]] (35.582787ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-c2zp4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c2zp4 to expose endpoints map[]
Mar  2 23:37:54.526: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c2zp4 exposes endpoints map[] (18.305322ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:37:54.574: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-c2zp4" for this suite.
Mar  2 23:38:02.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:38:03.938: INFO: namespace: e2e-tests-services-c2zp4, resource: packagemanifests, items remaining: 1
Mar  2 23:38:04.179: INFO: namespace: e2e-tests-services-c2zp4, resource: bindings, ignored listing per whitelist
Mar  2 23:38:05.117: INFO: namespace: e2e-tests-services-c2zp4 no longer exists
Mar  2 23:38:05.136: INFO: namespace: e2e-tests-services-c2zp4, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:38:05.153: INFO: namespace e2e-tests-services-c2zp4 deletion completed in 10.531277706s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:31.811 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:38:05.154: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  2 23:38:06.207: INFO: namespace e2e-tests-kubectl-7hz9g
Mar  2 23:38:06.207: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7hz9g'
Mar  2 23:38:06.733: INFO: stderr: ""
Mar  2 23:38:06.733: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  2 23:38:07.753: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:07.753: INFO: Found 0 / 1
Mar  2 23:38:08.755: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:08.755: INFO: Found 0 / 1
Mar  2 23:38:09.753: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:09.753: INFO: Found 0 / 1
Mar  2 23:38:10.752: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:10.752: INFO: Found 0 / 1
Mar  2 23:38:11.752: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:11.752: INFO: Found 0 / 1
Mar  2 23:38:12.751: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:12.751: INFO: Found 0 / 1
Mar  2 23:38:13.751: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:13.751: INFO: Found 0 / 1
Mar  2 23:38:14.753: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:14.753: INFO: Found 0 / 1
Mar  2 23:38:15.752: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:15.752: INFO: Found 1 / 1
Mar  2 23:38:15.752: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 23:38:15.770: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 23:38:15.770: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 23:38:15.770: INFO: wait on redis-master startup in e2e-tests-kubectl-7hz9g 
Mar  2 23:38:15.770: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-fxjfn redis-master --namespace=e2e-tests-kubectl-7hz9g'
Mar  2 23:38:16.018: INFO: stderr: ""
Mar  2 23:38:16.018: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Mar 23:38:14.713 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Mar 23:38:14.713 # Server started, Redis version 3.2.12\n1:M 02 Mar 23:38:14.713 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Mar 23:38:14.713 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  2 23:38:16.018: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-7hz9g'
Mar  2 23:38:16.235: INFO: stderr: ""
Mar  2 23:38:16.235: INFO: stdout: "service/rm2 exposed\n"
Mar  2 23:38:16.252: INFO: Service rm2 in namespace e2e-tests-kubectl-7hz9g found.
STEP: exposing service
Mar  2 23:38:18.288: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-7hz9g'
Mar  2 23:38:18.501: INFO: stderr: ""
Mar  2 23:38:18.501: INFO: stdout: "service/rm3 exposed\n"
Mar  2 23:38:18.519: INFO: Service rm3 in namespace e2e-tests-kubectl-7hz9g found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:38:20.554: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7hz9g" for this suite.
Mar  2 23:38:44.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:38:46.286: INFO: namespace: e2e-tests-kubectl-7hz9g, resource: bindings, ignored listing per whitelist
Mar  2 23:38:46.413: INFO: namespace: e2e-tests-kubectl-7hz9g, resource: packagemanifests, items remaining: 1
Mar  2 23:38:47.092: INFO: namespace: e2e-tests-kubectl-7hz9g no longer exists
Mar  2 23:38:47.110: INFO: namespace: e2e-tests-kubectl-7hz9g, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:38:47.127: INFO: namespace e2e-tests-kubectl-7hz9g deletion completed in 26.528610817s

• [SLOW TEST:41.974 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:38:47.128: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  2 23:38:48.192: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:48.687: INFO: stderr: ""
Mar  2 23:38:48.687: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:38:48.687: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:48.861: INFO: stderr: ""
Mar  2 23:38:48.861: INFO: stdout: "update-demo-nautilus-d2z7z update-demo-nautilus-d4dhw "
Mar  2 23:38:48.861: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d2z7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:49.038: INFO: stderr: ""
Mar  2 23:38:49.038: INFO: stdout: ""
Mar  2 23:38:49.038: INFO: update-demo-nautilus-d2z7z is created but not running
Mar  2 23:38:54.039: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:54.213: INFO: stderr: ""
Mar  2 23:38:54.213: INFO: stdout: "update-demo-nautilus-d2z7z update-demo-nautilus-d4dhw "
Mar  2 23:38:54.213: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d2z7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:54.387: INFO: stderr: ""
Mar  2 23:38:54.387: INFO: stdout: ""
Mar  2 23:38:54.387: INFO: update-demo-nautilus-d2z7z is created but not running
Mar  2 23:38:59.387: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:59.566: INFO: stderr: ""
Mar  2 23:38:59.566: INFO: stdout: "update-demo-nautilus-d2z7z update-demo-nautilus-d4dhw "
Mar  2 23:38:59.567: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d2z7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:59.736: INFO: stderr: ""
Mar  2 23:38:59.736: INFO: stdout: "true"
Mar  2 23:38:59.736: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d2z7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:38:59.903: INFO: stderr: ""
Mar  2 23:38:59.903: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:38:59.904: INFO: validating pod update-demo-nautilus-d2z7z
Mar  2 23:38:59.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:38:59.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:38:59.928: INFO: update-demo-nautilus-d2z7z is verified up and running
Mar  2 23:38:59.928: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:00.099: INFO: stderr: ""
Mar  2 23:39:00.099: INFO: stdout: "true"
Mar  2 23:39:00.100: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:00.272: INFO: stderr: ""
Mar  2 23:39:00.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:00.272: INFO: validating pod update-demo-nautilus-d4dhw
Mar  2 23:39:00.293: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:00.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:00.293: INFO: update-demo-nautilus-d4dhw is verified up and running
STEP: scaling down the replication controller
Mar  2 23:39:00.296: INFO: scanned /tmp/home for discovery docs: <nil>
Mar  2 23:39:00.296: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:01.582: INFO: stderr: ""
Mar  2 23:39:01.582: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:39:01.582: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:01.754: INFO: stderr: ""
Mar  2 23:39:01.754: INFO: stdout: "update-demo-nautilus-d2z7z update-demo-nautilus-d4dhw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 23:39:06.754: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:06.965: INFO: stderr: ""
Mar  2 23:39:06.965: INFO: stdout: "update-demo-nautilus-d4dhw "
Mar  2 23:39:06.965: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:07.138: INFO: stderr: ""
Mar  2 23:39:07.138: INFO: stdout: "true"
Mar  2 23:39:07.138: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:07.311: INFO: stderr: ""
Mar  2 23:39:07.311: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:07.311: INFO: validating pod update-demo-nautilus-d4dhw
Mar  2 23:39:07.330: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:07.330: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:07.330: INFO: update-demo-nautilus-d4dhw is verified up and running
STEP: scaling up the replication controller
Mar  2 23:39:07.333: INFO: scanned /tmp/home for discovery docs: <nil>
Mar  2 23:39:07.333: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:08.612: INFO: stderr: ""
Mar  2 23:39:08.612: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 23:39:08.612: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:08.796: INFO: stderr: ""
Mar  2 23:39:08.796: INFO: stdout: "update-demo-nautilus-d4dhw update-demo-nautilus-r26w9 "
Mar  2 23:39:08.796: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:08.966: INFO: stderr: ""
Mar  2 23:39:08.966: INFO: stdout: "true"
Mar  2 23:39:08.966: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:09.139: INFO: stderr: ""
Mar  2 23:39:09.139: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:09.139: INFO: validating pod update-demo-nautilus-d4dhw
Mar  2 23:39:09.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:09.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:09.157: INFO: update-demo-nautilus-d4dhw is verified up and running
Mar  2 23:39:09.158: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r26w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:09.327: INFO: stderr: ""
Mar  2 23:39:09.327: INFO: stdout: ""
Mar  2 23:39:09.327: INFO: update-demo-nautilus-r26w9 is created but not running
Mar  2 23:39:14.328: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:14.507: INFO: stderr: ""
Mar  2 23:39:14.507: INFO: stdout: "update-demo-nautilus-d4dhw update-demo-nautilus-r26w9 "
Mar  2 23:39:14.507: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:14.677: INFO: stderr: ""
Mar  2 23:39:14.677: INFO: stdout: "true"
Mar  2 23:39:14.677: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:14.849: INFO: stderr: ""
Mar  2 23:39:14.849: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:14.849: INFO: validating pod update-demo-nautilus-d4dhw
Mar  2 23:39:14.870: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:14.870: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:14.870: INFO: update-demo-nautilus-d4dhw is verified up and running
Mar  2 23:39:14.870: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r26w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:15.043: INFO: stderr: ""
Mar  2 23:39:15.043: INFO: stdout: ""
Mar  2 23:39:15.043: INFO: update-demo-nautilus-r26w9 is created but not running
Mar  2 23:39:20.043: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:20.222: INFO: stderr: ""
Mar  2 23:39:20.222: INFO: stdout: "update-demo-nautilus-d4dhw update-demo-nautilus-r26w9 "
Mar  2 23:39:20.222: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:20.404: INFO: stderr: ""
Mar  2 23:39:20.404: INFO: stdout: "true"
Mar  2 23:39:20.404: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-d4dhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:20.578: INFO: stderr: ""
Mar  2 23:39:20.578: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:20.578: INFO: validating pod update-demo-nautilus-d4dhw
Mar  2 23:39:20.596: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:20.596: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:20.596: INFO: update-demo-nautilus-d4dhw is verified up and running
Mar  2 23:39:20.596: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r26w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:20.766: INFO: stderr: ""
Mar  2 23:39:20.767: INFO: stdout: "true"
Mar  2 23:39:20.767: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r26w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:20.939: INFO: stderr: ""
Mar  2 23:39:20.939: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 23:39:20.939: INFO: validating pod update-demo-nautilus-r26w9
Mar  2 23:39:20.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 23:39:20.963: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 23:39:20.963: INFO: update-demo-nautilus-r26w9 is verified up and running
STEP: using delete to clean up resources
Mar  2 23:39:20.963: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:21.160: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 23:39:21.160: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 23:39:21.160: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-5wj9q'
Mar  2 23:39:21.349: INFO: stderr: "No resources found.\n"
Mar  2 23:39:21.349: INFO: stdout: ""
Mar  2 23:39:21.349: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-5wj9q -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 23:39:21.529: INFO: stderr: ""
Mar  2 23:39:21.529: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:39:21.529: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5wj9q" for this suite.
Mar  2 23:39:29.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:39:31.254: INFO: namespace: e2e-tests-kubectl-5wj9q, resource: packagemanifests, items remaining: 1
Mar  2 23:39:31.449: INFO: namespace: e2e-tests-kubectl-5wj9q, resource: bindings, ignored listing per whitelist
Mar  2 23:39:32.086: INFO: namespace: e2e-tests-kubectl-5wj9q no longer exists
Mar  2 23:39:32.104: INFO: namespace: e2e-tests-kubectl-5wj9q, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:39:32.122: INFO: namespace e2e-tests-kubectl-5wj9q deletion completed in 10.536922036s

• [SLOW TEST:44.995 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:39:32.122: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-lm84l
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-lm84l
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-lm84l
Mar  2 23:39:33.263: INFO: Found 0 stateful pods, waiting for 1
Mar  2 23:39:43.283: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 23:39:43.302: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:39:43.683: INFO: stderr: ""
Mar  2 23:39:43.683: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:39:43.683: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 23:39:43.701: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 23:39:53.719: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:39:53.719: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:39:53.791: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999533s
Mar  2 23:39:54.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98241847s
Mar  2 23:39:55.828: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.964462994s
Mar  2 23:39:56.847: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.945713315s
Mar  2 23:39:57.865: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.926948583s
Mar  2 23:39:58.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.908832513s
Mar  2 23:39:59.903: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.889605834s
Mar  2 23:40:00.921: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.871195446s
Mar  2 23:40:01.941: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.852943474s
Mar  2 23:40:02.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 833.00824ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-lm84l
Mar  2 23:40:03.983: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:40:04.438: INFO: stderr: ""
Mar  2 23:40:04.438: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:40:04.438: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:40:04.457: INFO: Found 1 stateful pods, waiting for 3
Mar  2 23:40:14.476: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:40:24.476: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:40:24.476: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:40:24.476: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  2 23:40:24.509: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:40:24.893: INFO: stderr: ""
Mar  2 23:40:24.893: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:40:24.893: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 23:40:24.893: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:40:25.277: INFO: stderr: ""
Mar  2 23:40:25.278: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:40:25.278: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 23:40:25.278: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:40:25.658: INFO: stderr: ""
Mar  2 23:40:25.658: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:40:25.658: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 23:40:25.658: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:40:25.677: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  2 23:40:35.714: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:40:35.714: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:40:35.714: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 23:40:35.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998958s
Mar  2 23:40:36.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981360952s
Mar  2 23:40:37.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962872513s
Mar  2 23:40:38.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943906231s
Mar  2 23:40:39.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925351216s
Mar  2 23:40:40.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.906898585s
Mar  2 23:40:41.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.888768785s
Mar  2 23:40:42.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.870265697s
Mar  2 23:40:43.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.851403065s
Mar  2 23:40:44.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 832.747906ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-lm84l
Mar  2 23:40:45.955: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:40:46.323: INFO: stderr: ""
Mar  2 23:40:46.323: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:40:46.323: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:40:46.323: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:40:46.716: INFO: stderr: ""
Mar  2 23:40:46.716: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:40:46.716: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:40:46.716: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-lm84l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:40:47.087: INFO: stderr: ""
Mar  2 23:40:47.087: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:40:47.087: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:40:47.087: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  2 23:41:07.165: INFO: Deleting all statefulset in ns e2e-tests-statefulset-lm84l
Mar  2 23:41:07.191: INFO: Scaling statefulset ss to 0
Mar  2 23:41:07.243: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:41:07.260: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:41:07.322: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-lm84l" for this suite.
Mar  2 23:41:15.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:41:16.968: INFO: namespace: e2e-tests-statefulset-lm84l, resource: bindings, ignored listing per whitelist
Mar  2 23:41:17.196: INFO: namespace: e2e-tests-statefulset-lm84l, resource: packagemanifests, items remaining: 1
Mar  2 23:41:17.856: INFO: namespace: e2e-tests-statefulset-lm84l no longer exists
Mar  2 23:41:17.875: INFO: namespace: e2e-tests-statefulset-lm84l, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:41:17.892: INFO: namespace e2e-tests-statefulset-lm84l deletion completed in 10.525108648s

• [SLOW TEST:105.770 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:41:17.892: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:41:18.941: INFO: Creating ReplicaSet my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353
Mar  2 23:41:18.994: INFO: Pod name my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353: Found 0 pods out of 1
Mar  2 23:41:24.012: INFO: Pod name my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353: Found 1 pods out of 1
Mar  2 23:41:24.012: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353" is running
Mar  2 23:41:28.050: INFO: Pod "my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353-hhw6z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-02 23:41:19 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-02 23:41:19 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-02 23:41:19 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-02 23:41:18 +0000 UTC Reason: Message:}])
Mar  2 23:41:28.050: INFO: Trying to dial the pod
Mar  2 23:41:33.109: INFO: Controller my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353: Got expected result from replica 1 [my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353-hhw6z]: "my-hostname-basic-ad7fce66-3d44-11e9-9008-0a58ac10f353-hhw6z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:41:33.109: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-qcnqc" for this suite.
Mar  2 23:41:39.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:41:40.238: INFO: namespace: e2e-tests-replicaset-qcnqc, resource: bindings, ignored listing per whitelist
Mar  2 23:41:40.830: INFO: namespace: e2e-tests-replicaset-qcnqc, resource: packagemanifests, items remaining: 1
Mar  2 23:41:41.644: INFO: namespace: e2e-tests-replicaset-qcnqc no longer exists
Mar  2 23:41:41.661: INFO: namespace: e2e-tests-replicaset-qcnqc, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:41:41.678: INFO: namespace e2e-tests-replicaset-qcnqc deletion completed in 8.52383684s

• [SLOW TEST:23.786 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:41:41.678: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar  2 23:41:42.730: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix428534372/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:41:42.809: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tk2w8" for this suite.
Mar  2 23:41:48.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:41:49.957: INFO: namespace: e2e-tests-kubectl-tk2w8, resource: packagemanifests, items remaining: 1
Mar  2 23:41:50.237: INFO: namespace: e2e-tests-kubectl-tk2w8, resource: bindings, ignored listing per whitelist
Mar  2 23:41:51.345: INFO: namespace: e2e-tests-kubectl-tk2w8 no longer exists
Mar  2 23:41:51.363: INFO: namespace: e2e-tests-kubectl-tk2w8, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:41:51.380: INFO: namespace e2e-tests-kubectl-tk2w8 deletion completed in 8.538290745s

• [SLOW TEST:9.702 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:41:51.381: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:42:52.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-dlgrf" for this suite.
Mar  2 23:43:16.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:43:18.563: INFO: namespace: e2e-tests-container-probe-dlgrf, resource: bindings, ignored listing per whitelist
Mar  2 23:43:18.979: INFO: namespace: e2e-tests-container-probe-dlgrf, resource: packagemanifests, items remaining: 1
Mar  2 23:43:19.661: INFO: namespace: e2e-tests-container-probe-dlgrf no longer exists
Mar  2 23:43:19.693: INFO: namespace: e2e-tests-container-probe-dlgrf, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:43:19.710: INFO: namespace e2e-tests-container-probe-dlgrf deletion completed in 27.121747203s

• [SLOW TEST:88.330 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:43:19.711: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 23:43:31.063387   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 23:43:31.063: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:43:31.063: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-lb9w8" for this suite.
Mar  2 23:43:39.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:43:40.915: INFO: namespace: e2e-tests-gc-lb9w8, resource: packagemanifests, items remaining: 1
Mar  2 23:43:41.232: INFO: namespace: e2e-tests-gc-lb9w8, resource: bindings, ignored listing per whitelist
Mar  2 23:43:41.582: INFO: namespace: e2e-tests-gc-lb9w8 no longer exists
Mar  2 23:43:41.601: INFO: namespace: e2e-tests-gc-lb9w8, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:43:41.618: INFO: namespace e2e-tests-gc-lb9w8 deletion completed in 10.524013592s

• [SLOW TEST:21.908 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:43:41.619: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-7h7dz
Mar  2 23:43:50.722: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-7h7dz
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 23:43:50.739: INFO: Initial restart count of pod liveness-http is 0
Mar  2 23:44:08.919: INFO: Restart count of pod e2e-tests-container-probe-7h7dz/liveness-http is now 1 (18.180191819s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:44:08.948: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-7h7dz" for this suite.
Mar  2 23:44:15.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:44:15.882: INFO: namespace: e2e-tests-container-probe-7h7dz, resource: packagemanifests, items remaining: 1
Mar  2 23:44:16.513: INFO: namespace: e2e-tests-container-probe-7h7dz, resource: bindings, ignored listing per whitelist
Mar  2 23:44:17.482: INFO: namespace: e2e-tests-container-probe-7h7dz no longer exists
Mar  2 23:44:17.501: INFO: namespace: e2e-tests-container-probe-7h7dz, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:44:17.518: INFO: namespace e2e-tests-container-probe-7h7dz deletion completed in 8.525172041s

• [SLOW TEST:35.899 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:44:17.518: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 23:44:18.599: INFO: Waiting up to 5m0s for pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-5lxz2" to be "success or failure"
Mar  2 23:44:18.619: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.095625ms
Mar  2 23:44:20.637: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037458889s
Mar  2 23:44:22.655: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055598522s
Mar  2 23:44:24.675: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075505197s
Mar  2 23:44:26.694: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094190322s
Mar  2 23:44:28.712: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112778645s
STEP: Saw pod success
Mar  2 23:44:28.712: INFO: Pod "pod-188df1c5-3d45-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:44:28.730: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-188df1c5-3d45-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:44:28.791: INFO: Waiting for pod pod-188df1c5-3d45-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:44:28.808: INFO: Pod pod-188df1c5-3d45-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:44:28.809: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5lxz2" for this suite.
Mar  2 23:44:34.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:44:36.154: INFO: namespace: e2e-tests-emptydir-5lxz2, resource: packagemanifests, items remaining: 1
Mar  2 23:44:36.726: INFO: namespace: e2e-tests-emptydir-5lxz2, resource: bindings, ignored listing per whitelist
Mar  2 23:44:37.343: INFO: namespace: e2e-tests-emptydir-5lxz2 no longer exists
Mar  2 23:44:37.361: INFO: namespace: e2e-tests-emptydir-5lxz2, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:44:37.378: INFO: namespace e2e-tests-emptydir-5lxz2 deletion completed in 8.525395471s

• [SLOW TEST:19.861 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:44:37.379: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 23:44:49.065: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2464c7fd-3d45-11e9-9008-0a58ac10f353"
Mar  2 23:44:49.065: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2464c7fd-3d45-11e9-9008-0a58ac10f353" in namespace "e2e-tests-pods-mjxhw" to be "terminated due to deadline exceeded"
Mar  2 23:44:49.090: INFO: Pod "pod-update-activedeadlineseconds-2464c7fd-3d45-11e9-9008-0a58ac10f353": Phase="Running", Reason="", readiness=true. Elapsed: 24.654147ms
Mar  2 23:44:51.107: INFO: Pod "pod-update-activedeadlineseconds-2464c7fd-3d45-11e9-9008-0a58ac10f353": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.042316268s
Mar  2 23:44:51.107: INFO: Pod "pod-update-activedeadlineseconds-2464c7fd-3d45-11e9-9008-0a58ac10f353" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:44:51.108: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mjxhw" for this suite.
Mar  2 23:44:57.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:44:58.947: INFO: namespace: e2e-tests-pods-mjxhw, resource: bindings, ignored listing per whitelist
Mar  2 23:44:59.177: INFO: namespace: e2e-tests-pods-mjxhw, resource: packagemanifests, items remaining: 1
Mar  2 23:44:59.643: INFO: namespace: e2e-tests-pods-mjxhw no longer exists
Mar  2 23:44:59.661: INFO: namespace: e2e-tests-pods-mjxhw, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:44:59.677: INFO: namespace e2e-tests-pods-mjxhw deletion completed in 8.525387924s

• [SLOW TEST:22.299 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:44:59.678: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  2 23:45:00.846: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-zz4tk,SelfLink:/api/v1/namespaces/e2e-tests-watch-zz4tk/configmaps/e2e-watch-test-watch-closed,UID:31bc0e0b-3d45-11e9-b620-0a843850fdce,ResourceVersion:84562,Generation:0,CreationTimestamp:2019-03-02 23:45:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 23:45:00.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-zz4tk,SelfLink:/api/v1/namespaces/e2e-tests-watch-zz4tk/configmaps/e2e-watch-test-watch-closed,UID:31bc0e0b-3d45-11e9-b620-0a843850fdce,ResourceVersion:84563,Generation:0,CreationTimestamp:2019-03-02 23:45:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 23:45:00.924: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-zz4tk,SelfLink:/api/v1/namespaces/e2e-tests-watch-zz4tk/configmaps/e2e-watch-test-watch-closed,UID:31bc0e0b-3d45-11e9-b620-0a843850fdce,ResourceVersion:84565,Generation:0,CreationTimestamp:2019-03-02 23:45:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 23:45:00.924: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-zz4tk,SelfLink:/api/v1/namespaces/e2e-tests-watch-zz4tk/configmaps/e2e-watch-test-watch-closed,UID:31bc0e0b-3d45-11e9-b620-0a843850fdce,ResourceVersion:84566,Generation:0,CreationTimestamp:2019-03-02 23:45:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:45:00.925: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-zz4tk" for this suite.
Mar  2 23:45:07.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:45:07.812: INFO: namespace: e2e-tests-watch-zz4tk, resource: bindings, ignored listing per whitelist
Mar  2 23:45:09.380: INFO: namespace: e2e-tests-watch-zz4tk, resource: packagemanifests, items remaining: 1
Mar  2 23:45:09.450: INFO: namespace: e2e-tests-watch-zz4tk no longer exists
Mar  2 23:45:09.470: INFO: namespace: e2e-tests-watch-zz4tk, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:45:09.487: INFO: namespace e2e-tests-watch-zz4tk deletion completed in 8.530697727s

• [SLOW TEST:9.810 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:45:09.488: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-s59h
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 23:45:10.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s59h" in namespace "e2e-tests-subpath-7fzxl" to be "success or failure"
Mar  2 23:45:10.621: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 17.522422ms
Mar  2 23:45:12.639: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035351444s
Mar  2 23:45:14.657: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053226667s
Mar  2 23:45:16.675: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070935979s
Mar  2 23:45:18.693: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088770857s
Mar  2 23:45:20.710: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Pending", Reason="", readiness=false. Elapsed: 10.106597425s
Mar  2 23:45:22.729: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 12.124892297s
Mar  2 23:45:24.747: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 14.14304392s
Mar  2 23:45:26.765: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 16.16097887s
Mar  2 23:45:28.783: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 18.17906833s
Mar  2 23:45:30.803: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 20.198628632s
Mar  2 23:45:32.820: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 22.216558882s
Mar  2 23:45:34.839: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 24.234606121s
Mar  2 23:45:36.857: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 26.25294405s
Mar  2 23:45:38.875: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 28.270694814s
Mar  2 23:45:40.893: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Running", Reason="", readiness=false. Elapsed: 30.288743146s
Mar  2 23:45:42.910: INFO: Pod "pod-subpath-test-downwardapi-s59h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.306412517s
STEP: Saw pod success
Mar  2 23:45:42.910: INFO: Pod "pod-subpath-test-downwardapi-s59h" satisfied condition "success or failure"
Mar  2 23:45:42.928: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-subpath-test-downwardapi-s59h container test-container-subpath-downwardapi-s59h: <nil>
STEP: delete the pod
Mar  2 23:45:42.984: INFO: Waiting for pod pod-subpath-test-downwardapi-s59h to disappear
Mar  2 23:45:43.001: INFO: Pod pod-subpath-test-downwardapi-s59h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-s59h
Mar  2 23:45:43.001: INFO: Deleting pod "pod-subpath-test-downwardapi-s59h" in namespace "e2e-tests-subpath-7fzxl"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:45:43.019: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7fzxl" for this suite.
Mar  2 23:45:49.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:45:50.130: INFO: namespace: e2e-tests-subpath-7fzxl, resource: bindings, ignored listing per whitelist
Mar  2 23:45:51.129: INFO: namespace: e2e-tests-subpath-7fzxl, resource: packagemanifests, items remaining: 1
Mar  2 23:45:51.553: INFO: namespace: e2e-tests-subpath-7fzxl no longer exists
Mar  2 23:45:51.571: INFO: namespace: e2e-tests-subpath-7fzxl, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:45:51.589: INFO: namespace e2e-tests-subpath-7fzxl deletion completed in 8.525440667s

• [SLOW TEST:42.101 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:45:51.589: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-509f2603-3d45-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:45:52.684: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-ntjpj" to be "success or failure"
Mar  2 23:45:52.709: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 24.735422ms
Mar  2 23:45:54.729: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045072078s
Mar  2 23:45:56.747: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063035518s
Mar  2 23:45:58.765: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080538409s
Mar  2 23:46:00.783: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09848441s
Mar  2 23:46:02.801: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.116481153s
STEP: Saw pod success
Mar  2 23:46:02.801: INFO: Pod "pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:46:02.820: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 23:46:02.879: INFO: Waiting for pod pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:46:02.896: INFO: Pod pod-projected-secrets-50a38278-3d45-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:46:02.896: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ntjpj" for this suite.
Mar  2 23:46:08.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:46:10.549: INFO: namespace: e2e-tests-projected-ntjpj, resource: bindings, ignored listing per whitelist
Mar  2 23:46:11.165: INFO: namespace: e2e-tests-projected-ntjpj, resource: packagemanifests, items remaining: 1
Mar  2 23:46:11.431: INFO: namespace: e2e-tests-projected-ntjpj no longer exists
Mar  2 23:46:11.450: INFO: namespace: e2e-tests-projected-ntjpj, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:46:11.467: INFO: namespace e2e-tests-projected-ntjpj deletion completed in 8.526338748s

• [SLOW TEST:19.878 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:46:11.467: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  2 23:46:23.169: INFO: Successfully updated pod "labelsupdate5c79b0b0-3d45-11e9-9008-0a58ac10f353"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:46:27.242: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8865k" for this suite.
Mar  2 23:46:51.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:46:52.304: INFO: namespace: e2e-tests-downward-api-8865k, resource: packagemanifests, items remaining: 1
Mar  2 23:46:52.948: INFO: namespace: e2e-tests-downward-api-8865k, resource: bindings, ignored listing per whitelist
Mar  2 23:46:53.776: INFO: namespace: e2e-tests-downward-api-8865k no longer exists
Mar  2 23:46:53.794: INFO: namespace: e2e-tests-downward-api-8865k, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:46:53.812: INFO: namespace e2e-tests-downward-api-8865k deletion completed in 26.525451118s

• [SLOW TEST:42.345 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:46:53.812: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:46:54.945: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 23:46:55.003: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:55.003: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:55.003: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:46:55.027: INFO: Number of nodes with available pods: 0
Mar  2 23:46:55.027: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:46:56.073: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:56.073: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:56.073: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:46:56.090: INFO: Number of nodes with available pods: 0
Mar  2 23:46:56.090: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:46:57.071: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:57.071: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:57.071: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:46:57.091: INFO: Number of nodes with available pods: 0
Mar  2 23:46:57.091: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:46:58.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:58.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:58.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:46:58.090: INFO: Number of nodes with available pods: 0
Mar  2 23:46:58.090: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:46:59.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:59.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:46:59.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:46:59.089: INFO: Number of nodes with available pods: 0
Mar  2 23:46:59.089: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:47:00.073: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:00.073: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:00.073: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:00.090: INFO: Number of nodes with available pods: 0
Mar  2 23:47:00.091: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:47:01.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:01.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:01.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:01.090: INFO: Number of nodes with available pods: 0
Mar  2 23:47:01.091: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:47:02.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:02.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:02.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:02.093: INFO: Number of nodes with available pods: 0
Mar  2 23:47:02.093: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:47:03.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:03.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:03.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:03.090: INFO: Number of nodes with available pods: 0
Mar  2 23:47:03.090: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  2 23:47:04.072: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:04.072: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:04.072: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:04.089: INFO: Number of nodes with available pods: 3
Mar  2 23:47:04.089: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  2 23:47:04.202: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:04.202: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:04.202: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:04.234: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:04.234: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:04.234: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:05.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:05.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:05.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:05.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:05.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:05.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:06.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:06.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:06.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:07.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:07.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:07.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:07.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:07.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:07.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:08.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:08.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:08.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:08.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:08.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:08.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:09.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:09.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:09.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:10.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:10.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:10.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:10.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:10.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:10.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:11.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:11.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:11.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:11.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:11.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:11.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:12.255: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:12.255: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:12.255: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:12.287: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:12.287: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:12.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:13.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:13.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:13.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:14.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:14.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:14.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:15.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:15.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:15.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:15.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:15.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:15.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:16.254: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:16.254: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:16.254: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:16.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:16.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:16.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:17.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:17.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:17.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:17.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:17.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:17.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:18.254: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:18.254: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:18.254: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:18.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:18.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:18.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:19.269: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:19.269: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:19.269: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:19.300: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:19.300: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:19.300: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:20.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:20.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:20.252: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:20.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:20.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:20.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:21.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:21.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:21.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:21.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:21.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:21.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:22.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:22.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:22.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:23.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:23.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:23.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:24.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:24.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:24.252: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:24.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:24.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:24.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:25.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:25.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:25.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:25.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:25.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:25.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:26.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:26.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:26.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:26.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:26.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:26.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:27.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:27.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:27.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:28.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:28.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:28.252: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:28.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:28.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:28.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:29.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:29.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:29.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:29.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:29.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:29.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:30.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:30.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:30.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:30.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:30.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:30.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:31.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:31.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:31.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:32.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:32.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:32.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:32.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:32.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:32.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:33.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:33.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:33.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:33.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:33.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:33.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:34.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:34.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:34.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:35.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:35.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:35.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:35.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:35.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:35.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:36.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:36.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:36.253: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:36.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:36.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:36.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:37.254: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:37.254: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:37.254: INFO: Wrong image for pod: daemon-set-n8mfw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:37.254: INFO: Pod daemon-set-n8mfw is not available
Mar  2 23:47:37.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:37.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:37.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:38.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:38.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:38.253: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:39.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:39.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:39.253: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:40.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:40.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:40.252: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:40.283: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:40.283: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:40.283: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:41.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:41.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:41.253: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:42.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:42.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:42.252: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:42.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:42.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:42.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:43.257: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:43.257: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:43.257: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:43.293: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:43.293: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:43.293: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:44.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:44.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:44.253: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:44.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:44.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:44.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:45.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:45.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:45.252: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:45.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:45.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:45.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:46.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:46.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:46.253: INFO: Pod daemon-set-xlbs8 is not available
Mar  2 23:47:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:47.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:47.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:48.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:48.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:48.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:48.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:48.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:49.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:49.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:49.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:49.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:49.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:50.259: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:50.259: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:51.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:51.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:52.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:52.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:52.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:52.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:52.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:53.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:53.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:53.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:53.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:53.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:54.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:54.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:54.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:54.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:54.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:55.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:55.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:55.289: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:55.289: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:55.289: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:56.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:56.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:56.287: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:56.287: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:56.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:57.254: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:57.254: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:57.287: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:57.287: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:57.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:58.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:58.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:58.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:58.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:58.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:47:59.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:59.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:47:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:47:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:00.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:00.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:00.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:00.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:00.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:01.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:01.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:02.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:02.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:02.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:02.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:02.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:03.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:03.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:03.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:03.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:03.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:04.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:04.252: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:04.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:04.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:04.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:05.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:05.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:05.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:05.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:05.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:06.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:06.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:06.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:07.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:07.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:07.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:07.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:07.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:08.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:08.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:08.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:08.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:08.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:09.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:09.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:10.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:10.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:10.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:10.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:10.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:11.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:11.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:11.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:11.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:11.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:12.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:12.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:12.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:12.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:12.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:13.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:13.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:13.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:14.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:14.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:14.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:15.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:15.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:15.287: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:15.287: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:15.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:16.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:16.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:16.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:16.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:16.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:17.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:17.253: INFO: Wrong image for pod: daemon-set-lrb84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:17.253: INFO: Pod daemon-set-lrb84 is not available
Mar  2 23:48:17.287: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:17.287: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:17.287: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:18.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:18.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:18.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:18.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:18.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:19.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:19.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:19.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:19.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:19.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:20.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:20.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:20.292: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:20.292: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:20.292: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:21.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:21.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:21.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:21.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:21.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:22.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:22.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:22.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:23.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:23.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:23.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:24.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:24.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:24.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:24.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:24.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:25.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:25.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:25.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:25.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:25.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:26.253: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:26.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:26.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:26.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:26.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:27.252: INFO: Pod daemon-set-2n76z is not available
Mar  2 23:48:27.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:27.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:28.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:28.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:28.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:28.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:29.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:29.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:29.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:29.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:30.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:30.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:30.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:30.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:31.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:31.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:32.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:32.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:32.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:32.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:33.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:33.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:33.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:33.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:34.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:34.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:35.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:35.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:35.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:35.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:36.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:36.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:36.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:36.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:37.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:37.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:37.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:37.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:38.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:38.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:39.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:39.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:40.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:40.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:40.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:40.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:41.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:41.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:42.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:42.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:42.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:42.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:43.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:43.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:43.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:43.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:44.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:44.286: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:44.286: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:44.286: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:45.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:45.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:45.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:45.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:46.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:46.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:47.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:47.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:48.254: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:48.289: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:48.289: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:48.289: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:49.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:49.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:49.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:49.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:50.258: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:50.291: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:51.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:51.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:52.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:52.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:52.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:52.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:53.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:53.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:53.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:53.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:54.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:54.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:54.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:54.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:55.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:55.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:55.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:55.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:56.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:56.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:56.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:56.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:57.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:57.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:57.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:57.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:58.252: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:58.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:58.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:58.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:48:59.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:48:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:48:59.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:00.253: INFO: Wrong image for pod: daemon-set-4q9wt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  2 23:49:00.253: INFO: Pod daemon-set-4q9wt is not available
Mar  2 23:49:00.285: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:00.285: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:00.285: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:01.252: INFO: Pod daemon-set-wtg42 is not available
Mar  2 23:49:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:01.284: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  2 23:49:01.315: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:01.316: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:01.316: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:01.333: INFO: Number of nodes with available pods: 2
Mar  2 23:49:01.333: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:02.379: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:02.379: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:02.379: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:02.396: INFO: Number of nodes with available pods: 2
Mar  2 23:49:02.396: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:03.379: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:03.379: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:03.379: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:03.397: INFO: Number of nodes with available pods: 2
Mar  2 23:49:03.397: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:04.380: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:04.380: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:04.380: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:04.398: INFO: Number of nodes with available pods: 2
Mar  2 23:49:04.398: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:05.379: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:05.379: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:05.379: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:05.396: INFO: Number of nodes with available pods: 2
Mar  2 23:49:05.396: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:06.380: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:06.380: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:06.380: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:06.398: INFO: Number of nodes with available pods: 2
Mar  2 23:49:06.398: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:07.379: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:07.379: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:07.379: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:07.397: INFO: Number of nodes with available pods: 2
Mar  2 23:49:07.397: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:08.383: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:08.383: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:08.383: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:08.401: INFO: Number of nodes with available pods: 2
Mar  2 23:49:08.401: INFO: Node ip-10-0-160-190.ec2.internal is running more than one daemon pod
Mar  2 23:49:09.379: INFO: DaemonSet pods can't tolerate node ip-10-0-143-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:09.379: INFO: DaemonSet pods can't tolerate node ip-10-0-145-176.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:13 +0000 UTC}], skip checking this node
Mar  2 23:49:09.379: INFO: DaemonSet pods can't tolerate node ip-10-0-161-194.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-03-02 22:24:14 +0000 UTC}], skip checking this node
Mar  2 23:49:09.398: INFO: Number of nodes with available pods: 3
Mar  2 23:49:09.398: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-qsmrf, will wait for the garbage collector to delete the pods
Mar  2 23:49:09.586: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.499987ms
Mar  2 23:49:09.687: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.383278ms
Mar  2 23:49:12.706: INFO: Number of nodes with available pods: 0
Mar  2 23:49:12.706: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 23:49:12.723: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qsmrf/daemonsets","resourceVersion":"87650"},"items":null}

Mar  2 23:49:12.740: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qsmrf/pods","resourceVersion":"87650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:49:12.823: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qsmrf" for this suite.
Mar  2 23:49:20.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:49:21.782: INFO: namespace: e2e-tests-daemonsets-qsmrf, resource: packagemanifests, items remaining: 1
Mar  2 23:49:22.701: INFO: namespace: e2e-tests-daemonsets-qsmrf, resource: bindings, ignored listing per whitelist
Mar  2 23:49:23.347: INFO: namespace: e2e-tests-daemonsets-qsmrf no longer exists
Mar  2 23:49:23.365: INFO: namespace: e2e-tests-daemonsets-qsmrf, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:49:23.382: INFO: namespace e2e-tests-daemonsets-qsmrf deletion completed in 10.52771384s

• [SLOW TEST:149.570 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:49:23.382: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar  2 23:49:24.465: INFO: Waiting up to 5m0s for pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353" in namespace "e2e-tests-var-expansion-cdskr" to be "success or failure"
Mar  2 23:49:24.488: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 22.974118ms
Mar  2 23:49:26.507: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041476405s
Mar  2 23:49:28.525: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059749356s
Mar  2 23:49:30.543: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077482071s
Mar  2 23:49:32.561: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095326945s
Mar  2 23:49:34.579: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113631803s
STEP: Saw pod success
Mar  2 23:49:34.579: INFO: Pod "var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:49:34.597: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 23:49:34.651: INFO: Waiting for pod var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:49:34.669: INFO: Pod var-expansion-cede9eff-3d45-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:49:34.669: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-cdskr" for this suite.
Mar  2 23:49:40.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:49:41.674: INFO: namespace: e2e-tests-var-expansion-cdskr, resource: bindings, ignored listing per whitelist
Mar  2 23:49:42.210: INFO: namespace: e2e-tests-var-expansion-cdskr, resource: packagemanifests, items remaining: 1
Mar  2 23:49:43.205: INFO: namespace: e2e-tests-var-expansion-cdskr no longer exists
Mar  2 23:49:43.223: INFO: namespace: e2e-tests-var-expansion-cdskr, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:49:43.241: INFO: namespace e2e-tests-var-expansion-cdskr deletion completed in 8.527431117s

• [SLOW TEST:19.859 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:49:43.241: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-96fh7/configmap-test-dab4f65a-3d45-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:49:44.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-96fh7" to be "success or failure"
Mar  2 23:49:44.359: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.8594ms
Mar  2 23:49:46.377: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035782161s
Mar  2 23:49:48.395: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053921988s
Mar  2 23:49:50.413: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071741583s
Mar  2 23:49:52.431: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089860478s
Mar  2 23:49:54.449: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107896946s
STEP: Saw pod success
Mar  2 23:49:54.449: INFO: Pod "pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:49:54.467: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353 container env-test: <nil>
STEP: delete the pod
Mar  2 23:49:54.523: INFO: Waiting for pod pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:49:54.541: INFO: Pod pod-configmaps-dab8c3b0-3d45-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:49:54.541: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-96fh7" for this suite.
Mar  2 23:50:00.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:50:01.956: INFO: namespace: e2e-tests-configmap-96fh7, resource: bindings, ignored listing per whitelist
Mar  2 23:50:02.124: INFO: namespace: e2e-tests-configmap-96fh7, resource: packagemanifests, items remaining: 1
Mar  2 23:50:03.075: INFO: namespace: e2e-tests-configmap-96fh7 no longer exists
Mar  2 23:50:03.094: INFO: namespace: e2e-tests-configmap-96fh7, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:50:03.111: INFO: namespace e2e-tests-configmap-96fh7 deletion completed in 8.525184287s

• [SLOW TEST:19.870 seconds]
[sig-api-machinery] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:50:03.111: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-8ctzs
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-8ctzs
STEP: Deleting pre-stop pod
Mar  2 23:50:31.348: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:50:31.371: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-8ctzs" for this suite.
Mar  2 23:51:11.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:51:12.936: INFO: namespace: e2e-tests-prestop-8ctzs, resource: bindings, ignored listing per whitelist
Mar  2 23:51:13.418: INFO: namespace: e2e-tests-prestop-8ctzs, resource: packagemanifests, items remaining: 1
Mar  2 23:51:13.906: INFO: namespace: e2e-tests-prestop-8ctzs no longer exists
Mar  2 23:51:13.924: INFO: namespace: e2e-tests-prestop-8ctzs, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:51:13.941: INFO: namespace e2e-tests-prestop-8ctzs deletion completed in 42.526474786s

• [SLOW TEST:70.831 seconds]
[k8s.io] [sig-node] PreStop
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:51:13.942: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-92jtz;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-92jtz;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-92jtz.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-92jtz.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 29.237.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.237.29_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 29.237.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.237.29_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-92jtz;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-92jtz;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-92jtz.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-92jtz.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-92jtz.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-92jtz.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-92jtz.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-92jtz.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 29.237.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.237.29_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 29.237.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.237.29_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 23:51:45.672: INFO: DNS probes using e2e-tests-dns-92jtz/dns-test-10cead41-3d46-11e9-9008-0a58ac10f353 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:51:45.803: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-92jtz" for this suite.
Mar  2 23:51:53.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:51:55.700: INFO: namespace: e2e-tests-dns-92jtz, resource: packagemanifests, items remaining: 1
Mar  2 23:51:55.717: INFO: namespace: e2e-tests-dns-92jtz, resource: bindings, ignored listing per whitelist
Mar  2 23:51:56.325: INFO: namespace: e2e-tests-dns-92jtz no longer exists
Mar  2 23:51:56.343: INFO: namespace: e2e-tests-dns-92jtz, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:51:56.360: INFO: namespace e2e-tests-dns-92jtz deletion completed in 10.52577642s

• [SLOW TEST:42.419 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:51:56.360: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar  2 23:51:57.381: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 23:51:57.450: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 23:51:57.468: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-63.ec2.internal before test
Mar  2 23:51:57.519: INFO: kube-state-metrics-67bf67646d-c76bn from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 23:51:57.519: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 23:51:57.519: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 23:51:57.519: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 23:51:57.519: INFO: multus-7z6tf from openshift-multus started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.519: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:51:57.519: INFO: node-exporter-68xtl from openshift-monitoring started at 2019-03-02 22:17:22 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.519: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.519: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:51:57.519: INFO: router-default-74d6bbbf54-6fv8g from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container router ready: true, restart count 0
Mar  2 23:51:57.520: INFO: telemeter-client-6db8c847b7-grhl9 from openshift-monitoring started at 2019-03-02 22:16:56 +0000 UTC (3 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 	Container reload ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 23:51:57.520: INFO: certified-operators-76b9f9bdfb-9nrwb from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container certified-operators ready: true, restart count 0
Mar  2 23:51:57.520: INFO: community-operators-768f567b68-sqssn from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container community-operators ready: true, restart count 0
Mar  2 23:51:57.520: INFO: ovs-hwm6x from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:51:57.520: INFO: tuned-4jp7z from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:23 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:51:57.520: INFO: node-ca-d9zfb from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:51:57.520: INFO: router-default-74d6bbbf54-dsl6h from openshift-ingress started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container router ready: true, restart count 0
Mar  2 23:51:57.520: INFO: dns-default-hfm86 from openshift-dns started at 2019-03-02 22:17:23 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:51:57.520: INFO: machine-config-daemon-4snhh from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:51:57.520: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-03-02 22:20:07 +0000 UTC (3 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:51:57.520: INFO: sdn-p29xm from openshift-sdn started at 2019-03-02 22:17:22 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:51:57.520: INFO: redhat-operators-8484568578-xdtzk from openshift-marketplace started at 2019-03-02 22:16:56 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.520: INFO: 	Container redhat-operators ready: true, restart count 0
Mar  2 23:51:57.520: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-148-56.ec2.internal before test
Mar  2 23:51:57.558: INFO: sdn-5tbhh from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:51:57.558: INFO: tuned-j48rm from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:51:57.558: INFO: ovs-2njbk from openshift-sdn started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:51:57.558: INFO: node-exporter-b5gbl from openshift-monitoring started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.558: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:51:57.558: INFO: machine-config-daemon-kqwc8 from openshift-machine-config-operator started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:51:57.558: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-03-02 22:19:55 +0000 UTC (3 container statuses recorded)
Mar  2 23:51:57.558: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:51:57.558: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:51:57.558: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:51:57.558: INFO: multus-kpz5d from openshift-multus started at 2019-03-02 22:17:25 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:51:57.559: INFO: dns-default-blbmc from openshift-dns started at 2019-03-02 22:17:25 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:51:57.559: INFO: node-ca-54mqp from openshift-image-registry started at 2019-03-02 22:18:06 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:51:57.559: INFO: grafana-5c65588698-xtfnz from openshift-monitoring started at 2019-03-02 22:18:45 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container grafana ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 23:51:57.559: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:51:57.559: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 23:51:57.559: INFO: prometheus-adapter-7b89fc5f5-cb2hj from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.559: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:51:57.559: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-160-190.ec2.internal before test
Mar  2 23:51:57.596: INFO: multus-9466n from openshift-multus started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 23:51:57.596: INFO: ovs-xtgl4 from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 23:51:57.596: INFO: node-exporter-n6h9d from openshift-monitoring started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 23:51:57.596: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-03-02 22:18:48 +0000 UTC (6 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container prometheus ready: true, restart count 1
Mar  2 23:51:57.596: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 23:51:57.596: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (3 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 23:51:57.596: INFO: dns-default-2f5st from openshift-dns started at 2019-03-02 22:17:27 +0000 UTC (2 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container dns ready: true, restart count 0
Mar  2 23:51:57.596: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 23:51:57.596: INFO: tuned-vptgs from openshift-cluster-node-tuning-operator started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container tuned ready: true, restart count 0
Mar  2 23:51:57.596: INFO: machine-config-daemon-mlhcn from openshift-machine-config-operator started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 23:51:57.596: INFO: prometheus-adapter-7b89fc5f5-v7747 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 23:51:57.596: INFO: sdn-n87hs from openshift-sdn started at 2019-03-02 22:17:27 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container sdn ready: true, restart count 0
Mar  2 23:51:57.596: INFO: node-ca-brndt from openshift-image-registry started at 2019-03-02 22:17:58 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 23:51:57.596: INFO: prometheus-operator-76f5fb67cd-2kb84 from openshift-monitoring started at 2019-03-02 22:19:39 +0000 UTC (1 container statuses recorded)
Mar  2 23:51:57.596: INFO: 	Container prometheus-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15884958e88dba0d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match node selector, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:51:58.864: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hzknp" for this suite.
Mar  2 23:52:04.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:52:06.411: INFO: namespace: e2e-tests-sched-pred-hzknp, resource: bindings, ignored listing per whitelist
Mar  2 23:52:06.984: INFO: namespace: e2e-tests-sched-pred-hzknp, resource: packagemanifests, items remaining: 1
Mar  2 23:52:07.403: INFO: namespace: e2e-tests-sched-pred-hzknp no longer exists
Mar  2 23:52:07.421: INFO: namespace: e2e-tests-sched-pred-hzknp, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:52:07.438: INFO: namespace e2e-tests-sched-pred-hzknp deletion completed in 8.529416197s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:11.078 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:52:07.438: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-whkxt
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar  2 23:52:08.563: INFO: Found 0 stateful pods, waiting for 3
Mar  2 23:52:18.584: INFO: Found 2 stateful pods, waiting for 3
Mar  2 23:52:28.582: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:52:28.582: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:52:28.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 23:52:38.582: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:52:38.582: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:52:38.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 23:52:38.635: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-whkxt ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:52:39.068: INFO: stderr: ""
Mar  2 23:52:39.068: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:52:39.068: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  2 23:52:49.194: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  2 23:52:49.253: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-whkxt ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:52:49.644: INFO: stderr: ""
Mar  2 23:52:49.644: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:52:49.644: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:52:59.758: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:52:59.758: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:52:59.758: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:52:59.758: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:09.797: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:53:09.797: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:09.797: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:19.795: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:53:19.795: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:19.795: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:29.795: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:53:29.795: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:39.794: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:53:39.794: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  2 23:53:49.794: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  2 23:53:59.794: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-whkxt ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  2 23:54:00.207: INFO: stderr: ""
Mar  2 23:54:00.207: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  2 23:54:00.207: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  2 23:54:10.327: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  2 23:54:10.380: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-whkxt ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  2 23:54:10.738: INFO: stderr: ""
Mar  2 23:54:10.738: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  2 23:54:10.738: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  2 23:54:20.844: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:54:20.844: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar  2 23:54:20.844: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar  2 23:54:30.880: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:54:30.880: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar  2 23:54:30.880: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar  2 23:54:40.880: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
Mar  2 23:54:40.880: INFO: Waiting for Pod e2e-tests-statefulset-whkxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar  2 23:54:50.880: INFO: Waiting for StatefulSet e2e-tests-statefulset-whkxt/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  2 23:55:00.881: INFO: Deleting all statefulset in ns e2e-tests-statefulset-whkxt
Mar  2 23:55:00.898: INFO: Scaling statefulset ss2 to 0
Mar  2 23:55:30.974: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 23:55:30.992: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:55:31.051: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-whkxt" for this suite.
Mar  2 23:55:39.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:55:40.921: INFO: namespace: e2e-tests-statefulset-whkxt, resource: packagemanifests, items remaining: 1
Mar  2 23:55:41.318: INFO: namespace: e2e-tests-statefulset-whkxt, resource: bindings, ignored listing per whitelist
Mar  2 23:55:41.585: INFO: namespace: e2e-tests-statefulset-whkxt no longer exists
Mar  2 23:55:41.603: INFO: namespace: e2e-tests-statefulset-whkxt, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:55:41.620: INFO: namespace e2e-tests-statefulset-whkxt deletion completed in 10.525054653s

• [SLOW TEST:214.182 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:55:41.620: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 23:55:42.800: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-g9m57'
Mar  2 23:55:44.065: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar  2 23:55:44.065: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Mar  2 23:55:46.101: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-g9m57'
Mar  2 23:55:46.309: INFO: stderr: ""
Mar  2 23:55:46.309: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:55:46.309: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g9m57" for this suite.
Mar  2 23:55:52.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:55:53.561: INFO: namespace: e2e-tests-kubectl-g9m57, resource: packagemanifests, items remaining: 1
Mar  2 23:55:54.186: INFO: namespace: e2e-tests-kubectl-g9m57, resource: bindings, ignored listing per whitelist
Mar  2 23:55:54.845: INFO: namespace: e2e-tests-kubectl-g9m57 no longer exists
Mar  2 23:55:54.864: INFO: namespace: e2e-tests-kubectl-g9m57, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:55:54.881: INFO: namespace e2e-tests-kubectl-g9m57 deletion completed in 8.527020534s

• [SLOW TEST:13.261 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:55:54.881: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  2 23:55:55.969: INFO: Waiting up to 5m0s for pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-9rc96" to be "success or failure"
Mar  2 23:55:55.987: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.844011ms
Mar  2 23:55:58.005: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03608287s
Mar  2 23:56:00.024: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054312921s
Mar  2 23:56:02.042: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072578581s
Mar  2 23:56:04.060: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.09109099s
STEP: Saw pod success
Mar  2 23:56:04.061: INFO: Pod "downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:56:04.078: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  2 23:56:04.135: INFO: Waiting for pod downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:56:04.152: INFO: Pod downward-api-b83a032a-3d46-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:56:04.152: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9rc96" for this suite.
Mar  2 23:56:10.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:56:11.606: INFO: namespace: e2e-tests-downward-api-9rc96, resource: packagemanifests, items remaining: 1
Mar  2 23:56:11.941: INFO: namespace: e2e-tests-downward-api-9rc96, resource: bindings, ignored listing per whitelist
Mar  2 23:56:12.688: INFO: namespace: e2e-tests-downward-api-9rc96 no longer exists
Mar  2 23:56:12.707: INFO: namespace: e2e-tests-downward-api-9rc96, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:56:12.724: INFO: namespace e2e-tests-downward-api-9rc96 deletion completed in 8.527480513s

• [SLOW TEST:17.843 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:56:12.725: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:56:20.083: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-rg8bc" for this suite.
Mar  2 23:56:26.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:56:27.397: INFO: namespace: e2e-tests-namespaces-rg8bc, resource: packagemanifests, items remaining: 1
Mar  2 23:56:27.825: INFO: namespace: e2e-tests-namespaces-rg8bc, resource: bindings, ignored listing per whitelist
Mar  2 23:56:28.634: INFO: namespace: e2e-tests-namespaces-rg8bc no longer exists
Mar  2 23:56:28.653: INFO: namespace: e2e-tests-namespaces-rg8bc, total namespaces: 48, active: 48, terminating: 0
Mar  2 23:56:28.670: INFO: namespace e2e-tests-namespaces-rg8bc deletion completed in 8.539047644s
STEP: Destroying namespace "e2e-tests-nsdeletetest-cc5zx" for this suite.
Mar  2 23:56:28.687: INFO: Namespace e2e-tests-nsdeletetest-cc5zx was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-qrlp6" for this suite.
Mar  2 23:56:34.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:56:36.418: INFO: namespace: e2e-tests-nsdeletetest-qrlp6, resource: packagemanifests, items remaining: 1
Mar  2 23:56:36.860: INFO: namespace: e2e-tests-nsdeletetest-qrlp6, resource: bindings, ignored listing per whitelist
Mar  2 23:56:37.177: INFO: namespace: e2e-tests-nsdeletetest-qrlp6 no longer exists
Mar  2 23:56:37.195: INFO: namespace: e2e-tests-nsdeletetest-qrlp6, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:56:37.213: INFO: namespace e2e-tests-nsdeletetest-qrlp6 deletion completed in 8.525505368s

• [SLOW TEST:24.489 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:56:37.213: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  2 23:56:39.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-n445k" to be "success or failure"
Mar  2 23:56:39.335: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.035692ms
Mar  2 23:56:41.353: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038360643s
Mar  2 23:56:43.371: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056449349s
Mar  2 23:56:45.390: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074878723s
Mar  2 23:56:47.411: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096048791s
Mar  2 23:56:49.429: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.114494545s
STEP: Saw pod success
Mar  2 23:56:49.429: INFO: Pod "downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:56:49.447: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  2 23:56:49.504: INFO: Waiting for pod downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:56:49.521: INFO: Pod downwardapi-volume-d1750e23-3d46-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:56:49.521: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n445k" for this suite.
Mar  2 23:56:55.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:56:56.639: INFO: namespace: e2e-tests-downward-api-n445k, resource: packagemanifests, items remaining: 1
Mar  2 23:56:57.608: INFO: namespace: e2e-tests-downward-api-n445k, resource: bindings, ignored listing per whitelist
Mar  2 23:56:58.061: INFO: namespace: e2e-tests-downward-api-n445k no longer exists
Mar  2 23:56:58.084: INFO: namespace: e2e-tests-downward-api-n445k, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:56:58.101: INFO: namespace e2e-tests-downward-api-n445k deletion completed in 8.535331315s

• [SLOW TEST:20.888 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:56:58.102: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0302 23:57:39.294352   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 23:57:39.294: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:57:39.294: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7mhn4" for this suite.
Mar  2 23:57:47.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:57:48.619: INFO: namespace: e2e-tests-gc-7mhn4, resource: packagemanifests, items remaining: 1
Mar  2 23:57:49.201: INFO: namespace: e2e-tests-gc-7mhn4, resource: bindings, ignored listing per whitelist
Mar  2 23:57:49.804: INFO: namespace: e2e-tests-gc-7mhn4 no longer exists
Mar  2 23:57:49.823: INFO: namespace: e2e-tests-gc-7mhn4, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:57:49.842: INFO: namespace e2e-tests-gc-7mhn4 deletion completed in 10.529490805s

• [SLOW TEST:51.741 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:57:49.842: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar  2 23:57:50.927: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-zrzcf" to be "success or failure"
Mar  2 23:57:50.948: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 21.433676ms
Mar  2 23:57:52.966: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039637031s
Mar  2 23:57:54.984: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057502695s
Mar  2 23:57:57.002: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075581431s
Mar  2 23:57:59.020: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09344768s
Mar  2 23:58:01.038: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111444199s
STEP: Saw pod success
Mar  2 23:58:01.038: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  2 23:58:01.056: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  2 23:58:01.112: INFO: Waiting for pod pod-host-path-test to disappear
Mar  2 23:58:01.130: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:58:01.130: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-zrzcf" for this suite.
Mar  2 23:58:07.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:58:08.181: INFO: namespace: e2e-tests-hostpath-zrzcf, resource: packagemanifests, items remaining: 1
Mar  2 23:58:09.287: INFO: namespace: e2e-tests-hostpath-zrzcf, resource: bindings, ignored listing per whitelist
Mar  2 23:58:09.667: INFO: namespace: e2e-tests-hostpath-zrzcf no longer exists
Mar  2 23:58:09.685: INFO: namespace: e2e-tests-hostpath-zrzcf, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:58:09.703: INFO: namespace e2e-tests-hostpath-zrzcf deletion completed in 8.527719314s

• [SLOW TEST:19.860 seconds]
[sig-storage] HostPath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:58:09.703: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 23:58:10.762: INFO: Waiting up to 5m0s for pod "pod-08924474-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-x9s89" to be "success or failure"
Mar  2 23:58:10.780: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.450166ms
Mar  2 23:58:12.799: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037137719s
Mar  2 23:58:14.817: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054950952s
Mar  2 23:58:16.835: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072730975s
Mar  2 23:58:18.853: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09062841s
Mar  2 23:58:20.871: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109158102s
STEP: Saw pod success
Mar  2 23:58:20.871: INFO: Pod "pod-08924474-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:58:20.889: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-08924474-3d47-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  2 23:58:20.945: INFO: Waiting for pod pod-08924474-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:58:20.962: INFO: Pod pod-08924474-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:58:20.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-x9s89" for this suite.
Mar  2 23:58:27.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:58:27.907: INFO: namespace: e2e-tests-emptydir-x9s89, resource: packagemanifests, items remaining: 1
Mar  2 23:58:28.973: INFO: namespace: e2e-tests-emptydir-x9s89, resource: bindings, ignored listing per whitelist
Mar  2 23:58:29.496: INFO: namespace: e2e-tests-emptydir-x9s89 no longer exists
Mar  2 23:58:29.515: INFO: namespace: e2e-tests-emptydir-x9s89, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:58:29.533: INFO: namespace e2e-tests-emptydir-x9s89 deletion completed in 8.52629631s

• [SLOW TEST:19.830 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:58:29.533: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-xhszl/configmap-test-148c1356-3d47-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  2 23:58:30.911: INFO: Waiting up to 5m0s for pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-xhszl" to be "success or failure"
Mar  2 23:58:30.930: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.715485ms
Mar  2 23:58:32.948: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036882982s
Mar  2 23:58:34.966: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054877165s
Mar  2 23:58:36.984: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072884068s
Mar  2 23:58:39.002: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091161773s
Mar  2 23:58:41.020: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109049716s
STEP: Saw pod success
Mar  2 23:58:41.020: INFO: Pod "pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  2 23:58:41.038: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353 container env-test: <nil>
STEP: delete the pod
Mar  2 23:58:41.095: INFO: Waiting for pod pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  2 23:58:41.112: INFO: Pod pod-configmaps-14936a22-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:58:41.112: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xhszl" for this suite.
Mar  2 23:58:47.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:58:48.014: INFO: namespace: e2e-tests-configmap-xhszl, resource: packagemanifests, items remaining: 1
Mar  2 23:58:48.605: INFO: namespace: e2e-tests-configmap-xhszl, resource: bindings, ignored listing per whitelist
Mar  2 23:58:49.647: INFO: namespace: e2e-tests-configmap-xhszl no longer exists
Mar  2 23:58:49.666: INFO: namespace: e2e-tests-configmap-xhszl, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:58:49.685: INFO: namespace e2e-tests-configmap-xhszl deletion completed in 8.528510061s

• [SLOW TEST:20.151 seconds]
[sig-api-machinery] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:58:49.685: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  2 23:58:50.768: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-206fdaeb-3d47-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:59:00.926: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wfl5d" for this suite.
Mar  2 23:59:25.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:59:25.936: INFO: namespace: e2e-tests-configmap-wfl5d, resource: bindings, ignored listing per whitelist
Mar  2 23:59:26.648: INFO: namespace: e2e-tests-configmap-wfl5d, resource: packagemanifests, items remaining: 1
Mar  2 23:59:27.478: INFO: namespace: e2e-tests-configmap-wfl5d no longer exists
Mar  2 23:59:27.497: INFO: namespace: e2e-tests-configmap-wfl5d, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:59:27.515: INFO: namespace e2e-tests-configmap-wfl5d deletion completed in 26.544946859s

• [SLOW TEST:37.830 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:59:27.515: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  2 23:59:28.563: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2pntm'
Mar  2 23:59:28.795: INFO: stderr: ""
Mar  2 23:59:28.795: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  2 23:59:38.846: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2pntm -o json'
Mar  2 23:59:39.018: INFO: stderr: ""
Mar  2 23:59:39.018: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"ips\\\": [\\n        \\\"10.129.2.104\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-03-02T23:59:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-2pntm\",\n        \"resourceVersion\": \"96204\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-2pntm/pods/e2e-test-nginx-pod\",\n        \"uid\": \"3716151f-3d47-11e9-8060-0e81de5d2554\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    },\n                    \"procMount\": \"Default\"\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-tnvd6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-vxxjd\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-160-190.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c43,c37\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-tnvd6\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-tnvd6\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-02T23:59:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-02T23:59:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-02T23:59:37Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-02T23:59:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://e88515ee30fed86e91740d5adaa9f01a60ba3e22b7f068e19e6a1ae9f8641342\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:b9734546761e49b453efce35ee523bbcaff1052d281516f133d41b090e26c0df\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-02T23:59:37Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.160.190\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.129.2.104\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-02T23:59:28Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  2 23:59:39.018: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-2pntm'
Mar  2 23:59:39.497: INFO: stderr: ""
Mar  2 23:59:39.497: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Mar  2 23:59:39.515: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2pntm'
Mar  2 23:59:41.116: INFO: stderr: ""
Mar  2 23:59:41.116: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  2 23:59:41.116: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2pntm" for this suite.
Mar  2 23:59:47.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 23:59:48.204: INFO: namespace: e2e-tests-kubectl-2pntm, resource: bindings, ignored listing per whitelist
Mar  2 23:59:48.445: INFO: namespace: e2e-tests-kubectl-2pntm, resource: packagemanifests, items remaining: 1
Mar  2 23:59:49.655: INFO: namespace: e2e-tests-kubectl-2pntm no longer exists
Mar  2 23:59:49.673: INFO: namespace: e2e-tests-kubectl-2pntm, total namespaces: 47, active: 47, terminating: 0
Mar  2 23:59:49.691: INFO: namespace e2e-tests-kubectl-2pntm deletion completed in 8.530646818s

• [SLOW TEST:22.176 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  2 23:59:49.691: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-44309f71-3d47-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  2 23:59:50.818: INFO: Waiting up to 5m0s for pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-vlh86" to be "success or failure"
Mar  2 23:59:50.838: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 19.653565ms
Mar  2 23:59:52.892: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073720394s
Mar  2 23:59:54.911: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093035304s
Mar  2 23:59:56.930: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111196156s
Mar  2 23:59:58.947: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.128927874s
Mar  3 00:00:00.965: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.146638528s
STEP: Saw pod success
Mar  3 00:00:00.966: INFO: Pod "pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:00:00.983: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353 container secret-env-test: <nil>
STEP: delete the pod
Mar  3 00:00:01.038: INFO: Waiting for pod pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:00:01.055: INFO: Pod pod-secrets-44354490-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:00:01.055: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vlh86" for this suite.
Mar  3 00:00:33.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:00:34.483: INFO: namespace: e2e-tests-secrets-vlh86, resource: bindings, ignored listing per whitelist
Mar  3 00:00:35.081: INFO: namespace: e2e-tests-secrets-vlh86, resource: packagemanifests, items remaining: 1
Mar  3 00:00:35.589: INFO: namespace: e2e-tests-secrets-vlh86 no longer exists
Mar  3 00:00:35.607: INFO: namespace: e2e-tests-secrets-vlh86, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:00:35.624: INFO: namespace e2e-tests-secrets-vlh86 deletion completed in 34.525040971s

• [SLOW TEST:45.933 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:00:35.625: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar  3 00:00:36.714: INFO: Waiting up to 5m0s for pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-containers-cs4g4" to be "success or failure"
Mar  3 00:00:36.732: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.520262ms
Mar  3 00:00:38.750: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035630421s
Mar  3 00:00:40.769: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054936877s
Mar  3 00:00:42.787: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073001744s
Mar  3 00:00:44.806: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091296102s
Mar  3 00:00:46.824: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109457964s
STEP: Saw pod success
Mar  3 00:00:46.824: INFO: Pod "client-containers-5f901013-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:00:46.842: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod client-containers-5f901013-3d47-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:00:46.898: INFO: Waiting for pod client-containers-5f901013-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:00:46.915: INFO: Pod client-containers-5f901013-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:00:46.915: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-cs4g4" for this suite.
Mar  3 00:00:57.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:00:58.198: INFO: namespace: e2e-tests-containers-cs4g4, resource: packagemanifests, items remaining: 1
Mar  3 00:00:59.139: INFO: namespace: e2e-tests-containers-cs4g4, resource: bindings, ignored listing per whitelist
Mar  3 00:00:59.463: INFO: namespace: e2e-tests-containers-cs4g4 no longer exists
Mar  3 00:00:59.481: INFO: namespace: e2e-tests-containers-cs4g4, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:00:59.499: INFO: namespace e2e-tests-containers-cs4g4 deletion completed in 12.538894106s

• [SLOW TEST:23.874 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:00:59.499: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-vtrcz.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-vtrcz.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-vtrcz.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-vtrcz.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-vtrcz.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-vtrcz.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 00:01:21.108: INFO: DNS probes using e2e-tests-dns-vtrcz/dns-test-6dc74b3f-3d47-11e9-9008-0a58ac10f353 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:01:21.191: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-vtrcz" for this suite.
E0303 00:01:40.087593   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
E0303 00:01:40.087601   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
E0303 00:01:40.087597   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
E0303 00:01:40.087594   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
E0303 00:01:40.087598   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
E0303 00:01:40.087598   27430 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=70821, ErrCode=NO_ERROR, debug=""
Mar  3 00:01:47.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:01:49.320: INFO: namespace: e2e-tests-dns-vtrcz, resource: packagemanifests, items remaining: 1
Mar  3 00:01:49.514: INFO: namespace: e2e-tests-dns-vtrcz, resource: bindings, ignored listing per whitelist
Mar  3 00:01:49.730: INFO: namespace: e2e-tests-dns-vtrcz no longer exists
Mar  3 00:01:49.749: INFO: namespace: e2e-tests-dns-vtrcz, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:01:49.765: INFO: namespace e2e-tests-dns-vtrcz deletion completed in 28.541068685s

• [SLOW TEST:50.266 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:01:49.766: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:01:50.968: INFO: (0) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 67.16335ms)
Mar  3 00:01:50.987: INFO: (1) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 19.067749ms)
Mar  3 00:01:51.006: INFO: (2) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 19.09051ms)
Mar  3 00:01:51.029: INFO: (3) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 22.304341ms)
Mar  3 00:01:51.048: INFO: (4) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 19.409629ms)
Mar  3 00:01:51.067: INFO: (5) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.882309ms)
Mar  3 00:01:51.087: INFO: (6) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 20.265898ms)
Mar  3 00:01:51.106: INFO: (7) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.826877ms)
Mar  3 00:01:51.131: INFO: (8) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 25.038501ms)
Mar  3 00:01:51.150: INFO: (9) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.840851ms)
Mar  3 00:01:51.169: INFO: (10) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.796141ms)
Mar  3 00:01:51.193: INFO: (11) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 23.890996ms)
Mar  3 00:01:51.212: INFO: (12) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.682224ms)
Mar  3 00:01:51.230: INFO: (13) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.482952ms)
Mar  3 00:01:51.249: INFO: (14) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.981085ms)
Mar  3 00:01:51.272: INFO: (15) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 22.976739ms)
Mar  3 00:01:51.291: INFO: (16) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.838409ms)
Mar  3 00:01:51.311: INFO: (17) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 19.061127ms)
Mar  3 00:01:51.331: INFO: (18) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 20.488697ms)
Mar  3 00:01:51.350: INFO: (19) /api/v1/nodes/ip-10-0-143-63.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.87452ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:01:51.350: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-hkbd6" for this suite.
Mar  3 00:01:57.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:01:57.881: INFO: namespace: e2e-tests-proxy-hkbd6, resource: bindings, ignored listing per whitelist
Mar  3 00:01:58.421: INFO: namespace: e2e-tests-proxy-hkbd6, resource: packagemanifests, items remaining: 1
Mar  3 00:01:58.989: INFO: namespace: e2e-tests-proxy-hkbd6 no longer exists
Mar  3 00:01:59.008: INFO: namespace: e2e-tests-proxy-hkbd6, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:01:59.025: INFO: namespace e2e-tests-proxy-hkbd6 deletion completed in 7.655461924s

• [SLOW TEST:9.259 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:01:59.025: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  3 00:02:00.102: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:02:11.619: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-p9z7h" for this suite.
Mar  3 00:02:55.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:02:56.584: INFO: namespace: e2e-tests-init-container-p9z7h, resource: packagemanifests, items remaining: 1
Mar  3 00:02:56.618: INFO: namespace: e2e-tests-init-container-p9z7h, resource: bindings, ignored listing per whitelist
Mar  3 00:02:58.159: INFO: namespace: e2e-tests-init-container-p9z7h no longer exists
Mar  3 00:02:58.180: INFO: namespace: e2e-tests-init-container-p9z7h, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:02:58.196: INFO: namespace e2e-tests-init-container-p9z7h deletion completed in 46.532210373s

• [SLOW TEST:59.171 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:02:58.197: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-b48a98b5-3d47-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  3 00:02:59.295: INFO: Waiting up to 5m0s for pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-njmmh" to be "success or failure"
Mar  3 00:02:59.312: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.821936ms
Mar  3 00:03:01.330: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035326665s
Mar  3 00:03:03.347: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052294515s
Mar  3 00:03:05.364: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069104932s
Mar  3 00:03:07.381: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086879136s
Mar  3 00:03:09.399: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104324284s
STEP: Saw pod success
Mar  3 00:03:09.400: INFO: Pod "pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:03:09.416: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 00:03:09.478: INFO: Waiting for pod pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:03:09.495: INFO: Pod pod-secrets-b48d5f40-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:03:09.495: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-njmmh" for this suite.
Mar  3 00:03:15.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:03:16.538: INFO: namespace: e2e-tests-secrets-njmmh, resource: packagemanifests, items remaining: 1
Mar  3 00:03:17.255: INFO: namespace: e2e-tests-secrets-njmmh, resource: bindings, ignored listing per whitelist
Mar  3 00:03:18.030: INFO: namespace: e2e-tests-secrets-njmmh no longer exists
Mar  3 00:03:18.050: INFO: namespace: e2e-tests-secrets-njmmh, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:03:18.067: INFO: namespace e2e-tests-secrets-njmmh deletion completed in 8.527207027s

• [SLOW TEST:19.870 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:03:18.067: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  3 00:03:29.851: INFO: Successfully updated pod "labelsupdatec06360d2-3d47-11e9-9008-0a58ac10f353"
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:03:31.899: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-64cst" for this suite.
Mar  3 00:03:56.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:03:56.753: INFO: namespace: e2e-tests-projected-64cst, resource: bindings, ignored listing per whitelist
Mar  3 00:03:57.709: INFO: namespace: e2e-tests-projected-64cst, resource: packagemanifests, items remaining: 1
Mar  3 00:03:58.441: INFO: namespace: e2e-tests-projected-64cst no longer exists
Mar  3 00:03:58.461: INFO: namespace: e2e-tests-projected-64cst, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:03:58.477: INFO: namespace e2e-tests-projected-64cst deletion completed in 26.53273311s

• [SLOW TEST:40.410 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:03:58.478: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  3 00:03:59.564: INFO: Waiting up to 5m0s for pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-lpzq9" to be "success or failure"
Mar  3 00:03:59.582: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.807724ms
Mar  3 00:04:01.599: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034879227s
Mar  3 00:04:03.618: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05385584s
Mar  3 00:04:05.636: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071915392s
Mar  3 00:04:07.654: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090317783s
Mar  3 00:04:09.673: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109350946s
STEP: Saw pod success
Mar  3 00:04:09.673: INFO: Pod "pod-d878f60f-3d47-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:04:09.690: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-d878f60f-3d47-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:04:09.741: INFO: Waiting for pod pod-d878f60f-3d47-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:04:09.758: INFO: Pod pod-d878f60f-3d47-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:04:09.758: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lpzq9" for this suite.
Mar  3 00:04:15.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:04:16.867: INFO: namespace: e2e-tests-emptydir-lpzq9, resource: packagemanifests, items remaining: 1
Mar  3 00:04:17.760: INFO: namespace: e2e-tests-emptydir-lpzq9, resource: bindings, ignored listing per whitelist
Mar  3 00:04:18.298: INFO: namespace: e2e-tests-emptydir-lpzq9 no longer exists
Mar  3 00:04:18.317: INFO: namespace: e2e-tests-emptydir-lpzq9, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:04:18.333: INFO: namespace e2e-tests-emptydir-lpzq9 deletion completed in 8.528982695s

• [SLOW TEST:19.856 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:04:18.334: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Mar  3 00:04:29.622: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:04:54.805: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-2kmw5" for this suite.
Mar  3 00:05:00.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:05:01.768: INFO: namespace: e2e-tests-namespaces-2kmw5, resource: bindings, ignored listing per whitelist
Mar  3 00:05:02.811: INFO: namespace: e2e-tests-namespaces-2kmw5, resource: packagemanifests, items remaining: 1
Mar  3 00:05:03.352: INFO: namespace: e2e-tests-namespaces-2kmw5 no longer exists
Mar  3 00:05:03.373: INFO: namespace: e2e-tests-namespaces-2kmw5, total namespaces: 48, active: 48, terminating: 0
Mar  3 00:05:03.390: INFO: namespace e2e-tests-namespaces-2kmw5 deletion completed in 8.537126481s
STEP: Destroying namespace "e2e-tests-nsdeletetest-bmk4j" for this suite.
Mar  3 00:05:03.411: INFO: Namespace e2e-tests-nsdeletetest-bmk4j was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-79w5w" for this suite.
Mar  3 00:05:09.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:05:10.595: INFO: namespace: e2e-tests-nsdeletetest-79w5w, resource: packagemanifests, items remaining: 1
Mar  3 00:05:11.270: INFO: namespace: e2e-tests-nsdeletetest-79w5w, resource: bindings, ignored listing per whitelist
Mar  3 00:05:11.904: INFO: namespace: e2e-tests-nsdeletetest-79w5w no longer exists
Mar  3 00:05:11.923: INFO: namespace: e2e-tests-nsdeletetest-79w5w, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:05:11.939: INFO: namespace e2e-tests-nsdeletetest-79w5w deletion completed in 8.528237674s

• [SLOW TEST:53.606 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:05:11.940: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-d4qw
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 00:05:13.181: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-d4qw" in namespace "e2e-tests-subpath-4vb4q" to be "success or failure"
Mar  3 00:05:13.198: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Pending", Reason="", readiness=false. Elapsed: 16.412899ms
Mar  3 00:05:15.215: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034129295s
Mar  3 00:05:17.234: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052257936s
Mar  3 00:05:19.251: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069557997s
Mar  3 00:05:21.280: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099164217s
Mar  3 00:05:23.298: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 10.116326743s
Mar  3 00:05:25.316: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 12.135111308s
Mar  3 00:05:27.337: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 14.155920519s
Mar  3 00:05:29.354: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 16.172885452s
Mar  3 00:05:31.371: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 18.190055975s
Mar  3 00:05:33.389: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 20.207610479s
Mar  3 00:05:35.406: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 22.224855908s
Mar  3 00:05:37.423: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 24.241943639s
Mar  3 00:05:39.442: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 26.26073707s
Mar  3 00:05:41.459: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Running", Reason="", readiness=false. Elapsed: 28.277946986s
Mar  3 00:05:43.477: INFO: Pod "pod-subpath-test-projected-d4qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.295703003s
STEP: Saw pod success
Mar  3 00:05:43.477: INFO: Pod "pod-subpath-test-projected-d4qw" satisfied condition "success or failure"
Mar  3 00:05:43.494: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-subpath-test-projected-d4qw container test-container-subpath-projected-d4qw: <nil>
STEP: delete the pod
Mar  3 00:05:43.544: INFO: Waiting for pod pod-subpath-test-projected-d4qw to disappear
Mar  3 00:05:43.561: INFO: Pod pod-subpath-test-projected-d4qw no longer exists
STEP: Deleting pod pod-subpath-test-projected-d4qw
Mar  3 00:05:43.561: INFO: Deleting pod "pod-subpath-test-projected-d4qw" in namespace "e2e-tests-subpath-4vb4q"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:05:43.577: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4vb4q" for this suite.
Mar  3 00:05:51.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:05:53.224: INFO: namespace: e2e-tests-subpath-4vb4q, resource: packagemanifests, items remaining: 1
Mar  3 00:05:53.514: INFO: namespace: e2e-tests-subpath-4vb4q, resource: bindings, ignored listing per whitelist
Mar  3 00:05:54.122: INFO: namespace: e2e-tests-subpath-4vb4q no longer exists
Mar  3 00:05:54.142: INFO: namespace: e2e-tests-subpath-4vb4q, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:05:54.158: INFO: namespace e2e-tests-subpath-4vb4q deletion completed in 10.535715937s

• [SLOW TEST:42.218 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:05:54.158: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-cbg7m
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  3 00:05:55.214: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  3 00:06:33.622: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.129.2.110:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cbg7m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  3 00:06:33.622: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  3 00:06:33.904: INFO: Found all expected endpoints: [netserver-0]
Mar  3 00:06:33.921: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.131.0.48:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cbg7m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  3 00:06:33.921: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  3 00:06:34.113: INFO: Found all expected endpoints: [netserver-1]
Mar  3 00:06:34.130: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.128.2.111:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cbg7m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  3 00:06:34.130: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Mar  3 00:06:34.331: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:06:34.332: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-cbg7m" for this suite.
Mar  3 00:06:58.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:06:59.658: INFO: namespace: e2e-tests-pod-network-test-cbg7m, resource: packagemanifests, items remaining: 1
Mar  3 00:07:00.557: INFO: namespace: e2e-tests-pod-network-test-cbg7m, resource: bindings, ignored listing per whitelist
Mar  3 00:07:00.870: INFO: namespace: e2e-tests-pod-network-test-cbg7m no longer exists
Mar  3 00:07:00.889: INFO: namespace: e2e-tests-pod-network-test-cbg7m, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:07:00.906: INFO: namespace e2e-tests-pod-network-test-cbg7m deletion completed in 26.528533235s

• [SLOW TEST:66.748 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:07:00.906: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-452edf72-3d48-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  3 00:07:01.979: INFO: Waiting up to 5m0s for pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353" in namespace "e2e-tests-secrets-pmw55" to be "success or failure"
Mar  3 00:07:01.997: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039285ms
Mar  3 00:07:04.014: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035313646s
Mar  3 00:07:06.031: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052676064s
Mar  3 00:07:08.048: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069771786s
Mar  3 00:07:10.066: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087664852s
Mar  3 00:07:12.084: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.105274295s
STEP: Saw pod success
Mar  3 00:07:12.084: INFO: Pod "pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:07:12.101: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 00:07:12.164: INFO: Waiting for pod pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:07:12.181: INFO: Pod pod-secrets-4531e9c1-3d48-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:07:12.181: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pmw55" for this suite.
Mar  3 00:07:18.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:07:19.033: INFO: namespace: e2e-tests-secrets-pmw55, resource: bindings, ignored listing per whitelist
Mar  3 00:07:20.100: INFO: namespace: e2e-tests-secrets-pmw55, resource: packagemanifests, items remaining: 1
Mar  3 00:07:20.719: INFO: namespace: e2e-tests-secrets-pmw55 no longer exists
Mar  3 00:07:20.739: INFO: namespace: e2e-tests-secrets-pmw55, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:07:20.755: INFO: namespace e2e-tests-secrets-pmw55 deletion completed in 8.528461043s

• [SLOW TEST:19.849 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:07:20.755: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  3 00:07:21.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-tmtmh" to be "success or failure"
Mar  3 00:07:21.869: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.436487ms
Mar  3 00:07:23.887: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035000617s
Mar  3 00:07:25.904: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052594704s
Mar  3 00:07:27.922: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069683787s
Mar  3 00:07:29.939: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086825888s
Mar  3 00:07:31.956: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104585171s
STEP: Saw pod success
Mar  3 00:07:31.957: INFO: Pod "downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:07:31.973: INFO: Trying to get logs from node ip-10-0-143-63.ec2.internal pod downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  3 00:07:32.034: INFO: Waiting for pod downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:07:32.050: INFO: Pod downwardapi-volume-510b210d-3d48-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:07:32.051: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tmtmh" for this suite.
Mar  3 00:07:40.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:07:41.161: INFO: namespace: e2e-tests-projected-tmtmh, resource: packagemanifests, items remaining: 1
Mar  3 00:07:42.570: INFO: namespace: e2e-tests-projected-tmtmh, resource: bindings, ignored listing per whitelist
Mar  3 00:07:42.588: INFO: namespace: e2e-tests-projected-tmtmh no longer exists
Mar  3 00:07:42.606: INFO: namespace: e2e-tests-projected-tmtmh, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:07:42.623: INFO: namespace e2e-tests-projected-tmtmh deletion completed in 10.526559785s

• [SLOW TEST:21.867 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:07:42.623: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar  3 00:07:43.730: INFO: Waiting up to 5m0s for pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353" in namespace "e2e-tests-containers-f4ddv" to be "success or failure"
Mar  3 00:07:43.748: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.285331ms
Mar  3 00:07:45.767: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036886859s
Mar  3 00:07:47.784: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05427789s
Mar  3 00:07:49.801: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07142586s
Mar  3 00:07:51.819: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08937496s
Mar  3 00:07:53.838: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107813376s
Mar  3 00:07:55.855: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.125214138s
STEP: Saw pod success
Mar  3 00:07:55.855: INFO: Pod "client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:07:55.872: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:07:55.920: INFO: Waiting for pod client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:07:55.937: INFO: Pod client-containers-5e1600be-3d48-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:07:55.937: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-f4ddv" for this suite.
Mar  3 00:08:04.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:08:04.839: INFO: namespace: e2e-tests-containers-f4ddv, resource: packagemanifests, items remaining: 1
Mar  3 00:08:05.920: INFO: namespace: e2e-tests-containers-f4ddv, resource: bindings, ignored listing per whitelist
Mar  3 00:08:07.152: INFO: namespace: e2e-tests-containers-f4ddv no longer exists
Mar  3 00:08:07.172: INFO: namespace: e2e-tests-containers-f4ddv, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:08:07.190: INFO: namespace e2e-tests-containers-f4ddv deletion completed in 11.20739569s

• [SLOW TEST:24.567 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:08:07.190: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  3 00:08:08.436: INFO: Waiting up to 5m0s for pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-kj7zv" to be "success or failure"
Mar  3 00:08:08.453: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 16.851347ms
Mar  3 00:08:10.485: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048900092s
Mar  3 00:08:12.502: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066385159s
Mar  3 00:08:14.520: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084479744s
Mar  3 00:08:16.537: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101710657s
Mar  3 00:08:18.561: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.125539322s
STEP: Saw pod success
Mar  3 00:08:18.561: INFO: Pod "pod-6cc9922d-3d48-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:08:18.587: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-6cc9922d-3d48-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:08:18.642: INFO: Waiting for pod pod-6cc9922d-3d48-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:08:18.659: INFO: Pod pod-6cc9922d-3d48-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:08:18.659: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kj7zv" for this suite.
Mar  3 00:08:26.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:08:27.929: INFO: namespace: e2e-tests-emptydir-kj7zv, resource: bindings, ignored listing per whitelist
Mar  3 00:08:28.505: INFO: namespace: e2e-tests-emptydir-kj7zv, resource: packagemanifests, items remaining: 1
Mar  3 00:08:29.424: INFO: namespace: e2e-tests-emptydir-kj7zv no longer exists
Mar  3 00:08:29.443: INFO: namespace: e2e-tests-emptydir-kj7zv, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:08:29.459: INFO: namespace e2e-tests-emptydir-kj7zv deletion completed in 10.754971909s

• [SLOW TEST:22.269 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:08:29.459: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0303 00:08:36.673628   27430 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  3 00:08:36.673: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:08:36.673: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kfvj2" for this suite.
Mar  3 00:08:42.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:08:43.516: INFO: namespace: e2e-tests-gc-kfvj2, resource: bindings, ignored listing per whitelist
Mar  3 00:08:43.620: INFO: namespace: e2e-tests-gc-kfvj2, resource: packagemanifests, items remaining: 1
Mar  3 00:08:45.187: INFO: namespace: e2e-tests-gc-kfvj2 no longer exists
Mar  3 00:08:45.210: INFO: namespace: e2e-tests-gc-kfvj2, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:08:45.226: INFO: namespace e2e-tests-gc-kfvj2 deletion completed in 8.53355551s

• [SLOW TEST:15.767 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:08:45.226: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:08:46.305: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-83678919-3d48-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-83678919-3d48-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:10:27.820: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5rfvm" for this suite.
Mar  3 00:11:09.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:11:10.832: INFO: namespace: e2e-tests-configmap-5rfvm, resource: packagemanifests, items remaining: 1
Mar  3 00:11:11.057: INFO: namespace: e2e-tests-configmap-5rfvm, resource: bindings, ignored listing per whitelist
Mar  3 00:11:12.367: INFO: namespace: e2e-tests-configmap-5rfvm no longer exists
Mar  3 00:11:12.386: INFO: namespace: e2e-tests-configmap-5rfvm, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:11:12.403: INFO: namespace e2e-tests-configmap-5rfvm deletion completed in 44.537764502s

• [SLOW TEST:147.177 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:11:12.403: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-c4f4p
Mar  3 00:11:23.709: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-c4f4p
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 00:11:23.726: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:15:23.815: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-c4f4p" for this suite.
Mar  3 00:15:29.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:15:30.802: INFO: namespace: e2e-tests-container-probe-c4f4p, resource: bindings, ignored listing per whitelist
Mar  3 00:15:31.651: INFO: namespace: e2e-tests-container-probe-c4f4p, resource: packagemanifests, items remaining: 1
Mar  3 00:15:32.352: INFO: namespace: e2e-tests-container-probe-c4f4p no longer exists
Mar  3 00:15:32.371: INFO: namespace: e2e-tests-container-probe-c4f4p, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:15:32.389: INFO: namespace e2e-tests-container-probe-c4f4p deletion completed in 8.528989995s

• [SLOW TEST:259.986 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:15:32.389: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  3 00:15:51.636: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 00:15:51.653: INFO: Pod pod-with-prestop-http-hook still exists
Mar  3 00:15:53.653: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 00:15:53.670: INFO: Pod pod-with-prestop-http-hook still exists
Mar  3 00:15:55.653: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 00:15:55.670: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:15:55.694: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8sckv" for this suite.
Mar  3 00:16:19.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:16:20.717: INFO: namespace: e2e-tests-container-lifecycle-hook-8sckv, resource: bindings, ignored listing per whitelist
Mar  3 00:16:21.321: INFO: namespace: e2e-tests-container-lifecycle-hook-8sckv, resource: packagemanifests, items remaining: 1
Mar  3 00:16:22.235: INFO: namespace: e2e-tests-container-lifecycle-hook-8sckv no longer exists
Mar  3 00:16:22.254: INFO: namespace: e2e-tests-container-lifecycle-hook-8sckv, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:16:22.270: INFO: namespace e2e-tests-container-lifecycle-hook-8sckv deletion completed in 26.531942192s

• [SLOW TEST:49.881 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:16:22.271: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar  3 00:16:23.350: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:24.873: INFO: stderr: ""
Mar  3 00:16:24.873: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 00:16:24.873: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:25.054: INFO: stderr: ""
Mar  3 00:16:25.055: INFO: stdout: "update-demo-nautilus-8gl6d update-demo-nautilus-h9jxg "
Mar  3 00:16:25.055: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-8gl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:25.226: INFO: stderr: ""
Mar  3 00:16:25.226: INFO: stdout: ""
Mar  3 00:16:25.226: INFO: update-demo-nautilus-8gl6d is created but not running
Mar  3 00:16:30.227: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:30.411: INFO: stderr: ""
Mar  3 00:16:30.411: INFO: stdout: "update-demo-nautilus-8gl6d update-demo-nautilus-h9jxg "
Mar  3 00:16:30.411: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-8gl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:30.600: INFO: stderr: ""
Mar  3 00:16:30.600: INFO: stdout: ""
Mar  3 00:16:30.600: INFO: update-demo-nautilus-8gl6d is created but not running
Mar  3 00:16:35.600: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:35.771: INFO: stderr: ""
Mar  3 00:16:35.771: INFO: stdout: "update-demo-nautilus-8gl6d update-demo-nautilus-h9jxg "
Mar  3 00:16:35.771: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-8gl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:35.941: INFO: stderr: ""
Mar  3 00:16:35.941: INFO: stdout: "true"
Mar  3 00:16:35.941: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-8gl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:36.119: INFO: stderr: ""
Mar  3 00:16:36.119: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 00:16:36.119: INFO: validating pod update-demo-nautilus-8gl6d
Mar  3 00:16:36.143: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 00:16:36.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 00:16:36.143: INFO: update-demo-nautilus-8gl6d is verified up and running
Mar  3 00:16:36.143: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-h9jxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:36.335: INFO: stderr: ""
Mar  3 00:16:36.335: INFO: stdout: "true"
Mar  3 00:16:36.335: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-h9jxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:16:36.509: INFO: stderr: ""
Mar  3 00:16:36.509: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 00:16:36.509: INFO: validating pod update-demo-nautilus-h9jxg
Mar  3 00:16:36.530: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 00:16:36.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 00:16:36.530: INFO: update-demo-nautilus-h9jxg is verified up and running
STEP: rolling-update to new replication controller
Mar  3 00:16:36.532: INFO: scanned /tmp/home for discovery docs: <nil>
Mar  3 00:16:36.533: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:11.293: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  3 00:17:11.293: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 00:17:11.293: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:11.493: INFO: stderr: ""
Mar  3 00:17:11.493: INFO: stdout: "update-demo-kitten-4nzzx update-demo-kitten-k7fpj "
Mar  3 00:17:11.493: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-4nzzx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:11.681: INFO: stderr: ""
Mar  3 00:17:11.681: INFO: stdout: "true"
Mar  3 00:17:11.681: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-4nzzx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:11.871: INFO: stderr: ""
Mar  3 00:17:11.871: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  3 00:17:11.871: INFO: validating pod update-demo-kitten-4nzzx
Mar  3 00:17:11.892: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  3 00:17:11.892: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  3 00:17:11.892: INFO: update-demo-kitten-4nzzx is verified up and running
Mar  3 00:17:11.892: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-k7fpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:12.063: INFO: stderr: ""
Mar  3 00:17:12.063: INFO: stdout: "true"
Mar  3 00:17:12.063: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-k7fpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sqmb2'
Mar  3 00:17:12.239: INFO: stderr: ""
Mar  3 00:17:12.239: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  3 00:17:12.239: INFO: validating pod update-demo-kitten-k7fpj
Mar  3 00:17:12.261: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  3 00:17:12.261: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  3 00:17:12.261: INFO: update-demo-kitten-k7fpj is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:17:12.261: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sqmb2" for this suite.
Mar  3 00:17:36.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:17:37.371: INFO: namespace: e2e-tests-kubectl-sqmb2, resource: bindings, ignored listing per whitelist
Mar  3 00:17:38.204: INFO: namespace: e2e-tests-kubectl-sqmb2, resource: packagemanifests, items remaining: 1
Mar  3 00:17:38.797: INFO: namespace: e2e-tests-kubectl-sqmb2 no longer exists
Mar  3 00:17:38.817: INFO: namespace: e2e-tests-kubectl-sqmb2, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:17:38.833: INFO: namespace e2e-tests-kubectl-sqmb2 deletion completed in 26.527418308s

• [SLOW TEST:76.562 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:17:38.833: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:17:39.913: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-jwgh4" for this suite.
Mar  3 00:17:45.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:17:47.544: INFO: namespace: e2e-tests-services-jwgh4, resource: packagemanifests, items remaining: 1
Mar  3 00:17:47.597: INFO: namespace: e2e-tests-services-jwgh4, resource: bindings, ignored listing per whitelist
Mar  3 00:17:48.424: INFO: namespace: e2e-tests-services-jwgh4 no longer exists
Mar  3 00:17:48.443: INFO: namespace: e2e-tests-services-jwgh4, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:17:48.459: INFO: namespace e2e-tests-services-jwgh4 deletion completed in 8.527725557s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:9.626 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:17:48.460: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c73193a7-3d49-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  3 00:17:49.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-sn9h4" to be "success or failure"
Mar  3 00:17:49.603: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 16.381704ms
Mar  3 00:17:51.620: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033398109s
Mar  3 00:17:53.637: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050224061s
Mar  3 00:17:55.654: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067196411s
Mar  3 00:17:57.671: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084332204s
Mar  3 00:17:59.689: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101610225s
STEP: Saw pod success
Mar  3 00:17:59.689: INFO: Pod "pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:17:59.705: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  3 00:17:59.752: INFO: Waiting for pod pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:17:59.768: INFO: Pod pod-configmaps-c7352cc2-3d49-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:17:59.768: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sn9h4" for this suite.
Mar  3 00:18:05.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:18:07.991: INFO: namespace: e2e-tests-configmap-sn9h4, resource: packagemanifests, items remaining: 1
Mar  3 00:18:08.240: INFO: namespace: e2e-tests-configmap-sn9h4, resource: bindings, ignored listing per whitelist
Mar  3 00:18:08.304: INFO: namespace: e2e-tests-configmap-sn9h4 no longer exists
Mar  3 00:18:08.323: INFO: namespace: e2e-tests-configmap-sn9h4, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:18:08.339: INFO: namespace e2e-tests-configmap-sn9h4 deletion completed in 8.525586256s

• [SLOW TEST:19.880 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:18:08.340: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar  3 00:18:09.502: INFO: Waiting up to 5m0s for pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353" in namespace "e2e-tests-containers-b2krr" to be "success or failure"
Mar  3 00:18:09.520: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.295364ms
Mar  3 00:18:11.539: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036939528s
Mar  3 00:18:13.556: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054109048s
Mar  3 00:18:15.573: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071248781s
Mar  3 00:18:17.590: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088206061s
Mar  3 00:18:19.608: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.105654186s
STEP: Saw pod success
Mar  3 00:18:19.608: INFO: Pod "client-containers-d313a971-3d49-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:18:19.624: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod client-containers-d313a971-3d49-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:18:19.719: INFO: Waiting for pod client-containers-d313a971-3d49-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:18:19.736: INFO: Pod client-containers-d313a971-3d49-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:18:19.736: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-b2krr" for this suite.
Mar  3 00:18:25.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:18:27.838: INFO: namespace: e2e-tests-containers-b2krr, resource: bindings, ignored listing per whitelist
Mar  3 00:18:28.158: INFO: namespace: e2e-tests-containers-b2krr, resource: packagemanifests, items remaining: 1
Mar  3 00:18:28.272: INFO: namespace: e2e-tests-containers-b2krr no longer exists
Mar  3 00:18:28.291: INFO: namespace: e2e-tests-containers-b2krr, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:18:28.308: INFO: namespace e2e-tests-containers-b2krr deletion completed in 8.526937665s

• [SLOW TEST:19.968 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:18:28.308: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  3 00:18:49.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 00:18:49.591: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 00:18:51.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 00:18:51.609: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 00:18:53.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 00:18:53.609: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 00:18:55.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 00:18:55.609: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 00:18:57.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 00:18:57.609: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:18:57.609: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-ltvln" for this suite.
Mar  3 00:19:21.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:19:23.145: INFO: namespace: e2e-tests-container-lifecycle-hook-ltvln, resource: packagemanifests, items remaining: 1
Mar  3 00:19:23.778: INFO: namespace: e2e-tests-container-lifecycle-hook-ltvln, resource: bindings, ignored listing per whitelist
Mar  3 00:19:24.148: INFO: namespace: e2e-tests-container-lifecycle-hook-ltvln no longer exists
Mar  3 00:19:24.167: INFO: namespace: e2e-tests-container-lifecycle-hook-ltvln, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:19:24.184: INFO: namespace e2e-tests-container-lifecycle-hook-ltvln deletion completed in 26.530124786s

• [SLOW TEST:55.875 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:19:24.184: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:19:25.256: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version --client'
Mar  3 00:19:25.344: INFO: stderr: ""
Mar  3 00:19:25.344: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12+\", GitVersion:\"v1.12.7-beta.0.5+b17cdb9d0b95c4\", GitCommit:\"b17cdb9d0b95c4812325739f5fd40edd9e3daa4d\", GitTreeState:\"clean\", BuildDate:\"2019-03-02T22:24:57Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar  3 00:19:25.359: INFO: Not supported for server versions before "1.12.7-beta.0.5+b17cdb9d0b95c4"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:19:25.361: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8dzcq" for this suite.
Mar  3 00:19:31.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:19:32.274: INFO: namespace: e2e-tests-kubectl-8dzcq, resource: bindings, ignored listing per whitelist
Mar  3 00:19:32.397: INFO: namespace: e2e-tests-kubectl-8dzcq, resource: packagemanifests, items remaining: 1
Mar  3 00:19:33.876: INFO: namespace: e2e-tests-kubectl-8dzcq no longer exists
Mar  3 00:19:33.896: INFO: namespace: e2e-tests-kubectl-8dzcq, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:19:33.912: INFO: namespace e2e-tests-kubectl-8dzcq deletion completed in 8.532537412s

S [SKIPPING] [9.729 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

    Mar  3 00:19:25.359: Not supported for server versions before "1.12.7-beta.0.5+b17cdb9d0b95c4"

    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:19:33.913: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar  3 00:19:45.497: INFO: Pod pod-hostip-064aca51-3d4a-11e9-9008-0a58ac10f353 has hostIP: 10.0.160.190
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:19:45.497: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-h5rzr" for this suite.
Mar  3 00:20:09.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:20:10.711: INFO: namespace: e2e-tests-pods-h5rzr, resource: bindings, ignored listing per whitelist
Mar  3 00:20:11.503: INFO: namespace: e2e-tests-pods-h5rzr, resource: packagemanifests, items remaining: 1
Mar  3 00:20:12.034: INFO: namespace: e2e-tests-pods-h5rzr no longer exists
Mar  3 00:20:12.053: INFO: namespace: e2e-tests-pods-h5rzr, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:20:12.069: INFO: namespace e2e-tests-pods-h5rzr deletion completed in 26.526952497s

• [SLOW TEST:38.156 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:20:12.069: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:20:13.158: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-1ccd1e01-3d4a-11e9-9008-0a58ac10f353
STEP: Creating configMap with name cm-test-opt-upd-1ccd1ed3-3d4a-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1ccd1e01-3d4a-11e9-9008-0a58ac10f353
STEP: Updating configmap cm-test-opt-upd-1ccd1ed3-3d4a-11e9-9008-0a58ac10f353
STEP: Creating configMap with name cm-test-opt-create-1ccd1f0f-3d4a-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:21:30.231: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-z4nfd" for this suite.
Mar  3 00:21:54.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:21:55.708: INFO: namespace: e2e-tests-configmap-z4nfd, resource: bindings, ignored listing per whitelist
Mar  3 00:21:56.337: INFO: namespace: e2e-tests-configmap-z4nfd, resource: packagemanifests, items remaining: 1
Mar  3 00:21:56.759: INFO: namespace: e2e-tests-configmap-z4nfd no longer exists
Mar  3 00:21:56.779: INFO: namespace: e2e-tests-configmap-z4nfd, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:21:56.795: INFO: namespace e2e-tests-configmap-z4nfd deletion completed in 26.532231682s

• [SLOW TEST:104.726 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:21:56.795: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5b37c00a-3d4a-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  3 00:21:57.926: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-hrfqn" to be "success or failure"
Mar  3 00:21:57.943: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.088125ms
Mar  3 00:21:59.961: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034502653s
Mar  3 00:22:01.981: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05443136s
Mar  3 00:22:03.998: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071896116s
Mar  3 00:22:06.015: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088986453s
Mar  3 00:22:08.032: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.106020501s
STEP: Saw pod success
Mar  3 00:22:08.032: INFO: Pod "pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:22:08.049: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  3 00:22:08.101: INFO: Waiting for pod pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:22:08.118: INFO: Pod pod-projected-configmaps-5b3accc6-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:22:08.118: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hrfqn" for this suite.
Mar  3 00:22:14.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:22:16.081: INFO: namespace: e2e-tests-projected-hrfqn, resource: bindings, ignored listing per whitelist
Mar  3 00:22:16.240: INFO: namespace: e2e-tests-projected-hrfqn, resource: packagemanifests, items remaining: 1
Mar  3 00:22:16.659: INFO: namespace: e2e-tests-projected-hrfqn no longer exists
Mar  3 00:22:16.678: INFO: namespace: e2e-tests-projected-hrfqn, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:22:16.694: INFO: namespace e2e-tests-projected-hrfqn deletion completed in 8.530664057s

• [SLOW TEST:19.900 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:22:16.695: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  3 00:22:17.800: INFO: Waiting up to 5m0s for pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-s5wwc" to be "success or failure"
Mar  3 00:22:17.821: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 20.444652ms
Mar  3 00:22:19.838: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037723183s
Mar  3 00:22:21.856: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055107352s
Mar  3 00:22:23.873: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07211607s
Mar  3 00:22:25.890: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089190471s
Mar  3 00:22:27.907: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.106454965s
STEP: Saw pod success
Mar  3 00:22:27.907: INFO: Pod "pod-6712b719-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:22:27.923: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-6712b719-3d4a-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:22:27.972: INFO: Waiting for pod pod-6712b719-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:22:27.988: INFO: Pod pod-6712b719-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:22:27.988: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s5wwc" for this suite.
Mar  3 00:22:34.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:22:35.641: INFO: namespace: e2e-tests-emptydir-s5wwc, resource: bindings, ignored listing per whitelist
Mar  3 00:22:36.158: INFO: namespace: e2e-tests-emptydir-s5wwc, resource: packagemanifests, items remaining: 1
Mar  3 00:22:36.528: INFO: namespace: e2e-tests-emptydir-s5wwc no longer exists
Mar  3 00:22:36.546: INFO: namespace: e2e-tests-emptydir-s5wwc, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:22:36.563: INFO: namespace e2e-tests-emptydir-s5wwc deletion completed in 8.529567419s

• [SLOW TEST:19.868 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:22:36.563: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:22:37.718: INFO: (0) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 61.226779ms)
Mar  3 00:22:37.736: INFO: (1) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.616811ms)
Mar  3 00:22:37.755: INFO: (2) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.350858ms)
Mar  3 00:22:37.773: INFO: (3) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.986285ms)
Mar  3 00:22:37.791: INFO: (4) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.019478ms)
Mar  3 00:22:37.809: INFO: (5) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.008646ms)
Mar  3 00:22:37.827: INFO: (6) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.205646ms)
Mar  3 00:22:37.846: INFO: (7) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.44084ms)
Mar  3 00:22:37.865: INFO: (8) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.798077ms)
Mar  3 00:22:37.882: INFO: (9) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.883026ms)
Mar  3 00:22:37.900: INFO: (10) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.772356ms)
Mar  3 00:22:37.919: INFO: (11) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.142731ms)
Mar  3 00:22:37.937: INFO: (12) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.938172ms)
Mar  3 00:22:37.954: INFO: (13) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.768487ms)
Mar  3 00:22:37.972: INFO: (14) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.815401ms)
Mar  3 00:22:37.991: INFO: (15) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.628351ms)
Mar  3 00:22:38.009: INFO: (16) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 18.112601ms)
Mar  3 00:22:38.027: INFO: (17) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.971155ms)
Mar  3 00:22:38.045: INFO: (18) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.777296ms)
Mar  3 00:22:38.063: INFO: (19) /api/v1/nodes/ip-10-0-143-63.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="btmp">btmp</a>
<a href="ceph/">ceph/</a>
<a href... (200; 17.833436ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:22:38.063: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-m4qhn" for this suite.
Mar  3 00:22:44.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:22:44.991: INFO: namespace: e2e-tests-proxy-m4qhn, resource: packagemanifests, items remaining: 1
Mar  3 00:22:45.447: INFO: namespace: e2e-tests-proxy-m4qhn, resource: bindings, ignored listing per whitelist
Mar  3 00:22:45.678: INFO: namespace: e2e-tests-proxy-m4qhn no longer exists
Mar  3 00:22:45.697: INFO: namespace: e2e-tests-proxy-m4qhn, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:22:45.713: INFO: namespace e2e-tests-proxy-m4qhn deletion completed in 7.632390351s

• [SLOW TEST:9.150 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:22:45.713: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  3 00:22:46.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-2nclv" to be "success or failure"
Mar  3 00:22:46.824: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.278637ms
Mar  3 00:22:48.841: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03429712s
Mar  3 00:22:50.858: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051207222s
Mar  3 00:22:52.875: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06847256s
Mar  3 00:22:54.892: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085587661s
Mar  3 00:22:56.910: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103102904s
STEP: Saw pod success
Mar  3 00:22:56.910: INFO: Pod "downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:22:56.926: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  3 00:22:56.974: INFO: Waiting for pod downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:22:56.991: INFO: Pod downwardapi-volume-785d25e1-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:22:56.991: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2nclv" for this suite.
Mar  3 00:23:03.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:23:04.410: INFO: namespace: e2e-tests-downward-api-2nclv, resource: packagemanifests, items remaining: 1
Mar  3 00:23:05.308: INFO: namespace: e2e-tests-downward-api-2nclv, resource: bindings, ignored listing per whitelist
Mar  3 00:23:05.533: INFO: namespace: e2e-tests-downward-api-2nclv no longer exists
Mar  3 00:23:05.552: INFO: namespace: e2e-tests-downward-api-2nclv, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:23:05.568: INFO: namespace e2e-tests-downward-api-2nclv deletion completed in 8.529209098s

• [SLOW TEST:19.855 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:23:05.568: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  3 00:23:06.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-9cvc2" to be "success or failure"
Mar  3 00:23:06.671: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.257773ms
Mar  3 00:23:08.689: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03534153s
Mar  3 00:23:10.707: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053059044s
Mar  3 00:23:12.724: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070014659s
Mar  3 00:23:14.741: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087173337s
Mar  3 00:23:16.759: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104710343s
STEP: Saw pod success
Mar  3 00:23:16.759: INFO: Pod "downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:23:16.777: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  3 00:23:16.894: INFO: Waiting for pod downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:23:16.910: INFO: Pod downwardapi-volume-8431333d-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:23:16.911: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9cvc2" for this suite.
Mar  3 00:23:23.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:23:24.010: INFO: namespace: e2e-tests-projected-9cvc2, resource: bindings, ignored listing per whitelist
Mar  3 00:23:24.479: INFO: namespace: e2e-tests-projected-9cvc2, resource: packagemanifests, items remaining: 1
Mar  3 00:23:25.466: INFO: namespace: e2e-tests-projected-9cvc2 no longer exists
Mar  3 00:23:25.499: INFO: namespace: e2e-tests-projected-9cvc2, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:23:25.515: INFO: namespace e2e-tests-projected-9cvc2 deletion completed in 8.543602373s

• [SLOW TEST:19.947 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:23:25.516: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353
Mar  3 00:23:26.640: INFO: Pod name my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353: Found 0 pods out of 1
Mar  3 00:23:31.658: INFO: Pod name my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353: Found 1 pods out of 1
Mar  3 00:23:31.658: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353" are running
Mar  3 00:23:35.692: INFO: Pod "my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353-ph5vr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-03 00:23:26 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-03 00:23:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-03 00:23:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-03 00:23:26 +0000 UTC Reason: Message:}])
Mar  3 00:23:35.692: INFO: Trying to dial the pod
Mar  3 00:23:40.755: INFO: Controller my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353: Got expected result from replica 1 [my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353-ph5vr]: "my-hostname-basic-901a2255-3d4a-11e9-9008-0a58ac10f353-ph5vr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:23:40.756: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-fj7d8" for this suite.
Mar  3 00:23:48.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:23:49.837: INFO: namespace: e2e-tests-replication-controller-fj7d8, resource: packagemanifests, items remaining: 1
Mar  3 00:23:50.595: INFO: namespace: e2e-tests-replication-controller-fj7d8, resource: bindings, ignored listing per whitelist
Mar  3 00:23:51.326: INFO: namespace: e2e-tests-replication-controller-fj7d8 no longer exists
Mar  3 00:23:51.359: INFO: namespace: e2e-tests-replication-controller-fj7d8, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:23:51.375: INFO: namespace e2e-tests-replication-controller-fj7d8 deletion completed in 10.560810109s

• [SLOW TEST:25.860 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:23:51.375: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  3 00:23:52.465: INFO: Waiting up to 5m0s for pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-clp6c" to be "success or failure"
Mar  3 00:23:52.482: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 16.776501ms
Mar  3 00:23:54.501: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03509177s
Mar  3 00:23:56.518: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052790262s
Mar  3 00:23:58.536: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070947756s
Mar  3 00:24:00.554: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088125479s
Mar  3 00:24:02.571: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.105279372s
STEP: Saw pod success
Mar  3 00:24:02.571: INFO: Pod "pod-9f80167c-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:24:02.587: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-9f80167c-3d4a-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:24:02.636: INFO: Waiting for pod pod-9f80167c-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:24:02.653: INFO: Pod pod-9f80167c-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:24:02.653: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-clp6c" for this suite.
Mar  3 00:24:08.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:24:09.833: INFO: namespace: e2e-tests-emptydir-clp6c, resource: bindings, ignored listing per whitelist
Mar  3 00:24:10.145: INFO: namespace: e2e-tests-emptydir-clp6c, resource: packagemanifests, items remaining: 1
Mar  3 00:24:11.207: INFO: namespace: e2e-tests-emptydir-clp6c no longer exists
Mar  3 00:24:11.240: INFO: namespace: e2e-tests-emptydir-clp6c, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:24:11.257: INFO: namespace e2e-tests-emptydir-clp6c deletion completed in 8.544619914s

• [SLOW TEST:19.881 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:24:11.257: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  3 00:24:12.352: INFO: Waiting up to 5m0s for pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-pv2cc" to be "success or failure"
Mar  3 00:24:12.369: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.741163ms
Mar  3 00:24:14.387: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034948383s
Mar  3 00:24:16.405: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053708247s
Mar  3 00:24:18.426: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074184896s
Mar  3 00:24:20.443: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091424761s
Mar  3 00:24:22.460: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108488716s
STEP: Saw pod success
Mar  3 00:24:22.460: INFO: Pod "downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:24:22.477: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  3 00:24:22.523: INFO: Waiting for pod downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:24:22.540: INFO: Pod downward-api-ab5a3fbb-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:24:22.540: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pv2cc" for this suite.
Mar  3 00:24:28.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:24:30.127: INFO: namespace: e2e-tests-downward-api-pv2cc, resource: packagemanifests, items remaining: 1
Mar  3 00:24:30.740: INFO: namespace: e2e-tests-downward-api-pv2cc, resource: bindings, ignored listing per whitelist
Mar  3 00:24:31.077: INFO: namespace: e2e-tests-downward-api-pv2cc no longer exists
Mar  3 00:24:31.110: INFO: namespace: e2e-tests-downward-api-pv2cc, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:24:31.127: INFO: namespace e2e-tests-downward-api-pv2cc deletion completed in 8.541933376s

• [SLOW TEST:19.870 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:24:31.127: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-b734d0f6-3d4a-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume configMaps
Mar  3 00:24:32.259: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-configmap-h7srp" to be "success or failure"
Mar  3 00:24:32.278: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.937214ms
Mar  3 00:24:34.295: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036367694s
Mar  3 00:24:36.316: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056490077s
Mar  3 00:24:38.333: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073586164s
Mar  3 00:24:40.350: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090859956s
Mar  3 00:24:42.367: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108112411s
STEP: Saw pod success
Mar  3 00:24:42.367: INFO: Pod "pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:24:42.384: INFO: Trying to get logs from node ip-10-0-148-56.ec2.internal pod pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  3 00:24:42.432: INFO: Waiting for pod pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:24:42.448: INFO: Pod pod-configmaps-b7381491-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:24:42.448: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h7srp" for this suite.
Mar  3 00:24:48.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:24:49.890: INFO: namespace: e2e-tests-configmap-h7srp, resource: bindings, ignored listing per whitelist
Mar  3 00:24:50.734: INFO: namespace: e2e-tests-configmap-h7srp, resource: packagemanifests, items remaining: 1
Mar  3 00:24:50.998: INFO: namespace: e2e-tests-configmap-h7srp no longer exists
Mar  3 00:24:51.031: INFO: namespace: e2e-tests-configmap-h7srp, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:24:51.047: INFO: namespace e2e-tests-configmap-h7srp deletion completed in 8.541076859s

• [SLOW TEST:19.921 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:24:51.048: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar  3 00:24:52.271: INFO: Waiting up to 5m0s for pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-var-expansion-2gtzd" to be "success or failure"
Mar  3 00:24:52.287: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 16.420298ms
Mar  3 00:24:54.305: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033557544s
Mar  3 00:24:56.323: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051698908s
Mar  3 00:24:58.348: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076983499s
Mar  3 00:25:00.365: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094185005s
Mar  3 00:25:02.382: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111520872s
STEP: Saw pod success
Mar  3 00:25:02.383: INFO: Pod "var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:25:02.401: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353 container dapi-container: <nil>
STEP: delete the pod
Mar  3 00:25:02.448: INFO: Waiting for pod var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:25:02.465: INFO: Pod var-expansion-c3255a1a-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:25:02.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-2gtzd" for this suite.
Mar  3 00:25:08.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:25:09.772: INFO: namespace: e2e-tests-var-expansion-2gtzd, resource: packagemanifests, items remaining: 1
Mar  3 00:25:10.837: INFO: namespace: e2e-tests-var-expansion-2gtzd, resource: bindings, ignored listing per whitelist
Mar  3 00:25:11.004: INFO: namespace: e2e-tests-var-expansion-2gtzd no longer exists
Mar  3 00:25:11.036: INFO: namespace: e2e-tests-var-expansion-2gtzd, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:25:11.052: INFO: namespace e2e-tests-var-expansion-2gtzd deletion completed in 8.541559803s

• [SLOW TEST:20.005 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:25:11.052: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:25:12.285: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cf100974-3d4a-11e9-97ac-0e81de5d2554", Controller:(*bool)(0xc42348bf46), BlockOwnerDeletion:(*bool)(0xc42348bf47)}}
Mar  3 00:25:12.307: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cf03f298-3d4a-11e9-97ac-0e81de5d2554", Controller:(*bool)(0xc422fc4032), BlockOwnerDeletion:(*bool)(0xc422fc4033)}}
Mar  3 00:25:12.326: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"cf08b96c-3d4a-11e9-97ac-0e81de5d2554", Controller:(*bool)(0xc422fda182), BlockOwnerDeletion:(*bool)(0xc422fda183)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:25:17.363: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-j4rkt" for this suite.
Mar  3 00:25:23.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:25:24.712: INFO: namespace: e2e-tests-gc-j4rkt, resource: bindings, ignored listing per whitelist
Mar  3 00:25:25.901: INFO: namespace: e2e-tests-gc-j4rkt, resource: packagemanifests, items remaining: 1
Mar  3 00:25:25.919: INFO: namespace: e2e-tests-gc-j4rkt no longer exists
Mar  3 00:25:25.953: INFO: namespace: e2e-tests-gc-j4rkt, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:25:25.970: INFO: namespace e2e-tests-gc-j4rkt deletion completed in 8.54797877s

• [SLOW TEST:14.918 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:25:25.970: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d7e1bed3-3d4a-11e9-9008-0a58ac10f353
STEP: Creating a pod to test consume secrets
Mar  3 00:25:27.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353" in namespace "e2e-tests-projected-rf8gz" to be "success or failure"
Mar  3 00:25:27.120: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 18.683321ms
Mar  3 00:25:29.140: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038523586s
Mar  3 00:25:31.157: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055662699s
Mar  3 00:25:33.175: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072908403s
Mar  3 00:25:35.192: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090309367s
Mar  3 00:25:37.209: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107331667s
STEP: Saw pod success
Mar  3 00:25:37.209: INFO: Pod "pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:25:37.226: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 00:25:37.274: INFO: Waiting for pod pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:25:37.290: INFO: Pod pod-projected-secrets-d7e57ba7-3d4a-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:25:37.290: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rf8gz" for this suite.
Mar  3 00:25:43.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:25:44.291: INFO: namespace: e2e-tests-projected-rf8gz, resource: packagemanifests, items remaining: 1
Mar  3 00:25:45.610: INFO: namespace: e2e-tests-projected-rf8gz, resource: bindings, ignored listing per whitelist
Mar  3 00:25:45.829: INFO: namespace: e2e-tests-projected-rf8gz no longer exists
Mar  3 00:25:45.864: INFO: namespace: e2e-tests-projected-rf8gz, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:25:45.880: INFO: namespace e2e-tests-projected-rf8gz deletion completed in 8.54456746s

• [SLOW TEST:19.910 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:25:45.880: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:25:47.104: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  3 00:25:47.140: INFO: Number of nodes with available pods: 0
Mar  3 00:25:47.140: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  3 00:25:47.240: INFO: Number of nodes with available pods: 0
Mar  3 00:25:47.240: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:48.257: INFO: Number of nodes with available pods: 0
Mar  3 00:25:48.257: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:49.258: INFO: Number of nodes with available pods: 0
Mar  3 00:25:49.258: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:50.258: INFO: Number of nodes with available pods: 0
Mar  3 00:25:50.258: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:51.257: INFO: Number of nodes with available pods: 0
Mar  3 00:25:51.257: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:52.257: INFO: Number of nodes with available pods: 0
Mar  3 00:25:52.257: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:53.257: INFO: Number of nodes with available pods: 0
Mar  3 00:25:53.258: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:54.257: INFO: Number of nodes with available pods: 0
Mar  3 00:25:54.257: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:55.258: INFO: Number of nodes with available pods: 1
Mar  3 00:25:55.258: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  3 00:25:55.331: INFO: Number of nodes with available pods: 0
Mar  3 00:25:55.331: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  3 00:25:55.370: INFO: Number of nodes with available pods: 0
Mar  3 00:25:55.370: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:56.387: INFO: Number of nodes with available pods: 0
Mar  3 00:25:56.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:57.387: INFO: Number of nodes with available pods: 0
Mar  3 00:25:57.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:58.388: INFO: Number of nodes with available pods: 0
Mar  3 00:25:58.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:25:59.387: INFO: Number of nodes with available pods: 0
Mar  3 00:25:59.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:00.389: INFO: Number of nodes with available pods: 0
Mar  3 00:26:00.389: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:01.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:01.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:02.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:02.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:03.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:03.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:04.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:04.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:05.388: INFO: Number of nodes with available pods: 0
Mar  3 00:26:05.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:06.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:06.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:07.389: INFO: Number of nodes with available pods: 0
Mar  3 00:26:07.389: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:08.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:08.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:09.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:09.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:10.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:10.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:11.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:11.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:12.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:12.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:13.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:13.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:14.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:14.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:15.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:15.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:16.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:16.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:17.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:17.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:18.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:18.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:19.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:19.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:20.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:20.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:21.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:21.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:22.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:22.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:23.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:23.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:24.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:24.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:25.388: INFO: Number of nodes with available pods: 0
Mar  3 00:26:25.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:26.389: INFO: Number of nodes with available pods: 0
Mar  3 00:26:26.389: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:27.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:27.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:28.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:28.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:29.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:29.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:30.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:30.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:31.390: INFO: Number of nodes with available pods: 0
Mar  3 00:26:31.390: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:32.388: INFO: Number of nodes with available pods: 0
Mar  3 00:26:32.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:33.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:33.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:34.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:34.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:35.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:35.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:36.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:36.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:37.388: INFO: Number of nodes with available pods: 0
Mar  3 00:26:37.388: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:38.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:38.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:39.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:39.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:40.387: INFO: Number of nodes with available pods: 0
Mar  3 00:26:40.387: INFO: Node ip-10-0-143-63.ec2.internal is running more than one daemon pod
Mar  3 00:26:41.387: INFO: Number of nodes with available pods: 1
Mar  3 00:26:41.388: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-wcrqj, will wait for the garbage collector to delete the pods
Mar  3 00:26:41.510: INFO: Deleting {extensions DaemonSet} daemon-set took: 22.0446ms
Mar  3 00:26:41.610: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.343453ms
Mar  3 00:27:15.428: INFO: Number of nodes with available pods: 0
Mar  3 00:27:15.429: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 00:27:15.445: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-wcrqj/daemonsets","resourceVersion":"118304"},"items":null}

Mar  3 00:27:15.463: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-wcrqj/pods","resourceVersion":"118304"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:27:15.565: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-wcrqj" for this suite.
Mar  3 00:27:21.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:27:23.381: INFO: namespace: e2e-tests-daemonsets-wcrqj, resource: bindings, ignored listing per whitelist
Mar  3 00:27:23.471: INFO: namespace: e2e-tests-daemonsets-wcrqj, resource: packagemanifests, items remaining: 1
Mar  3 00:27:24.104: INFO: namespace: e2e-tests-daemonsets-wcrqj no longer exists
Mar  3 00:27:24.125: INFO: namespace: e2e-tests-daemonsets-wcrqj, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:27:24.142: INFO: namespace e2e-tests-daemonsets-wcrqj deletion completed in 8.532290782s

• [SLOW TEST:98.262 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:27:24.143: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:27:35.341: INFO: Waiting up to 5m0s for pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353" in namespace "e2e-tests-pods-lk64p" to be "success or failure"
Mar  3 00:27:35.359: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.984604ms
Mar  3 00:27:37.377: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035222848s
Mar  3 00:27:39.394: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052270771s
Mar  3 00:27:41.411: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069614123s
Mar  3 00:27:43.428: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08676426s
Mar  3 00:27:45.445: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104124091s
STEP: Saw pod success
Mar  3 00:27:45.445: INFO: Pod "client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:27:45.462: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353 container env3cont: <nil>
STEP: delete the pod
Mar  3 00:27:45.515: INFO: Waiting for pod client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:27:45.531: INFO: Pod client-envvars-24585c87-3d4b-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:27:45.532: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lk64p" for this suite.
Mar  3 00:28:25.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:28:26.468: INFO: namespace: e2e-tests-pods-lk64p, resource: packagemanifests, items remaining: 1
Mar  3 00:28:27.101: INFO: namespace: e2e-tests-pods-lk64p, resource: bindings, ignored listing per whitelist
Mar  3 00:28:28.070: INFO: namespace: e2e-tests-pods-lk64p no longer exists
Mar  3 00:28:28.090: INFO: namespace: e2e-tests-pods-lk64p, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:28:28.107: INFO: namespace e2e-tests-pods-lk64p deletion completed in 42.529857s

• [SLOW TEST:63.964 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:28:28.107: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  3 00:28:29.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353" in namespace "e2e-tests-downward-api-lp4ww" to be "success or failure"
Mar  3 00:28:29.224: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 16.990884ms
Mar  3 00:28:31.242: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035533611s
Mar  3 00:28:33.259: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052801327s
Mar  3 00:28:35.277: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06994153s
Mar  3 00:28:37.296: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089109287s
Mar  3 00:28:39.313: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.106460702s
STEP: Saw pod success
Mar  3 00:28:39.313: INFO: Pod "downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:28:39.330: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353 container client-container: <nil>
STEP: delete the pod
Mar  3 00:28:39.380: INFO: Waiting for pod downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:28:39.397: INFO: Pod downwardapi-volume-44723d54-3d4b-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:28:39.397: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lp4ww" for this suite.
Mar  3 00:28:45.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:28:47.367: INFO: namespace: e2e-tests-downward-api-lp4ww, resource: packagemanifests, items remaining: 1
Mar  3 00:28:47.401: INFO: namespace: e2e-tests-downward-api-lp4ww, resource: bindings, ignored listing per whitelist
Mar  3 00:28:47.932: INFO: namespace: e2e-tests-downward-api-lp4ww no longer exists
Mar  3 00:28:47.950: INFO: namespace: e2e-tests-downward-api-lp4ww, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:28:47.966: INFO: namespace e2e-tests-downward-api-lp4ww deletion completed in 8.524777952s

• [SLOW TEST:19.859 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:28:47.967: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  3 00:28:49.060: INFO: Waiting up to 5m0s for pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353" in namespace "e2e-tests-emptydir-4xxrf" to be "success or failure"
Mar  3 00:28:49.078: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 17.577412ms
Mar  3 00:28:51.115: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055451863s
Mar  3 00:28:53.132: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072432894s
Mar  3 00:28:55.149: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089414167s
Mar  3 00:28:57.167: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10713664s
Mar  3 00:28:59.187: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.126940376s
STEP: Saw pod success
Mar  3 00:28:59.187: INFO: Pod "pod-50479a2b-3d4b-11e9-9008-0a58ac10f353" satisfied condition "success or failure"
Mar  3 00:28:59.204: INFO: Trying to get logs from node ip-10-0-160-190.ec2.internal pod pod-50479a2b-3d4b-11e9-9008-0a58ac10f353 container test-container: <nil>
STEP: delete the pod
Mar  3 00:28:59.253: INFO: Waiting for pod pod-50479a2b-3d4b-11e9-9008-0a58ac10f353 to disappear
Mar  3 00:28:59.269: INFO: Pod pod-50479a2b-3d4b-11e9-9008-0a58ac10f353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:28:59.269: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4xxrf" for this suite.
Mar  3 00:29:05.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:29:06.229: INFO: namespace: e2e-tests-emptydir-4xxrf, resource: packagemanifests, items remaining: 1
Mar  3 00:29:07.523: INFO: namespace: e2e-tests-emptydir-4xxrf, resource: bindings, ignored listing per whitelist
Mar  3 00:29:07.816: INFO: namespace: e2e-tests-emptydir-4xxrf no longer exists
Mar  3 00:29:07.849: INFO: namespace: e2e-tests-emptydir-4xxrf, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:29:07.865: INFO: namespace e2e-tests-emptydir-4xxrf deletion completed in 8.550862452s

• [SLOW TEST:19.899 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:29:07.865: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-kjdsd
I0303 00:29:08.946578   27430 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-kjdsd, replica count: 1
I0303 00:29:09.997350   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:10.997663   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:11.998003   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:12.998266   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:13.998592   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:14.998874   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:15.999185   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:16.999543   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 00:29:17.999858   27430 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 00:29:18.126: INFO: Created: latency-svc-6rprh
Mar  3 00:29:18.142: INFO: Got endpoints: latency-svc-6rprh [42.619872ms]
Mar  3 00:29:18.173: INFO: Created: latency-svc-lvm7c
Mar  3 00:29:18.179: INFO: Got endpoints: latency-svc-lvm7c [36.55626ms]
Mar  3 00:29:18.185: INFO: Created: latency-svc-nstjd
Mar  3 00:29:18.190: INFO: Got endpoints: latency-svc-nstjd [47.175377ms]
Mar  3 00:29:18.199: INFO: Created: latency-svc-xx9qc
Mar  3 00:29:18.205: INFO: Got endpoints: latency-svc-xx9qc [62.562404ms]
Mar  3 00:29:18.210: INFO: Created: latency-svc-tzpbw
Mar  3 00:29:18.215: INFO: Got endpoints: latency-svc-tzpbw [72.138154ms]
Mar  3 00:29:18.221: INFO: Created: latency-svc-drnrm
Mar  3 00:29:18.228: INFO: Got endpoints: latency-svc-drnrm [85.566349ms]
Mar  3 00:29:18.240: INFO: Created: latency-svc-7lx9w
Mar  3 00:29:18.246: INFO: Got endpoints: latency-svc-7lx9w [102.943666ms]
Mar  3 00:29:18.252: INFO: Created: latency-svc-xw8jr
Mar  3 00:29:18.258: INFO: Got endpoints: latency-svc-xw8jr [114.675207ms]
Mar  3 00:29:18.270: INFO: Created: latency-svc-2q7cx
Mar  3 00:29:18.275: INFO: Got endpoints: latency-svc-2q7cx [132.372263ms]
Mar  3 00:29:18.287: INFO: Created: latency-svc-75cp4
Mar  3 00:29:18.289: INFO: Got endpoints: latency-svc-75cp4 [146.713726ms]
Mar  3 00:29:18.294: INFO: Created: latency-svc-zhmw9
Mar  3 00:29:18.302: INFO: Got endpoints: latency-svc-zhmw9 [158.971978ms]
Mar  3 00:29:18.308: INFO: Created: latency-svc-wjvc9
Mar  3 00:29:18.315: INFO: Got endpoints: latency-svc-wjvc9 [171.835169ms]
Mar  3 00:29:18.321: INFO: Created: latency-svc-wwdtb
Mar  3 00:29:18.326: INFO: Got endpoints: latency-svc-wwdtb [183.340492ms]
Mar  3 00:29:18.332: INFO: Created: latency-svc-nltct
Mar  3 00:29:18.339: INFO: Got endpoints: latency-svc-nltct [195.692104ms]
Mar  3 00:29:18.343: INFO: Created: latency-svc-zpmbg
Mar  3 00:29:18.350: INFO: Got endpoints: latency-svc-zpmbg [206.77438ms]
Mar  3 00:29:18.355: INFO: Created: latency-svc-4bxgj
Mar  3 00:29:18.360: INFO: Got endpoints: latency-svc-4bxgj [216.984995ms]
Mar  3 00:29:18.368: INFO: Created: latency-svc-lxlgr
Mar  3 00:29:18.374: INFO: Got endpoints: latency-svc-lxlgr [194.843626ms]
Mar  3 00:29:18.382: INFO: Created: latency-svc-2c9fg
Mar  3 00:29:18.389: INFO: Got endpoints: latency-svc-2c9fg [199.370146ms]
Mar  3 00:29:18.396: INFO: Created: latency-svc-tjvcn
Mar  3 00:29:18.404: INFO: Got endpoints: latency-svc-tjvcn [198.819612ms]
Mar  3 00:29:18.405: INFO: Created: latency-svc-dpv4q
Mar  3 00:29:18.417: INFO: Got endpoints: latency-svc-dpv4q [201.718676ms]
Mar  3 00:29:18.420: INFO: Created: latency-svc-m9g2j
Mar  3 00:29:18.428: INFO: Got endpoints: latency-svc-m9g2j [199.211115ms]
Mar  3 00:29:18.435: INFO: Created: latency-svc-j52gz
Mar  3 00:29:18.441: INFO: Got endpoints: latency-svc-j52gz [195.019036ms]
Mar  3 00:29:18.447: INFO: Created: latency-svc-ckss7
Mar  3 00:29:18.452: INFO: Got endpoints: latency-svc-ckss7 [194.367878ms]
Mar  3 00:29:18.456: INFO: Created: latency-svc-98l74
Mar  3 00:29:18.464: INFO: Got endpoints: latency-svc-98l74 [188.540851ms]
Mar  3 00:29:18.468: INFO: Created: latency-svc-dv6k4
Mar  3 00:29:18.475: INFO: Got endpoints: latency-svc-dv6k4 [185.574485ms]
Mar  3 00:29:18.488: INFO: Created: latency-svc-cxlxl
Mar  3 00:29:18.493: INFO: Got endpoints: latency-svc-cxlxl [191.234447ms]
Mar  3 00:29:18.499: INFO: Created: latency-svc-x4ppq
Mar  3 00:29:18.503: INFO: Got endpoints: latency-svc-x4ppq [188.437788ms]
Mar  3 00:29:18.515: INFO: Created: latency-svc-nfr2k
Mar  3 00:29:18.528: INFO: Got endpoints: latency-svc-nfr2k [201.73511ms]
Mar  3 00:29:18.535: INFO: Created: latency-svc-slm9z
Mar  3 00:29:18.552: INFO: Got endpoints: latency-svc-slm9z [213.637448ms]
Mar  3 00:29:18.553: INFO: Created: latency-svc-p9tgg
Mar  3 00:29:18.559: INFO: Got endpoints: latency-svc-p9tgg [209.565095ms]
Mar  3 00:29:18.563: INFO: Created: latency-svc-57ccc
Mar  3 00:29:18.567: INFO: Got endpoints: latency-svc-57ccc [207.40248ms]
Mar  3 00:29:18.579: INFO: Created: latency-svc-59gxg
Mar  3 00:29:18.583: INFO: Got endpoints: latency-svc-59gxg [209.249316ms]
Mar  3 00:29:18.610: INFO: Created: latency-svc-rk9vw
Mar  3 00:29:18.616: INFO: Got endpoints: latency-svc-rk9vw [226.961239ms]
Mar  3 00:29:18.624: INFO: Created: latency-svc-6cl8n
Mar  3 00:29:18.630: INFO: Got endpoints: latency-svc-6cl8n [225.305257ms]
Mar  3 00:29:18.636: INFO: Created: latency-svc-b4cnv
Mar  3 00:29:18.641: INFO: Got endpoints: latency-svc-b4cnv [224.798305ms]
Mar  3 00:29:18.648: INFO: Created: latency-svc-mcfbl
Mar  3 00:29:18.655: INFO: Got endpoints: latency-svc-mcfbl [227.107498ms]
Mar  3 00:29:18.660: INFO: Created: latency-svc-z4drd
Mar  3 00:29:18.666: INFO: Got endpoints: latency-svc-z4drd [224.999413ms]
Mar  3 00:29:18.671: INFO: Created: latency-svc-j8b4w
Mar  3 00:29:18.678: INFO: Got endpoints: latency-svc-j8b4w [225.627963ms]
Mar  3 00:29:18.684: INFO: Created: latency-svc-trlbm
Mar  3 00:29:18.688: INFO: Got endpoints: latency-svc-trlbm [224.141954ms]
Mar  3 00:29:18.695: INFO: Created: latency-svc-r9928
Mar  3 00:29:18.702: INFO: Got endpoints: latency-svc-r9928 [226.993903ms]
Mar  3 00:29:18.711: INFO: Created: latency-svc-p7dc8
Mar  3 00:29:18.722: INFO: Got endpoints: latency-svc-p7dc8 [228.700039ms]
Mar  3 00:29:18.729: INFO: Created: latency-svc-lk7sr
Mar  3 00:29:18.738: INFO: Got endpoints: latency-svc-lk7sr [235.264359ms]
Mar  3 00:29:18.749: INFO: Created: latency-svc-d8sfh
Mar  3 00:29:18.758: INFO: Created: latency-svc-q9x84
Mar  3 00:29:18.770: INFO: Created: latency-svc-brxrb
Mar  3 00:29:18.779: INFO: Created: latency-svc-5q558
Mar  3 00:29:18.783: INFO: Got endpoints: latency-svc-d8sfh [254.490966ms]
Mar  3 00:29:18.792: INFO: Created: latency-svc-c9zvx
Mar  3 00:29:18.805: INFO: Created: latency-svc-wzhsj
Mar  3 00:29:18.825: INFO: Created: latency-svc-gb4mf
Mar  3 00:29:18.834: INFO: Got endpoints: latency-svc-q9x84 [281.746021ms]
Mar  3 00:29:18.837: INFO: Created: latency-svc-wx7vn
Mar  3 00:29:18.854: INFO: Created: latency-svc-gtzdx
Mar  3 00:29:18.867: INFO: Created: latency-svc-vzgvh
Mar  3 00:29:18.875: INFO: Created: latency-svc-qnpzr
Mar  3 00:29:18.882: INFO: Got endpoints: latency-svc-brxrb [322.970419ms]
Mar  3 00:29:18.887: INFO: Created: latency-svc-zchpv
Mar  3 00:29:18.907: INFO: Created: latency-svc-r2x4n
Mar  3 00:29:18.919: INFO: Created: latency-svc-qs9bg
Mar  3 00:29:18.929: INFO: Created: latency-svc-sthth
Mar  3 00:29:18.936: INFO: Got endpoints: latency-svc-5q558 [368.601031ms]
Mar  3 00:29:18.947: INFO: Created: latency-svc-2652t
Mar  3 00:29:18.955: INFO: Created: latency-svc-gnvh5
Mar  3 00:29:18.966: INFO: Created: latency-svc-qjnh4
Mar  3 00:29:18.982: INFO: Created: latency-svc-zxw55
Mar  3 00:29:18.987: INFO: Got endpoints: latency-svc-c9zvx [403.749732ms]
Mar  3 00:29:19.014: INFO: Created: latency-svc-b2z5b
Mar  3 00:29:19.031: INFO: Got endpoints: latency-svc-wzhsj [414.281811ms]
Mar  3 00:29:19.068: INFO: Created: latency-svc-vd8mr
Mar  3 00:29:19.081: INFO: Got endpoints: latency-svc-gb4mf [451.628656ms]
Mar  3 00:29:19.112: INFO: Created: latency-svc-fp9kn
Mar  3 00:29:19.132: INFO: Got endpoints: latency-svc-wx7vn [490.323503ms]
Mar  3 00:29:19.165: INFO: Created: latency-svc-kkqbf
Mar  3 00:29:19.181: INFO: Got endpoints: latency-svc-gtzdx [526.400437ms]
Mar  3 00:29:19.210: INFO: Created: latency-svc-ckfvm
Mar  3 00:29:19.231: INFO: Got endpoints: latency-svc-vzgvh [565.080203ms]
Mar  3 00:29:19.266: INFO: Created: latency-svc-fvlk6
Mar  3 00:29:19.281: INFO: Got endpoints: latency-svc-qnpzr [602.801029ms]
Mar  3 00:29:19.310: INFO: Created: latency-svc-qcwk4
Mar  3 00:29:19.332: INFO: Got endpoints: latency-svc-zchpv [643.527299ms]
Mar  3 00:29:19.360: INFO: Created: latency-svc-vs6d9
Mar  3 00:29:19.381: INFO: Got endpoints: latency-svc-r2x4n [678.691531ms]
Mar  3 00:29:19.414: INFO: Created: latency-svc-tnttj
Mar  3 00:29:19.436: INFO: Got endpoints: latency-svc-qs9bg [714.337674ms]
Mar  3 00:29:19.469: INFO: Created: latency-svc-m6m2k
Mar  3 00:29:19.483: INFO: Got endpoints: latency-svc-sthth [744.635339ms]
Mar  3 00:29:19.510: INFO: Created: latency-svc-h8plq
Mar  3 00:29:19.530: INFO: Got endpoints: latency-svc-2652t [747.516632ms]
Mar  3 00:29:19.556: INFO: Created: latency-svc-m4wb5
Mar  3 00:29:19.581: INFO: Got endpoints: latency-svc-gnvh5 [747.004549ms]
Mar  3 00:29:19.611: INFO: Created: latency-svc-l5w78
Mar  3 00:29:19.634: INFO: Got endpoints: latency-svc-qjnh4 [751.780978ms]
Mar  3 00:29:19.663: INFO: Created: latency-svc-lnwwf
Mar  3 00:29:19.681: INFO: Got endpoints: latency-svc-zxw55 [744.56825ms]
Mar  3 00:29:19.716: INFO: Created: latency-svc-qnwwx
Mar  3 00:29:19.730: INFO: Got endpoints: latency-svc-b2z5b [743.219409ms]
Mar  3 00:29:19.761: INFO: Created: latency-svc-r6vjg
Mar  3 00:29:19.781: INFO: Got endpoints: latency-svc-vd8mr [750.196428ms]
Mar  3 00:29:19.810: INFO: Created: latency-svc-sq68w
Mar  3 00:29:19.831: INFO: Got endpoints: latency-svc-fp9kn [749.153831ms]
Mar  3 00:29:19.863: INFO: Created: latency-svc-97cvs
Mar  3 00:29:19.881: INFO: Got endpoints: latency-svc-kkqbf [749.257446ms]
Mar  3 00:29:19.908: INFO: Created: latency-svc-gnmxc
Mar  3 00:29:19.930: INFO: Got endpoints: latency-svc-ckfvm [748.940609ms]
Mar  3 00:29:19.957: INFO: Created: latency-svc-2v6dz
Mar  3 00:29:19.981: INFO: Got endpoints: latency-svc-fvlk6 [750.299269ms]
Mar  3 00:29:20.023: INFO: Created: latency-svc-bpn9p
Mar  3 00:29:20.031: INFO: Got endpoints: latency-svc-qcwk4 [749.96013ms]
Mar  3 00:29:20.082: INFO: Created: latency-svc-jsx5n
Mar  3 00:29:20.095: INFO: Got endpoints: latency-svc-vs6d9 [763.594506ms]
Mar  3 00:29:20.138: INFO: Created: latency-svc-fb4jh
Mar  3 00:29:20.139: INFO: Got endpoints: latency-svc-tnttj [758.169124ms]
Mar  3 00:29:20.199: INFO: Got endpoints: latency-svc-m6m2k [762.897491ms]
Mar  3 00:29:20.226: INFO: Created: latency-svc-574qd
Mar  3 00:29:20.234: INFO: Got endpoints: latency-svc-h8plq [750.271676ms]
Mar  3 00:29:20.252: INFO: Created: latency-svc-t5lr9
Mar  3 00:29:20.277: INFO: Created: latency-svc-j9p84
Mar  3 00:29:20.281: INFO: Got endpoints: latency-svc-m4wb5 [751.132667ms]
Mar  3 00:29:20.326: INFO: Created: latency-svc-n64lg
Mar  3 00:29:20.331: INFO: Got endpoints: latency-svc-l5w78 [750.132491ms]
Mar  3 00:29:20.361: INFO: Created: latency-svc-glnvg
Mar  3 00:29:20.382: INFO: Got endpoints: latency-svc-lnwwf [747.868271ms]
Mar  3 00:29:20.432: INFO: Got endpoints: latency-svc-qnwwx [751.206435ms]
Mar  3 00:29:20.442: INFO: Created: latency-svc-tkn92
Mar  3 00:29:20.464: INFO: Created: latency-svc-khjrn
Mar  3 00:29:20.484: INFO: Got endpoints: latency-svc-r6vjg [753.083456ms]
Mar  3 00:29:20.517: INFO: Created: latency-svc-t47n2
Mar  3 00:29:20.531: INFO: Got endpoints: latency-svc-sq68w [749.970623ms]
Mar  3 00:29:20.559: INFO: Created: latency-svc-w2kc9
Mar  3 00:29:20.581: INFO: Got endpoints: latency-svc-97cvs [750.193902ms]
Mar  3 00:29:20.609: INFO: Created: latency-svc-5npwn
Mar  3 00:29:20.632: INFO: Got endpoints: latency-svc-gnmxc [750.613784ms]
Mar  3 00:29:20.661: INFO: Created: latency-svc-k65zc
Mar  3 00:29:20.681: INFO: Got endpoints: latency-svc-2v6dz [750.75851ms]
Mar  3 00:29:20.710: INFO: Created: latency-svc-rm4f4
Mar  3 00:29:20.731: INFO: Got endpoints: latency-svc-bpn9p [749.833104ms]
Mar  3 00:29:20.759: INFO: Created: latency-svc-rj6pz
Mar  3 00:29:20.780: INFO: Got endpoints: latency-svc-jsx5n [749.709687ms]
Mar  3 00:29:20.810: INFO: Created: latency-svc-mv8sr
Mar  3 00:29:20.830: INFO: Got endpoints: latency-svc-fb4jh [735.081387ms]
Mar  3 00:29:20.865: INFO: Created: latency-svc-jxhcc
Mar  3 00:29:20.881: INFO: Got endpoints: latency-svc-574qd [741.513119ms]
Mar  3 00:29:20.907: INFO: Created: latency-svc-6sf5v
Mar  3 00:29:20.932: INFO: Got endpoints: latency-svc-t5lr9 [732.945379ms]
Mar  3 00:29:20.973: INFO: Created: latency-svc-r6v75
Mar  3 00:29:20.981: INFO: Got endpoints: latency-svc-j9p84 [747.644321ms]
Mar  3 00:29:21.013: INFO: Created: latency-svc-hm5dm
Mar  3 00:29:21.031: INFO: Got endpoints: latency-svc-n64lg [749.468135ms]
Mar  3 00:29:21.060: INFO: Created: latency-svc-vsmts
Mar  3 00:29:21.081: INFO: Got endpoints: latency-svc-glnvg [749.62547ms]
Mar  3 00:29:21.112: INFO: Created: latency-svc-tvk9g
Mar  3 00:29:21.131: INFO: Got endpoints: latency-svc-tkn92 [748.444007ms]
Mar  3 00:29:21.162: INFO: Created: latency-svc-wq48q
Mar  3 00:29:21.181: INFO: Got endpoints: latency-svc-khjrn [748.885009ms]
Mar  3 00:29:21.209: INFO: Created: latency-svc-6t68w
Mar  3 00:29:21.230: INFO: Got endpoints: latency-svc-t47n2 [746.725114ms]
Mar  3 00:29:21.259: INFO: Created: latency-svc-xxf2h
Mar  3 00:29:21.283: INFO: Got endpoints: latency-svc-w2kc9 [752.377596ms]
Mar  3 00:29:21.313: INFO: Created: latency-svc-xxk8z
Mar  3 00:29:21.334: INFO: Got endpoints: latency-svc-5npwn [753.18957ms]
Mar  3 00:29:21.363: INFO: Created: latency-svc-vmkzb
Mar  3 00:29:21.382: INFO: Got endpoints: latency-svc-k65zc [749.663227ms]
Mar  3 00:29:21.412: INFO: Created: latency-svc-2s48z
Mar  3 00:29:21.431: INFO: Got endpoints: latency-svc-rm4f4 [749.856581ms]
Mar  3 00:29:21.459: INFO: Created: latency-svc-5lg66
Mar  3 00:29:21.482: INFO: Got endpoints: latency-svc-rj6pz [750.401582ms]
Mar  3 00:29:21.509: INFO: Created: latency-svc-tzhdr
Mar  3 00:29:21.531: INFO: Got endpoints: latency-svc-mv8sr [750.1855ms]
Mar  3 00:29:21.559: INFO: Created: latency-svc-f4drd
Mar  3 00:29:21.581: INFO: Got endpoints: latency-svc-jxhcc [750.47446ms]
Mar  3 00:29:21.614: INFO: Created: latency-svc-psd8m
Mar  3 00:29:21.632: INFO: Got endpoints: latency-svc-6sf5v [751.498874ms]
Mar  3 00:29:21.663: INFO: Created: latency-svc-9z2g2
Mar  3 00:29:21.687: INFO: Got endpoints: latency-svc-r6v75 [755.135237ms]
Mar  3 00:29:21.724: INFO: Created: latency-svc-krlth
Mar  3 00:29:21.731: INFO: Got endpoints: latency-svc-hm5dm [749.735783ms]
Mar  3 00:29:21.762: INFO: Created: latency-svc-phnrt
Mar  3 00:29:21.782: INFO: Got endpoints: latency-svc-vsmts [750.841596ms]
Mar  3 00:29:21.811: INFO: Created: latency-svc-q47jw
Mar  3 00:29:21.837: INFO: Got endpoints: latency-svc-tvk9g [755.963976ms]
Mar  3 00:29:21.866: INFO: Created: latency-svc-qpvzb
Mar  3 00:29:21.881: INFO: Got endpoints: latency-svc-wq48q [750.21703ms]
Mar  3 00:29:21.909: INFO: Created: latency-svc-ss9r9
Mar  3 00:29:21.930: INFO: Got endpoints: latency-svc-6t68w [749.241473ms]
Mar  3 00:29:21.959: INFO: Created: latency-svc-c62gt
Mar  3 00:29:21.980: INFO: Got endpoints: latency-svc-xxf2h [749.916024ms]
Mar  3 00:29:22.016: INFO: Created: latency-svc-kfxtb
Mar  3 00:29:22.032: INFO: Got endpoints: latency-svc-xxk8z [748.841487ms]
Mar  3 00:29:22.060: INFO: Created: latency-svc-2svg4
Mar  3 00:29:22.085: INFO: Got endpoints: latency-svc-vmkzb [750.843428ms]
Mar  3 00:29:22.112: INFO: Created: latency-svc-bvmfm
Mar  3 00:29:22.133: INFO: Got endpoints: latency-svc-2s48z [751.184143ms]
Mar  3 00:29:22.159: INFO: Created: latency-svc-zswv7
Mar  3 00:29:22.182: INFO: Got endpoints: latency-svc-5lg66 [750.422413ms]
Mar  3 00:29:22.208: INFO: Created: latency-svc-gwb5s
Mar  3 00:29:22.231: INFO: Got endpoints: latency-svc-tzhdr [748.681893ms]
Mar  3 00:29:22.262: INFO: Created: latency-svc-fsvkc
Mar  3 00:29:22.281: INFO: Got endpoints: latency-svc-f4drd [749.804058ms]
Mar  3 00:29:22.311: INFO: Created: latency-svc-frftr
Mar  3 00:29:22.331: INFO: Got endpoints: latency-svc-psd8m [749.576781ms]
Mar  3 00:29:22.361: INFO: Created: latency-svc-pcfnh
Mar  3 00:29:22.382: INFO: Got endpoints: latency-svc-9z2g2 [749.811206ms]
Mar  3 00:29:22.411: INFO: Created: latency-svc-fxbkx
Mar  3 00:29:22.431: INFO: Got endpoints: latency-svc-krlth [743.605272ms]
Mar  3 00:29:22.465: INFO: Created: latency-svc-rcb4v
Mar  3 00:29:22.482: INFO: Got endpoints: latency-svc-phnrt [750.985785ms]
Mar  3 00:29:22.512: INFO: Created: latency-svc-bktt8
Mar  3 00:29:22.530: INFO: Got endpoints: latency-svc-q47jw [748.417302ms]
Mar  3 00:29:22.559: INFO: Created: latency-svc-x54wp
Mar  3 00:29:22.581: INFO: Got endpoints: latency-svc-qpvzb [744.031891ms]
Mar  3 00:29:22.611: INFO: Created: latency-svc-clbqm
Mar  3 00:29:22.632: INFO: Got endpoints: latency-svc-ss9r9 [750.573632ms]
Mar  3 00:29:22.659: INFO: Created: latency-svc-vcrjd
Mar  3 00:29:22.682: INFO: Got endpoints: latency-svc-c62gt [751.900314ms]
Mar  3 00:29:22.718: INFO: Created: latency-svc-cnfpg
Mar  3 00:29:22.731: INFO: Got endpoints: latency-svc-kfxtb [750.558486ms]
Mar  3 00:29:22.761: INFO: Created: latency-svc-czq95
Mar  3 00:29:22.782: INFO: Got endpoints: latency-svc-2svg4 [750.146414ms]
Mar  3 00:29:22.811: INFO: Created: latency-svc-vxs5k
Mar  3 00:29:22.830: INFO: Got endpoints: latency-svc-bvmfm [745.049516ms]
Mar  3 00:29:22.857: INFO: Created: latency-svc-j5lm4
Mar  3 00:29:22.882: INFO: Got endpoints: latency-svc-zswv7 [748.97218ms]
Mar  3 00:29:22.907: INFO: Created: latency-svc-8vgtd
Mar  3 00:29:22.933: INFO: Got endpoints: latency-svc-gwb5s [751.019056ms]
Mar  3 00:29:22.960: INFO: Created: latency-svc-sqkjs
Mar  3 00:29:22.989: INFO: Got endpoints: latency-svc-fsvkc [758.105621ms]
Mar  3 00:29:23.019: INFO: Created: latency-svc-j8c7x
Mar  3 00:29:23.030: INFO: Got endpoints: latency-svc-frftr [749.812781ms]
Mar  3 00:29:23.057: INFO: Created: latency-svc-8cxvn
Mar  3 00:29:23.081: INFO: Got endpoints: latency-svc-pcfnh [749.888689ms]
Mar  3 00:29:23.111: INFO: Created: latency-svc-4mhw4
Mar  3 00:29:23.135: INFO: Got endpoints: latency-svc-fxbkx [753.200165ms]
Mar  3 00:29:23.167: INFO: Created: latency-svc-5wc4t
Mar  3 00:29:23.186: INFO: Got endpoints: latency-svc-rcb4v [754.374127ms]
Mar  3 00:29:23.213: INFO: Created: latency-svc-vp6vz
Mar  3 00:29:23.231: INFO: Got endpoints: latency-svc-bktt8 [748.468888ms]
Mar  3 00:29:23.260: INFO: Created: latency-svc-vcnr2
Mar  3 00:29:23.283: INFO: Got endpoints: latency-svc-x54wp [752.744323ms]
Mar  3 00:29:23.314: INFO: Created: latency-svc-wcp89
Mar  3 00:29:23.331: INFO: Got endpoints: latency-svc-clbqm [749.408095ms]
Mar  3 00:29:23.361: INFO: Created: latency-svc-28h8p
Mar  3 00:29:23.381: INFO: Got endpoints: latency-svc-vcrjd [749.826174ms]
Mar  3 00:29:23.412: INFO: Created: latency-svc-xhxlf
Mar  3 00:29:23.431: INFO: Got endpoints: latency-svc-cnfpg [748.447991ms]
Mar  3 00:29:23.462: INFO: Created: latency-svc-pr49x
Mar  3 00:29:23.480: INFO: Got endpoints: latency-svc-czq95 [748.895519ms]
Mar  3 00:29:23.511: INFO: Created: latency-svc-npb9s
Mar  3 00:29:23.531: INFO: Got endpoints: latency-svc-vxs5k [747.964479ms]
Mar  3 00:29:23.563: INFO: Created: latency-svc-77x8w
Mar  3 00:29:23.581: INFO: Got endpoints: latency-svc-j5lm4 [750.342469ms]
Mar  3 00:29:23.608: INFO: Created: latency-svc-7p4k6
Mar  3 00:29:23.632: INFO: Got endpoints: latency-svc-8vgtd [749.881151ms]
Mar  3 00:29:23.659: INFO: Created: latency-svc-nw84g
Mar  3 00:29:23.680: INFO: Got endpoints: latency-svc-sqkjs [747.591512ms]
Mar  3 00:29:23.726: INFO: Created: latency-svc-q86wt
Mar  3 00:29:23.732: INFO: Got endpoints: latency-svc-j8c7x [743.321656ms]
Mar  3 00:29:23.766: INFO: Created: latency-svc-tvgts
Mar  3 00:29:23.783: INFO: Got endpoints: latency-svc-8cxvn [752.557872ms]
Mar  3 00:29:23.813: INFO: Created: latency-svc-6pbk8
Mar  3 00:29:23.834: INFO: Got endpoints: latency-svc-4mhw4 [753.400153ms]
Mar  3 00:29:23.863: INFO: Created: latency-svc-cbgrm
Mar  3 00:29:23.880: INFO: Got endpoints: latency-svc-5wc4t [744.667936ms]
Mar  3 00:29:23.907: INFO: Created: latency-svc-w6pwf
Mar  3 00:29:23.931: INFO: Got endpoints: latency-svc-vp6vz [745.644979ms]
Mar  3 00:29:23.958: INFO: Created: latency-svc-5lmbr
Mar  3 00:29:23.981: INFO: Got endpoints: latency-svc-vcnr2 [749.851299ms]
Mar  3 00:29:24.008: INFO: Created: latency-svc-k4qdm
Mar  3 00:29:24.031: INFO: Got endpoints: latency-svc-wcp89 [747.837912ms]
Mar  3 00:29:24.066: INFO: Created: latency-svc-xd8rn
Mar  3 00:29:24.082: INFO: Got endpoints: latency-svc-28h8p [750.756819ms]
Mar  3 00:29:24.133: INFO: Got endpoints: latency-svc-xhxlf [751.195826ms]
Mar  3 00:29:24.161: INFO: Created: latency-svc-s8lf6
Mar  3 00:29:24.170: INFO: Created: latency-svc-wbvgf
Mar  3 00:29:24.180: INFO: Got endpoints: latency-svc-pr49x [749.488613ms]
Mar  3 00:29:24.207: INFO: Created: latency-svc-wmjht
Mar  3 00:29:24.231: INFO: Got endpoints: latency-svc-npb9s [750.898874ms]
Mar  3 00:29:24.260: INFO: Created: latency-svc-twpcx
Mar  3 00:29:24.280: INFO: Got endpoints: latency-svc-77x8w [749.810327ms]
Mar  3 00:29:24.307: INFO: Created: latency-svc-688bj
Mar  3 00:29:24.330: INFO: Got endpoints: latency-svc-7p4k6 [749.686288ms]
Mar  3 00:29:24.360: INFO: Created: latency-svc-zqt4v
Mar  3 00:29:24.380: INFO: Got endpoints: latency-svc-nw84g [748.44159ms]
Mar  3 00:29:24.410: INFO: Created: latency-svc-dh2cs
Mar  3 00:29:24.433: INFO: Got endpoints: latency-svc-q86wt [752.445245ms]
Mar  3 00:29:24.463: INFO: Created: latency-svc-mm6q6
Mar  3 00:29:24.483: INFO: Got endpoints: latency-svc-tvgts [750.265892ms]
Mar  3 00:29:24.511: INFO: Created: latency-svc-dst9z
Mar  3 00:29:24.531: INFO: Got endpoints: latency-svc-6pbk8 [747.671626ms]
Mar  3 00:29:24.564: INFO: Created: latency-svc-zpvfp
Mar  3 00:29:24.581: INFO: Got endpoints: latency-svc-cbgrm [746.325622ms]
Mar  3 00:29:24.607: INFO: Created: latency-svc-q4gmz
Mar  3 00:29:24.631: INFO: Got endpoints: latency-svc-w6pwf [750.742033ms]
Mar  3 00:29:24.661: INFO: Created: latency-svc-xqjjc
Mar  3 00:29:24.681: INFO: Got endpoints: latency-svc-5lmbr [749.877009ms]
Mar  3 00:29:24.708: INFO: Created: latency-svc-pvl4h
Mar  3 00:29:24.730: INFO: Got endpoints: latency-svc-k4qdm [749.790811ms]
Mar  3 00:29:24.759: INFO: Created: latency-svc-kr9qd
Mar  3 00:29:24.781: INFO: Got endpoints: latency-svc-xd8rn [749.790853ms]
Mar  3 00:29:24.820: INFO: Created: latency-svc-l6z4p
Mar  3 00:29:24.831: INFO: Got endpoints: latency-svc-s8lf6 [749.116012ms]
Mar  3 00:29:24.860: INFO: Created: latency-svc-22vqv
Mar  3 00:29:24.881: INFO: Got endpoints: latency-svc-wbvgf [748.443917ms]
Mar  3 00:29:24.925: INFO: Created: latency-svc-2bsxw
Mar  3 00:29:24.931: INFO: Got endpoints: latency-svc-wmjht [750.186777ms]
Mar  3 00:29:24.958: INFO: Created: latency-svc-pdlm7
Mar  3 00:29:24.983: INFO: Got endpoints: latency-svc-twpcx [752.310209ms]
Mar  3 00:29:25.013: INFO: Created: latency-svc-2h4xr
Mar  3 00:29:25.031: INFO: Got endpoints: latency-svc-688bj [750.96464ms]
Mar  3 00:29:25.062: INFO: Created: latency-svc-w9zfb
Mar  3 00:29:25.080: INFO: Got endpoints: latency-svc-zqt4v [749.813583ms]
Mar  3 00:29:25.109: INFO: Created: latency-svc-5qnzz
Mar  3 00:29:25.131: INFO: Got endpoints: latency-svc-dh2cs [750.616927ms]
Mar  3 00:29:25.168: INFO: Created: latency-svc-8s655
Mar  3 00:29:25.181: INFO: Got endpoints: latency-svc-mm6q6 [748.183243ms]
Mar  3 00:29:25.211: INFO: Created: latency-svc-ck6fm
Mar  3 00:29:25.230: INFO: Got endpoints: latency-svc-dst9z [747.709942ms]
Mar  3 00:29:25.261: INFO: Created: latency-svc-ln5r9
Mar  3 00:29:25.280: INFO: Got endpoints: latency-svc-zpvfp [749.490147ms]
Mar  3 00:29:25.332: INFO: Created: latency-svc-56pj9
Mar  3 00:29:25.332: INFO: Got endpoints: latency-svc-q4gmz [751.795002ms]
Mar  3 00:29:25.364: INFO: Created: latency-svc-ndl8s
Mar  3 00:29:25.391: INFO: Got endpoints: latency-svc-xqjjc [760.191074ms]
Mar  3 00:29:25.425: INFO: Created: latency-svc-6dn2b
Mar  3 00:29:25.432: INFO: Got endpoints: latency-svc-pvl4h [750.247472ms]
Mar  3 00:29:25.474: INFO: Created: latency-svc-q7z48
Mar  3 00:29:25.486: INFO: Got endpoints: latency-svc-kr9qd [755.704735ms]
Mar  3 00:29:25.516: INFO: Created: latency-svc-t2ntc
Mar  3 00:29:25.531: INFO: Got endpoints: latency-svc-l6z4p [749.914394ms]
Mar  3 00:29:25.562: INFO: Created: latency-svc-997fm
Mar  3 00:29:25.581: INFO: Got endpoints: latency-svc-22vqv [750.290546ms]
Mar  3 00:29:25.609: INFO: Created: latency-svc-cq5ts
Mar  3 00:29:25.630: INFO: Got endpoints: latency-svc-2bsxw [749.169319ms]
Mar  3 00:29:25.659: INFO: Created: latency-svc-sg7tq
Mar  3 00:29:25.686: INFO: Got endpoints: latency-svc-pdlm7 [755.292224ms]
Mar  3 00:29:25.720: INFO: Created: latency-svc-kpzhs
Mar  3 00:29:25.732: INFO: Got endpoints: latency-svc-2h4xr [748.102931ms]
Mar  3 00:29:25.759: INFO: Created: latency-svc-kz4sr
Mar  3 00:29:25.781: INFO: Got endpoints: latency-svc-w9zfb [749.123877ms]
Mar  3 00:29:25.811: INFO: Created: latency-svc-vq96w
Mar  3 00:29:25.831: INFO: Got endpoints: latency-svc-5qnzz [750.280437ms]
Mar  3 00:29:25.864: INFO: Created: latency-svc-4kzt9
Mar  3 00:29:25.880: INFO: Got endpoints: latency-svc-8s655 [749.027963ms]
Mar  3 00:29:25.911: INFO: Created: latency-svc-7nr4l
Mar  3 00:29:25.931: INFO: Got endpoints: latency-svc-ck6fm [749.515237ms]
Mar  3 00:29:25.961: INFO: Created: latency-svc-xqsw2
Mar  3 00:29:25.980: INFO: Got endpoints: latency-svc-ln5r9 [750.050012ms]
Mar  3 00:29:26.032: INFO: Got endpoints: latency-svc-56pj9 [751.945267ms]
Mar  3 00:29:26.081: INFO: Got endpoints: latency-svc-ndl8s [748.560731ms]
Mar  3 00:29:26.131: INFO: Got endpoints: latency-svc-6dn2b [739.412307ms]
Mar  3 00:29:26.181: INFO: Got endpoints: latency-svc-q7z48 [749.698799ms]
Mar  3 00:29:26.233: INFO: Got endpoints: latency-svc-t2ntc [746.798231ms]
Mar  3 00:29:26.284: INFO: Got endpoints: latency-svc-997fm [753.336575ms]
Mar  3 00:29:26.331: INFO: Got endpoints: latency-svc-cq5ts [750.31954ms]
Mar  3 00:29:26.381: INFO: Got endpoints: latency-svc-sg7tq [750.817656ms]
Mar  3 00:29:26.431: INFO: Got endpoints: latency-svc-kpzhs [745.02577ms]
Mar  3 00:29:26.481: INFO: Got endpoints: latency-svc-kz4sr [749.627759ms]
Mar  3 00:29:26.535: INFO: Got endpoints: latency-svc-vq96w [753.9072ms]
Mar  3 00:29:26.582: INFO: Got endpoints: latency-svc-4kzt9 [750.929668ms]
Mar  3 00:29:26.631: INFO: Got endpoints: latency-svc-7nr4l [750.669296ms]
Mar  3 00:29:26.682: INFO: Got endpoints: latency-svc-xqsw2 [751.102071ms]
Mar  3 00:29:26.682: INFO: Latencies: [36.55626ms 47.175377ms 62.562404ms 72.138154ms 85.566349ms 102.943666ms 114.675207ms 132.372263ms 146.713726ms 158.971978ms 171.835169ms 183.340492ms 185.574485ms 188.437788ms 188.540851ms 191.234447ms 194.367878ms 194.843626ms 195.019036ms 195.692104ms 198.819612ms 199.211115ms 199.370146ms 201.718676ms 201.73511ms 206.77438ms 207.40248ms 209.249316ms 209.565095ms 213.637448ms 216.984995ms 224.141954ms 224.798305ms 224.999413ms 225.305257ms 225.627963ms 226.961239ms 226.993903ms 227.107498ms 228.700039ms 235.264359ms 254.490966ms 281.746021ms 322.970419ms 368.601031ms 403.749732ms 414.281811ms 451.628656ms 490.323503ms 526.400437ms 565.080203ms 602.801029ms 643.527299ms 678.691531ms 714.337674ms 732.945379ms 735.081387ms 739.412307ms 741.513119ms 743.219409ms 743.321656ms 743.605272ms 744.031891ms 744.56825ms 744.635339ms 744.667936ms 745.02577ms 745.049516ms 745.644979ms 746.325622ms 746.725114ms 746.798231ms 747.004549ms 747.516632ms 747.591512ms 747.644321ms 747.671626ms 747.709942ms 747.837912ms 747.868271ms 747.964479ms 748.102931ms 748.183243ms 748.417302ms 748.44159ms 748.443917ms 748.444007ms 748.447991ms 748.468888ms 748.560731ms 748.681893ms 748.841487ms 748.885009ms 748.895519ms 748.940609ms 748.97218ms 749.027963ms 749.116012ms 749.123877ms 749.153831ms 749.169319ms 749.241473ms 749.257446ms 749.408095ms 749.468135ms 749.488613ms 749.490147ms 749.515237ms 749.576781ms 749.62547ms 749.627759ms 749.663227ms 749.686288ms 749.698799ms 749.709687ms 749.735783ms 749.790811ms 749.790853ms 749.804058ms 749.810327ms 749.811206ms 749.812781ms 749.813583ms 749.826174ms 749.833104ms 749.851299ms 749.856581ms 749.877009ms 749.881151ms 749.888689ms 749.914394ms 749.916024ms 749.96013ms 749.970623ms 750.050012ms 750.132491ms 750.146414ms 750.1855ms 750.186777ms 750.193902ms 750.196428ms 750.21703ms 750.247472ms 750.265892ms 750.271676ms 750.280437ms 750.290546ms 750.299269ms 750.31954ms 750.342469ms 750.401582ms 750.422413ms 750.47446ms 750.558486ms 750.573632ms 750.613784ms 750.616927ms 750.669296ms 750.742033ms 750.756819ms 750.75851ms 750.817656ms 750.841596ms 750.843428ms 750.898874ms 750.929668ms 750.96464ms 750.985785ms 751.019056ms 751.102071ms 751.132667ms 751.184143ms 751.195826ms 751.206435ms 751.498874ms 751.780978ms 751.795002ms 751.900314ms 751.945267ms 752.310209ms 752.377596ms 752.445245ms 752.557872ms 752.744323ms 753.083456ms 753.18957ms 753.200165ms 753.336575ms 753.400153ms 753.9072ms 754.374127ms 755.135237ms 755.292224ms 755.704735ms 755.963976ms 758.105621ms 758.169124ms 760.191074ms 762.897491ms 763.594506ms]
Mar  3 00:29:26.682: INFO: 50 %ile: 749.169319ms
Mar  3 00:29:26.682: INFO: 90 %ile: 752.377596ms
Mar  3 00:29:26.682: INFO: 99 %ile: 762.897491ms
Mar  3 00:29:26.682: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:29:26.682: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-kjdsd" for this suite.
Mar  3 00:29:40.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:29:42.236: INFO: namespace: e2e-tests-svc-latency-kjdsd, resource: bindings, ignored listing per whitelist
Mar  3 00:29:43.543: INFO: namespace: e2e-tests-svc-latency-kjdsd, resource: packagemanifests, items remaining: 1
Mar  3 00:29:43.666: INFO: namespace: e2e-tests-svc-latency-kjdsd no longer exists
Mar  3 00:29:43.698: INFO: namespace: e2e-tests-svc-latency-kjdsd, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:29:43.714: INFO: namespace e2e-tests-svc-latency-kjdsd deletion completed in 16.985539819s

• [SLOW TEST:35.849 seconds]
[sig-network] Service endpoints latency
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:29:43.715: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  3 00:29:54.882: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-71842a43-3d4b-11e9-9008-0a58ac10f353,GenerateName:,Namespace:e2e-tests-events-mxghg,SelfLink:/api/v1/namespaces/e2e-tests-events-mxghg/pods/send-events-71842a43-3d4b-11e9-9008-0a58ac10f353,UID:71864b85-3d4b-11e9-97ac-0e81de5d2554,ResourceVersion:121659,Generation:0,CreationTimestamp:2019-03-03 00:29:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 783934643,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.131"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bvcjh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bvcjh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-bvcjh true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c27,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-5sgb5}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222596e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422259700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:29:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:29:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:29:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:29:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.131,StartTime:2019-03-03 00:29:44 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-03 00:29:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://2b72d8a4b13054c6c77c93f58834cade3ca2fd25fb34c34f04f9630dbf0809c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  3 00:29:56.901: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  3 00:29:58.918: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:29:58.938: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-mxghg" for this suite.
Mar  3 00:30:39.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:30:40.353: INFO: namespace: e2e-tests-events-mxghg, resource: bindings, ignored listing per whitelist
Mar  3 00:30:40.638: INFO: namespace: e2e-tests-events-mxghg, resource: packagemanifests, items remaining: 1
Mar  3 00:30:41.482: INFO: namespace: e2e-tests-events-mxghg no longer exists
Mar  3 00:30:41.502: INFO: namespace: e2e-tests-events-mxghg, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:30:41.518: INFO: namespace e2e-tests-events-mxghg deletion completed in 42.531605757s

• [SLOW TEST:57.803 seconds]
[k8s.io] [sig-node] Events
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:30:41.518: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:30:42.652: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zq8g9" for this suite.
Mar  3 00:31:06.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:31:08.757: INFO: namespace: e2e-tests-pods-zq8g9, resource: packagemanifests, items remaining: 1
Mar  3 00:31:08.807: INFO: namespace: e2e-tests-pods-zq8g9, resource: bindings, ignored listing per whitelist
Mar  3 00:31:09.199: INFO: namespace: e2e-tests-pods-zq8g9 no longer exists
Mar  3 00:31:09.218: INFO: namespace: e2e-tests-pods-zq8g9, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:31:09.234: INFO: namespace e2e-tests-pods-zq8g9 deletion completed in 26.53687222s

• [SLOW TEST:27.716 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:31:09.234: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:31:10.443: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar  3 00:31:10.479: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k4dpw/daemonsets","resourceVersion":"122648"},"items":null}

Mar  3 00:31:10.495: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k4dpw/pods","resourceVersion":"122649"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:31:10.577: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k4dpw" for this suite.
Mar  3 00:31:16.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:31:17.675: INFO: namespace: e2e-tests-daemonsets-k4dpw, resource: bindings, ignored listing per whitelist
Mar  3 00:31:18.509: INFO: namespace: e2e-tests-daemonsets-k4dpw, resource: packagemanifests, items remaining: 1
Mar  3 00:31:19.106: INFO: namespace: e2e-tests-daemonsets-k4dpw no longer exists
Mar  3 00:31:19.127: INFO: namespace: e2e-tests-daemonsets-k4dpw, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:31:19.143: INFO: namespace e2e-tests-daemonsets-k4dpw deletion completed in 8.5348381s

S [SKIPPING] [9.909 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar  3 00:31:10.444: Requires at least 2 nodes (not -1)

  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:31:19.143: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  3 00:31:40.506: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:40.523: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:42.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:42.540: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:44.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:44.542: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:46.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:46.542: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:48.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:48.540: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:50.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:50.540: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:52.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:52.540: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:54.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:54.540: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:56.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:56.541: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:31:58.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:31:58.541: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 00:32:00.523: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 00:32:00.540: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:32:00.562: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-49txv" for this suite.
Mar  3 00:32:24.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:32:25.492: INFO: namespace: e2e-tests-container-lifecycle-hook-49txv, resource: packagemanifests, items remaining: 1
Mar  3 00:32:25.774: INFO: namespace: e2e-tests-container-lifecycle-hook-49txv, resource: bindings, ignored listing per whitelist
Mar  3 00:32:27.124: INFO: namespace: e2e-tests-container-lifecycle-hook-49txv no longer exists
Mar  3 00:32:27.143: INFO: namespace: e2e-tests-container-lifecycle-hook-49txv, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:32:27.160: INFO: namespace e2e-tests-container-lifecycle-hook-49txv deletion completed in 26.544175263s

• [SLOW TEST:68.017 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:32:27.160: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:32:28.233: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d2f094ac-3d4b-11e9-9008-0a58ac10f353
STEP: Creating secret with name s-test-opt-upd-d2f09511-3d4b-11e9-9008-0a58ac10f353
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d2f094ac-3d4b-11e9-9008-0a58ac10f353
STEP: Updating secret s-test-opt-upd-d2f09511-3d4b-11e9-9008-0a58ac10f353
STEP: Creating secret with name s-test-opt-create-d2f0954c-3d4b-11e9-9008-0a58ac10f353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:33:45.299: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hn9sg" for this suite.
Mar  3 00:34:09.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:34:10.438: INFO: namespace: e2e-tests-secrets-hn9sg, resource: packagemanifests, items remaining: 1
Mar  3 00:34:10.990: INFO: namespace: e2e-tests-secrets-hn9sg, resource: bindings, ignored listing per whitelist
Mar  3 00:34:11.835: INFO: namespace: e2e-tests-secrets-hn9sg no longer exists
Mar  3 00:34:11.854: INFO: namespace: e2e-tests-secrets-hn9sg, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:34:11.870: INFO: namespace e2e-tests-secrets-hn9sg deletion completed in 26.526614705s

• [SLOW TEST:104.711 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar  3 00:34:11.871: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  3 00:34:12.951: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  3 00:34:13.011: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  3 00:34:18.029: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  3 00:34:24.063: INFO: Creating deployment "test-rolling-update-deployment"
Mar  3 00:34:24.087: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  3 00:34:24.130: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar  3 00:34:26.169: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  3 00:34:26.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 00:34:28.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 00:34:30.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 00:34:32.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687170064, loc:(*time.Location)(0x6c3af00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 00:34:34.206: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  3 00:34:34.260: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-vfz7q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vfz7q/deployments/test-rolling-update-deployment,UID:17fba3fa-3d4c-11e9-97ac-0e81de5d2554,ResourceVersion:125046,Generation:1,CreationTimestamp:2019-03-03 00:34:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-03 00:34:24 +0000 UTC 2019-03-03 00:34:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-03 00:34:33 +0000 UTC 2019-03-03 00:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  3 00:34:34.279: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-vfz7q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vfz7q/replicasets/test-rolling-update-deployment-65b7695dcf,UID:18023349-3d4c-11e9-ae4f-12d7f6a43fda,ResourceVersion:125035,Generation:1,CreationTimestamp:2019-03-03 00:34:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 17fba3fa-3d4c-11e9-97ac-0e81de5d2554 0xc420bca897 0xc420bca898}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  3 00:34:34.279: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  3 00:34:34.279: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-vfz7q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vfz7q/replicasets/test-rolling-update-controller,UID:115c053b-3d4c-11e9-97ac-0e81de5d2554,ResourceVersion:125045,Generation:2,CreationTimestamp:2019-03-03 00:34:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 17fba3fa-3d4c-11e9-97ac-0e81de5d2554 0xc420bca7ce 0xc420bca7cf}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  3 00:34:34.296: INFO: Pod "test-rolling-update-deployment-65b7695dcf-mrscp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-mrscp,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-vfz7q,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vfz7q/pods/test-rolling-update-deployment-65b7695dcf-mrscp,UID:18057431-3d4c-11e9-ae4f-12d7f6a43fda,ResourceVersion:125034,Generation:0,CreationTimestamp:2019-03-03 00:34:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "ips": [
        "10.129.2.134"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 18023349-3d4c-11e9-ae4f-12d7f6a43fda 0xc421df5087 0xc421df5088}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kdkpm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kdkpm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-kdkpm true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-160-190.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-ktkfc}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421df5370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421df5390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:34:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:34:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:34:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-03 00:34:24 +0000 UTC  }],Message:,Reason:,HostIP:10.0.160.190,PodIP:10.129.2.134,StartTime:2019-03-03 00:34:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-03 00:34:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://65d7c187458b1a2e70d2f6ab88f7961c3124936ed40a5a9c6acd2b6b2f8cd0b5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar  3 00:34:34.296: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vfz7q" for this suite.
Mar  3 00:34:42.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  3 00:34:43.473: INFO: namespace: e2e-tests-deployment-vfz7q, resource: bindings, ignored listing per whitelist
Mar  3 00:34:44.407: INFO: namespace: e2e-tests-deployment-vfz7q, resource: packagemanifests, items remaining: 1
Mar  3 00:34:45.099: INFO: namespace: e2e-tests-deployment-vfz7q no longer exists
Mar  3 00:34:45.133: INFO: namespace: e2e-tests-deployment-vfz7q, total namespaces: 47, active: 47, terminating: 0
Mar  3 00:34:45.149: INFO: namespace e2e-tests-deployment-vfz7q deletion completed in 10.808040674s

• [SLOW TEST:33.279 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSMar  3 00:34:45.149: INFO: Running AfterSuite actions on all node
Mar  3 00:34:45.149: INFO: Running AfterSuite actions on node 1
Mar  3 00:34:45.149: INFO: Dumping logs locally to: /tmp/artifacts
Mar  3 00:34:45.150: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 187 of 2011 Specs in 7531.935 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Pending | 1824 Skipped PASS
