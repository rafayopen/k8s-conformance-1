I0716 12:55:20.558135      18 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-453034999
I0716 12:55:20.558361      18 e2e.go:224] Starting e2e run "f76bfab9-a7c8-11e9-9e69-7a63f556cc5d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1563281719 - Will randomize all specs
Will run 201 of 1946 specs

Jul 16 12:55:20.799: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 12:55:20.803: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 16 12:55:20.816: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 16 12:55:20.868: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 16 12:55:20.868: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jul 16 12:55:20.868: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 16 12:55:22.256: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (1 seconds elapsed)
Jul 16 12:55:22.256: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-qingcloud-node' (1 seconds elapsed)
Jul 16 12:55:22.257: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (1 seconds elapsed)
Jul 16 12:55:22.257: INFO: e2e test version: v1.13.0
Jul 16 12:55:22.260: INFO: kube-apiserver version: v1.13.5
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:55:22.260: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
Jul 16 12:55:25.237: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 12:55:25.269: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-skbkw" to be "success or failure"
Jul 16 12:55:25.271: INFO: Pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546689ms
Jul 16 12:55:27.275: INFO: Pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006078827s
Jul 16 12:55:29.281: INFO: Pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012508965s
Jul 16 12:55:31.286: INFO: Pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017039735s
STEP: Saw pod success
Jul 16 12:55:31.286: INFO: Pod "downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 12:55:31.289: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 12:55:31.322: INFO: Waiting for pod downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d to disappear
Jul 16 12:55:31.325: INFO: Pod downwardapi-volume-faacfa06-a7c8-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:55:31.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-skbkw" for this suite.
Jul 16 12:55:37.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:55:38.963: INFO: namespace: e2e-tests-projected-skbkw, resource: bindings, ignored listing per whitelist
Jul 16 12:55:39.669: INFO: namespace e2e-tests-projected-skbkw deletion completed in 8.340330623s

• [SLOW TEST:17.409 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:55:39.670: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-kghvr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 12:55:40.046: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 12:56:08.176: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.107:8080/dial?request=hostName&protocol=http&host=10.233.83.233&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-kghvr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 12:56:08.176: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 12:56:08.484: INFO: Waiting for endpoints: map[]
Jul 16 12:56:08.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.107:8080/dial?request=hostName&protocol=http&host=10.233.100.106&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-kghvr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 12:56:08.488: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 12:56:08.724: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:56:08.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-kghvr" for this suite.
Jul 16 12:56:32.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:56:33.466: INFO: namespace: e2e-tests-pod-network-test-kghvr, resource: bindings, ignored listing per whitelist
Jul 16 12:56:35.065: INFO: namespace e2e-tests-pod-network-test-kghvr deletion completed in 26.332079481s

• [SLOW TEST:55.396 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:56:35.066: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 16 12:56:35.161: INFO: Waiting up to 5m0s for pod "pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-hhrmr" to be "success or failure"
Jul 16 12:56:35.165: INFO: Pod "pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50112ms
Jul 16 12:56:37.169: INFO: Pod "pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007851253s
Jul 16 12:56:39.174: INFO: Pod "pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012332184s
STEP: Saw pod success
Jul 16 12:56:39.174: INFO: Pod "pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 12:56:39.177: INFO: Trying to get logs from node i-taxl6ow1 pod pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 12:56:39.199: INFO: Waiting for pod pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 12:56:39.202: INFO: Pod pod-24581eb9-a7c9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:56:39.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hhrmr" for this suite.
Jul 16 12:56:45.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:56:45.361: INFO: namespace: e2e-tests-emptydir-hhrmr, resource: bindings, ignored listing per whitelist
Jul 16 12:56:47.566: INFO: namespace e2e-tests-emptydir-hhrmr deletion completed in 8.358948256s

• [SLOW TEST:12.500 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:56:47.569: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 16 12:56:55.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:56:55.755: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 12:56:57.755: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:56:57.762: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 12:56:59.755: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:56:59.759: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 12:57:01.755: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:57:01.761: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 12:57:03.755: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:57:03.759: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 12:57:05.755: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 12:57:05.764: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:57:05.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-rms96" for this suite.
Jul 16 12:57:29.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:57:29.820: INFO: namespace: e2e-tests-container-lifecycle-hook-rms96, resource: bindings, ignored listing per whitelist
Jul 16 12:57:32.109: INFO: namespace e2e-tests-container-lifecycle-hook-rms96 deletion completed in 26.327707117s

• [SLOW TEST:44.541 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:57:32.112: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-4658d072-a7c9-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 12:57:32.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-k2lc2" to be "success or failure"
Jul 16 12:57:32.214: INFO: Pod "pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.992467ms
Jul 16 12:57:34.218: INFO: Pod "pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007569491s
Jul 16 12:57:36.222: INFO: Pod "pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011941216s
STEP: Saw pod success
Jul 16 12:57:36.222: INFO: Pod "pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 12:57:36.225: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 12:57:36.256: INFO: Waiting for pod pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 12:57:36.263: INFO: Pod pod-configmaps-46593865-a7c9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:57:36.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k2lc2" for this suite.
Jul 16 12:57:42.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:57:42.444: INFO: namespace: e2e-tests-configmap-k2lc2, resource: bindings, ignored listing per whitelist
Jul 16 12:57:44.604: INFO: namespace e2e-tests-configmap-k2lc2 deletion completed in 8.336742006s

• [SLOW TEST:12.492 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:57:44.605: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0716 12:58:24.717512      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 12:58:24.717: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:58:24.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-rgqfw" for this suite.
Jul 16 12:58:30.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:58:30.784: INFO: namespace: e2e-tests-gc-rgqfw, resource: bindings, ignored listing per whitelist
Jul 16 12:58:33.064: INFO: namespace e2e-tests-gc-rgqfw deletion completed in 8.34178179s

• [SLOW TEST:48.459 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:58:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Jul 16 12:58:35.253: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:58:59.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-2fv5b" for this suite.
Jul 16 12:59:05.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:59:05.408: INFO: namespace: e2e-tests-namespaces-2fv5b, resource: bindings, ignored listing per whitelist
Jul 16 12:59:07.709: INFO: namespace e2e-tests-namespaces-2fv5b deletion completed in 8.34121529s
STEP: Destroying namespace "e2e-tests-nsdeletetest-x8xk7" for this suite.
Jul 16 12:59:07.715: INFO: Namespace e2e-tests-nsdeletetest-x8xk7 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-fn5fz" for this suite.
Jul 16 12:59:13.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:59:14.992: INFO: namespace: e2e-tests-nsdeletetest-fn5fz, resource: bindings, ignored listing per whitelist
Jul 16 12:59:16.042: INFO: namespace e2e-tests-nsdeletetest-fn5fz deletion completed in 8.326189146s

• [SLOW TEST:42.977 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:59:16.044: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 12:59:16.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-52lxw'
Jul 16 12:59:16.580: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 12:59:16.580: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Jul 16 12:59:18.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-52lxw'
Jul 16 12:59:18.725: INFO: stderr: ""
Jul 16 12:59:18.725: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 12:59:18.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-52lxw" for this suite.
Jul 16 12:59:42.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 12:59:45.070: INFO: namespace: e2e-tests-kubectl-52lxw, resource: bindings, ignored listing per whitelist
Jul 16 12:59:45.119: INFO: namespace e2e-tests-kubectl-52lxw deletion completed in 26.387447198s

• [SLOW TEST:29.076 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 12:59:45.119: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-959eb2a3-a7c9-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 12:59:45.207: INFO: Waiting up to 5m0s for pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-xl882" to be "success or failure"
Jul 16 12:59:45.210: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856238ms
Jul 16 12:59:47.214: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007409231s
Jul 16 12:59:49.218: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011039385s
Jul 16 12:59:51.222: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015150586s
Jul 16 12:59:53.225: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018898316s
Jul 16 12:59:55.228: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021955267s
Jul 16 12:59:57.232: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02541364s
Jul 16 12:59:59.237: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030363997s
Jul 16 13:00:01.241: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034597643s
Jul 16 13:00:03.247: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.04002794s
Jul 16 13:00:05.250: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.043828542s
Jul 16 13:00:07.261: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.054246235s
Jul 16 13:00:09.266: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.059301124s
Jul 16 13:00:11.272: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.065104704s
Jul 16 13:00:13.275: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.068544949s
Jul 16 13:00:15.279: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.072311106s
Jul 16 13:00:17.283: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 32.076046172s
Jul 16 13:00:19.287: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.080124911s
Jul 16 13:00:21.290: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.083557004s
STEP: Saw pod success
Jul 16 13:00:21.290: INFO: Pod "pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:00:21.293: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d container secret-env-test: <nil>
STEP: delete the pod
Jul 16 13:00:21.328: INFO: Waiting for pod pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:00:21.332: INFO: Pod pod-secrets-959f33e7-a7c9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:00:21.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xl882" for this suite.
Jul 16 13:00:27.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:00:27.422: INFO: namespace: e2e-tests-secrets-xl882, resource: bindings, ignored listing per whitelist
Jul 16 13:00:29.671: INFO: namespace e2e-tests-secrets-xl882 deletion completed in 8.329334042s

• [SLOW TEST:44.552 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:00:29.672: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:00:29.782: INFO: (0) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.645599ms)
Jul 16 13:00:29.785: INFO: (1) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.325439ms)
Jul 16 13:00:29.789: INFO: (2) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.679516ms)
Jul 16 13:00:29.793: INFO: (3) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.161772ms)
Jul 16 13:00:29.798: INFO: (4) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.21847ms)
Jul 16 13:00:29.808: INFO: (5) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.89044ms)
Jul 16 13:00:29.811: INFO: (6) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.629892ms)
Jul 16 13:00:29.815: INFO: (7) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.477389ms)
Jul 16 13:00:29.819: INFO: (8) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.781981ms)
Jul 16 13:00:29.822: INFO: (9) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.488876ms)
Jul 16 13:00:29.825: INFO: (10) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.350564ms)
Jul 16 13:00:29.829: INFO: (11) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.450684ms)
Jul 16 13:00:29.832: INFO: (12) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.460987ms)
Jul 16 13:00:29.837: INFO: (13) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.22248ms)
Jul 16 13:00:29.841: INFO: (14) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.922612ms)
Jul 16 13:00:29.844: INFO: (15) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.319109ms)
Jul 16 13:00:29.848: INFO: (16) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.739249ms)
Jul 16 13:00:29.852: INFO: (17) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.961741ms)
Jul 16 13:00:29.856: INFO: (18) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.986068ms)
Jul 16 13:00:29.869: INFO: (19) /api/v1/nodes/i-7znxt5so/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.173932ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:00:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-wmct5" for this suite.
Jul 16 13:00:35.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:00:35.929: INFO: namespace: e2e-tests-proxy-wmct5, resource: bindings, ignored listing per whitelist
Jul 16 13:00:36.099: INFO: namespace e2e-tests-proxy-wmct5 deletion completed in 6.224438101s

• [SLOW TEST:6.427 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:00:36.100: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b40119d1-a7c9-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:00:36.183: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-ts67t" to be "success or failure"
Jul 16 13:00:36.186: INFO: Pod "pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622514ms
Jul 16 13:00:38.190: INFO: Pod "pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006671777s
STEP: Saw pod success
Jul 16 13:00:38.190: INFO: Pod "pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:00:38.192: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:00:38.266: INFO: Waiting for pod pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:00:38.268: INFO: Pod pod-configmaps-b4018774-a7c9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:00:38.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ts67t" for this suite.
Jul 16 13:00:44.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:00:44.956: INFO: namespace: e2e-tests-configmap-ts67t, resource: bindings, ignored listing per whitelist
Jul 16 13:00:46.606: INFO: namespace e2e-tests-configmap-ts67t deletion completed in 8.334890742s

• [SLOW TEST:10.506 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:00:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:00:46.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5w8cw" for this suite.
Jul 16 13:01:08.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:01:08.762: INFO: namespace: e2e-tests-pods-5w8cw, resource: bindings, ignored listing per whitelist
Jul 16 13:01:11.059: INFO: namespace e2e-tests-pods-5w8cw deletion completed in 24.340946584s

• [SLOW TEST:24.453 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:01:11.060: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 16 13:01:19.188: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:19.188: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:19.489: INFO: Exec stderr: ""
Jul 16 13:01:19.489: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:19.489: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:19.771: INFO: Exec stderr: ""
Jul 16 13:01:19.771: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:19.771: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:19.978: INFO: Exec stderr: ""
Jul 16 13:01:19.978: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:19.978: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:20.184: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 16 13:01:20.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:20.448: INFO: Exec stderr: ""
Jul 16 13:01:20.461: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:20.462: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:20.684: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 16 13:01:20.694: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:20.694: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:20.926: INFO: Exec stderr: ""
Jul 16 13:01:20.926: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:20.926: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:21.143: INFO: Exec stderr: ""
Jul 16 13:01:21.143: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:21.402: INFO: Exec stderr: ""
Jul 16 13:01:21.402: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9vm28 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:01:21.402: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:01:21.652: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:01:21.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-9vm28" for this suite.
Jul 16 13:02:01.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:02:01.689: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-9vm28, resource: bindings, ignored listing per whitelist
Jul 16 13:02:03.992: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-9vm28 deletion completed in 42.335374502s

• [SLOW TEST:52.932 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:02:03.993: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:02:08.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-s58wl" for this suite.
Jul 16 13:02:58.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:02:58.169: INFO: namespace: e2e-tests-kubelet-test-s58wl, resource: bindings, ignored listing per whitelist
Jul 16 13:03:00.462: INFO: namespace e2e-tests-kubelet-test-s58wl deletion completed in 52.340161581s

• [SLOW TEST:56.470 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:03:00.464: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-0a11ee4e-a7ca-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:03:00.590: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-fspwg" to be "success or failure"
Jul 16 13:03:00.602: INFO: Pod "pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.452304ms
Jul 16 13:03:02.606: INFO: Pod "pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015949833s
Jul 16 13:03:04.610: INFO: Pod "pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01983393s
STEP: Saw pod success
Jul 16 13:03:04.610: INFO: Pod "pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:03:04.612: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:03:04.647: INFO: Waiting for pod pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:03:04.650: INFO: Pod pod-projected-secrets-0a12ce3e-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:03:04.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fspwg" for this suite.
Jul 16 13:03:10.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:03:10.776: INFO: namespace: e2e-tests-projected-fspwg, resource: bindings, ignored listing per whitelist
Jul 16 13:03:13.002: INFO: namespace e2e-tests-projected-fspwg deletion completed in 8.346433363s

• [SLOW TEST:12.538 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:03:13.002: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Jul 16 13:03:13.080: INFO: Waiting up to 5m0s for pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-containers-k75vf" to be "success or failure"
Jul 16 13:03:13.094: INFO: Pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022326ms
Jul 16 13:03:15.097: INFO: Pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017743147s
Jul 16 13:03:17.103: INFO: Pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023755855s
Jul 16 13:03:19.107: INFO: Pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027543489s
STEP: Saw pod success
Jul 16 13:03:19.107: INFO: Pod "client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:03:19.110: INFO: Trying to get logs from node i-taxl6ow1 pod client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:03:19.134: INFO: Waiting for pod client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:03:19.137: INFO: Pod client-containers-1185f604-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:03:19.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-k75vf" for this suite.
Jul 16 13:03:25.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:03:27.236: INFO: namespace: e2e-tests-containers-k75vf, resource: bindings, ignored listing per whitelist
Jul 16 13:03:27.488: INFO: namespace e2e-tests-containers-k75vf deletion completed in 8.346184125s

• [SLOW TEST:14.486 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:03:27.489: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 13:03:27.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-tt2zc'
Jul 16 13:03:27.793: INFO: stderr: ""
Jul 16 13:03:27.793: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Jul 16 13:03:27.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tt2zc'
Jul 16 13:03:35.660: INFO: stderr: ""
Jul 16 13:03:35.667: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:03:35.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tt2zc" for this suite.
Jul 16 13:03:41.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:03:41.849: INFO: namespace: e2e-tests-kubectl-tt2zc, resource: bindings, ignored listing per whitelist
Jul 16 13:03:44.090: INFO: namespace e2e-tests-kubectl-tt2zc deletion completed in 8.416524869s

• [SLOW TEST:16.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:03:44.092: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-b5l67
Jul 16 13:03:46.234: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-b5l67
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 13:03:46.237: INFO: Initial restart count of pod liveness-exec is 0
Jul 16 13:04:40.352: INFO: Restart count of pod e2e-tests-container-probe-b5l67/liveness-exec is now 1 (54.115614506s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:04:40.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b5l67" for this suite.
Jul 16 13:04:46.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:04:46.795: INFO: namespace: e2e-tests-container-probe-b5l67, resource: bindings, ignored listing per whitelist
Jul 16 13:04:48.695: INFO: namespace e2e-tests-container-probe-b5l67 deletion completed in 8.325654003s

• [SLOW TEST:64.603 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:04:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4a932c1f-a7ca-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:04:48.806: INFO: Waiting up to 5m0s for pod "pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-kfhtm" to be "success or failure"
Jul 16 13:04:48.811: INFO: Pod "pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.205999ms
Jul 16 13:04:50.821: INFO: Pod "pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015071271s
STEP: Saw pod success
Jul 16 13:04:50.821: INFO: Pod "pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:04:50.824: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:04:50.851: INFO: Waiting for pod pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:04:50.854: INFO: Pod pod-secrets-4a94897f-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:04:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kfhtm" for this suite.
Jul 16 13:04:56.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:04:57.293: INFO: namespace: e2e-tests-secrets-kfhtm, resource: bindings, ignored listing per whitelist
Jul 16 13:04:59.192: INFO: namespace e2e-tests-secrets-kfhtm deletion completed in 8.332073929s

• [SLOW TEST:10.497 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:04:59.197: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Jul 16 13:04:59.294: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-sq7xh" to be "success or failure"
Jul 16 13:04:59.301: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346426ms
Jul 16 13:05:01.306: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012090847s
Jul 16 13:05:03.310: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015980153s
STEP: Saw pod success
Jul 16 13:05:03.310: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 16 13:05:03.313: INFO: Trying to get logs from node i-taxl6ow1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 16 13:05:03.332: INFO: Waiting for pod pod-host-path-test to disappear
Jul 16 13:05:03.335: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:05:03.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-sq7xh" for this suite.
Jul 16 13:05:09.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:05:09.441: INFO: namespace: e2e-tests-hostpath-sq7xh, resource: bindings, ignored listing per whitelist
Jul 16 13:05:11.676: INFO: namespace e2e-tests-hostpath-sq7xh deletion completed in 8.329115219s

• [SLOW TEST:12.479 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:05:11.677: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-58488011-a7ca-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:05:11.812: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-wqsc6" to be "success or failure"
Jul 16 13:05:11.819: INFO: Pod "pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280315ms
Jul 16 13:05:13.824: INFO: Pod "pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011806259s
Jul 16 13:05:15.827: INFO: Pod "pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0150782s
STEP: Saw pod success
Jul 16 13:05:15.827: INFO: Pod "pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:05:15.830: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:05:15.862: INFO: Waiting for pod pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:05:15.866: INFO: Pod pod-projected-secrets-5848fe8a-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:05:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wqsc6" for this suite.
Jul 16 13:05:21.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:05:23.759: INFO: namespace: e2e-tests-projected-wqsc6, resource: bindings, ignored listing per whitelist
Jul 16 13:05:24.199: INFO: namespace e2e-tests-projected-wqsc6 deletion completed in 8.327996227s

• [SLOW TEST:12.522 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:05:24.201: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 16 13:05:24.326: INFO: Waiting up to 5m0s for pod "pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-pfphs" to be "success or failure"
Jul 16 13:05:24.330: INFO: Pod "pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165554ms
Jul 16 13:05:26.333: INFO: Pod "pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007335005s
STEP: Saw pod success
Jul 16 13:05:26.333: INFO: Pod "pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:05:26.335: INFO: Trying to get logs from node i-taxl6ow1 pod pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:05:26.365: INFO: Waiting for pod pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:05:26.367: INFO: Pod pod-5fc0714a-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:05:26.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pfphs" for this suite.
Jul 16 13:05:32.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:05:32.461: INFO: namespace: e2e-tests-emptydir-pfphs, resource: bindings, ignored listing per whitelist
Jul 16 13:05:34.709: INFO: namespace e2e-tests-emptydir-pfphs deletion completed in 8.337600156s

• [SLOW TEST:10.508 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:05:34.709: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-660169c8-a7ca-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:05:34.826: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-nsvwl" to be "success or failure"
Jul 16 13:05:34.830: INFO: Pod "pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709245ms
Jul 16 13:05:36.836: INFO: Pod "pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009735818s
STEP: Saw pod success
Jul 16 13:05:36.836: INFO: Pod "pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:05:36.838: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:05:36.856: INFO: Waiting for pod pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:05:36.859: INFO: Pod pod-projected-configmaps-6602351b-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:05:36.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nsvwl" for this suite.
Jul 16 13:05:42.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:05:42.951: INFO: namespace: e2e-tests-projected-nsvwl, resource: bindings, ignored listing per whitelist
Jul 16 13:05:45.200: INFO: namespace e2e-tests-projected-nsvwl deletion completed in 8.329837642s

• [SLOW TEST:10.491 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:05:45.200: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-nwsbw
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 13:05:45.294: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 13:06:05.363: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.83:8080/dial?request=hostName&protocol=udp&host=10.233.83.237&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-nwsbw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:06:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:06:05.624: INFO: Waiting for endpoints: map[]
Jul 16 13:06:05.627: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.83:8080/dial?request=hostName&protocol=udp&host=10.233.100.84&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-nwsbw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:06:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:06:05.829: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:06:05.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-nwsbw" for this suite.
Jul 16 13:06:29.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:30.874: INFO: namespace: e2e-tests-pod-network-test-nwsbw, resource: bindings, ignored listing per whitelist
Jul 16 13:06:32.174: INFO: namespace e2e-tests-pod-network-test-nwsbw deletion completed in 26.337486532s

• [SLOW TEST:46.974 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:06:32.175: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Jul 16 13:06:32.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 --namespace=e2e-tests-kubectl-cm5qn run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 16 13:06:34.501: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 16 13:06:34.501: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:06:36.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cm5qn" for this suite.
Jul 16 13:06:42.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:44.754: INFO: namespace: e2e-tests-kubectl-cm5qn, resource: bindings, ignored listing per whitelist
Jul 16 13:06:44.854: INFO: namespace e2e-tests-kubectl-cm5qn deletion completed in 8.33923055s

• [SLOW TEST:12.679 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:06:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:06:44.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-7hsp7" to be "success or failure"
Jul 16 13:06:44.953: INFO: Pod "downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49327ms
Jul 16 13:06:46.956: INFO: Pod "downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008707521s
Jul 16 13:06:48.959: INFO: Pod "downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012084731s
STEP: Saw pod success
Jul 16 13:06:48.960: INFO: Pod "downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:06:48.962: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:06:48.982: INFO: Waiting for pod downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:06:48.985: INFO: Pod downwardapi-volume-8fcdee84-a7ca-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:06:48.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7hsp7" for this suite.
Jul 16 13:06:55.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:55.142: INFO: namespace: e2e-tests-downward-api-7hsp7, resource: bindings, ignored listing per whitelist
Jul 16 13:06:57.323: INFO: namespace e2e-tests-downward-api-7hsp7 deletion completed in 8.330852802s

• [SLOW TEST:12.467 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:06:57.323: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-d74m2
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Jul 16 13:06:57.441: INFO: Found 0 stateful pods, waiting for 3
Jul 16 13:07:07.446: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:07.446: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:07.446: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 16 13:07:07.472: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 16 13:07:17.513: INFO: Updating stateful set ss2
Jul 16 13:07:17.531: INFO: Waiting for Pod e2e-tests-statefulset-d74m2/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jul 16 13:07:27.602: INFO: Found 1 stateful pods, waiting for 3
Jul 16 13:07:37.610: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:37.610: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:37.610: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 16 13:07:47.610: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:47.610: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:47.610: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 16 13:07:57.609: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:57.609: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:07:57.609: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 16 13:08:07.609: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:08:07.609: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:08:07.609: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 16 13:08:07.639: INFO: Updating stateful set ss2
Jul 16 13:08:07.661: INFO: Waiting for Pod e2e-tests-statefulset-d74m2/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 16 13:08:17.690: INFO: Updating stateful set ss2
Jul 16 13:08:17.697: INFO: Waiting for StatefulSet e2e-tests-statefulset-d74m2/ss2 to complete update
Jul 16 13:08:17.697: INFO: Waiting for Pod e2e-tests-statefulset-d74m2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 16 13:08:27.704: INFO: Waiting for StatefulSet e2e-tests-statefulset-d74m2/ss2 to complete update
Jul 16 13:08:27.704: INFO: Waiting for Pod e2e-tests-statefulset-d74m2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 16 13:08:37.703: INFO: Waiting for StatefulSet e2e-tests-statefulset-d74m2/ss2 to complete update
Jul 16 13:08:37.703: INFO: Waiting for Pod e2e-tests-statefulset-d74m2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 16 13:08:47.714: INFO: Waiting for StatefulSet e2e-tests-statefulset-d74m2/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 16 13:08:57.706: INFO: Deleting all statefulset in ns e2e-tests-statefulset-d74m2
Jul 16 13:08:57.708: INFO: Scaling statefulset ss2 to 0
Jul 16 13:09:17.734: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 13:09:17.737: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:09:17.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-d74m2" for this suite.
Jul 16 13:09:23.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:09:23.796: INFO: namespace: e2e-tests-statefulset-d74m2, resource: bindings, ignored listing per whitelist
Jul 16 13:09:26.080: INFO: namespace e2e-tests-statefulset-d74m2 deletion completed in 8.323983042s

• [SLOW TEST:148.757 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:09:26.082: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jul 16 13:09:30.708: INFO: Successfully updated pod "annotationupdateefe7d774-a7ca-11e9-9e69-7a63f556cc5d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:09:32.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6dcsn" for this suite.
Jul 16 13:09:56.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:09:58.633: INFO: namespace: e2e-tests-projected-6dcsn, resource: bindings, ignored listing per whitelist
Jul 16 13:09:59.091: INFO: namespace e2e-tests-projected-6dcsn deletion completed in 26.345323068s

• [SLOW TEST:33.009 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:09:59.093: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jul 16 13:10:03.763: INFO: Successfully updated pod "annotationupdate0398fb7c-a7cb-11e9-9e69-7a63f556cc5d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:10:05.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7sv9w" for this suite.
Jul 16 13:10:29.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:10:29.930: INFO: namespace: e2e-tests-downward-api-7sv9w, resource: bindings, ignored listing per whitelist
Jul 16 13:10:32.140: INFO: namespace e2e-tests-downward-api-7sv9w deletion completed in 26.335279652s

• [SLOW TEST:33.047 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:10:32.140: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:10:58.251: INFO: Container started at 2019-07-16 13:10:35 +0000 UTC, pod became ready at 2019-07-16 13:10:57 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:10:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9bj87" for this suite.
Jul 16 13:11:22.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:11:22.326: INFO: namespace: e2e-tests-container-probe-9bj87, resource: bindings, ignored listing per whitelist
Jul 16 13:11:24.585: INFO: namespace e2e-tests-container-probe-9bj87 deletion completed in 26.328643556s

• [SLOW TEST:52.445 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:11:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:11:24.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-dzgnw" to be "success or failure"
Jul 16 13:11:24.678: INFO: Pod "downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26672ms
Jul 16 13:11:26.682: INFO: Pod "downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005735143s
STEP: Saw pod success
Jul 16 13:11:26.682: INFO: Pod "downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:11:26.684: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:11:26.702: INFO: Waiting for pod downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:11:26.705: INFO: Pod downwardapi-volume-36899eb0-a7cb-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:11:26.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dzgnw" for this suite.
Jul 16 13:11:32.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:11:33.902: INFO: namespace: e2e-tests-downward-api-dzgnw, resource: bindings, ignored listing per whitelist
Jul 16 13:11:35.052: INFO: namespace e2e-tests-downward-api-dzgnw deletion completed in 8.338579944s

• [SLOW TEST:10.467 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:11:35.053: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-s8bx7
Jul 16 13:11:37.159: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-s8bx7
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 13:11:37.163: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:15:37.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-s8bx7" for this suite.
Jul 16 13:15:43.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:15:43.920: INFO: namespace: e2e-tests-container-probe-s8bx7, resource: bindings, ignored listing per whitelist
Jul 16 13:15:46.149: INFO: namespace e2e-tests-container-probe-s8bx7 deletion completed in 8.35086757s

• [SLOW TEST:251.096 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:15:46.149: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d2729552-a7cb-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:15:46.267: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-b8wdt" to be "success or failure"
Jul 16 13:15:46.271: INFO: Pod "pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00075ms
Jul 16 13:15:48.275: INFO: Pod "pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007891198s
STEP: Saw pod success
Jul 16 13:15:48.275: INFO: Pod "pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:15:48.278: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:15:48.298: INFO: Waiting for pod pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:15:48.301: INFO: Pod pod-projected-configmaps-d2752159-a7cb-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:15:48.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b8wdt" for this suite.
Jul 16 13:15:54.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:15:55.932: INFO: namespace: e2e-tests-projected-b8wdt, resource: bindings, ignored listing per whitelist
Jul 16 13:15:56.631: INFO: namespace e2e-tests-projected-b8wdt deletion completed in 8.325353828s

• [SLOW TEST:10.482 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:15:56.632: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:16:02.748: INFO: Waiting up to 5m0s for pod "client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-pods-q7gqw" to be "success or failure"
Jul 16 13:16:02.754: INFO: Pod "client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.444114ms
Jul 16 13:16:04.759: INFO: Pod "client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010819301s
Jul 16 13:16:06.762: INFO: Pod "client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014152221s
STEP: Saw pod success
Jul 16 13:16:06.762: INFO: Pod "client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:16:06.764: INFO: Trying to get logs from node i-taxl6ow1 pod client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d container env3cont: <nil>
STEP: delete the pod
Jul 16 13:16:06.797: INFO: Waiting for pod client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:16:06.800: INFO: Pod client-envvars-dc47ec17-a7cb-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:16:06.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-q7gqw" for this suite.
Jul 16 13:16:46.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:46.913: INFO: namespace: e2e-tests-pods-q7gqw, resource: bindings, ignored listing per whitelist
Jul 16 13:16:49.141: INFO: namespace e2e-tests-pods-q7gqw deletion completed in 42.334197627s

• [SLOW TEST:52.509 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:16:49.145: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-f7ff0985-a7cb-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f7ff0985-a7cb-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:18:13.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wxk7j" for this suite.
Jul 16 13:18:35.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:18:36.808: INFO: namespace: e2e-tests-configmap-wxk7j, resource: bindings, ignored listing per whitelist
Jul 16 13:18:38.207: INFO: namespace e2e-tests-configmap-wxk7j deletion completed in 24.327145931s

• [SLOW TEST:109.062 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:18:38.209: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-vspv
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 13:18:38.318: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vspv" in namespace "e2e-tests-subpath-b9bzl" to be "success or failure"
Jul 16 13:18:38.320: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365422ms
Jul 16 13:18:40.325: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007002029s
Jul 16 13:18:42.329: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 4.011136908s
Jul 16 13:18:44.333: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 6.015048312s
Jul 16 13:18:46.365: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 8.046800202s
Jul 16 13:18:48.369: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 10.051077152s
Jul 16 13:18:50.372: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 12.054288291s
Jul 16 13:18:52.376: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 14.057712315s
Jul 16 13:18:54.380: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 16.061552919s
Jul 16 13:18:56.384: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 18.066222925s
Jul 16 13:18:58.388: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 20.070488607s
Jul 16 13:19:00.393: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Running", Reason="", readiness=false. Elapsed: 22.075095851s
Jul 16 13:19:02.397: INFO: Pod "pod-subpath-test-configmap-vspv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.079218266s
STEP: Saw pod success
Jul 16 13:19:02.397: INFO: Pod "pod-subpath-test-configmap-vspv" satisfied condition "success or failure"
Jul 16 13:19:02.401: INFO: Trying to get logs from node i-taxl6ow1 pod pod-subpath-test-configmap-vspv container test-container-subpath-configmap-vspv: <nil>
STEP: delete the pod
Jul 16 13:19:02.443: INFO: Waiting for pod pod-subpath-test-configmap-vspv to disappear
Jul 16 13:19:02.446: INFO: Pod pod-subpath-test-configmap-vspv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vspv
Jul 16 13:19:02.447: INFO: Deleting pod "pod-subpath-test-configmap-vspv" in namespace "e2e-tests-subpath-b9bzl"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:19:02.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-b9bzl" for this suite.
Jul 16 13:19:08.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:19:10.215: INFO: namespace: e2e-tests-subpath-b9bzl, resource: bindings, ignored listing per whitelist
Jul 16 13:19:10.808: INFO: namespace e2e-tests-subpath-b9bzl deletion completed in 8.347156528s

• [SLOW TEST:32.600 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:19:10.811: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-4c8d65c5-a7cc-11e9-9e69-7a63f556cc5d
STEP: Creating secret with name s-test-opt-upd-4c8d6775-a7cc-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4c8d65c5-a7cc-11e9-9e69-7a63f556cc5d
STEP: Updating secret s-test-opt-upd-4c8d6775-a7cc-11e9-9e69-7a63f556cc5d
STEP: Creating secret with name s-test-opt-create-4c8d679f-a7cc-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:19:19.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jqnpv" for this suite.
Jul 16 13:19:41.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:19:41.692: INFO: namespace: e2e-tests-projected-jqnpv, resource: bindings, ignored listing per whitelist
Jul 16 13:19:43.600: INFO: namespace e2e-tests-projected-jqnpv deletion completed in 24.340621207s

• [SLOW TEST:32.793 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:19:43.604: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:19:43.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-7rvh9" to be "success or failure"
Jul 16 13:19:43.766: INFO: Pod "downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.565195ms
Jul 16 13:19:45.771: INFO: Pod "downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011124567s
Jul 16 13:19:47.775: INFO: Pod "downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01523165s
STEP: Saw pod success
Jul 16 13:19:47.775: INFO: Pod "downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:19:47.778: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:19:47.800: INFO: Waiting for pod downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:19:47.808: INFO: Pod downwardapi-volume-6001d2e0-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:19:47.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7rvh9" for this suite.
Jul 16 13:19:53.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:19:55.543: INFO: namespace: e2e-tests-projected-7rvh9, resource: bindings, ignored listing per whitelist
Jul 16 13:19:56.144: INFO: namespace e2e-tests-projected-7rvh9 deletion completed in 8.330499165s

• [SLOW TEST:12.540 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:19:56.145: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:19:56.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-cvkz9" to be "success or failure"
Jul 16 13:19:56.241: INFO: Pod "downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.885504ms
Jul 16 13:19:58.245: INFO: Pod "downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017714518s
STEP: Saw pod success
Jul 16 13:19:58.245: INFO: Pod "downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:19:58.248: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:19:58.282: INFO: Waiting for pod downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:19:58.285: INFO: Pod downwardapi-volume-677219be-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:19:58.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cvkz9" for this suite.
Jul 16 13:20:04.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:20:04.376: INFO: namespace: e2e-tests-downward-api-cvkz9, resource: bindings, ignored listing per whitelist
Jul 16 13:20:06.626: INFO: namespace e2e-tests-downward-api-cvkz9 deletion completed in 8.334331128s

• [SLOW TEST:10.482 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:20:06.627: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Jul 16 13:20:06.718: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 16 13:20:06.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:07.443: INFO: stderr: ""
Jul 16 13:20:07.443: INFO: stdout: "service/redis-slave created\n"
Jul 16 13:20:07.443: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 16 13:20:07.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:07.800: INFO: stderr: ""
Jul 16 13:20:07.800: INFO: stdout: "service/redis-master created\n"
Jul 16 13:20:07.801: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 16 13:20:07.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:08.080: INFO: stderr: ""
Jul 16 13:20:08.080: INFO: stdout: "service/frontend created\n"
Jul 16 13:20:08.081: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 16 13:20:08.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:08.356: INFO: stderr: ""
Jul 16 13:20:08.356: INFO: stdout: "deployment.extensions/frontend created\n"
Jul 16 13:20:08.356: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 16 13:20:08.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:08.666: INFO: stderr: ""
Jul 16 13:20:08.675: INFO: stdout: "deployment.extensions/redis-master created\n"
Jul 16 13:20:08.685: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 16 13:20:08.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:08.946: INFO: stderr: ""
Jul 16 13:20:08.946: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Jul 16 13:20:08.946: INFO: Waiting for all frontend pods to be Running.
Jul 16 13:20:43.998: INFO: Waiting for frontend to serve content.
Jul 16 13:20:45.024: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 16 13:20:50.052: INFO: Trying to add a new entry to the guestbook.
Jul 16 13:20:50.065: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 16 13:20:50.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.222: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.222: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:20:50.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.363: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.363: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:20:50.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.504: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.504: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:20:50.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.642: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.642: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:20:50.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.788: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.788: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:20:50.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67h8q'
Jul 16 13:20:50.935: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:20:50.935: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:20:50.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-67h8q" for this suite.
Jul 16 13:21:30.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:21:32.883: INFO: namespace: e2e-tests-kubectl-67h8q, resource: bindings, ignored listing per whitelist
Jul 16 13:21:33.283: INFO: namespace e2e-tests-kubectl-67h8q deletion completed in 42.340395438s

• [SLOW TEST:86.656 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:21:33.283: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 16 13:21:33.383: INFO: Waiting up to 5m0s for pod "pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-8wdvr" to be "success or failure"
Jul 16 13:21:33.386: INFO: Pod "pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.739596ms
Jul 16 13:21:35.390: INFO: Pod "pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006477731s
Jul 16 13:21:37.393: INFO: Pod "pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010007202s
STEP: Saw pod success
Jul 16 13:21:37.393: INFO: Pod "pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:21:37.397: INFO: Trying to get logs from node i-taxl6ow1 pod pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:21:37.414: INFO: Waiting for pod pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:21:37.416: INFO: Pod pod-a15a7733-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:21:37.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8wdvr" for this suite.
Jul 16 13:21:43.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:21:43.491: INFO: namespace: e2e-tests-emptydir-8wdvr, resource: bindings, ignored listing per whitelist
Jul 16 13:21:45.747: INFO: namespace e2e-tests-emptydir-8wdvr deletion completed in 8.326392352s

• [SLOW TEST:12.464 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:21:45.748: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Jul 16 13:21:45.834: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-453034999 proxy --unix-socket=/tmp/kubectl-proxy-unix421154026/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:21:45.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7k47h" for this suite.
Jul 16 13:21:51.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:21:51.939: INFO: namespace: e2e-tests-kubectl-7k47h, resource: bindings, ignored listing per whitelist
Jul 16 13:21:54.243: INFO: namespace e2e-tests-kubectl-7k47h deletion completed in 8.334732086s

• [SLOW TEST:8.495 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:21:54.244: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:21:54.366: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:21:56.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qxkgm" for this suite.
Jul 16 13:22:36.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:22:37.090: INFO: namespace: e2e-tests-pods-qxkgm, resource: bindings, ignored listing per whitelist
Jul 16 13:22:38.740: INFO: namespace e2e-tests-pods-qxkgm deletion completed in 42.334085715s

• [SLOW TEST:44.496 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:22:38.740: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:22:38.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-l2d59" to be "success or failure"
Jul 16 13:22:38.828: INFO: Pod "downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996493ms
Jul 16 13:22:40.832: INFO: Pod "downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006637281s
STEP: Saw pod success
Jul 16 13:22:40.832: INFO: Pod "downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:22:40.834: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:22:40.856: INFO: Waiting for pod downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:22:40.859: INFO: Pod downwardapi-volume-c85cd058-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:22:40.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l2d59" for this suite.
Jul 16 13:22:46.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:22:46.969: INFO: namespace: e2e-tests-projected-l2d59, resource: bindings, ignored listing per whitelist
Jul 16 13:22:49.207: INFO: namespace e2e-tests-projected-l2d59 deletion completed in 8.343888974s

• [SLOW TEST:10.467 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:22:49.208: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:22:49.296: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 16 13:22:49.302: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 16 13:22:54.309: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 13:22:54.309: INFO: Creating deployment "test-rolling-update-deployment"
Jul 16 13:22:54.314: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 16 13:22:54.320: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 16 13:22:56.328: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 16 13:22:56.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880174, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880174, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880174, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880174, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:22:58.334: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 16 13:22:58.342: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-x45b4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x45b4/deployments/test-rolling-update-deployment,UID:d198843f-a7cc-11e9-a7d7-5254228aab6e,ResourceVersion:14982,Generation:1,CreationTimestamp:2019-07-16 13:22:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-16 13:22:54 +0000 UTC 2019-07-16 13:22:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-16 13:22:56 +0000 UTC 2019-07-16 13:22:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 16 13:22:58.347: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-x45b4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x45b4/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:d19b7c03-a7cc-11e9-a7d7-5254228aab6e,ResourceVersion:14973,Generation:1,CreationTimestamp:2019-07-16 13:22:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d198843f-a7cc-11e9-a7d7-5254228aab6e 0xc00207bcd7 0xc00207bcd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 13:22:58.347: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 16 13:22:58.348: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-x45b4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x45b4/replicasets/test-rolling-update-controller,UID:ce9b721d-a7cc-11e9-a7d7-5254228aab6e,ResourceVersion:14981,Generation:2,CreationTimestamp:2019-07-16 13:22:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d198843f-a7cc-11e9-a7d7-5254228aab6e 0xc00207bc17 0xc00207bc18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:22:58.352: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-l6sxz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-l6sxz,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-x45b4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-x45b4/pods/test-rolling-update-deployment-68b55d7bc6-l6sxz,UID:d19c1fc3-a7cc-11e9-a7d7-5254228aab6e,ResourceVersion:14972,Generation:0,CreationTimestamp:2019-07-16 13:22:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 d19b7c03-a7cc-11e9-a7d7-5254228aab6e 0xc000ddbc57 0xc000ddbc58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-88pbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-88pbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-88pbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ddbea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ddbec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:22:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:22:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:22:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:22:54 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.99,StartTime:2019-07-16 13:22:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-16 13:22:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://873393f6d411f90c267acdd3d57ca9e7998cf6011e7cdb81c245bd413e0e4efb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:22:58.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-x45b4" for this suite.
Jul 16 13:23:04.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:04.410: INFO: namespace: e2e-tests-deployment-x45b4, resource: bindings, ignored listing per whitelist
Jul 16 13:23:06.684: INFO: namespace e2e-tests-deployment-x45b4 deletion completed in 8.327755432s

• [SLOW TEST:17.476 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:23:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:23:06.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-vdnzh" to be "success or failure"
Jul 16 13:23:06.778: INFO: Pod "downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.597416ms
Jul 16 13:23:08.783: INFO: Pod "downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010740924s
STEP: Saw pod success
Jul 16 13:23:08.783: INFO: Pod "downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:23:08.786: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:23:08.810: INFO: Waiting for pod downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:23:08.814: INFO: Pod downwardapi-volume-d904d55c-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:23:08.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vdnzh" for this suite.
Jul 16 13:23:14.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:16.864: INFO: namespace: e2e-tests-downward-api-vdnzh, resource: bindings, ignored listing per whitelist
Jul 16 13:23:17.172: INFO: namespace e2e-tests-downward-api-vdnzh deletion completed in 8.353575786s

• [SLOW TEST:10.486 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:23:17.172: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-df450511-a7cc-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:23:17.266: INFO: Waiting up to 5m0s for pod "pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-gzrnb" to be "success or failure"
Jul 16 13:23:17.270: INFO: Pod "pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.452077ms
Jul 16 13:23:19.273: INFO: Pod "pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006733481s
Jul 16 13:23:21.277: INFO: Pod "pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009855909s
STEP: Saw pod success
Jul 16 13:23:21.277: INFO: Pod "pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:23:21.278: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:23:21.298: INFO: Waiting for pod pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:23:21.300: INFO: Pod pod-configmaps-df45ddf3-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:23:21.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gzrnb" for this suite.
Jul 16 13:23:27.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:27.403: INFO: namespace: e2e-tests-configmap-gzrnb, resource: bindings, ignored listing per whitelist
Jul 16 13:23:29.633: INFO: namespace e2e-tests-configmap-gzrnb deletion completed in 8.328234578s

• [SLOW TEST:12.460 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:23:29.633: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-lpzl
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 13:23:29.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-lpzl" in namespace "e2e-tests-subpath-cctmz" to be "success or failure"
Jul 16 13:23:29.758: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.592121ms
Jul 16 13:23:31.763: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011874838s
Jul 16 13:23:33.767: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 4.015809518s
Jul 16 13:23:35.771: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 6.019958019s
Jul 16 13:23:37.775: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 8.024307426s
Jul 16 13:23:39.779: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 10.028131307s
Jul 16 13:23:41.783: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 12.032008875s
Jul 16 13:23:43.786: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 14.035055591s
Jul 16 13:23:45.790: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 16.038754465s
Jul 16 13:23:47.793: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 18.042701504s
Jul 16 13:23:49.797: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 20.045925694s
Jul 16 13:23:51.801: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Running", Reason="", readiness=false. Elapsed: 22.049879402s
Jul 16 13:23:53.804: INFO: Pod "pod-subpath-test-projected-lpzl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05362302s
STEP: Saw pod success
Jul 16 13:23:53.804: INFO: Pod "pod-subpath-test-projected-lpzl" satisfied condition "success or failure"
Jul 16 13:23:53.807: INFO: Trying to get logs from node i-taxl6ow1 pod pod-subpath-test-projected-lpzl container test-container-subpath-projected-lpzl: <nil>
STEP: delete the pod
Jul 16 13:23:53.827: INFO: Waiting for pod pod-subpath-test-projected-lpzl to disappear
Jul 16 13:23:53.830: INFO: Pod pod-subpath-test-projected-lpzl no longer exists
STEP: Deleting pod pod-subpath-test-projected-lpzl
Jul 16 13:23:53.830: INFO: Deleting pod "pod-subpath-test-projected-lpzl" in namespace "e2e-tests-subpath-cctmz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:23:53.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-cctmz" for this suite.
Jul 16 13:23:59.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:24:01.683: INFO: namespace: e2e-tests-subpath-cctmz, resource: bindings, ignored listing per whitelist
Jul 16 13:24:02.178: INFO: namespace e2e-tests-subpath-cctmz deletion completed in 8.340429153s

• [SLOW TEST:32.545 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:24:02.179: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:24:02.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-lftt5" to be "success or failure"
Jul 16 13:24:02.295: INFO: Pod "downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717892ms
Jul 16 13:24:04.298: INFO: Pod "downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005824931s
Jul 16 13:24:06.301: INFO: Pod "downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009378456s
STEP: Saw pod success
Jul 16 13:24:06.301: INFO: Pod "downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:24:06.304: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:24:06.322: INFO: Waiting for pod downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:24:06.326: INFO: Pod downwardapi-volume-fa1c785e-a7cc-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:24:06.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lftt5" for this suite.
Jul 16 13:24:12.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:24:12.435: INFO: namespace: e2e-tests-projected-lftt5, resource: bindings, ignored listing per whitelist
Jul 16 13:24:14.665: INFO: namespace e2e-tests-projected-lftt5 deletion completed in 8.334799682s

• [SLOW TEST:12.487 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:24:14.666: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Jul 16 13:24:14.737: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-453034999 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:24:14.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hpkz6" for this suite.
Jul 16 13:24:20.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:24:20.877: INFO: namespace: e2e-tests-kubectl-hpkz6, resource: bindings, ignored listing per whitelist
Jul 16 13:24:23.163: INFO: namespace e2e-tests-kubectl-hpkz6 deletion completed in 8.335657335s

• [SLOW TEST:8.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:24:23.165: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:24:23.286: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Jul 16 13:24:23.293: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-hmjhz/daemonsets","resourceVersion":"15324"},"items":null}

Jul 16 13:24:23.295: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-hmjhz/pods","resourceVersion":"15324"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:24:23.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-hmjhz" for this suite.
Jul 16 13:24:29.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:24:29.346: INFO: namespace: e2e-tests-daemonsets-hmjhz, resource: bindings, ignored listing per whitelist
Jul 16 13:24:31.648: INFO: namespace e2e-tests-daemonsets-hmjhz deletion completed in 8.339395709s

S [SKIPPING] [8.483 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Jul 16 13:24:23.286: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:24:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-snpsl
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-snpsl
STEP: Deleting pre-stop pod
Jul 16 13:24:46.826: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:24:46.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-snpsl" for this suite.
Jul 16 13:25:26.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:25:26.935: INFO: namespace: e2e-tests-prestop-snpsl, resource: bindings, ignored listing per whitelist
Jul 16 13:25:29.166: INFO: namespace e2e-tests-prestop-snpsl deletion completed in 42.330886674s

• [SLOW TEST:57.517 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:25:29.166: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:25:29.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-n4qdx" to be "success or failure"
Jul 16 13:25:29.259: INFO: Pod "downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023635ms
Jul 16 13:25:31.263: INFO: Pod "downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007688572s
STEP: Saw pod success
Jul 16 13:25:31.263: INFO: Pod "downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:25:31.266: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:25:31.293: INFO: Waiting for pod downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:25:31.299: INFO: Pod downwardapi-volume-2df1d4c4-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:25:31.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n4qdx" for this suite.
Jul 16 13:25:37.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:25:37.401: INFO: namespace: e2e-tests-projected-n4qdx, resource: bindings, ignored listing per whitelist
Jul 16 13:25:39.643: INFO: namespace e2e-tests-projected-n4qdx deletion completed in 8.33828633s

• [SLOW TEST:10.477 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:25:39.644: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3432afce-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3432afce-a7cd-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:25:43.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zl529" for this suite.
Jul 16 13:26:07.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:26:08.726: INFO: namespace: e2e-tests-projected-zl529, resource: bindings, ignored listing per whitelist
Jul 16 13:26:10.126: INFO: namespace e2e-tests-projected-zl529 deletion completed in 26.330998152s

• [SLOW TEST:30.482 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:26:10.127: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jul 16 13:26:10.267: INFO: Waiting up to 5m0s for pod "downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-vsrrb" to be "success or failure"
Jul 16 13:26:10.273: INFO: Pod "downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.528659ms
Jul 16 13:26:12.277: INFO: Pod "downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009448301s
STEP: Saw pod success
Jul 16 13:26:12.277: INFO: Pod "downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:26:12.279: INFO: Trying to get logs from node i-taxl6ow1 pod downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:26:12.304: INFO: Waiting for pod downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:26:12.307: INFO: Pod downward-api-4663569d-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:26:12.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vsrrb" for this suite.
Jul 16 13:26:18.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:26:19.690: INFO: namespace: e2e-tests-downward-api-vsrrb, resource: bindings, ignored listing per whitelist
Jul 16 13:26:20.640: INFO: namespace e2e-tests-downward-api-vsrrb deletion completed in 8.328505569s

• [SLOW TEST:10.513 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:26:20.640: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 16 13:26:20.753: INFO: Waiting up to 5m0s for pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-rpxxw" to be "success or failure"
Jul 16 13:26:20.758: INFO: Pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503788ms
Jul 16 13:26:22.762: INFO: Pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007966325s
Jul 16 13:26:24.765: INFO: Pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011194431s
Jul 16 13:26:26.769: INFO: Pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015047701s
STEP: Saw pod success
Jul 16 13:26:26.769: INFO: Pod "pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:26:26.773: INFO: Trying to get logs from node i-taxl6ow1 pod pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:26:26.802: INFO: Waiting for pod pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:26:26.804: INFO: Pod pod-4ca38ef0-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:26:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rpxxw" for this suite.
Jul 16 13:26:32.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:26:34.385: INFO: namespace: e2e-tests-emptydir-rpxxw, resource: bindings, ignored listing per whitelist
Jul 16 13:26:35.135: INFO: namespace e2e-tests-emptydir-rpxxw deletion completed in 8.327668887s

• [SLOW TEST:14.495 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:26:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-554526ba-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating secret with name s-test-opt-upd-55452712-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-554526ba-a7cd-11e9-9e69-7a63f556cc5d
STEP: Updating secret s-test-opt-upd-55452712-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating secret with name s-test-opt-create-55452733-a7cd-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:26:43.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wmww5" for this suite.
Jul 16 13:27:07.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:27:07.579: INFO: namespace: e2e-tests-secrets-wmww5, resource: bindings, ignored listing per whitelist
Jul 16 13:27:09.680: INFO: namespace e2e-tests-secrets-wmww5 deletion completed in 26.329682919s

• [SLOW TEST:34.544 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:27:09.682: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 16 13:27:09.774: INFO: Waiting up to 5m0s for pod "pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-65kzr" to be "success or failure"
Jul 16 13:27:09.778: INFO: Pod "pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225914ms
Jul 16 13:27:11.782: INFO: Pod "pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00733907s
STEP: Saw pod success
Jul 16 13:27:11.782: INFO: Pod "pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:27:11.786: INFO: Trying to get logs from node i-taxl6ow1 pod pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:27:11.810: INFO: Waiting for pod pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:27:11.814: INFO: Pod pod-69dc4124-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:27:11.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-65kzr" for this suite.
Jul 16 13:27:17.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:27:17.963: INFO: namespace: e2e-tests-emptydir-65kzr, resource: bindings, ignored listing per whitelist
Jul 16 13:27:20.166: INFO: namespace e2e-tests-emptydir-65kzr deletion completed in 8.346088292s

• [SLOW TEST:10.484 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:27:20.167: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-mzv8
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 13:27:20.293: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mzv8" in namespace "e2e-tests-subpath-b64kq" to be "success or failure"
Jul 16 13:27:20.298: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.606166ms
Jul 16 13:27:22.302: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008601123s
Jul 16 13:27:24.305: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 4.011852167s
Jul 16 13:27:26.308: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 6.014936055s
Jul 16 13:27:28.312: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 8.018699513s
Jul 16 13:27:30.316: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 10.022741528s
Jul 16 13:27:32.319: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 12.026165823s
Jul 16 13:27:34.322: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 14.029329071s
Jul 16 13:27:36.326: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 16.033271629s
Jul 16 13:27:38.332: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 18.038587975s
Jul 16 13:27:40.335: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 20.042428901s
Jul 16 13:27:42.339: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Running", Reason="", readiness=false. Elapsed: 22.045828674s
Jul 16 13:27:44.343: INFO: Pod "pod-subpath-test-downwardapi-mzv8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050014791s
STEP: Saw pod success
Jul 16 13:27:44.343: INFO: Pod "pod-subpath-test-downwardapi-mzv8" satisfied condition "success or failure"
Jul 16 13:27:44.346: INFO: Trying to get logs from node i-taxl6ow1 pod pod-subpath-test-downwardapi-mzv8 container test-container-subpath-downwardapi-mzv8: <nil>
STEP: delete the pod
Jul 16 13:27:44.371: INFO: Waiting for pod pod-subpath-test-downwardapi-mzv8 to disappear
Jul 16 13:27:44.375: INFO: Pod pod-subpath-test-downwardapi-mzv8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mzv8
Jul 16 13:27:44.375: INFO: Deleting pod "pod-subpath-test-downwardapi-mzv8" in namespace "e2e-tests-subpath-b64kq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:27:44.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-b64kq" for this suite.
Jul 16 13:27:50.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:27:52.524: INFO: namespace: e2e-tests-subpath-b64kq, resource: bindings, ignored listing per whitelist
Jul 16 13:27:52.722: INFO: namespace e2e-tests-subpath-b64kq deletion completed in 8.338574291s

• [SLOW TEST:32.555 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:27:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-83823be0-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:27:52.808: INFO: Waiting up to 5m0s for pod "pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-8nhdm" to be "success or failure"
Jul 16 13:27:52.815: INFO: Pod "pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.556472ms
Jul 16 13:27:54.820: INFO: Pod "pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011763373s
Jul 16 13:27:56.824: INFO: Pod "pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015894896s
STEP: Saw pod success
Jul 16 13:27:56.824: INFO: Pod "pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:27:56.827: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:27:56.853: INFO: Waiting for pod pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:27:56.856: INFO: Pod pod-secrets-8382cf70-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:27:56.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8nhdm" for this suite.
Jul 16 13:28:02.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:03.397: INFO: namespace: e2e-tests-secrets-8nhdm, resource: bindings, ignored listing per whitelist
Jul 16 13:28:05.196: INFO: namespace e2e-tests-secrets-8nhdm deletion completed in 8.334327902s

• [SLOW TEST:12.474 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:28:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 16 13:28:05.274: INFO: Waiting up to 5m0s for pod "pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-r5vbt" to be "success or failure"
Jul 16 13:28:05.276: INFO: Pod "pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227077ms
Jul 16 13:28:07.280: INFO: Pod "pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005629827s
Jul 16 13:28:09.286: INFO: Pod "pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012129638s
STEP: Saw pod success
Jul 16 13:28:09.286: INFO: Pod "pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:28:09.289: INFO: Trying to get logs from node i-taxl6ow1 pod pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:28:09.306: INFO: Waiting for pod pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:28:09.309: INFO: Pod pod-8af0f4aa-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:28:09.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r5vbt" for this suite.
Jul 16 13:28:15.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:15.894: INFO: namespace: e2e-tests-emptydir-r5vbt, resource: bindings, ignored listing per whitelist
Jul 16 13:28:17.643: INFO: namespace e2e-tests-emptydir-r5vbt deletion completed in 8.329233837s

• [SLOW TEST:12.448 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:28:17.643: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-c9f26/configmap-test-926775a6-a7cd-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:28:17.802: INFO: Waiting up to 5m0s for pod "pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-c9f26" to be "success or failure"
Jul 16 13:28:17.807: INFO: Pod "pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194146ms
Jul 16 13:28:19.812: INFO: Pod "pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009528762s
STEP: Saw pod success
Jul 16 13:28:19.812: INFO: Pod "pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:28:19.816: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d container env-test: <nil>
STEP: delete the pod
Jul 16 13:28:19.835: INFO: Waiting for pod pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:28:19.839: INFO: Pod pod-configmaps-9267f621-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:28:19.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c9f26" for this suite.
Jul 16 13:28:25.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:26.132: INFO: namespace: e2e-tests-configmap-c9f26, resource: bindings, ignored listing per whitelist
Jul 16 13:28:28.181: INFO: namespace e2e-tests-configmap-c9f26 deletion completed in 8.337447566s

• [SLOW TEST:10.538 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:28:28.181: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jul 16 13:28:28.272: INFO: Waiting up to 5m0s for pod "downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-6vnck" to be "success or failure"
Jul 16 13:28:28.277: INFO: Pod "downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.656831ms
Jul 16 13:28:30.280: INFO: Pod "downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008259185s
STEP: Saw pod success
Jul 16 13:28:30.281: INFO: Pod "downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:28:30.283: INFO: Trying to get logs from node i-taxl6ow1 pod downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:28:30.304: INFO: Waiting for pod downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:28:30.306: INFO: Pod downward-api-98a62657-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:28:30.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6vnck" for this suite.
Jul 16 13:28:36.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:37.748: INFO: namespace: e2e-tests-downward-api-6vnck, resource: bindings, ignored listing per whitelist
Jul 16 13:28:38.648: INFO: namespace e2e-tests-downward-api-6vnck deletion completed in 8.33689186s

• [SLOW TEST:10.467 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:28:38.649: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 16 13:28:38.732: INFO: Waiting up to 5m0s for pod "pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-xfpwx" to be "success or failure"
Jul 16 13:28:38.734: INFO: Pod "pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074051ms
Jul 16 13:28:40.739: INFO: Pod "pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00735966s
Jul 16 13:28:42.743: INFO: Pod "pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010902396s
STEP: Saw pod success
Jul 16 13:28:42.743: INFO: Pod "pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:28:42.745: INFO: Trying to get logs from node i-taxl6ow1 pod pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:28:42.765: INFO: Waiting for pod pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:28:42.767: INFO: Pod pod-9ee1cc16-a7cd-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:28:42.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xfpwx" for this suite.
Jul 16 13:28:48.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:48.892: INFO: namespace: e2e-tests-emptydir-xfpwx, resource: bindings, ignored listing per whitelist
Jul 16 13:28:51.098: INFO: namespace e2e-tests-emptydir-xfpwx deletion completed in 8.326873536s

• [SLOW TEST:12.450 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:28:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:29:51.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-swgvv" for this suite.
Jul 16 13:30:15.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:30:15.306: INFO: namespace: e2e-tests-container-probe-swgvv, resource: bindings, ignored listing per whitelist
Jul 16 13:30:17.560: INFO: namespace e2e-tests-container-probe-swgvv deletion completed in 26.332456272s

• [SLOW TEST:86.459 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:30:17.560: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 16 13:30:21.686: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-d9db3d3d-a7cd-11e9-9e69-7a63f556cc5d,GenerateName:,Namespace:e2e-tests-events-4lng2,SelfLink:/api/v1/namespaces/e2e-tests-events-4lng2/pods/send-events-d9db3d3d-a7cd-11e9-9e69-7a63f556cc5d,UID:d9dba513-a7cd-11e9-a7d7-5254228aab6e,ResourceVersion:16451,Generation:0,CreationTimestamp:2019-07-16 13:30:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 666905950,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xpnnc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xpnnc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-xpnnc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000841990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008419b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:30:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:30:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:30:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:30:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.120,StartTime:2019-07-16 13:30:17 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-16 13:30:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://40730353f63df3a3499559a55115ae4f4641aff6174fb400907cb99d28b0ce24}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 16 13:30:23.690: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 16 13:30:25.695: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:30:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-4lng2" for this suite.
Jul 16 13:31:05.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:05.847: INFO: namespace: e2e-tests-events-4lng2, resource: bindings, ignored listing per whitelist
Jul 16 13:31:08.053: INFO: namespace e2e-tests-events-4lng2 deletion completed in 42.342794513s

• [SLOW TEST:50.493 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:31:08.053: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-nsq7
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 13:31:08.150: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nsq7" in namespace "e2e-tests-subpath-s6ksj" to be "success or failure"
Jul 16 13:31:08.153: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076141ms
Jul 16 13:31:10.158: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007578429s
Jul 16 13:31:12.162: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 4.011966002s
Jul 16 13:31:14.166: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 6.015972044s
Jul 16 13:31:16.172: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 8.02205426s
Jul 16 13:31:18.177: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 10.026391205s
Jul 16 13:31:20.181: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 12.030902979s
Jul 16 13:31:22.185: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 14.034498622s
Jul 16 13:31:24.188: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 16.038050334s
Jul 16 13:31:26.192: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 18.041907921s
Jul 16 13:31:28.195: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 20.044981339s
Jul 16 13:31:30.199: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Running", Reason="", readiness=false. Elapsed: 22.04865181s
Jul 16 13:31:32.202: INFO: Pod "pod-subpath-test-configmap-nsq7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.051710422s
STEP: Saw pod success
Jul 16 13:31:32.202: INFO: Pod "pod-subpath-test-configmap-nsq7" satisfied condition "success or failure"
Jul 16 13:31:32.205: INFO: Trying to get logs from node i-taxl6ow1 pod pod-subpath-test-configmap-nsq7 container test-container-subpath-configmap-nsq7: <nil>
STEP: delete the pod
Jul 16 13:31:32.242: INFO: Waiting for pod pod-subpath-test-configmap-nsq7 to disappear
Jul 16 13:31:32.246: INFO: Pod pod-subpath-test-configmap-nsq7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nsq7
Jul 16 13:31:32.246: INFO: Deleting pod "pod-subpath-test-configmap-nsq7" in namespace "e2e-tests-subpath-s6ksj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:31:32.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-s6ksj" for this suite.
Jul 16 13:31:38.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:38.284: INFO: namespace: e2e-tests-subpath-s6ksj, resource: bindings, ignored listing per whitelist
Jul 16 13:31:40.591: INFO: namespace e2e-tests-subpath-s6ksj deletion completed in 8.334368632s

• [SLOW TEST:32.538 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:31:40.593: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 16 13:31:40.713: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 13:31:40.734: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 13:31:40.739: INFO: 
Logging pods the kubelet thinks is on node i-7znxt5so before test
Jul 16 13:31:40.792: INFO: openpitrix-task-db-ctrl-job-f5jjj from openpitrix-system started at 2019-07-16 12:40:25 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container openpitrix-task-db-ctrl ready: false, restart count 4
Jul 16 13:31:40.792: INFO: notification-deployment-79974b56c9-ppqc2 from kubesphere-alerting-system started at 2019-07-16 12:40:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container notification ready: true, restart count 0
Jul 16 13:31:40.792: INFO: istio-telemetry-758d9c786f-xtfm2 from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container mixer ready: true, restart count 3
Jul 16 13:31:40.792: INFO: kube-state-metrics-7f9c44c88-ljpct from kubesphere-monitoring-system started at 2019-07-16 12:40:14 +0000 UTC (4 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 16 13:31:40.792: INFO: csi-qingcloud-controller-0 from kube-system started at 2019-07-16 12:37:54 +0000 UTC (3 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 13:31:40.792: INFO: openpitrix-repo-manager-deployment-84fd5b5fdf-fbr58 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container openpitrix-repo-manager ready: true, restart count 0
Jul 16 13:31:40.792: INFO: openpitrix-runtime-manager-deployment-5fcbb6f447-vjw7t from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container openpitrix-runtime-manager ready: true, restart count 0
Jul 16 13:31:40.792: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:31 +0000 UTC (3 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 13:31:40.792: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 13:31:40.792: INFO: notification-db-init-job-5zrd7 from kubesphere-alerting-system started at 2019-07-16 12:40:36 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container notification-db-init ready: false, restart count 0
Jul 16 13:31:40.792: INFO: alerting-client-64b7bb9d99-mhx29 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container alerting-client ready: true, restart count 0
Jul 16 13:31:40.792: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-6968x from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 13:31:40.792: INFO: calico-node-r5c8r from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 13:31:40.792: INFO: openpitrix-app-db-ctrl-job-4gk2k from openpitrix-system started at 2019-07-16 12:40:21 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container openpitrix-app-db-ctrl ready: false, restart count 4
Jul 16 13:31:40.792: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 13:31:40.792: INFO: istio-policy-b87497cf4-n4kkt from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.792: INFO: 	Container mixer ready: true, restart count 2
Jul 16 13:31:40.792: INFO: ks-jenkins-7f67d57746-k87dc from kubesphere-devops-system started at 2019-07-16 12:43:04 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.792: INFO: 	Container ks-jenkins ready: true, restart count 0
Jul 16 13:31:40.793: INFO: metrics-server-85d786dd4d-448wj from kube-system started at 2019-07-16 12:38:51 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container metrics-server ready: true, restart count 0
Jul 16 13:31:40.793: INFO: istio-init-crd-10-c6klc from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container istio-init-crd-10 ready: false, restart count 0
Jul 16 13:31:40.793: INFO: ks-docs-77c4796dc9-q4bdj from kubesphere-system started at 2019-07-16 12:39:46 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container ks-docs ready: true, restart count 0
Jul 16 13:31:40.793: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 13:31:40.793: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 13:31:40.793: INFO: alerting-manager-5cf779dc56-47pg6 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container alerting-manager ready: true, restart count 0
Jul 16 13:31:40.793: INFO: openpitrix-iam-service-deployment-864df9fb6f-kbvs8 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container openpitrix-iam-service ready: true, restart count 0
Jul 16 13:31:40.793: INFO: prometheus-operator-6f5d67dcd4-sc6ft from kubesphere-monitoring-system started at 2019-07-16 12:39:26 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 16 13:31:40.793: INFO: node-exporter-92tpb from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 13:31:40.793: INFO: istio-galley-689b548d98-jm8l2 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container galley ready: true, restart count 0
Jul 16 13:31:40.793: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2019-07-16 12:49:11 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 13:31:40.793: INFO: coredns-bbbb94784-9rtpg from kube-system started at 2019-07-16 12:18:09 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container coredns ready: true, restart count 0
Jul 16 13:31:40.793: INFO: notification-db-ctrl-job-c82wv from kubesphere-alerting-system started at 2019-07-16 12:40:37 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Jul 16 13:31:40.793: INFO: alerting-watcher-5867dc5ffc-6c22k from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container alerting-watcher ready: true, restart count 0
Jul 16 13:31:40.793: INFO: istio-ingressgateway-5477b7ffd9-6zjgw from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.793: INFO: alerting-db-init-job-zgqlj from kubesphere-alerting-system started at 2019-07-16 12:38:35 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container alerting-db-init ready: false, restart count 0
Jul 16 13:31:40.793: INFO: csi-qingcloud-node-mfxls from kube-system started at 2019-07-16 12:37:54 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 13:31:40.793: INFO: istio-init-crd-11-7plk7 from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container istio-init-crd-11 ready: false, restart count 0
Jul 16 13:31:40.793: INFO: logging-fluentbit-operator-555dd47d6d-8pnkc from kubesphere-logging-system started at 2019-07-16 12:39:35 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container fluentbit-operator ready: true, restart count 0
Jul 16 13:31:40.793: INFO: openpitrix-iam-db-ctrl-job-6zwht from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container openpitrix-iam-db-ctrl ready: false, restart count 3
Jul 16 13:31:40.793: INFO: openpitrix-job-db-ctrl-job-rhhwx from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container openpitrix-job-db-ctrl ready: false, restart count 5
Jul 16 13:31:40.793: INFO: alerting-executor-78764b5795-fkxjn from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 13:31:40.793: INFO: kubectl-admin-d784b7777-zl544 from kubesphere-controls-system started at 2019-07-16 12:41:39 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container kubectl ready: true, restart count 0
Jul 16 13:31:40.793: INFO: tiller-deploy-6f9697dfd9-j9hch from kube-system started at 2019-07-16 12:18:13 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container tiller ready: true, restart count 0
Jul 16 13:31:40.793: INFO: fluent-bit-4bldx from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 13:31:40.793: INFO: alerting-db-ctrl-job-pxdbk from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container alerting-db-ctrl ready: false, restart count 0
Jul 16 13:31:40.793: INFO: uc-jenkins-update-center-598f8d6549-4nmnj from kubesphere-devops-system started at 2019-07-16 12:42:29 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container jenkins-update-center ready: true, restart count 0
Jul 16 13:31:40.793: INFO: istio-pilot-9685dfc4b-xxlch from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container discovery ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.793: INFO: kube-proxy-q6k4p from kube-system started at 2019-07-16 12:19:15 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.793: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 13:31:40.793: INFO: 
Logging pods the kubelet thinks is on node i-taxl6ow1 before test
Jul 16 13:31:40.826: INFO: openpitrix-etcd-deployment-54bc9bb948-s6wmk from openpitrix-system started at 2019-07-16 12:39:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-etcd ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-cluster-db-ctrl-job-mqp4p from openpitrix-system started at 2019-07-16 12:40:22 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-cluster-db-ctrl ready: false, restart count 2
Jul 16 13:31:40.826: INFO: openpitrix-db-init-job-4g8dq from openpitrix-system started at 2019-07-16 12:40:26 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-db-init ready: false, restart count 0
Jul 16 13:31:40.826: INFO: jaeger-collector-f579b55fb-ljwcp from istio-system started at 2019-07-16 12:47:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container jaeger-collector ready: true, restart count 3
Jul 16 13:31:40.826: INFO: calico-kube-controllers-767548c9d9-mqjxp from kube-system started at 2019-07-16 12:17:28 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 16 13:31:40.826: INFO: default-http-backend-96d94689d-sl8zp from kubesphere-controls-system started at 2019-07-16 12:38:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 16 13:31:40.826: INFO: alerting-executor-78764b5795-v7tvg from kubesphere-alerting-system started at 2019-07-16 12:40:57 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 13:31:40.826: INFO: istio-citadel-5f886dc9b4-hxx8p from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container citadel ready: true, restart count 0
Jul 16 13:31:40.826: INFO: istio-sidecar-injector-74666b458c-v8r9r from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
Jul 16 13:31:40.826: INFO: istio-pilot-9685dfc4b-f8qhv from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container discovery ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: node-exporter-hkg6g from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 13:31:40.826: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:36 +0000 UTC (3 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 13:31:40.826: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-task-manager-deployment-59578dc9d6-rghp8 from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-task-manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-app-manager-deployment-595dcd76f-58qvb from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-app-manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-category-manager-deployment-7968d789d6-lljbl from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-category-manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: ks-devops-db-init-job-h9xvn from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container devops-db-init ready: false, restart count 0
Jul 16 13:31:40.826: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 13:31:40.826: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 13:31:40.826: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 13:31:40.826: INFO: istio-telemetry-758d9c786f-zkkh6 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container mixer ready: true, restart count 4
Jul 16 13:31:40.826: INFO: istio-ingressgateway-5477b7ffd9-mqd6p from istio-system started at 2019-07-16 12:42:54 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: jaeger-query-6b5d8f7bcd-pxqtq from istio-system started at 2019-07-16 12:47:02 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container jaeger-agent ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container jaeger-query ready: true, restart count 1
Jul 16 13:31:40.826: INFO: openpitrix-minio-deployment-84d5f9c94b-v4gkn from openpitrix-system started at 2019-07-16 12:39:12 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-minio ready: true, restart count 0
Jul 16 13:31:40.826: INFO: fluent-bit-r8xb4 from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-api-gateway-deployment-587cc46874-gwm9j from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-api-gateway ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-cluster-manager-deployment-6dcd96b68b-ptl6w from openpitrix-system started at 2019-07-16 12:38:20 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-cluster-manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-repo-indexer-deployment-5f4c895b54-vqj26 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-repo-indexer ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-runtime-db-ctrl-job-gcr9h from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-runtime-db-ctrl ready: false, restart count 5
Jul 16 13:31:40.826: INFO: istio-policy-b87497cf4-nstpl from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container mixer ready: true, restart count 2
Jul 16 13:31:40.826: INFO: dns-autoscaler-89c7bbd57-7x9kr from kube-system started at 2019-07-16 12:18:07 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container autoscaler ready: true, restart count 0
Jul 16 13:31:40.826: INFO: calico-node-js6px from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-job-manager-deployment-588858bcb9-vrlv2 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-job-manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: openpitrix-repo-db-ctrl-job-69hdm from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container openpitrix-repo-db-ctrl ready: false, restart count 5
Jul 16 13:31:40.826: INFO: jaeger-operator-8cb967c86-pcznr from istio-system started at 2019-07-16 12:42:40 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container jaeger-operator ready: true, restart count 0
Jul 16 13:31:40.826: INFO: kube-proxy-xn8lr from kube-system started at 2019-07-16 12:18:35 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: ks-sonarqube-sonarqube-9c5876cd8-jvl46 from kubesphere-devops-system started at 2019-07-16 12:38:03 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container sonarqube ready: true, restart count 4
Jul 16 13:31:40.826: INFO: istio-galley-689b548d98-26dvv from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container galley ready: true, restart count 0
Jul 16 13:31:40.826: INFO: csi-qingcloud-node-vzj9n from kube-system started at 2019-07-16 12:37:58 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 13:31:40.826: INFO: s2ioperator-0 from kubesphere-devops-system started at 2019-07-16 12:39:09 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container manager ready: true, restart count 0
Jul 16 13:31:40.826: INFO: ks-devops-db-ctrl-job-4dwlz from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container devops-db-ctrl ready: false, restart count 0
Jul 16 13:31:40.826: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-dpktl from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:31:40.826: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 13:31:40.826: INFO: ks-sonarqube-postgresql-5d58f5574b-5r955 from kubesphere-devops-system started at 2019-07-16 12:38:58 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container ks-sonarqube-postgresql ready: true, restart count 0
Jul 16 13:31:40.826: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 12:54:10 +0000 UTC (1 container statuses recorded)
Jul 16 13:31:40.826: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b1e66cdaca3f26], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:31:41.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-b4xkk" for this suite.
Jul 16 13:31:47.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:48.063: INFO: namespace: e2e-tests-sched-pred-b4xkk, resource: bindings, ignored listing per whitelist
Jul 16 13:31:50.221: INFO: namespace e2e-tests-sched-pred-b4xkk deletion completed in 8.337868452s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.629 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:31:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:31:50.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-sb62x" to be "success or failure"
Jul 16 13:31:50.342: INFO: Pod "downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599237ms
Jul 16 13:31:52.346: INFO: Pod "downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010211471s
STEP: Saw pod success
Jul 16 13:31:52.346: INFO: Pod "downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:31:52.348: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:31:52.370: INFO: Waiting for pod downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:31:52.374: INFO: Pod downwardapi-volume-1115f760-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:31:52.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sb62x" for this suite.
Jul 16 13:31:58.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:58.412: INFO: namespace: e2e-tests-projected-sb62x, resource: bindings, ignored listing per whitelist
Jul 16 13:32:00.716: INFO: namespace e2e-tests-projected-sb62x deletion completed in 8.337313056s

• [SLOW TEST:10.494 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:32:00.718: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 16 13:32:05.354: INFO: Successfully updated pod "pod-update-activedeadlineseconds-17576372-a7ce-11e9-9e69-7a63f556cc5d"
Jul 16 13:32:05.354: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-17576372-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-pods-cq622" to be "terminated due to deadline exceeded"
Jul 16 13:32:05.356: INFO: Pod "pod-update-activedeadlineseconds-17576372-a7ce-11e9-9e69-7a63f556cc5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.191709ms
Jul 16 13:32:07.360: INFO: Pod "pod-update-activedeadlineseconds-17576372-a7ce-11e9-9e69-7a63f556cc5d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005931209s
Jul 16 13:32:07.360: INFO: Pod "pod-update-activedeadlineseconds-17576372-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:32:07.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-cq622" for this suite.
Jul 16 13:32:13.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:32:13.703: INFO: namespace: e2e-tests-pods-cq622, resource: bindings, ignored listing per whitelist
Jul 16 13:32:15.704: INFO: namespace e2e-tests-pods-cq622 deletion completed in 8.333682368s

• [SLOW TEST:14.986 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:32:15.704: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Jul 16 13:32:15.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 api-versions'
Jul 16 13:32:15.900: INFO: stderr: ""
Jul 16 13:32:15.900: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.istio.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\nconfig.istio.io/v1alpha2\ncoordination.k8s.io/v1beta1\ndevops.kubesphere.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\njaegertracing.io/v1\nlogging.kubesphere.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.istio.io/v1alpha3\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbac.istio.io/v1alpha1\nscheduling.k8s.io/v1beta1\nservicemesh.kubesphere.io/v1alpha2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntenant.kubesphere.io/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:32:15.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-569pk" for this suite.
Jul 16 13:32:21.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:32:23.848: INFO: namespace: e2e-tests-kubectl-569pk, resource: bindings, ignored listing per whitelist
Jul 16 13:32:24.248: INFO: namespace e2e-tests-kubectl-569pk deletion completed in 8.343299917s

• [SLOW TEST:8.544 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:32:24.249: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jul 16 13:32:24.870: INFO: Waiting up to 5m0s for pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw" in namespace "e2e-tests-svcaccounts-gbhng" to be "success or failure"
Jul 16 13:32:24.873: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015126ms
Jul 16 13:32:26.876: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006112141s
Jul 16 13:32:28.879: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009559398s
STEP: Saw pod success
Jul 16 13:32:28.879: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw" satisfied condition "success or failure"
Jul 16 13:32:28.881: INFO: Trying to get logs from node i-taxl6ow1 pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw container token-test: <nil>
STEP: delete the pod
Jul 16 13:32:28.899: INFO: Waiting for pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw to disappear
Jul 16 13:32:28.901: INFO: Pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-wcgzw no longer exists
STEP: Creating a pod to test consume service account root CA
Jul 16 13:32:28.904: INFO: Waiting up to 5m0s for pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq" in namespace "e2e-tests-svcaccounts-gbhng" to be "success or failure"
Jul 16 13:32:28.907: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.028169ms
Jul 16 13:32:30.911: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006581891s
Jul 16 13:32:32.915: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011070444s
STEP: Saw pod success
Jul 16 13:32:32.915: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq" satisfied condition "success or failure"
Jul 16 13:32:32.925: INFO: Trying to get logs from node i-taxl6ow1 pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq container root-ca-test: <nil>
STEP: delete the pod
Jul 16 13:32:32.946: INFO: Waiting for pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq to disappear
Jul 16 13:32:32.949: INFO: Pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-jvjgq no longer exists
STEP: Creating a pod to test consume service account namespace
Jul 16 13:32:32.955: INFO: Waiting up to 5m0s for pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd" in namespace "e2e-tests-svcaccounts-gbhng" to be "success or failure"
Jul 16 13:32:32.961: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.9999ms
Jul 16 13:32:34.968: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012428253s
Jul 16 13:32:36.973: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017489804s
STEP: Saw pod success
Jul 16 13:32:36.973: INFO: Pod "pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd" satisfied condition "success or failure"
Jul 16 13:32:36.976: INFO: Trying to get logs from node i-taxl6ow1 pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd container namespace-test: <nil>
STEP: delete the pod
Jul 16 13:32:37.008: INFO: Waiting for pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd to disappear
Jul 16 13:32:37.021: INFO: Pod pod-service-account-25ab2ff1-a7ce-11e9-9e69-7a63f556cc5d-9f5zd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:32:37.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-gbhng" for this suite.
Jul 16 13:32:43.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:32:43.083: INFO: namespace: e2e-tests-svcaccounts-gbhng, resource: bindings, ignored listing per whitelist
Jul 16 13:32:45.363: INFO: namespace e2e-tests-svcaccounts-gbhng deletion completed in 8.328172063s

• [SLOW TEST:21.115 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:32:45.364: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 16 13:32:49.519: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:49.522: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:32:51.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:51.527: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:32:53.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:53.526: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:32:55.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:55.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:32:57.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:57.527: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:32:59.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:32:59.526: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:33:01.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:33:01.538: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:33:03.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:33:03.526: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:33:05.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:33:05.526: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:33:07.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:33:07.526: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 13:33:09.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 13:33:09.527: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:33:09.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-gfrbd" for this suite.
Jul 16 13:33:33.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:33:33.684: INFO: namespace: e2e-tests-container-lifecycle-hook-gfrbd, resource: bindings, ignored listing per whitelist
Jul 16 13:33:35.869: INFO: namespace e2e-tests-container-lifecycle-hook-gfrbd deletion completed in 26.336126037s

• [SLOW TEST:50.506 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:33:35.869: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jul 16 13:33:35.947: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:33:41.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-shbv5" for this suite.
Jul 16 13:34:05.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:06.279: INFO: namespace: e2e-tests-init-container-shbv5, resource: bindings, ignored listing per whitelist
Jul 16 13:34:07.539: INFO: namespace e2e-tests-init-container-shbv5 deletion completed in 26.348660802s

• [SLOW TEST:31.669 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:34:07.539: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-62f61be0-a7ce-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:34:07.706: INFO: Waiting up to 5m0s for pod "pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-l5649" to be "success or failure"
Jul 16 13:34:07.711: INFO: Pod "pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118629ms
Jul 16 13:34:09.715: INFO: Pod "pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009281879s
STEP: Saw pod success
Jul 16 13:34:09.715: INFO: Pod "pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:34:09.719: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:34:09.739: INFO: Waiting for pod pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:34:09.742: INFO: Pod pod-secrets-62f6ab33-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:34:09.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-l5649" for this suite.
Jul 16 13:34:15.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:17.884: INFO: namespace: e2e-tests-secrets-l5649, resource: bindings, ignored listing per whitelist
Jul 16 13:34:18.085: INFO: namespace e2e-tests-secrets-l5649 deletion completed in 8.338805844s

• [SLOW TEST:10.547 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:34:18.087: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:34:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-fwrnm" for this suite.
Jul 16 13:34:48.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:48.571: INFO: namespace: e2e-tests-container-runtime-fwrnm, resource: bindings, ignored listing per whitelist
Jul 16 13:34:50.745: INFO: namespace e2e-tests-container-runtime-fwrnm deletion completed in 8.353491525s

• [SLOW TEST:32.658 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:34:50.746: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0716 13:35:00.980741      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:35:00.980: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tvgrb" for this suite.
Jul 16 13:35:07.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:07.124: INFO: namespace: e2e-tests-gc-tvgrb, resource: bindings, ignored listing per whitelist
Jul 16 13:35:09.325: INFO: namespace e2e-tests-gc-tvgrb deletion completed in 8.340451883s

• [SLOW TEST:18.580 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:35:09.328: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Jul 16 13:35:09.408: INFO: Waiting up to 5m0s for pod "var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-var-expansion-qv69q" to be "success or failure"
Jul 16 13:35:09.411: INFO: Pod "var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863593ms
Jul 16 13:35:11.415: INFO: Pod "var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006627311s
STEP: Saw pod success
Jul 16 13:35:11.415: INFO: Pod "var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:35:11.419: INFO: Trying to get logs from node i-taxl6ow1 pod var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:35:11.442: INFO: Waiting for pod var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:35:11.445: INFO: Pod var-expansion-87be8737-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qv69q" for this suite.
Jul 16 13:35:17.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:19.611: INFO: namespace: e2e-tests-var-expansion-qv69q, resource: bindings, ignored listing per whitelist
Jul 16 13:35:19.817: INFO: namespace e2e-tests-var-expansion-qv69q deletion completed in 8.367491779s

• [SLOW TEST:10.489 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:35:19.818: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jul 16 13:35:19.918: INFO: Waiting up to 5m0s for pod "downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-jr76p" to be "success or failure"
Jul 16 13:35:19.922: INFO: Pod "downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.585548ms
Jul 16 13:35:21.931: INFO: Pod "downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012712175s
STEP: Saw pod success
Jul 16 13:35:21.931: INFO: Pod "downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:35:21.933: INFO: Trying to get logs from node i-taxl6ow1 pod downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:35:21.962: INFO: Waiting for pod downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:35:21.965: INFO: Pod downward-api-8e027054-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jr76p" for this suite.
Jul 16 13:35:27.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:28.081: INFO: namespace: e2e-tests-downward-api-jr76p, resource: bindings, ignored listing per whitelist
Jul 16 13:35:30.299: INFO: namespace e2e-tests-downward-api-jr76p deletion completed in 8.328582412s

• [SLOW TEST:10.480 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:35:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Jul 16 13:35:30.412: INFO: Waiting up to 5m0s for pod "client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-containers-stcns" to be "success or failure"
Jul 16 13:35:30.419: INFO: Pod "client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.586646ms
Jul 16 13:35:32.422: INFO: Pod "client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009856557s
Jul 16 13:35:34.426: INFO: Pod "client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013744862s
STEP: Saw pod success
Jul 16 13:35:34.426: INFO: Pod "client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:35:34.429: INFO: Trying to get logs from node i-taxl6ow1 pod client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:35:34.456: INFO: Waiting for pod client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:35:34.459: INFO: Pod client-containers-944244e9-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:34.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-stcns" for this suite.
Jul 16 13:35:40.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:41.397: INFO: namespace: e2e-tests-containers-stcns, resource: bindings, ignored listing per whitelist
Jul 16 13:35:42.795: INFO: namespace e2e-tests-containers-stcns deletion completed in 8.331326235s

• [SLOW TEST:12.496 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:35:42.795: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Jul 16 13:35:42.888: INFO: Waiting up to 5m0s for pod "client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-containers-rfmhk" to be "success or failure"
Jul 16 13:35:42.892: INFO: Pod "client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250905ms
Jul 16 13:35:44.897: INFO: Pod "client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008758357s
Jul 16 13:35:46.901: INFO: Pod "client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012656919s
STEP: Saw pod success
Jul 16 13:35:46.901: INFO: Pod "client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:35:46.905: INFO: Trying to get logs from node i-taxl6ow1 pod client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:35:46.921: INFO: Waiting for pod client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:35:46.930: INFO: Pod client-containers-9bb2d151-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:46.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-rfmhk" for this suite.
Jul 16 13:35:52.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:53.056: INFO: namespace: e2e-tests-containers-rfmhk, resource: bindings, ignored listing per whitelist
Jul 16 13:35:55.266: INFO: namespace e2e-tests-containers-rfmhk deletion completed in 8.331378217s

• [SLOW TEST:12.471 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:35:55.268: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-a3226ccd-a7ce-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:35:55.421: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-br899" to be "success or failure"
Jul 16 13:35:55.426: INFO: Pod "pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763956ms
Jul 16 13:35:57.429: INFO: Pod "pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007816534s
STEP: Saw pod success
Jul 16 13:35:57.429: INFO: Pod "pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:35:57.432: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:35:57.477: INFO: Waiting for pod pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:35:57.480: INFO: Pod pod-projected-secrets-a32af247-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:35:57.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-br899" for this suite.
Jul 16 13:36:03.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:03.580: INFO: namespace: e2e-tests-projected-br899, resource: bindings, ignored listing per whitelist
Jul 16 13:36:05.820: INFO: namespace e2e-tests-projected-br899 deletion completed in 8.334861676s

• [SLOW TEST:10.553 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:36:05.822: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jul 16 13:36:05.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:06.490: INFO: stderr: ""
Jul 16 13:36:06.490: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:36:06.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:06.621: INFO: stderr: ""
Jul 16 13:36:06.621: INFO: stdout: "update-demo-nautilus-nxhkt update-demo-nautilus-zbmg4 "
Jul 16 13:36:06.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-nxhkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:06.732: INFO: stderr: ""
Jul 16 13:36:06.732: INFO: stdout: ""
Jul 16 13:36:06.732: INFO: update-demo-nautilus-nxhkt is created but not running
Jul 16 13:36:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:11.863: INFO: stderr: ""
Jul 16 13:36:11.863: INFO: stdout: "update-demo-nautilus-nxhkt update-demo-nautilus-zbmg4 "
Jul 16 13:36:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-nxhkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:11.959: INFO: stderr: ""
Jul 16 13:36:11.959: INFO: stdout: "true"
Jul 16 13:36:11.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-nxhkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:12.066: INFO: stderr: ""
Jul 16 13:36:12.066: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:36:12.066: INFO: validating pod update-demo-nautilus-nxhkt
Jul 16 13:36:12.073: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:36:12.073: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:36:12.073: INFO: update-demo-nautilus-nxhkt is verified up and running
Jul 16 13:36:12.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-zbmg4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:12.206: INFO: stderr: ""
Jul 16 13:36:12.206: INFO: stdout: "true"
Jul 16 13:36:12.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-zbmg4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:12.395: INFO: stderr: ""
Jul 16 13:36:12.395: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:36:12.395: INFO: validating pod update-demo-nautilus-zbmg4
Jul 16 13:36:12.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:36:12.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:36:12.402: INFO: update-demo-nautilus-zbmg4 is verified up and running
STEP: using delete to clean up resources
Jul 16 13:36:12.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:12.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:36:12.524: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 16 13:36:12.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:12.695: INFO: stderr: "No resources found.\n"
Jul 16 13:36:12.695: INFO: stdout: ""
Jul 16 13:36:12.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -l name=update-demo --namespace=e2e-tests-kubectl-pjm7x -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:36:12.798: INFO: stderr: ""
Jul 16 13:36:12.798: INFO: stdout: "update-demo-nautilus-nxhkt\nupdate-demo-nautilus-zbmg4\n"
Jul 16 13:36:13.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-pjm7x'
Jul 16 13:36:13.451: INFO: stderr: "No resources found.\n"
Jul 16 13:36:13.451: INFO: stdout: ""
Jul 16 13:36:13.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -l name=update-demo --namespace=e2e-tests-kubectl-pjm7x -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:36:13.559: INFO: stderr: ""
Jul 16 13:36:13.559: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:36:13.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pjm7x" for this suite.
Jul 16 13:36:35.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:37.488: INFO: namespace: e2e-tests-kubectl-pjm7x, resource: bindings, ignored listing per whitelist
Jul 16 13:36:37.889: INFO: namespace e2e-tests-kubectl-pjm7x deletion completed in 24.324341274s

• [SLOW TEST:32.067 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:36:37.889: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-bc8df159-a7ce-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:36:38.014: INFO: Waiting up to 5m0s for pod "pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-qtm2q" to be "success or failure"
Jul 16 13:36:38.019: INFO: Pod "pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.964743ms
Jul 16 13:36:40.025: INFO: Pod "pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010611251s
STEP: Saw pod success
Jul 16 13:36:40.025: INFO: Pod "pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:36:40.030: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:36:40.093: INFO: Waiting for pod pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:36:40.100: INFO: Pod pod-secrets-bc8ea2b9-a7ce-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:36:40.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qtm2q" for this suite.
Jul 16 13:36:46.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:47.891: INFO: namespace: e2e-tests-secrets-qtm2q, resource: bindings, ignored listing per whitelist
Jul 16 13:36:48.440: INFO: namespace e2e-tests-secrets-qtm2q deletion completed in 8.331833165s

• [SLOW TEST:10.551 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:36:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 16 13:36:48.576: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vqhsr,SelfLink:/api/v1/namespaces/e2e-tests-watch-vqhsr/configmaps/e2e-watch-test-watch-closed,UID:c2d9d71f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18138,Generation:0,CreationTimestamp:2019-07-16 13:36:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:36:48.576: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vqhsr,SelfLink:/api/v1/namespaces/e2e-tests-watch-vqhsr/configmaps/e2e-watch-test-watch-closed,UID:c2d9d71f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18139,Generation:0,CreationTimestamp:2019-07-16 13:36:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 16 13:36:48.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vqhsr,SelfLink:/api/v1/namespaces/e2e-tests-watch-vqhsr/configmaps/e2e-watch-test-watch-closed,UID:c2d9d71f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18140,Generation:0,CreationTimestamp:2019-07-16 13:36:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 13:36:48.589: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vqhsr,SelfLink:/api/v1/namespaces/e2e-tests-watch-vqhsr/configmaps/e2e-watch-test-watch-closed,UID:c2d9d71f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18141,Generation:0,CreationTimestamp:2019-07-16 13:36:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:36:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-vqhsr" for this suite.
Jul 16 13:36:54.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:56.686: INFO: namespace: e2e-tests-watch-vqhsr, resource: bindings, ignored listing per whitelist
Jul 16 13:36:56.936: INFO: namespace e2e-tests-watch-vqhsr deletion completed in 8.336386005s

• [SLOW TEST:8.495 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:36:56.936: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:36:57.062: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 16 13:37:02.066: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 13:37:02.066: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 16 13:37:02.177: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-vrm6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vrm6p/deployments/test-cleanup-deployment,UID:cae82529-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18191,Generation:1,CreationTimestamp:2019-07-16 13:37:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul 16 13:37:02.203: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-vrm6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vrm6p/replicasets/test-cleanup-deployment-7dbbfcf846,UID:caf4ba1b-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18194,Generation:1,CreationTimestamp:2019-07-16 13:37:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment cae82529-a7ce-11e9-a7d7-5254228aab6e 0xc001f269b7 0xc001f269b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:37:02.203: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 16 13:37:02.203: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-vrm6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vrm6p/replicasets/test-cleanup-controller,UID:c7e90268-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18193,Generation:1,CreationTimestamp:2019-07-16 13:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment cae82529-a7ce-11e9-a7d7-5254228aab6e 0xc001f268f7 0xc001f268f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 13:37:02.220: INFO: Pod "test-cleanup-controller-f8mpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-f8mpj,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-vrm6p,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vrm6p/pods/test-cleanup-controller-f8mpj,UID:c7eadd9a-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18185,Generation:0,CreationTimestamp:2019-07-16 13:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller c7e90268-a7ce-11e9-a7d7-5254228aab6e 0xc002156837 0xc002156838}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dkpzs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dkpzs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dkpzs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021568b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021568d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:36:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:36:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:36:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:36:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.115,StartTime:2019-07-16 13:36:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:36:58 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://92abdac162e8db3154f39876fd9f40a3a856c35e8269a499a84e20a1c389cc50}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:02.221: INFO: Pod "test-cleanup-deployment-7dbbfcf846-9fnz7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-9fnz7,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-vrm6p,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vrm6p/pods/test-cleanup-deployment-7dbbfcf846-9fnz7,UID:caf6e706-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18197,Generation:0,CreationTimestamp:2019-07-16 13:37:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 caf4ba1b-a7ce-11e9-a7d7-5254228aab6e 0xc002156997 0xc002156998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dkpzs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dkpzs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dkpzs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002156a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002156a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:37:02.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vrm6p" for this suite.
Jul 16 13:37:08.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:09.202: INFO: namespace: e2e-tests-deployment-vrm6p, resource: bindings, ignored listing per whitelist
Jul 16 13:37:10.603: INFO: namespace e2e-tests-deployment-vrm6p deletion completed in 8.373139098s

• [SLOW TEST:13.667 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:37:10.604: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:37:10.693: INFO: Creating deployment "nginx-deployment"
Jul 16 13:37:10.696: INFO: Waiting for observed generation 1
Jul 16 13:37:12.703: INFO: Waiting for all required pods to come up
Jul 16 13:37:12.707: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 16 13:37:22.722: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 16 13:37:22.727: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 16 13:37:22.735: INFO: Updating deployment nginx-deployment
Jul 16 13:37:22.735: INFO: Waiting for observed generation 2
Jul 16 13:37:24.740: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 16 13:37:24.743: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 16 13:37:24.746: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 16 13:37:24.754: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 16 13:37:24.754: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 16 13:37:24.756: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 16 13:37:24.760: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 16 13:37:24.760: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 16 13:37:24.765: INFO: Updating deployment nginx-deployment
Jul 16 13:37:24.765: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 16 13:37:24.774: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 16 13:37:26.783: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 16 13:37:26.788: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d8fkw/deployments/nginx-deployment,UID:d00a313c-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18566,Generation:3,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-16 13:37:24 +0000 UTC 2019-07-16 13:37:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-16 13:37:24 +0000 UTC 2019-07-16 13:37:10 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 16 13:37:26.792: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d8fkw/replicasets/nginx-deployment-65bbdb5f8,UID:d737a8d0-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18554,Generation:3,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d00a313c-a7ce-11e9-a7d7-5254228aab6e 0xc000b74627 0xc000b74628}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:37:26.792: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 16 13:37:26.792: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d8fkw/replicasets/nginx-deployment-555b55d965,UID:d00ac23d-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18559,Generation:3,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d00a313c-a7ce-11e9-a7d7-5254228aab6e 0xc000b74567 0xc000b74568}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 16 13:37:26.799: INFO: Pod "nginx-deployment-555b55d965-4jf67" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4jf67,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-4jf67,UID:d00f5225-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18372,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b74f47 0xc000b74f48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b74fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b74fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:10.233.83.248,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://da3209d3ffa42d3ec0c5b138b1373117908537173e295dc0cbfe5d81adae2bb5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.799: INFO: Pod "nginx-deployment-555b55d965-6p4ft" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6p4ft,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-6p4ft,UID:d00cea61-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18415,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b750a0 0xc000b750a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.119,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://11caa6121b7f62134f1bbffd57ec38e9223971dead715ca451ec1817b86409d3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.800: INFO: Pod "nginx-deployment-555b55d965-7jqxh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7jqxh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-7jqxh,UID:d8711329-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18529,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b751f0 0xc000b751f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.800: INFO: Pod "nginx-deployment-555b55d965-9nhvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9nhvj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-9nhvj,UID:d86fe59b-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18528,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b752f0 0xc000b752f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.800: INFO: Pod "nginx-deployment-555b55d965-btvjb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-btvjb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-btvjb,UID:d00d8bf8-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18384,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b753f0 0xc000b753f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.117,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a8dc5a66b27443701f616497105f3f9b0ddb1ba4ce28968f55656819cfac8ba4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.801: INFO: Pod "nginx-deployment-555b55d965-cfszd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cfszd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-cfszd,UID:d8753bbe-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18572,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75540 0xc000b75541}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b755b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b755d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.801: INFO: Pod "nginx-deployment-555b55d965-cndcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cndcq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-cndcq,UID:d86e787d-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18587,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75680 0xc000b75681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b756f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.801: INFO: Pod "nginx-deployment-555b55d965-cxgxz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cxgxz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-cxgxz,UID:d00f6021-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18394,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b757c0 0xc000b757c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.118,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f3f9792dcfc059daad14c1c73aa9365185790392fa403897688938d14b74ffe2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.801: INFO: Pod "nginx-deployment-555b55d965-hvtp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hvtp5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-hvtp5,UID:d875a800-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18556,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75910 0xc000b75911}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b759a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.801: INFO: Pod "nginx-deployment-555b55d965-jnc5w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jnc5w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-jnc5w,UID:d8758813-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18576,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75a10 0xc000b75a11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.802: INFO: Pod "nginx-deployment-555b55d965-ljh5f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ljh5f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-ljh5f,UID:d871216d-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18534,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75b50 0xc000b75b51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.802: INFO: Pod "nginx-deployment-555b55d965-lmqnh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lmqnh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-lmqnh,UID:d0114705-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18404,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75c50 0xc000b75c51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.121,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0059ec970262b72e3e57fde664baef535431e37ca910ddb27259b55e40e695ef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.802: INFO: Pod "nginx-deployment-555b55d965-pf5zf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-pf5zf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-pf5zf,UID:d00d8842-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18351,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75da0 0xc000b75da1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:10.233.83.247,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8785a7bdc6b2e52facbf2a3baa645754ce8d962a0af989b4f1d94c0109bec042}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.802: INFO: Pod "nginx-deployment-555b55d965-px7mm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-px7mm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-px7mm,UID:d0114dcb-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18409,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc000b75ef0 0xc000b75ef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b75f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b75f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.120,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1a40d7c19582f6863da906d3902b29f72d315d39ffa028af0772829356f311ec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.802: INFO: Pod "nginx-deployment-555b55d965-qn5gd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qn5gd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-qn5gd,UID:d87598ae-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18612,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e040 0xc00259e041}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.803: INFO: Pod "nginx-deployment-555b55d965-r5fmm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-r5fmm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-r5fmm,UID:d875c0ce-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18594,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e180 0xc00259e181}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.803: INFO: Pod "nginx-deployment-555b55d965-tskn8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tskn8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-tskn8,UID:d8715bab-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18535,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e2c0 0xc00259e2c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.803: INFO: Pod "nginx-deployment-555b55d965-vdksf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vdksf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-vdksf,UID:d86f5737-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18514,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e3c0 0xc00259e3c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.803: INFO: Pod "nginx-deployment-555b55d965-vvb6h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vvb6h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-vvb6h,UID:d00f0d25-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18349,Generation:0,CreationTimestamp:2019-07-16 13:37:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e4c0 0xc00259e4c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:10.233.83.246,StartTime:2019-07-16 13:37:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 13:37:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://246c59551c7c72d4a83215f365c1602fa77b033cc13e1d9fb8468e2754be42f8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.803: INFO: Pod "nginx-deployment-555b55d965-w2v2s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w2v2s,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-555b55d965-w2v2s,UID:d8714f24-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18538,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 d00ac23d-a7ce-11e9-a7d7-5254228aab6e 0xc00259e610 0xc00259e611}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e6a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.804: INFO: Pod "nginx-deployment-65bbdb5f8-64bwt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-64bwt,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-64bwt,UID:d7412d09-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18499,Generation:0,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259e710 0xc00259e711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.804: INFO: Pod "nginx-deployment-65bbdb5f8-6bx2l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6bx2l,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-6bx2l,UID:d73844cf-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18456,Generation:0,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259e870 0xc00259e871}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259e8f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259e910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.804: INFO: Pod "nginx-deployment-65bbdb5f8-bzng4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bzng4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-bzng4,UID:d87457c9-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18546,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259e9d0 0xc00259e9d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259ea50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259ea70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.804: INFO: Pod "nginx-deployment-65bbdb5f8-ct2w2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ct2w2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-ct2w2,UID:d871401f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18533,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259eae0 0xc00259eae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259eb60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259eb80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.804: INFO: Pod "nginx-deployment-65bbdb5f8-fg256" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-fg256,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-fg256,UID:d739270f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18467,Generation:0,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259ebf0 0xc00259ebf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259ec70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259ec90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.805: INFO: Pod "nginx-deployment-65bbdb5f8-gjtsf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gjtsf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-gjtsf,UID:d86fa520-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18527,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259ed50 0xc00259ed51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259edd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259edf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.805: INFO: Pod "nginx-deployment-65bbdb5f8-hgdkd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hgdkd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-hgdkd,UID:d7390823-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18455,Generation:0,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259ee60 0xc00259ee61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259eee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259ef00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.805: INFO: Pod "nginx-deployment-65bbdb5f8-ldwfr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ldwfr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-ldwfr,UID:d86f852f-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18552,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259efc0 0xc00259efc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.805: INFO: Pod "nginx-deployment-65bbdb5f8-n2nz2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-n2nz2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-n2nz2,UID:d86ed62b-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18613,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259f120 0xc00259f121}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.806: INFO: Pod "nginx-deployment-65bbdb5f8-pklt6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pklt6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-pklt6,UID:d8711e58-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18562,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259f280 0xc00259f281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-7znxt5so,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.6,PodIP:,StartTime:2019-07-16 13:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.806: INFO: Pod "nginx-deployment-65bbdb5f8-rkg7c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rkg7c,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-rkg7c,UID:d87106bd-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18531,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259f3e0 0xc00259f3e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.806: INFO: Pod "nginx-deployment-65bbdb5f8-sjlcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sjlcq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-sjlcq,UID:d8714afe-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18532,Generation:0,CreationTimestamp:2019-07-16 13:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259f4f0 0xc00259f4f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 13:37:26.806: INFO: Pod "nginx-deployment-65bbdb5f8-th95w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-th95w,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-d8fkw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d8fkw/pods/nginx-deployment-65bbdb5f8-th95w,UID:d7408039-a7ce-11e9-a7d7-5254228aab6e,ResourceVersion:18488,Generation:0,CreationTimestamp:2019-07-16 13:37:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d737a8d0-a7ce-11e9-a7d7-5254228aab6e 0xc00259f600 0xc00259f601}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-skvxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-skvxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-skvxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00259f680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00259f6a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-07-16 13:37:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:37:26.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-d8fkw" for this suite.
Jul 16 13:37:34.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:36.102: INFO: namespace: e2e-tests-deployment-d8fkw, resource: bindings, ignored listing per whitelist
Jul 16 13:37:37.151: INFO: namespace e2e-tests-deployment-d8fkw deletion completed in 10.331348626s

• [SLOW TEST:26.547 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:37:37.151: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jul 16 13:37:43.278: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-dfddc619-a7ce-11e9-9e69-7a63f556cc5d", GenerateName:"", Namespace:"e2e-tests-pods-fz2ss", SelfLink:"/api/v1/namespaces/e2e-tests-pods-fz2ss/pods/pod-submit-remove-dfddc619-a7ce-11e9-9e69-7a63f556cc5d", UID:"dfdf0780-a7ce-11e9-a7d7-5254228aab6e", ResourceVersion:"19041", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63698881057, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"246575070"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-glvwd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001ea4540), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-glvwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001e1aed8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"i-taxl6ow1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0017b73e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e1af20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e1af40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001e1af48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001e1af4c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881057, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881060, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881060, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881057, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.7", PodIP:"10.233.100.107", StartTime:(*v1.Time)(0xc001575aa0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001575b40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7", ContainerID:"docker://6f2e46829308de9270e8b422d995281a6d996199744ebd25e27db7716a9b25f2"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 16 13:37:48.309: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:37:48.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fz2ss" for this suite.
Jul 16 13:37:54.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:54.345: INFO: namespace: e2e-tests-pods-fz2ss, resource: bindings, ignored listing per whitelist
Jul 16 13:37:56.648: INFO: namespace e2e-tests-pods-fz2ss deletion completed in 8.330923654s

• [SLOW TEST:19.497 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:37:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Jul 16 13:37:58.761: INFO: Pod pod-hostip-eb7b235c-a7ce-11e9-9e69-7a63f556cc5d has hostIP: 192.168.0.7
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:37:58.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-sbzdm" for this suite.
Jul 16 13:38:22.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:25.045: INFO: namespace: e2e-tests-pods-sbzdm, resource: bindings, ignored listing per whitelist
Jul 16 13:38:25.097: INFO: namespace e2e-tests-pods-sbzdm deletion completed in 26.332618801s

• [SLOW TEST:28.448 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:38:25.098: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-9mfsx
Jul 16 13:40:25.198: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-9mfsx
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 13:40:25.200: INFO: Initial restart count of pod liveness-http is 0
Jul 16 13:40:39.255: INFO: Restart count of pod e2e-tests-container-probe-9mfsx/liveness-http is now 1 (14.054406797s elapsed)
Jul 16 13:40:59.300: INFO: Restart count of pod e2e-tests-container-probe-9mfsx/liveness-http is now 2 (34.099889502s elapsed)
Jul 16 13:41:19.339: INFO: Restart count of pod e2e-tests-container-probe-9mfsx/liveness-http is now 3 (54.138276684s elapsed)
Jul 16 13:41:39.392: INFO: Restart count of pod e2e-tests-container-probe-9mfsx/liveness-http is now 4 (1m14.19167995s elapsed)
Jul 16 13:42:49.539: INFO: Restart count of pod e2e-tests-container-probe-9mfsx/liveness-http is now 5 (2m24.338950775s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:42:49.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9mfsx" for this suite.
Jul 16 13:42:55.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:42:55.658: INFO: namespace: e2e-tests-container-probe-9mfsx, resource: bindings, ignored listing per whitelist
Jul 16 13:42:57.924: INFO: namespace e2e-tests-container-probe-9mfsx deletion completed in 8.339745848s

• [SLOW TEST:272.827 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:42:57.927: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hxxvm
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Jul 16 13:42:58.026: INFO: Found 0 stateful pods, waiting for 3
Jul 16 13:43:08.030: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:43:08.030: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:43:08.030: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:43:08.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hxxvm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:43:08.353: INFO: stderr: ""
Jul 16 13:43:08.353: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:43:08.353: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 16 13:43:08.382: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 16 13:43:18.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hxxvm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:43:18.690: INFO: stderr: ""
Jul 16 13:43:18.690: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 13:43:18.690: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 13:43:38.705: INFO: Waiting for StatefulSet e2e-tests-statefulset-hxxvm/ss2 to complete update
Jul 16 13:43:38.705: INFO: Waiting for Pod e2e-tests-statefulset-hxxvm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jul 16 13:43:48.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hxxvm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:43:49.040: INFO: stderr: ""
Jul 16 13:43:49.040: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:43:49.040: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 13:43:59.073: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 16 13:44:09.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hxxvm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:44:09.466: INFO: stderr: ""
Jul 16 13:44:09.466: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 13:44:09.466: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 16 13:44:29.485: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hxxvm
Jul 16 13:44:29.488: INFO: Scaling statefulset ss2 to 0
Jul 16 13:44:49.508: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 13:44:49.512: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:44:49.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hxxvm" for this suite.
Jul 16 13:44:55.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:44:56.725: INFO: namespace: e2e-tests-statefulset-hxxvm, resource: bindings, ignored listing per whitelist
Jul 16 13:44:57.877: INFO: namespace e2e-tests-statefulset-hxxvm deletion completed in 8.341237628s

• [SLOW TEST:119.951 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:44:57.879: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-r7j58
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-r7j58 to expose endpoints map[]
Jul 16 13:44:58.004: INFO: Get endpoints failed (3.217792ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 16 13:44:59.008: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-r7j58 exposes endpoints map[] (1.008002844s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-r7j58
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-r7j58 to expose endpoints map[pod1:[80]]
Jul 16 13:45:01.040: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-r7j58 exposes endpoints map[pod1:[80]] (2.02434305s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-r7j58
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-r7j58 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 16 13:45:05.108: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-r7j58 exposes endpoints map[pod1:[80] pod2:[80]] (4.063504048s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-r7j58
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-r7j58 to expose endpoints map[pod2:[80]]
Jul 16 13:45:06.147: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-r7j58 exposes endpoints map[pod2:[80]] (1.033283872s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-r7j58
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-r7j58 to expose endpoints map[]
Jul 16 13:45:07.163: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-r7j58 exposes endpoints map[] (1.00847902s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:45:07.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-r7j58" for this suite.
Jul 16 13:45:31.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:45:32.828: INFO: namespace: e2e-tests-services-r7j58, resource: bindings, ignored listing per whitelist
Jul 16 13:45:33.529: INFO: namespace e2e-tests-services-r7j58 deletion completed in 26.333464937s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:35.651 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:45:33.531: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-fbce8634-a7cf-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:45:33.633: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-4sb7p" to be "success or failure"
Jul 16 13:45:33.639: INFO: Pod "pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.36823ms
Jul 16 13:45:35.643: INFO: Pod "pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00930857s
STEP: Saw pod success
Jul 16 13:45:35.643: INFO: Pod "pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:45:35.646: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:45:35.684: INFO: Waiting for pod pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:45:35.687: INFO: Pod pod-projected-configmaps-fbcf7d5a-a7cf-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:45:35.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4sb7p" for this suite.
Jul 16 13:45:41.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:45:41.850: INFO: namespace: e2e-tests-projected-4sb7p, resource: bindings, ignored listing per whitelist
Jul 16 13:45:44.029: INFO: namespace e2e-tests-projected-4sb7p deletion completed in 8.338794935s

• [SLOW TEST:10.499 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:45:44.030: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:45:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:45:46.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-skfp6" for this suite.
Jul 16 13:46:26.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:46:28.069: INFO: namespace: e2e-tests-pods-skfp6, resource: bindings, ignored listing per whitelist
Jul 16 13:46:28.671: INFO: namespace e2e-tests-pods-skfp6 deletion completed in 42.331568324s

• [SLOW TEST:44.642 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:46:28.672: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:46:32.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-p2sw5" for this suite.
Jul 16 13:46:38.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:46:38.931: INFO: namespace: e2e-tests-emptydir-wrapper-p2sw5, resource: bindings, ignored listing per whitelist
Jul 16 13:46:41.144: INFO: namespace e2e-tests-emptydir-wrapper-p2sw5 deletion completed in 8.328624867s

• [SLOW TEST:12.472 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:46:41.144: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-2hzff
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2hzff to expose endpoints map[]
Jul 16 13:46:41.276: INFO: Get endpoints failed (2.656168ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 16 13:46:42.280: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2hzff exposes endpoints map[] (1.006999777s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-2hzff
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2hzff to expose endpoints map[pod1:[100]]
Jul 16 13:46:44.311: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2hzff exposes endpoints map[pod1:[100]] (2.019267344s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-2hzff
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2hzff to expose endpoints map[pod1:[100] pod2:[101]]
Jul 16 13:46:47.353: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2hzff exposes endpoints map[pod1:[100] pod2:[101]] (3.038559741s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-2hzff
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2hzff to expose endpoints map[pod2:[101]]
Jul 16 13:46:47.370: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2hzff exposes endpoints map[pod2:[101]] (8.366942ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-2hzff
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2hzff to expose endpoints map[]
Jul 16 13:46:48.382: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2hzff exposes endpoints map[] (1.006208924s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:46:48.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-2hzff" for this suite.
Jul 16 13:47:10.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:10.431: INFO: namespace: e2e-tests-services-2hzff, resource: bindings, ignored listing per whitelist
Jul 16 13:47:12.728: INFO: namespace e2e-tests-services-2hzff deletion completed in 24.3289321s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:31.584 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:47:12.728: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 16 13:47:12.833: INFO: Waiting up to 5m0s for pod "pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-jnt8r" to be "success or failure"
Jul 16 13:47:12.837: INFO: Pod "pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855256ms
Jul 16 13:47:14.840: INFO: Pod "pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006762658s
Jul 16 13:47:16.845: INFO: Pod "pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205186s
STEP: Saw pod success
Jul 16 13:47:16.845: INFO: Pod "pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:47:16.849: INFO: Trying to get logs from node i-taxl6ow1 pod pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:47:16.867: INFO: Waiting for pod pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:47:16.871: INFO: Pod pod-36f008d6-a7d0-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:47:16.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jnt8r" for this suite.
Jul 16 13:47:22.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:22.959: INFO: namespace: e2e-tests-emptydir-jnt8r, resource: bindings, ignored listing per whitelist
Jul 16 13:47:25.211: INFO: namespace e2e-tests-emptydir-jnt8r deletion completed in 8.336628318s

• [SLOW TEST:12.483 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:47:25.212: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jul 16 13:47:25.290: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:47:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-hfdnz" for this suite.
Jul 16 13:47:35.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:36.410: INFO: namespace: e2e-tests-init-container-hfdnz, resource: bindings, ignored listing per whitelist
Jul 16 13:47:37.611: INFO: namespace e2e-tests-init-container-hfdnz deletion completed in 8.328854171s

• [SLOW TEST:12.399 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:47:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 16 13:47:37.706: INFO: Waiting up to 5m0s for pod "pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-tszfd" to be "success or failure"
Jul 16 13:47:37.711: INFO: Pod "pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.988663ms
Jul 16 13:47:39.715: INFO: Pod "pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008912199s
Jul 16 13:47:41.720: INFO: Pod "pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013790826s
STEP: Saw pod success
Jul 16 13:47:41.720: INFO: Pod "pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:47:41.723: INFO: Trying to get logs from node i-taxl6ow1 pod pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:47:41.741: INFO: Waiting for pod pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:47:41.744: INFO: Pod pod-45c392c3-a7d0-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:47:41.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tszfd" for this suite.
Jul 16 13:47:47.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:48.191: INFO: namespace: e2e-tests-emptydir-tszfd, resource: bindings, ignored listing per whitelist
Jul 16 13:47:50.093: INFO: namespace e2e-tests-emptydir-tszfd deletion completed in 8.341780046s

• [SLOW TEST:12.482 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:47:50.093: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 13:47:50.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-gw52q'
Jul 16 13:47:50.864: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 13:47:50.864: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Jul 16 13:47:50.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-gw52q'
Jul 16 13:47:51.003: INFO: stderr: ""
Jul 16 13:47:51.003: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:47:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gw52q" for this suite.
Jul 16 13:47:57.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:57.155: INFO: namespace: e2e-tests-kubectl-gw52q, resource: bindings, ignored listing per whitelist
Jul 16 13:47:59.348: INFO: namespace e2e-tests-kubectl-gw52q deletion completed in 8.340217073s

• [SLOW TEST:9.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:47:59.348: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-52bb23d8-a7d0-11e9-9e69-7a63f556cc5d
STEP: Creating configMap with name cm-test-opt-upd-52bb242d-a7d0-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-52bb23d8-a7d0-11e9-9e69-7a63f556cc5d
STEP: Updating configmap cm-test-opt-upd-52bb242d-a7d0-11e9-9e69-7a63f556cc5d
STEP: Creating configMap with name cm-test-opt-create-52bb2443-a7d0-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:48:05.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nhkvq" for this suite.
Jul 16 13:48:29.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:48:30.487: INFO: namespace: e2e-tests-configmap-nhkvq, resource: bindings, ignored listing per whitelist
Jul 16 13:48:31.937: INFO: namespace e2e-tests-configmap-nhkvq deletion completed in 26.327328896s

• [SLOW TEST:32.589 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:48:31.938: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0716 13:49:02.563900      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:49:02.563: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:49:02.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hjgf5" for this suite.
Jul 16 13:49:08.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:49:10.735: INFO: namespace: e2e-tests-gc-hjgf5, resource: bindings, ignored listing per whitelist
Jul 16 13:49:10.924: INFO: namespace e2e-tests-gc-hjgf5 deletion completed in 8.351599976s

• [SLOW TEST:38.986 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:49:10.926: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 16 13:49:15.556: INFO: Successfully updated pod "pod-update-7d62c08d-a7d0-11e9-9e69-7a63f556cc5d"
STEP: verifying the updated pod is in kubernetes
Jul 16 13:49:15.562: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:49:15.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wkxfr" for this suite.
Jul 16 13:49:39.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:49:39.739: INFO: namespace: e2e-tests-pods-wkxfr, resource: bindings, ignored listing per whitelist
Jul 16 13:49:41.888: INFO: namespace e2e-tests-pods-wkxfr deletion completed in 26.322009383s

• [SLOW TEST:30.963 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:49:41.888: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:49:41.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 version --client'
Jul 16 13:49:42.039: INFO: stderr: ""
Jul 16 13:49:42.039: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jul 16 13:49:42.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-7gd5g'
Jul 16 13:49:42.329: INFO: stderr: ""
Jul 16 13:49:42.329: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 16 13:49:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-7gd5g'
Jul 16 13:49:42.619: INFO: stderr: ""
Jul 16 13:49:42.619: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 13:49:43.623: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:49:43.623: INFO: Found 0 / 1
Jul 16 13:49:44.623: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:49:44.623: INFO: Found 1 / 1
Jul 16 13:49:44.623: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 13:49:44.626: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:49:44.626: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 13:49:44.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 describe pod redis-master-n8s28 --namespace=e2e-tests-kubectl-7gd5g'
Jul 16 13:49:44.778: INFO: stderr: ""
Jul 16 13:49:44.778: INFO: stdout: "Name:               redis-master-n8s28\nNamespace:          e2e-tests-kubectl-7gd5g\nPriority:           0\nPriorityClassName:  <none>\nNode:               i-taxl6ow1/192.168.0.7\nStart Time:         Tue, 16 Jul 2019 13:49:42 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.233.100.125\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://ad13b02a5f9e0aee7a891866d9a8c54d56f8c5703314c3e1d51e9c51c71a9042\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 Jul 2019 13:49:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-srwrc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-srwrc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-srwrc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                 Message\n  ----    ------     ----  ----                 -------\n  Normal  Scheduled  2s    default-scheduler    Successfully assigned e2e-tests-kubectl-7gd5g/redis-master-n8s28 to i-taxl6ow1\n  Normal  Pulled     1s    kubelet, i-taxl6ow1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, i-taxl6ow1  Created container\n  Normal  Started    1s    kubelet, i-taxl6ow1  Started container\n"
Jul 16 13:49:44.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 describe rc redis-master --namespace=e2e-tests-kubectl-7gd5g'
Jul 16 13:49:44.954: INFO: stderr: ""
Jul 16 13:49:44.954: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-7gd5g\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-n8s28\n"
Jul 16 13:49:44.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 describe service redis-master --namespace=e2e-tests-kubectl-7gd5g'
Jul 16 13:49:45.078: INFO: stderr: ""
Jul 16 13:49:45.078: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-7gd5g\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.23.178\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.100.125:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 16 13:49:45.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 describe node i-7znxt5so'
Jul 16 13:49:45.254: INFO: stderr: ""
Jul 16 13:49:45.254: INFO: stdout: "Name:               i-7znxt5so\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=i-7znxt5so\n                    node-role.kubernetes.io/node=\n                    role=node\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.6\n                    csi.volume.kubernetes.io/nodeid: {\"csi-qingcloud\":\"i-7znxt5so\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 Jul 2019 12:16:45 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 16 Jul 2019 13:49:38 +0000   Tue, 16 Jul 2019 12:16:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 16 Jul 2019 13:49:38 +0000   Tue, 16 Jul 2019 12:16:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 16 Jul 2019 13:49:38 +0000   Tue, 16 Jul 2019 12:16:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 16 Jul 2019 13:49:38 +0000   Tue, 16 Jul 2019 12:17:05 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.0.6\n  Hostname:    i-7znxt5so\nCapacity:\n cpu:                8\n ephemeral-storage:  103079248Ki\n hugepages-2Mi:      0\n memory:             16431708Ki\n pods:               110\nAllocatable:\n cpu:                7800m\n ephemeral-storage:  94997834800\n hugepages-2Mi:      0\n memory:             15829308Ki\n pods:               110\nSystem Info:\n Machine ID:                    650537faf29b36e0df09ddb15d2db814\n System UUID:                   5AA02EDC-8520-37AB-A6A9-98BEB66B6857\n Boot ID:                       ef82b234-0458-49a0-ac75-db51e1653015\n Kernel Version:                4.4.0-131-generic\n OS Image:                      Ubuntu 16.04.5 LTS\n Operating System:              linux\n Architecture:                  amd64\n Container Runtime Version:     docker://18.6.2\n Kubelet Version:               v1.13.5\n Kube-Proxy Version:            v1.13.5\nPodCIDR:                        10.233.66.0/24\nNon-terminated Pods:            (34 in total)\n  Namespace                     Name                                                       CPU Requests  CPU Limits   Memory Requests  Memory Limits     AGE\n  ---------                     ----                                                       ------------  ----------   ---------------  -------------     ---\n  heptio-sonobuoy               sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-6968x    0 (0%)        0 (0%)       0 (0%)           0 (0%)            55m\n  istio-system                  istio-galley-689b548d98-jm8l2                              10m (0%)      0 (0%)       0 (0%)           0 (0%)            67m\n  istio-system                  istio-ingressgateway-5477b7ffd9-6zjgw                      10m (0%)      0 (0%)       0 (0%)           0 (0%)            67m\n  istio-system                  istio-pilot-9685dfc4b-xxlch                                600m (7%)     2 (25%)      2176Mi (14%)     128Mi (0%)        67m\n  istio-system                  istio-policy-b87497cf4-n4kkt                               110m (1%)     2 (25%)      128Mi (0%)       128Mi (0%)        67m\n  istio-system                  istio-telemetry-758d9c786f-xtfm2                           600m (7%)     6800m (87%)  628Mi (4%)       4134217728 (25%)  66m\n  kube-system                   calico-node-r5c8r                                          150m (1%)     300m (3%)    64M (0%)         500M (3%)         92m\n  kube-system                   coredns-bbbb94784-9rtpg                                    100m (1%)     0 (0%)       70Mi (0%)        170Mi (1%)        91m\n  kube-system                   csi-qingcloud-controller-0                                 0 (0%)        0 (0%)       0 (0%)           0 (0%)            71m\n  kube-system                   csi-qingcloud-node-mfxls                                   0 (0%)        0 (0%)       0 (0%)           0 (0%)            71m\n  kube-system                   kube-proxy-q6k4p                                           0 (0%)        0 (0%)       0 (0%)           0 (0%)            90m\n  kube-system                   metrics-server-85d786dd4d-448wj                            0 (0%)        0 (0%)       0 (0%)           0 (0%)            70m\n  kube-system                   tiller-deploy-6f9697dfd9-j9hch                             0 (0%)        0 (0%)       0 (0%)           0 (0%)            91m\n  kubesphere-alerting-system    alerting-client-64b7bb9d99-mhx29                           10m (0%)      500m (6%)    10Mi (0%)        500Mi (3%)        68m\n  kubesphere-alerting-system    alerting-executor-78764b5795-fkxjn                         20m (0%)      3 (38%)      20Mi (0%)        2500Mi (16%)      68m\n  kubesphere-alerting-system    alerting-manager-5cf779dc56-47pg6                          10m (0%)      500m (6%)    10Mi (0%)        500Mi (3%)        68m\n  kubesphere-alerting-system    alerting-watcher-5867dc5ffc-6c22k                          10m (0%)      500m (6%)    10Mi (0%)        500Mi (3%)        68m\n  kubesphere-alerting-system    notification-deployment-79974b56c9-ppqc2                   10m (0%)      1 (12%)      10Mi (0%)        1000Mi (6%)       69m\n  kubesphere-controls-system    kubectl-admin-d784b7777-zl544                              0 (0%)        0 (0%)       0 (0%)           0 (0%)            68m\n  kubesphere-devops-system      ks-jenkins-7f67d57746-k87dc                                200m (2%)     1 (12%)      4Gi (26%)        8Gi (52%)         67m\n  kubesphere-devops-system      uc-jenkins-update-center-598f8d6549-4nmnj                  0 (0%)        0 (0%)       0 (0%)           0 (0%)            67m\n  kubesphere-logging-system     elasticsearch-logging-data-1                               25m (0%)      4 (51%)      1536Mi (9%)      0 (0%)            60m\n  kubesphere-logging-system     elasticsearch-logging-discovery-0                          25m (0%)      2 (25%)      512Mi (3%)       0 (0%)            70m\n  kubesphere-logging-system     fluent-bit-4bldx                                           0 (0%)        0 (0%)       0 (0%)           0 (0%)            70m\n  kubesphere-logging-system     logging-fluentbit-operator-555dd47d6d-8pnkc                25m (0%)      250m (3%)    180Mi (1%)       180Mi (1%)        70m\n  kubesphere-monitoring-system  kube-state-metrics-7f9c44c88-ljpct                         136m (1%)     106m (1%)    310Mi (2%)       240Mi (1%)        69m\n  kubesphere-monitoring-system  node-exporter-92tpb                                        112m (1%)     270m (3%)    200Mi (1%)       220Mi (1%)        70m\n  kubesphere-monitoring-system  prometheus-k8s-0                                           155m (1%)     2575m (33%)  460Mi (2%)       2108Mi (13%)      70m\n  kubesphere-monitoring-system  prometheus-k8s-system-0                                    135m (1%)     2575m (33%)  460Mi (2%)       2108Mi (13%)      70m\n  kubesphere-monitoring-system  prometheus-operator-6f5d67dcd4-sc6ft                       80m (1%)      200m (2%)    100Mi (0%)       200Mi (1%)        70m\n  kubesphere-system             ks-docs-77c4796dc9-q4bdj                                   10m (0%)      100m (1%)    10Mi (0%)        100Mi (0%)        69m\n  openpitrix-system             openpitrix-iam-service-deployment-864df9fb6f-kbvs8         50m (0%)      500m (6%)    100Mi (0%)       500Mi (3%)        71m\n  openpitrix-system             openpitrix-repo-manager-deployment-84fd5b5fdf-fbr58        50m (0%)      500m (6%)    100Mi (0%)       500Mi (3%)        71m\n  openpitrix-system             openpitrix-runtime-manager-deployment-5fcbb6f447-vjw7t     50m (0%)      500m (6%)    100Mi (0%)       500Mi (3%)        71m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                2693m (34%)       31176m (399%)\n  memory             11557924Ki (73%)  25893047552 (159%)\n  ephemeral-storage  0 (0%)            0 (0%)\nEvents:              <none>\n"
Jul 16 13:49:45.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 describe namespace e2e-tests-kubectl-7gd5g'
Jul 16 13:49:45.394: INFO: stderr: ""
Jul 16 13:49:45.394: INFO: stdout: "Name:         e2e-tests-kubectl-7gd5g\nLabels:       e2e-framework=kubectl\n              e2e-run=f76bfab9-a7c8-11e9-9e69-7a63f556cc5d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:49:45.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7gd5g" for this suite.
Jul 16 13:50:09.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:50:11.179: INFO: namespace: e2e-tests-kubectl-7gd5g, resource: bindings, ignored listing per whitelist
Jul 16 13:50:11.729: INFO: namespace e2e-tests-kubectl-7gd5g deletion completed in 26.32980163s

• [SLOW TEST:29.841 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:50:11.730: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 16 13:50:11.847: INFO: Waiting up to 5m0s for pod "pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-7v4hf" to be "success or failure"
Jul 16 13:50:11.851: INFO: Pod "pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616611ms
Jul 16 13:50:13.854: INFO: Pod "pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007409796s
STEP: Saw pod success
Jul 16 13:50:13.855: INFO: Pod "pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:50:13.859: INFO: Trying to get logs from node i-taxl6ow1 pod pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:50:13.880: INFO: Waiting for pod pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:50:13.887: INFO: Pod pod-a1a3d040-a7d0-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:50:13.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7v4hf" for this suite.
Jul 16 13:50:19.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:50:19.925: INFO: namespace: e2e-tests-emptydir-7v4hf, resource: bindings, ignored listing per whitelist
Jul 16 13:50:22.214: INFO: namespace e2e-tests-emptydir-7v4hf deletion completed in 8.322290882s

• [SLOW TEST:10.484 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:50:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Jul 16 13:50:22.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:22.581: INFO: stderr: ""
Jul 16 13:50:22.581: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:50:22.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:22.707: INFO: stderr: ""
Jul 16 13:50:22.707: INFO: stdout: "update-demo-nautilus-mhrpc update-demo-nautilus-qpf8f "
Jul 16 13:50:22.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-mhrpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:22.838: INFO: stderr: ""
Jul 16 13:50:22.838: INFO: stdout: ""
Jul 16 13:50:22.838: INFO: update-demo-nautilus-mhrpc is created but not running
Jul 16 13:50:27.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:27.995: INFO: stderr: ""
Jul 16 13:50:27.995: INFO: stdout: "update-demo-nautilus-mhrpc update-demo-nautilus-qpf8f "
Jul 16 13:50:27.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-mhrpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:28.098: INFO: stderr: ""
Jul 16 13:50:28.098: INFO: stdout: "true"
Jul 16 13:50:28.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-mhrpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:28.194: INFO: stderr: ""
Jul 16 13:50:28.194: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:50:28.194: INFO: validating pod update-demo-nautilus-mhrpc
Jul 16 13:50:28.200: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:50:28.200: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:50:28.200: INFO: update-demo-nautilus-mhrpc is verified up and running
Jul 16 13:50:28.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-qpf8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:28.331: INFO: stderr: ""
Jul 16 13:50:28.331: INFO: stdout: "true"
Jul 16 13:50:28.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-qpf8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:28.442: INFO: stderr: ""
Jul 16 13:50:28.442: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:50:28.442: INFO: validating pod update-demo-nautilus-qpf8f
Jul 16 13:50:28.451: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:50:28.451: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:50:28.451: INFO: update-demo-nautilus-qpf8f is verified up and running
STEP: rolling-update to new replication controller
Jul 16 13:50:28.456: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:50:28.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:51.953: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 16 13:50:51.953: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:50:51.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:52.096: INFO: stderr: ""
Jul 16 13:50:52.096: INFO: stdout: "update-demo-kitten-2kcbh update-demo-kitten-xrfjx "
Jul 16 13:50:52.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-kitten-2kcbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:52.203: INFO: stderr: ""
Jul 16 13:50:52.203: INFO: stdout: "true"
Jul 16 13:50:52.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-kitten-2kcbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:52.309: INFO: stderr: ""
Jul 16 13:50:52.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 16 13:50:52.309: INFO: validating pod update-demo-kitten-2kcbh
Jul 16 13:50:52.314: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 16 13:50:52.314: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 16 13:50:52.314: INFO: update-demo-kitten-2kcbh is verified up and running
Jul 16 13:50:52.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-kitten-xrfjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:52.418: INFO: stderr: ""
Jul 16 13:50:52.418: INFO: stdout: "true"
Jul 16 13:50:52.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-kitten-xrfjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-g6hv6'
Jul 16 13:50:52.529: INFO: stderr: ""
Jul 16 13:50:52.529: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 16 13:50:52.529: INFO: validating pod update-demo-kitten-xrfjx
Jul 16 13:50:52.538: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 16 13:50:52.538: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 16 13:50:52.538: INFO: update-demo-kitten-xrfjx is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:50:52.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g6hv6" for this suite.
Jul 16 13:51:16.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:51:17.178: INFO: namespace: e2e-tests-kubectl-g6hv6, resource: bindings, ignored listing per whitelist
Jul 16 13:51:18.877: INFO: namespace e2e-tests-kubectl-g6hv6 deletion completed in 26.332735881s

• [SLOW TEST:56.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:51:18.879: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-c9a41daf-a7d0-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 13:51:18.960: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-jrshh" to be "success or failure"
Jul 16 13:51:18.965: INFO: Pod "pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99276ms
Jul 16 13:51:20.979: INFO: Pod "pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017659745s
Jul 16 13:51:22.983: INFO: Pod "pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02114645s
STEP: Saw pod success
Jul 16 13:51:22.983: INFO: Pod "pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:51:22.985: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:51:23.001: INFO: Waiting for pod pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:51:23.003: INFO: Pod pod-projected-secrets-c9a488be-a7d0-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:51:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jrshh" for this suite.
Jul 16 13:51:29.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:51:29.037: INFO: namespace: e2e-tests-projected-jrshh, resource: bindings, ignored listing per whitelist
Jul 16 13:51:31.333: INFO: namespace e2e-tests-projected-jrshh deletion completed in 8.325598769s

• [SLOW TEST:12.454 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:51:31.333: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:51:31.411: INFO: Creating ReplicaSet my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d
Jul 16 13:51:31.431: INFO: Pod name my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d: Found 0 pods out of 1
Jul 16 13:51:36.435: INFO: Pod name my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d: Found 1 pods out of 1
Jul 16 13:51:36.435: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d" is running
Jul 16 13:51:36.438: INFO: Pod "my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d-7bn96" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:51:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:51:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:51:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:51:31 +0000 UTC Reason: Message:}])
Jul 16 13:51:36.438: INFO: Trying to dial the pod
Jul 16 13:51:41.453: INFO: Controller my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d: Got expected result from replica 1 [my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d-7bn96]: "my-hostname-basic-d1112bba-a7d0-11e9-9e69-7a63f556cc5d-7bn96", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:51:41.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-rclv4" for this suite.
Jul 16 13:51:47.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:51:49.349: INFO: namespace: e2e-tests-replicaset-rclv4, resource: bindings, ignored listing per whitelist
Jul 16 13:51:49.800: INFO: namespace e2e-tests-replicaset-rclv4 deletion completed in 8.34026867s

• [SLOW TEST:18.466 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:51:49.800: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0716 13:51:55.969814      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:51:55.969: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:51:55.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x9kkn" for this suite.
Jul 16 13:52:04.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:05.637: INFO: namespace: e2e-tests-gc-x9kkn, resource: bindings, ignored listing per whitelist
Jul 16 13:52:06.386: INFO: namespace e2e-tests-gc-x9kkn deletion completed in 10.409239694s

• [SLOW TEST:16.586 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:52:06.387: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:52:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:52:07.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-l8rtz" for this suite.
Jul 16 13:52:13.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:13.214: INFO: namespace: e2e-tests-custom-resource-definition-l8rtz, resource: bindings, ignored listing per whitelist
Jul 16 13:52:15.364: INFO: namespace e2e-tests-custom-resource-definition-l8rtz deletion completed in 8.329506219s

• [SLOW TEST:8.977 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:52:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 16 13:52:15.457: INFO: Waiting up to 5m0s for pod "pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-r82z7" to be "success or failure"
Jul 16 13:52:15.460: INFO: Pod "pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586373ms
Jul 16 13:52:17.464: INFO: Pod "pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006672596s
Jul 16 13:52:19.468: INFO: Pod "pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0107748s
STEP: Saw pod success
Jul 16 13:52:19.468: INFO: Pod "pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:52:19.481: INFO: Trying to get logs from node i-taxl6ow1 pod pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:52:19.525: INFO: Waiting for pod pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:52:19.528: INFO: Pod pod-eb500e86-a7d0-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:52:19.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r82z7" for this suite.
Jul 16 13:52:25.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:27.214: INFO: namespace: e2e-tests-emptydir-r82z7, resource: bindings, ignored listing per whitelist
Jul 16 13:52:27.863: INFO: namespace e2e-tests-emptydir-r82z7 deletion completed in 8.330451948s

• [SLOW TEST:12.499 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:52:27.864: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:52:27.976: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 13:52:27.992: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:28.004: INFO: Number of nodes with available pods: 0
Jul 16 13:52:28.004: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:52:29.008: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:29.011: INFO: Number of nodes with available pods: 0
Jul 16 13:52:29.011: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:52:30.011: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:30.014: INFO: Number of nodes with available pods: 0
Jul 16 13:52:30.014: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:52:31.009: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:31.013: INFO: Number of nodes with available pods: 1
Jul 16 13:52:31.014: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:52:32.011: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:32.014: INFO: Number of nodes with available pods: 1
Jul 16 13:52:32.014: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:52:33.009: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:33.011: INFO: Number of nodes with available pods: 2
Jul 16 13:52:33.012: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 16 13:52:33.035: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:33.035: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:33.044: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:34.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:34.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:34.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:35.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:35.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:35.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:36.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:36.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:36.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:37.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:37.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:37.065: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:38.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:38.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:38.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:39.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:39.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:39.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:40.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:40.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:40.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:41.051: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:41.051: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:41.057: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:42.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:42.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:42.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:43.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:43.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:43.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:44.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:44.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:44.060: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:45.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:45.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:45.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:46.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:46.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:46.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:47.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:47.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:47.062: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:48.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:48.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:48.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:49.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:49.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:49.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:50.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:50.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:50.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:51.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:51.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:51.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:52.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:52.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:52.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:53.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:53.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:53.057: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:54.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:54.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:54.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:55.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:55.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:55.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:56.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:56.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:56.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:57.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:57.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:57.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:58.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:58.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:58.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:52:59.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:59.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:52:59.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:00.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:00.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:00.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:01.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:01.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:01.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:02.051: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:02.051: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:02.058: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:03.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:03.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:03.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:04.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:04.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:04.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:05.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:05.049: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:05.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:05.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:06.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:06.049: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:06.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:06.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:07.054: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:07.054: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:07.054: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:07.062: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:08.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:08.049: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:08.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:08.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:09.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:09.048: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:09.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:09.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:10.050: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:10.050: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:10.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:10.061: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:11.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:11.049: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:11.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:11.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:12.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:12.050: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:12.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:12.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:13.049: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:13.049: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:13.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:13.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:14.053: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:14.053: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:14.054: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:14.059: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:15.048: INFO: Wrong image for pod: daemon-set-fvpmk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:15.048: INFO: Pod daemon-set-fvpmk is not available
Jul 16 13:53:15.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:15.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:16.048: INFO: Pod daemon-set-pgmtg is not available
Jul 16 13:53:16.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:16.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:17.051: INFO: Pod daemon-set-pgmtg is not available
Jul 16 13:53:17.051: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:17.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:18.049: INFO: Pod daemon-set-pgmtg is not available
Jul 16 13:53:18.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:18.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:19.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:19.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:20.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:20.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:21.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:21.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:22.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:22.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:23.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:23.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:24.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:24.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:25.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:25.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:26.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:26.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:27.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:27.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:28.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:28.078: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:29.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:29.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:30.053: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:30.060: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:31.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:31.060: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:32.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:32.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:33.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:33.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:34.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:34.052: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:35.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:35.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:36.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:36.084: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:37.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:37.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:38.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:38.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:39.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:39.062: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:40.050: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:40.056: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:41.053: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:41.058: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:42.057: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:42.085: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:43.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:43.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:44.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:44.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:45.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:45.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:46.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:46.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:47.054: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:47.059: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:48.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:48.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:49.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:49.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:50.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:50.055: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:51.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:51.049: INFO: Pod daemon-set-vl89w is not available
Jul 16 13:53:51.065: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:52.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:52.048: INFO: Pod daemon-set-vl89w is not available
Jul 16 13:53:52.063: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:53.048: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:53.049: INFO: Pod daemon-set-vl89w is not available
Jul 16 13:53:53.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:54.049: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:54.049: INFO: Pod daemon-set-vl89w is not available
Jul 16 13:53:54.053: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:55.052: INFO: Wrong image for pod: daemon-set-vl89w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jul 16 13:53:55.052: INFO: Pod daemon-set-vl89w is not available
Jul 16 13:53:55.058: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:56.049: INFO: Pod daemon-set-mlvq2 is not available
Jul 16 13:53:56.054: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 16 13:53:56.058: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:56.061: INFO: Number of nodes with available pods: 1
Jul 16 13:53:56.061: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:53:57.066: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:57.069: INFO: Number of nodes with available pods: 1
Jul 16 13:53:57.069: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:53:58.078: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:58.082: INFO: Number of nodes with available pods: 1
Jul 16 13:53:58.082: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:53:59.066: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:53:59.073: INFO: Number of nodes with available pods: 1
Jul 16 13:53:59.074: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:54:00.068: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:00.072: INFO: Number of nodes with available pods: 2
Jul 16 13:54:00.072: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-5t6c4, will wait for the garbage collector to delete the pods
Jul 16 13:54:00.154: INFO: Deleting DaemonSet.extensions daemon-set took: 8.908285ms
Jul 16 13:54:00.255: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.5275ms
Jul 16 13:54:05.659: INFO: Number of nodes with available pods: 0
Jul 16 13:54:05.659: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 13:54:05.662: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-5t6c4/daemonsets","resourceVersion":"22372"},"items":null}

Jul 16 13:54:05.664: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-5t6c4/pods","resourceVersion":"22372"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:54:05.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-5t6c4" for this suite.
Jul 16 13:54:11.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:54:12.034: INFO: namespace: e2e-tests-daemonsets-5t6c4, resource: bindings, ignored listing per whitelist
Jul 16 13:54:14.034: INFO: namespace e2e-tests-daemonsets-5t6c4 deletion completed in 8.345250785s

• [SLOW TEST:106.171 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:54:14.035: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 13:54:14.149: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:14.152: INFO: Number of nodes with available pods: 0
Jul 16 13:54:14.152: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:54:15.158: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:15.160: INFO: Number of nodes with available pods: 0
Jul 16 13:54:15.160: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:54:16.158: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:16.161: INFO: Number of nodes with available pods: 0
Jul 16 13:54:16.161: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 13:54:17.159: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:17.163: INFO: Number of nodes with available pods: 2
Jul 16 13:54:17.163: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 16 13:54:17.192: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:17.207: INFO: Number of nodes with available pods: 1
Jul 16 13:54:17.207: INFO: Node i-taxl6ow1 is running more than one daemon pod
Jul 16 13:54:18.212: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:18.215: INFO: Number of nodes with available pods: 1
Jul 16 13:54:18.215: INFO: Node i-taxl6ow1 is running more than one daemon pod
Jul 16 13:54:19.215: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 13:54:19.219: INFO: Number of nodes with available pods: 2
Jul 16 13:54:19.219: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-5xf7s, will wait for the garbage collector to delete the pods
Jul 16 13:54:19.301: INFO: Deleting DaemonSet.extensions daemon-set took: 5.430445ms
Jul 16 13:54:19.401: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.229266ms
Jul 16 13:54:55.804: INFO: Number of nodes with available pods: 0
Jul 16 13:54:55.804: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 13:54:55.806: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-5xf7s/daemonsets","resourceVersion":"22575"},"items":null}

Jul 16 13:54:55.808: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-5xf7s/pods","resourceVersion":"22575"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:54:55.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-5xf7s" for this suite.
Jul 16 13:55:01.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:55:01.983: INFO: namespace: e2e-tests-daemonsets-5xf7s, resource: bindings, ignored listing per whitelist
Jul 16 13:55:04.162: INFO: namespace e2e-tests-daemonsets-5xf7s deletion completed in 8.338129299s

• [SLOW TEST:50.127 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:55:04.164: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4ff663f9-a7d1-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 13:55:04.320: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-8rjcs" to be "success or failure"
Jul 16 13:55:04.324: INFO: Pod "pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308647ms
Jul 16 13:55:06.330: INFO: Pod "pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009256695s
STEP: Saw pod success
Jul 16 13:55:06.330: INFO: Pod "pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:55:06.333: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:55:06.356: INFO: Waiting for pod pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:55:06.359: INFO: Pod pod-projected-configmaps-4ff7671d-a7d1-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:55:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8rjcs" for this suite.
Jul 16 13:55:12.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:55:12.552: INFO: namespace: e2e-tests-projected-8rjcs, resource: bindings, ignored listing per whitelist
Jul 16 13:55:14.699: INFO: namespace e2e-tests-projected-8rjcs deletion completed in 8.336578366s

• [SLOW TEST:10.535 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:55:14.700: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 16 13:55:14.816: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 16 13:55:19.820: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:55:19.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-4z2lt" for this suite.
Jul 16 13:55:25.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:55:26.675: INFO: namespace: e2e-tests-replication-controller-4z2lt, resource: bindings, ignored listing per whitelist
Jul 16 13:55:28.177: INFO: namespace e2e-tests-replication-controller-4z2lt deletion completed in 8.338602922s

• [SLOW TEST:13.478 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:55:28.178: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-h7gjn.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-h7gjn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-h7gjn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-h7gjn.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-h7gjn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-h7gjn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 13:56:14.415: INFO: DNS probes using e2e-tests-dns-h7gjn/dns-test-5e3f43ae-a7d1-11e9-9e69-7a63f556cc5d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:56:14.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-h7gjn" for this suite.
Jul 16 13:56:20.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:56:20.631: INFO: namespace: e2e-tests-dns-h7gjn, resource: bindings, ignored listing per whitelist
Jul 16 13:56:22.779: INFO: namespace e2e-tests-dns-h7gjn deletion completed in 8.341550999s

• [SLOW TEST:54.601 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:56:22.781: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jul 16 13:56:22.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:23.144: INFO: stderr: ""
Jul 16 13:56:23.144: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:56:23.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:23.290: INFO: stderr: ""
Jul 16 13:56:23.291: INFO: stdout: "update-demo-nautilus-f9ngl update-demo-nautilus-znt7n "
Jul 16 13:56:23.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-f9ngl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:23.402: INFO: stderr: ""
Jul 16 13:56:23.402: INFO: stdout: ""
Jul 16 13:56:23.402: INFO: update-demo-nautilus-f9ngl is created but not running
Jul 16 13:56:28.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:28.517: INFO: stderr: ""
Jul 16 13:56:28.517: INFO: stdout: "update-demo-nautilus-f9ngl update-demo-nautilus-znt7n "
Jul 16 13:56:28.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-f9ngl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:28.608: INFO: stderr: ""
Jul 16 13:56:28.608: INFO: stdout: "true"
Jul 16 13:56:28.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-f9ngl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:28.713: INFO: stderr: ""
Jul 16 13:56:28.713: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:56:28.713: INFO: validating pod update-demo-nautilus-f9ngl
Jul 16 13:56:28.727: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:56:28.727: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:56:28.727: INFO: update-demo-nautilus-f9ngl is verified up and running
Jul 16 13:56:28.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:28.835: INFO: stderr: ""
Jul 16 13:56:28.835: INFO: stdout: "true"
Jul 16 13:56:28.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:28.931: INFO: stderr: ""
Jul 16 13:56:28.931: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:56:28.932: INFO: validating pod update-demo-nautilus-znt7n
Jul 16 13:56:28.936: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:56:28.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:56:28.936: INFO: update-demo-nautilus-znt7n is verified up and running
STEP: scaling down the replication controller
Jul 16 13:56:28.940: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:56:28.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:30.122: INFO: stderr: ""
Jul 16 13:56:30.122: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:56:30.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:30.275: INFO: stderr: ""
Jul 16 13:56:30.275: INFO: stdout: "update-demo-nautilus-f9ngl update-demo-nautilus-znt7n "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 16 13:56:35.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:35.399: INFO: stderr: ""
Jul 16 13:56:35.399: INFO: stdout: "update-demo-nautilus-znt7n "
Jul 16 13:56:35.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:35.522: INFO: stderr: ""
Jul 16 13:56:35.522: INFO: stdout: "true"
Jul 16 13:56:35.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:35.640: INFO: stderr: ""
Jul 16 13:56:35.640: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:56:35.640: INFO: validating pod update-demo-nautilus-znt7n
Jul 16 13:56:35.644: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:56:35.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:56:35.644: INFO: update-demo-nautilus-znt7n is verified up and running
STEP: scaling up the replication controller
Jul 16 13:56:35.650: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:56:35.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:36.813: INFO: stderr: ""
Jul 16 13:56:36.813: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:56:36.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:36.935: INFO: stderr: ""
Jul 16 13:56:36.935: INFO: stdout: "update-demo-nautilus-l46dh update-demo-nautilus-znt7n "
Jul 16 13:56:36.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-l46dh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:37.082: INFO: stderr: ""
Jul 16 13:56:37.082: INFO: stdout: ""
Jul 16 13:56:37.082: INFO: update-demo-nautilus-l46dh is created but not running
Jul 16 13:56:42.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.219: INFO: stderr: ""
Jul 16 13:56:42.220: INFO: stdout: "update-demo-nautilus-l46dh update-demo-nautilus-znt7n "
Jul 16 13:56:42.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-l46dh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.336: INFO: stderr: ""
Jul 16 13:56:42.337: INFO: stdout: "true"
Jul 16 13:56:42.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-l46dh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.431: INFO: stderr: ""
Jul 16 13:56:42.431: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:56:42.431: INFO: validating pod update-demo-nautilus-l46dh
Jul 16 13:56:42.437: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:56:42.437: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:56:42.437: INFO: update-demo-nautilus-l46dh is verified up and running
Jul 16 13:56:42.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.542: INFO: stderr: ""
Jul 16 13:56:42.542: INFO: stdout: "true"
Jul 16 13:56:42.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods update-demo-nautilus-znt7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.640: INFO: stderr: ""
Jul 16 13:56:42.640: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:56:42.640: INFO: validating pod update-demo-nautilus-znt7n
Jul 16 13:56:42.644: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:56:42.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:56:42.644: INFO: update-demo-nautilus-znt7n is verified up and running
STEP: using delete to clean up resources
Jul 16 13:56:42.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.767: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:56:42.767: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 16 13:56:42.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-8rdv6'
Jul 16 13:56:42.894: INFO: stderr: "No resources found.\n"
Jul 16 13:56:42.894: INFO: stdout: ""
Jul 16 13:56:42.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -l name=update-demo --namespace=e2e-tests-kubectl-8rdv6 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:56:42.994: INFO: stderr: ""
Jul 16 13:56:42.994: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:56:42.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8rdv6" for this suite.
Jul 16 13:57:05.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:57:05.074: INFO: namespace: e2e-tests-kubectl-8rdv6, resource: bindings, ignored listing per whitelist
Jul 16 13:57:07.332: INFO: namespace e2e-tests-kubectl-8rdv6 deletion completed in 24.333158686s

• [SLOW TEST:44.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:57:07.332: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:57:07.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-rgbvx" to be "success or failure"
Jul 16 13:57:07.457: INFO: Pod "downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.387317ms
Jul 16 13:57:09.461: INFO: Pod "downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014835042s
STEP: Saw pod success
Jul 16 13:57:09.462: INFO: Pod "downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:57:09.464: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 13:57:09.496: INFO: Waiting for pod downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:57:09.499: INFO: Pod downwardapi-volume-995ab926-a7d1-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:57:09.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rgbvx" for this suite.
Jul 16 13:57:15.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:57:16.446: INFO: namespace: e2e-tests-downward-api-rgbvx, resource: bindings, ignored listing per whitelist
Jul 16 13:57:17.845: INFO: namespace e2e-tests-downward-api-rgbvx deletion completed in 8.341299876s

• [SLOW TEST:10.513 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:57:17.846: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 13:57:17.977: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9fa1ad0c-a7d1-11e9-a7d7-5254228aab6e", Controller:(*bool)(0xc001402b9e), BlockOwnerDeletion:(*bool)(0xc001402b9f)}}
Jul 16 13:57:17.982: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9f9d6c0a-a7d1-11e9-a7d7-5254228aab6e", Controller:(*bool)(0xc001a67cce), BlockOwnerDeletion:(*bool)(0xc001a67ccf)}}
Jul 16 13:57:17.990: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9fa02a98-a7d1-11e9-a7d7-5254228aab6e", Controller:(*bool)(0xc00226211e), BlockOwnerDeletion:(*bool)(0xc00226211f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:57:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d9tt2" for this suite.
Jul 16 13:57:29.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:57:29.610: INFO: namespace: e2e-tests-gc-d9tt2, resource: bindings, ignored listing per whitelist
Jul 16 13:57:31.337: INFO: namespace e2e-tests-gc-d9tt2 deletion completed in 8.330998684s

• [SLOW TEST:13.492 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:57:31.337: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-b4j8s
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 13:57:31.424: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 13:57:57.497: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.233.83.239 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-b4j8s PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:57:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:57:58.770: INFO: Found all expected endpoints: [netserver-0]
Jul 16 13:57:58.773: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.233.100.119 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-b4j8s PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:57:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 13:57:59.972: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:57:59.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-b4j8s" for this suite.
Jul 16 13:58:21.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:58:22.014: INFO: namespace: e2e-tests-pod-network-test-b4j8s, resource: bindings, ignored listing per whitelist
Jul 16 13:58:24.318: INFO: namespace e2e-tests-pod-network-test-b4j8s deletion completed in 24.332730421s

• [SLOW TEST:52.981 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:58:24.318: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 16 13:58:47.425: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:58:48.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-kkxc6" for this suite.
Jul 16 13:59:12.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:59:12.496: INFO: namespace: e2e-tests-replicaset-kkxc6, resource: bindings, ignored listing per whitelist
Jul 16 13:59:14.787: INFO: namespace e2e-tests-replicaset-kkxc6 deletion completed in 26.340119232s

• [SLOW TEST:50.468 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:59:14.787: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 16 13:59:14.867: INFO: Waiting up to 5m0s for pod "pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-tgw8f" to be "success or failure"
Jul 16 13:59:14.869: INFO: Pod "pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586119ms
Jul 16 13:59:16.873: INFO: Pod "pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005957161s
STEP: Saw pod success
Jul 16 13:59:16.873: INFO: Pod "pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 13:59:16.875: INFO: Trying to get logs from node i-taxl6ow1 pod pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 13:59:16.899: INFO: Waiting for pod pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d to disappear
Jul 16 13:59:16.902: INFO: Pod pod-e54defdb-a7d1-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:59:16.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tgw8f" for this suite.
Jul 16 13:59:22.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:59:23.023: INFO: namespace: e2e-tests-emptydir-tgw8f, resource: bindings, ignored listing per whitelist
Jul 16 13:59:25.243: INFO: namespace e2e-tests-emptydir-tgw8f deletion completed in 8.335615599s

• [SLOW TEST:10.456 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 13:59:25.244: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 13:59:48.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-dlcx2" for this suite.
Jul 16 14:00:12.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:00:12.507: INFO: namespace: e2e-tests-replication-controller-dlcx2, resource: bindings, ignored listing per whitelist
Jul 16 14:00:14.701: INFO: namespace e2e-tests-replication-controller-dlcx2 deletion completed in 26.333984815s

• [SLOW TEST:49.458 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:00:14.702: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-090703e2-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:00:14.801: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-qm2z9" to be "success or failure"
Jul 16 14:00:14.803: INFO: Pod "pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227517ms
Jul 16 14:00:16.807: INFO: Pod "pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006397307s
Jul 16 14:00:18.812: INFO: Pod "pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010987082s
STEP: Saw pod success
Jul 16 14:00:18.812: INFO: Pod "pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:00:18.817: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:00:18.842: INFO: Waiting for pod pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:00:18.846: INFO: Pod pod-projected-configmaps-09076f5f-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:00:18.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qm2z9" for this suite.
Jul 16 14:00:24.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:00:25.842: INFO: namespace: e2e-tests-projected-qm2z9, resource: bindings, ignored listing per whitelist
Jul 16 14:00:27.193: INFO: namespace e2e-tests-projected-qm2z9 deletion completed in 8.339667973s

• [SLOW TEST:12.492 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:00:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-1079a6cb-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:00:27.298: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-zdffn" to be "success or failure"
Jul 16 14:00:27.304: INFO: Pod "pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.266727ms
Jul 16 14:00:29.308: INFO: Pod "pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009996025s
Jul 16 14:00:31.312: INFO: Pod "pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014022538s
STEP: Saw pod success
Jul 16 14:00:31.312: INFO: Pod "pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:00:31.315: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:00:31.335: INFO: Waiting for pod pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:00:31.339: INFO: Pod pod-projected-configmaps-107a2389-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:00:31.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zdffn" for this suite.
Jul 16 14:00:37.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:00:37.432: INFO: namespace: e2e-tests-projected-zdffn, resource: bindings, ignored listing per whitelist
Jul 16 14:00:39.696: INFO: namespace e2e-tests-projected-zdffn deletion completed in 8.352981085s

• [SLOW TEST:12.503 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:00:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 14:00:39.779: INFO: Creating deployment "test-recreate-deployment"
Jul 16 14:00:39.782: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 16 14:00:39.789: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jul 16 14:00:41.796: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 16 14:00:41.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882439, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882439, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882439, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882439, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:00:43.803: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 16 14:00:43.810: INFO: Updating deployment test-recreate-deployment
Jul 16 14:00:43.810: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 16 14:00:43.873: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-8sh97,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8sh97/deployments/test-recreate-deployment,UID:17ebb11d-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:23827,Generation:2,CreationTimestamp:2019-07-16 14:00:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-16 14:00:43 +0000 UTC 2019-07-16 14:00:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-16 14:00:43 +0000 UTC 2019-07-16 14:00:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 16 14:00:43.876: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-8sh97,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8sh97/replicasets/test-recreate-deployment-697fbf54bf,UID:1a57850d-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:23826,Generation:1,CreationTimestamp:2019-07-16 14:00:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 17ebb11d-a7d2-11e9-a7d7-5254228aab6e 0xc0006942a7 0xc0006942a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:00:43.876: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 16 14:00:43.876: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-8sh97,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8sh97/replicasets/test-recreate-deployment-5dfdcc846d,UID:17ed5557-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:23816,Generation:2,CreationTimestamp:2019-07-16 14:00:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 17ebb11d-a7d2-11e9-a7d7-5254228aab6e 0xc0006940e7 0xc0006940e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:00:43.880: INFO: Pod "test-recreate-deployment-697fbf54bf-4b2sb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-4b2sb,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-8sh97,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8sh97/pods/test-recreate-deployment-697fbf54bf-4b2sb,UID:1a5832e9-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:23822,Generation:0,CreationTimestamp:2019-07-16 14:00:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 1a57850d-a7d2-11e9-a7d7-5254228aab6e 0xc001b60337 0xc001b60338}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-k2295 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k2295,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k2295 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b604c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b604e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:00:43 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:00:43.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-8sh97" for this suite.
Jul 16 14:00:49.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:00:49.942: INFO: namespace: e2e-tests-deployment-8sh97, resource: bindings, ignored listing per whitelist
Jul 16 14:00:52.216: INFO: namespace e2e-tests-deployment-8sh97 deletion completed in 8.331206919s

• [SLOW TEST:12.519 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:00:52.216: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 16 14:00:58.368: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 14:00:58.371: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 14:01:00.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 14:01:00.376: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 14:01:02.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 14:01:02.384: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 14:01:04.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 14:01:04.375: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 14:01:06.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 14:01:06.379: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:01:06.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-9xqtk" for this suite.
Jul 16 14:01:28.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:01:30.020: INFO: namespace: e2e-tests-container-lifecycle-hook-9xqtk, resource: bindings, ignored listing per whitelist
Jul 16 14:01:30.721: INFO: namespace e2e-tests-container-lifecycle-hook-9xqtk deletion completed in 24.328198546s

• [SLOW TEST:38.504 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:01:30.721: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jul 16 14:01:30.824: INFO: Waiting up to 5m0s for pod "downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-bshb8" to be "success or failure"
Jul 16 14:01:30.827: INFO: Pod "downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835205ms
Jul 16 14:01:32.834: INFO: Pod "downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009827024s
STEP: Saw pod success
Jul 16 14:01:32.834: INFO: Pod "downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:01:32.838: INFO: Trying to get logs from node i-taxl6ow1 pod downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:01:32.874: INFO: Waiting for pod downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:01:32.876: INFO: Pod downward-api-365726cd-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:01:32.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bshb8" for this suite.
Jul 16 14:01:38.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:01:39.014: INFO: namespace: e2e-tests-downward-api-bshb8, resource: bindings, ignored listing per whitelist
Jul 16 14:01:41.219: INFO: namespace e2e-tests-downward-api-bshb8 deletion completed in 8.333915295s

• [SLOW TEST:10.498 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:01:41.220: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 16 14:01:41.327: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 14:01:41.338: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 14:01:41.341: INFO: 
Logging pods the kubelet thinks is on node i-7znxt5so before test
Jul 16 14:01:41.361: INFO: istio-pilot-9685dfc4b-xxlch from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container discovery ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.361: INFO: uc-jenkins-update-center-598f8d6549-4nmnj from kubesphere-devops-system started at 2019-07-16 12:42:29 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container jenkins-update-center ready: true, restart count 0
Jul 16 14:01:41.361: INFO: notification-deployment-79974b56c9-ppqc2 from kubesphere-alerting-system started at 2019-07-16 12:40:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container notification ready: true, restart count 0
Jul 16 14:01:41.361: INFO: kube-state-metrics-7f9c44c88-ljpct from kubesphere-monitoring-system started at 2019-07-16 12:40:14 +0000 UTC (4 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 16 14:01:41.361: INFO: openpitrix-runtime-manager-deployment-5fcbb6f447-vjw7t from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container openpitrix-runtime-manager ready: true, restart count 0
Jul 16 14:01:41.361: INFO: notification-db-init-job-5zrd7 from kubesphere-alerting-system started at 2019-07-16 12:40:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container notification-db-init ready: false, restart count 0
Jul 16 14:01:41.361: INFO: alerting-client-64b7bb9d99-mhx29 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container alerting-client ready: true, restart count 0
Jul 16 14:01:41.361: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-6968x from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 14:01:41.361: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 14:01:41.361: INFO: csi-qingcloud-controller-0 from kube-system started at 2019-07-16 12:37:54 +0000 UTC (3 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:01:41.361: INFO: ks-jenkins-7f67d57746-k87dc from kubesphere-devops-system started at 2019-07-16 12:43:04 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container ks-jenkins ready: true, restart count 0
Jul 16 14:01:41.361: INFO: istio-policy-b87497cf4-n4kkt from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.361: INFO: 	Container mixer ready: true, restart count 2
Jul 16 14:01:41.361: INFO: istio-ingressgateway-5477b7ffd9-5p6wp from istio-system started at 2019-07-16 14:01:11 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.361: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.361: INFO: alerting-manager-5cf779dc56-47pg6 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container alerting-manager ready: true, restart count 0
Jul 16 14:01:41.362: INFO: istio-galley-689b548d98-jm8l2 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container galley ready: true, restart count 0
Jul 16 14:01:41.362: INFO: coredns-bbbb94784-9rtpg from kube-system started at 2019-07-16 12:18:09 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container coredns ready: true, restart count 0
Jul 16 14:01:41.362: INFO: alerting-watcher-5867dc5ffc-6c22k from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container alerting-watcher ready: true, restart count 0
Jul 16 14:01:41.362: INFO: istio-ingressgateway-5477b7ffd9-6zjgw from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.362: INFO: alerting-db-init-job-zgqlj from kubesphere-alerting-system started at 2019-07-16 12:38:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container alerting-db-init ready: false, restart count 0
Jul 16 14:01:41.362: INFO: openpitrix-iam-db-ctrl-job-6zwht from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-iam-db-ctrl ready: false, restart count 3
Jul 16 14:01:41.362: INFO: kubectl-admin-d784b7777-zl544 from kubesphere-controls-system started at 2019-07-16 12:41:39 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container kubectl ready: true, restart count 0
Jul 16 14:01:41.362: INFO: csi-qingcloud-node-mfxls from kube-system started at 2019-07-16 12:37:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 14:01:41.362: INFO: fluent-bit-4bldx from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 14:01:41.362: INFO: alerting-db-ctrl-job-pxdbk from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container alerting-db-ctrl ready: false, restart count 0
Jul 16 14:01:41.362: INFO: kube-proxy-q6k4p from kube-system started at 2019-07-16 12:19:15 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:01:41.362: INFO: istio-telemetry-758d9c786f-xtfm2 from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container mixer ready: true, restart count 3
Jul 16 14:01:41.362: INFO: openpitrix-task-db-ctrl-job-f5jjj from openpitrix-system started at 2019-07-16 12:40:25 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-task-db-ctrl ready: false, restart count 4
Jul 16 14:01:41.362: INFO: openpitrix-repo-manager-deployment-84fd5b5fdf-fbr58 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-repo-manager ready: true, restart count 0
Jul 16 14:01:41.362: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:31 +0000 UTC (3 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:01:41.362: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:01:41.362: INFO: calico-node-r5c8r from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:01:41.362: INFO: openpitrix-app-db-ctrl-job-4gk2k from openpitrix-system started at 2019-07-16 12:40:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-app-db-ctrl ready: false, restart count 4
Jul 16 14:01:41.362: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:01:41.362: INFO: metrics-server-85d786dd4d-448wj from kube-system started at 2019-07-16 12:38:51 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container metrics-server ready: true, restart count 0
Jul 16 14:01:41.362: INFO: istio-init-crd-10-c6klc from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container istio-init-crd-10 ready: false, restart count 0
Jul 16 14:01:41.362: INFO: ks-docs-77c4796dc9-q4bdj from kubesphere-system started at 2019-07-16 12:39:46 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container ks-docs ready: true, restart count 0
Jul 16 14:01:41.362: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:01:41.362: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:01:41.362: INFO: openpitrix-iam-service-deployment-864df9fb6f-kbvs8 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-iam-service ready: true, restart count 0
Jul 16 14:01:41.362: INFO: node-exporter-92tpb from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:01:41.362: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2019-07-16 12:49:11 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:01:41.362: INFO: prometheus-operator-6f5d67dcd4-sc6ft from kubesphere-monitoring-system started at 2019-07-16 12:39:26 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 16 14:01:41.362: INFO: notification-db-ctrl-job-c82wv from kubesphere-alerting-system started at 2019-07-16 12:40:37 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Jul 16 14:01:41.362: INFO: istio-init-crd-11-7plk7 from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container istio-init-crd-11 ready: false, restart count 0
Jul 16 14:01:41.362: INFO: logging-fluentbit-operator-555dd47d6d-8pnkc from kubesphere-logging-system started at 2019-07-16 12:39:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container fluentbit-operator ready: true, restart count 0
Jul 16 14:01:41.362: INFO: openpitrix-job-db-ctrl-job-rhhwx from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container openpitrix-job-db-ctrl ready: false, restart count 5
Jul 16 14:01:41.362: INFO: alerting-executor-78764b5795-fkxjn from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 14:01:41.362: INFO: tiller-deploy-6f9697dfd9-j9hch from kube-system started at 2019-07-16 12:18:13 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.362: INFO: 	Container tiller ready: true, restart count 0
Jul 16 14:01:41.362: INFO: 
Logging pods the kubelet thinks is on node i-taxl6ow1 before test
Jul 16 14:01:41.388: INFO: openpitrix-app-manager-deployment-595dcd76f-58qvb from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container openpitrix-app-manager ready: true, restart count 0
Jul 16 14:01:41.388: INFO: openpitrix-minio-deployment-84d5f9c94b-v4gkn from openpitrix-system started at 2019-07-16 12:39:12 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container openpitrix-minio ready: true, restart count 0
Jul 16 14:01:41.388: INFO: fluent-bit-r8xb4 from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 14:01:41.388: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 14:01:41.388: INFO: istio-ingressgateway-5477b7ffd9-mqd6p from istio-system started at 2019-07-16 12:42:54 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.388: INFO: jaeger-query-6b5d8f7bcd-pxqtq from istio-system started at 2019-07-16 12:47:02 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container jaeger-agent ready: true, restart count 0
Jul 16 14:01:41.388: INFO: 	Container jaeger-query ready: true, restart count 1
Jul 16 14:01:41.388: INFO: dns-autoscaler-89c7bbd57-7x9kr from kube-system started at 2019-07-16 12:18:07 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container autoscaler ready: true, restart count 0
Jul 16 14:01:41.388: INFO: calico-node-js6px from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:01:41.388: INFO: openpitrix-api-gateway-deployment-587cc46874-gwm9j from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container openpitrix-api-gateway ready: true, restart count 0
Jul 16 14:01:41.388: INFO: openpitrix-runtime-db-ctrl-job-gcr9h from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container openpitrix-runtime-db-ctrl ready: false, restart count 5
Jul 16 14:01:41.388: INFO: ks-sonarqube-sonarqube-9c5876cd8-jvl46 from kubesphere-devops-system started at 2019-07-16 12:38:03 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container sonarqube ready: true, restart count 4
Jul 16 14:01:41.388: INFO: csi-qingcloud-node-vzj9n from kube-system started at 2019-07-16 12:37:58 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:01:41.388: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 14:01:41.388: INFO: s2ioperator-0 from kubesphere-devops-system started at 2019-07-16 12:39:09 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.388: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:01:41.388: INFO: 	Container manager ready: true, restart count 0
Jul 16 14:01:41.388: INFO: istio-galley-689b548d98-26dvv from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container galley ready: true, restart count 0
Jul 16 14:01:41.389: INFO: ks-sonarqube-postgresql-5d58f5574b-5r955 from kubesphere-devops-system started at 2019-07-16 12:38:58 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container ks-sonarqube-postgresql ready: true, restart count 0
Jul 16 14:01:41.389: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 12:54:10 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: calico-kube-controllers-767548c9d9-mqjxp from kube-system started at 2019-07-16 12:17:28 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 16 14:01:41.389: INFO: jaeger-collector-f579b55fb-ljwcp from istio-system started at 2019-07-16 12:47:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container jaeger-collector ready: true, restart count 3
Jul 16 14:01:41.389: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:36 +0000 UTC (3 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:01:41.389: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:01:41.389: INFO: istio-pilot-9685dfc4b-f8qhv from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container discovery ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-category-manager-deployment-7968d789d6-lljbl from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-category-manager ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-task-manager-deployment-59578dc9d6-rghp8 from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-task-manager ready: true, restart count 0
Jul 16 14:01:41.389: INFO: ks-devops-db-init-job-h9xvn from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container devops-db-init ready: false, restart count 0
Jul 16 14:01:41.389: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:01:41.389: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:01:41.389: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:01:41.389: INFO: istio-telemetry-758d9c786f-zkkh6 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container mixer ready: true, restart count 4
Jul 16 14:01:41.389: INFO: openpitrix-cluster-manager-deployment-6dcd96b68b-ptl6w from openpitrix-system started at 2019-07-16 12:38:20 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-cluster-manager ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-repo-indexer-deployment-5f4c895b54-vqj26 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-repo-indexer ready: true, restart count 0
Jul 16 14:01:41.389: INFO: istio-policy-b87497cf4-nstpl from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container mixer ready: true, restart count 2
Jul 16 14:01:41.389: INFO: istio-ingressgateway-5477b7ffd9-xt7sk from istio-system started at 2019-07-16 13:47:12 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: kube-proxy-xn8lr from kube-system started at 2019-07-16 12:18:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-job-manager-deployment-588858bcb9-vrlv2 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-job-manager ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-repo-db-ctrl-job-69hdm from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-repo-db-ctrl ready: false, restart count 5
Jul 16 14:01:41.389: INFO: jaeger-operator-8cb967c86-pcznr from istio-system started at 2019-07-16 12:42:40 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container jaeger-operator ready: true, restart count 0
Jul 16 14:01:41.389: INFO: ks-devops-db-ctrl-job-4dwlz from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container devops-db-ctrl ready: false, restart count 0
Jul 16 14:01:41.389: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-dpktl from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 14:01:41.389: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 14:01:41.389: INFO: default-http-backend-96d94689d-sl8zp from kubesphere-controls-system started at 2019-07-16 12:38:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-etcd-deployment-54bc9bb948-s6wmk from openpitrix-system started at 2019-07-16 12:39:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-etcd ready: true, restart count 0
Jul 16 14:01:41.389: INFO: openpitrix-cluster-db-ctrl-job-mqp4p from openpitrix-system started at 2019-07-16 12:40:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-cluster-db-ctrl ready: false, restart count 2
Jul 16 14:01:41.389: INFO: openpitrix-db-init-job-4g8dq from openpitrix-system started at 2019-07-16 12:40:26 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container openpitrix-db-init ready: false, restart count 0
Jul 16 14:01:41.389: INFO: node-exporter-hkg6g from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:01:41.389: INFO: alerting-executor-78764b5795-v7tvg from kubesphere-alerting-system started at 2019-07-16 12:40:57 +0000 UTC (2 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 14:01:41.389: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 14:01:41.389: INFO: istio-citadel-5f886dc9b4-hxx8p from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container citadel ready: true, restart count 0
Jul 16 14:01:41.389: INFO: istio-sidecar-injector-74666b458c-v8r9r from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:01:41.389: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3ddc0cf1-a7d2-11e9-9e69-7a63f556cc5d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-3ddc0cf1-a7d2-11e9-9e69-7a63f556cc5d off the node i-taxl6ow1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3ddc0cf1-a7d2-11e9-9e69-7a63f556cc5d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:01:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-xhf97" for this suite.
Jul 16 14:02:09.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:09.555: INFO: namespace: e2e-tests-sched-pred-xhf97, resource: bindings, ignored listing per whitelist
Jul 16 14:02:11.804: INFO: namespace e2e-tests-sched-pred-xhf97 deletion completed in 24.32836448s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:30.585 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:02:11.805: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-4ed32670-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:02:11.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-h4l55" to be "success or failure"
Jul 16 14:02:11.916: INFO: Pod "pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511501ms
Jul 16 14:02:13.921: INFO: Pod "pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141539s
Jul 16 14:02:15.924: INFO: Pod "pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010471042s
STEP: Saw pod success
Jul 16 14:02:15.924: INFO: Pod "pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:02:15.926: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:02:15.954: INFO: Waiting for pod pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:02:15.957: INFO: Pod pod-configmaps-4ed40a25-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:02:15.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h4l55" for this suite.
Jul 16 14:02:21.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:22.027: INFO: namespace: e2e-tests-configmap-h4l55, resource: bindings, ignored listing per whitelist
Jul 16 14:02:24.289: INFO: namespace e2e-tests-configmap-h4l55 deletion completed in 8.326660719s

• [SLOW TEST:12.485 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:02:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-lh6t
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 14:02:24.390: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lh6t" in namespace "e2e-tests-subpath-jh2hs" to be "success or failure"
Jul 16 14:02:24.402: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.091337ms
Jul 16 14:02:26.406: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01559538s
Jul 16 14:02:28.411: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 4.020420614s
Jul 16 14:02:30.415: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 6.02448261s
Jul 16 14:02:32.418: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 8.027937937s
Jul 16 14:02:34.421: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 10.031041651s
Jul 16 14:02:36.425: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 12.034687872s
Jul 16 14:02:38.429: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 14.038515381s
Jul 16 14:02:40.433: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 16.043163898s
Jul 16 14:02:42.439: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 18.048561936s
Jul 16 14:02:44.443: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 20.053239321s
Jul 16 14:02:46.448: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Running", Reason="", readiness=false. Elapsed: 22.057738264s
Jul 16 14:02:48.453: INFO: Pod "pod-subpath-test-secret-lh6t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.062283327s
STEP: Saw pod success
Jul 16 14:02:48.453: INFO: Pod "pod-subpath-test-secret-lh6t" satisfied condition "success or failure"
Jul 16 14:02:48.458: INFO: Trying to get logs from node i-taxl6ow1 pod pod-subpath-test-secret-lh6t container test-container-subpath-secret-lh6t: <nil>
STEP: delete the pod
Jul 16 14:02:48.485: INFO: Waiting for pod pod-subpath-test-secret-lh6t to disappear
Jul 16 14:02:48.487: INFO: Pod pod-subpath-test-secret-lh6t no longer exists
STEP: Deleting pod pod-subpath-test-secret-lh6t
Jul 16 14:02:48.487: INFO: Deleting pod "pod-subpath-test-secret-lh6t" in namespace "e2e-tests-subpath-jh2hs"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:02:48.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-jh2hs" for this suite.
Jul 16 14:02:54.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:54.536: INFO: namespace: e2e-tests-subpath-jh2hs, resource: bindings, ignored listing per whitelist
Jul 16 14:02:56.825: INFO: namespace e2e-tests-subpath-jh2hs deletion completed in 8.330291046s

• [SLOW TEST:32.535 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:02:56.825: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:02:56.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-nqnj4'
Jul 16 14:02:57.341: INFO: stderr: ""
Jul 16 14:02:57.341: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 16 14:03:02.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-nqnj4 -o json'
Jul 16 14:03:02.558: INFO: stderr: ""
Jul 16 14:03:02.559: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-16T14:02:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-nqnj4\",\n        \"resourceVersion\": \"24369\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-nqnj4/pods/e2e-test-nginx-pod\",\n        \"uid\": \"69e66369-a7d2-11e9-a7d7-5254228aab6e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4xdwv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"i-taxl6ow1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4xdwv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4xdwv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:02:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:02:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:02:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:02:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://3ed708954b66641b0b457546f041a03b284cf6e61d8b01df5702e1ff50259c8b\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-16T14:02:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.100.82\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-16T14:02:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 16 14:03:02.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 replace -f - --namespace=e2e-tests-kubectl-nqnj4'
Jul 16 14:03:02.849: INFO: stderr: ""
Jul 16 14:03:02.849: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Jul 16 14:03:02.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-nqnj4'
Jul 16 14:03:15.664: INFO: stderr: ""
Jul 16 14:03:15.664: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:03:15.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nqnj4" for this suite.
Jul 16 14:03:21.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:03:21.733: INFO: namespace: e2e-tests-kubectl-nqnj4, resource: bindings, ignored listing per whitelist
Jul 16 14:03:24.012: INFO: namespace e2e-tests-kubectl-nqnj4 deletion completed in 8.34343742s

• [SLOW TEST:27.187 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:03:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:03:24.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-z6k4n'
Jul 16 14:03:24.223: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 14:03:24.224: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 16 14:03:24.242: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-6nn8q]
Jul 16 14:03:24.242: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-6nn8q" in namespace "e2e-tests-kubectl-z6k4n" to be "running and ready"
Jul 16 14:03:24.248: INFO: Pod "e2e-test-nginx-rc-6nn8q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.556675ms
Jul 16 14:03:26.252: INFO: Pod "e2e-test-nginx-rc-6nn8q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034782s
Jul 16 14:03:28.271: INFO: Pod "e2e-test-nginx-rc-6nn8q": Phase="Running", Reason="", readiness=true. Elapsed: 4.028757323s
Jul 16 14:03:28.271: INFO: Pod "e2e-test-nginx-rc-6nn8q" satisfied condition "running and ready"
Jul 16 14:03:28.271: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-6nn8q]
Jul 16 14:03:28.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z6k4n'
Jul 16 14:03:28.416: INFO: stderr: ""
Jul 16 14:03:28.416: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Jul 16 14:03:28.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z6k4n'
Jul 16 14:03:28.545: INFO: stderr: ""
Jul 16 14:03:28.545: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:03:28.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z6k4n" for this suite.
Jul 16 14:03:50.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:03:50.684: INFO: namespace: e2e-tests-kubectl-z6k4n, resource: bindings, ignored listing per whitelist
Jul 16 14:03:52.894: INFO: namespace e2e-tests-kubectl-z6k4n deletion completed in 24.34009932s

• [SLOW TEST:28.882 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:03:52.895: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-vt5b8.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 81.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.81_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-vt5b8;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-vt5b8.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-vt5b8.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-vt5b8.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 81.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.81_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 14:03:57.049: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.054: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.059: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8 from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.070: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8 from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.075: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.081: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.097: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.144: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.151: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.171: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-vt5b8 from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.179: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8 from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.185: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.191: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.198: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.202: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc from pod e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d: the server could not find the requested resource (get pods dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d)
Jul 16 14:03:57.224: INFO: Lookups using e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8 wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8 wheezy_udp@dns-test-service.e2e-tests-dns-vt5b8.svc wheezy_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-vt5b8 jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8 jessie_udp@dns-test-service.e2e-tests-dns-vt5b8.svc jessie_tcp@dns-test-service.e2e-tests-dns-vt5b8.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-vt5b8.svc]

Jul 16 14:04:02.566: INFO: DNS probes using e2e-tests-dns-vt5b8/dns-test-8b1797cd-a7d2-11e9-9e69-7a63f556cc5d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:04:02.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-vt5b8" for this suite.
Jul 16 14:04:08.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:10.239: INFO: namespace: e2e-tests-dns-vt5b8, resource: bindings, ignored listing per whitelist
Jul 16 14:04:10.989: INFO: namespace e2e-tests-dns-vt5b8 deletion completed in 8.339044792s

• [SLOW TEST:18.094 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:04:10.989: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 14:04:11.083: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 16 14:04:16.088: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 14:04:16.088: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 16 14:04:18.092: INFO: Creating deployment "test-rollover-deployment"
Jul 16 14:04:18.100: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 16 14:04:20.108: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 16 14:04:20.116: INFO: Ensure that both replica sets have 1 created replica
Jul 16 14:04:20.123: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 16 14:04:20.145: INFO: Updating deployment test-rollover-deployment
Jul 16 14:04:20.145: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 16 14:04:22.159: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 16 14:04:22.168: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 16 14:04:22.175: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:04:22.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882662, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:24.183: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:04:24.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882662, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:26.186: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:04:26.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882662, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:28.182: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:04:28.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882662, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:30.190: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:04:30.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882662, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:32.188: INFO: 
Jul 16 14:04:32.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882672, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882658, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:04:34.182: INFO: 
Jul 16 14:04:34.183: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 16 14:04:34.191: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-cctzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cctzw/deployments/test-rollover-deployment,UID:9a0bca1a-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:24729,Generation:2,CreationTimestamp:2019-07-16 14:04:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-16 14:04:18 +0000 UTC 2019-07-16 14:04:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-16 14:04:32 +0000 UTC 2019-07-16 14:04:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 16 14:04:34.194: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-cctzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cctzw/replicasets/test-rollover-deployment-6b7f9d6597,UID:9b441ef2-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:24720,Generation:2,CreationTimestamp:2019-07-16 14:04:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9a0bca1a-a7d2-11e9-a7d7-5254228aab6e 0xc002fe96d7 0xc002fe96d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 14:04:34.194: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 16 14:04:34.195: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-cctzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cctzw/replicasets/test-rollover-controller,UID:95dc9e5f-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:24728,Generation:2,CreationTimestamp:2019-07-16 14:04:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9a0bca1a-a7d2-11e9-a7d7-5254228aab6e 0xc002fe9547 0xc002fe9548}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:04:34.196: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-cctzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cctzw/replicasets/test-rollover-deployment-6586df867b,UID:9a0e2ab1-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:24682,Generation:2,CreationTimestamp:2019-07-16 14:04:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9a0bca1a-a7d2-11e9-a7d7-5254228aab6e 0xc002fe9607 0xc002fe9608}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:04:34.199: INFO: Pod "test-rollover-deployment-6b7f9d6597-f4bjl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-f4bjl,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-cctzw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cctzw/pods/test-rollover-deployment-6b7f9d6597-f4bjl,UID:9b4999d4-a7d2-11e9-a7d7-5254228aab6e,ResourceVersion:24703,Generation:0,CreationTimestamp:2019-07-16 14:04:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 9b441ef2-a7d2-11e9-a7d7-5254228aab6e 0xc0028830a7 0xc0028830a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-qsx5g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qsx5g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qsx5g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-taxl6ow1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002883120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002883180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:04:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:04:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:04:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:04:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.100.109,StartTime:2019-07-16 14:04:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-16 14:04:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://820608f46d0c358b44cbbd9be79cc1091489bae5c670211c79e386454ad83e87}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:04:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-cctzw" for this suite.
Jul 16 14:04:40.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:40.444: INFO: namespace: e2e-tests-deployment-cctzw, resource: bindings, ignored listing per whitelist
Jul 16 14:04:42.548: INFO: namespace e2e-tests-deployment-cctzw deletion completed in 8.345115801s

• [SLOW TEST:31.559 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:04:42.550: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-a8ac7648-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:04:42.649: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-nlzkn" to be "success or failure"
Jul 16 14:04:42.655: INFO: Pod "pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79451ms
Jul 16 14:04:44.659: INFO: Pod "pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554074s
Jul 16 14:04:46.662: INFO: Pod "pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013392825s
STEP: Saw pod success
Jul 16 14:04:46.663: INFO: Pod "pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:04:46.667: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:04:46.696: INFO: Waiting for pod pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:04:46.699: INFO: Pod pod-projected-configmaps-a8ad4766-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:04:46.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nlzkn" for this suite.
Jul 16 14:04:52.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:52.810: INFO: namespace: e2e-tests-projected-nlzkn, resource: bindings, ignored listing per whitelist
Jul 16 14:04:55.035: INFO: namespace e2e-tests-projected-nlzkn deletion completed in 8.331579876s

• [SLOW TEST:12.485 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:04:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-mxlwv/configmap-test-b022f70b-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:04:55.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-mxlwv" to be "success or failure"
Jul 16 14:04:55.175: INFO: Pod "pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.294943ms
Jul 16 14:04:57.180: INFO: Pod "pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013909839s
Jul 16 14:04:59.183: INFO: Pod "pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017273728s
STEP: Saw pod success
Jul 16 14:04:59.183: INFO: Pod "pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:04:59.186: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d container env-test: <nil>
STEP: delete the pod
Jul 16 14:04:59.211: INFO: Waiting for pod pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:04:59.215: INFO: Pod pod-configmaps-b0237710-a7d2-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:04:59.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mxlwv" for this suite.
Jul 16 14:05:05.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:05:05.260: INFO: namespace: e2e-tests-configmap-mxlwv, resource: bindings, ignored listing per whitelist
Jul 16 14:05:07.551: INFO: namespace e2e-tests-configmap-mxlwv deletion completed in 8.328258464s

• [SLOW TEST:12.515 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:05:07.552: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-b79ef30b-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating configMap with name cm-test-opt-upd-b79ef366-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b79ef30b-a7d2-11e9-9e69-7a63f556cc5d
STEP: Updating configmap cm-test-opt-upd-b79ef366-a7d2-11e9-9e69-7a63f556cc5d
STEP: Creating configMap with name cm-test-opt-create-b79ef381-a7d2-11e9-9e69-7a63f556cc5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:05:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ftqf6" for this suite.
Jul 16 14:05:37.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:05:39.439: INFO: namespace: e2e-tests-projected-ftqf6, resource: bindings, ignored listing per whitelist
Jul 16 14:05:40.246: INFO: namespace e2e-tests-projected-ftqf6 deletion completed in 26.340761663s

• [SLOW TEST:32.694 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:05:40.248: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-7bcvm
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-7bcvm
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-7bcvm
Jul 16 14:05:40.374: INFO: Found 0 stateful pods, waiting for 1
Jul 16 14:05:50.379: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 16 14:05:50.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:05:50.753: INFO: stderr: ""
Jul 16 14:05:50.753: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:05:50.753: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:05:50.757: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 16 14:06:00.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:06:00.763: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:06:00.785: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:00.785: INFO: ss-0  i-taxl6ow1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  }]
Jul 16 14:06:00.785: INFO: ss-1              Pending         []
Jul 16 14:06:00.785: INFO: 
Jul 16 14:06:00.785: INFO: StatefulSet ss has not reached scale 3, at 2
Jul 16 14:06:01.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992366985s
Jul 16 14:06:02.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98713928s
Jul 16 14:06:03.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978184529s
Jul 16 14:06:04.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9734602s
Jul 16 14:06:05.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969191963s
Jul 16 14:06:06.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965106042s
Jul 16 14:06:07.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96050107s
Jul 16 14:06:08.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956360782s
Jul 16 14:06:09.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.672726ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-7bcvm
Jul 16 14:06:10.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:06:11.185: INFO: stderr: ""
Jul 16 14:06:11.185: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:06:11.185: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:06:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:06:11.484: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jul 16 14:06:11.485: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:06:11.485: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:06:11.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:06:11.835: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jul 16 14:06:11.835: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:06:11.835: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:06:11.839: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 16 14:06:21.849: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:06:21.849: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:06:21.849: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 16 14:06:21.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:06:22.158: INFO: stderr: ""
Jul 16 14:06:22.158: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:06:22.158: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:06:22.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:06:22.515: INFO: stderr: ""
Jul 16 14:06:22.515: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:06:22.515: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:06:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:06:22.814: INFO: stderr: ""
Jul 16 14:06:22.814: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:06:22.814: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:06:22.814: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:06:22.817: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 16 14:06:32.824: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:06:32.824: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:06:32.824: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:06:32.840: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:32.840: INFO: ss-0  i-taxl6ow1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  }]
Jul 16 14:06:32.840: INFO: ss-1  i-7znxt5so  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:32.841: INFO: ss-2  i-taxl6ow1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:32.841: INFO: 
Jul 16 14:06:32.841: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 14:06:33.846: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:33.846: INFO: ss-0  i-taxl6ow1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  }]
Jul 16 14:06:33.846: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:33.846: INFO: ss-2  i-taxl6ow1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:33.846: INFO: 
Jul 16 14:06:33.846: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 14:06:34.850: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:34.850: INFO: ss-0  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:05:40 +0000 UTC  }]
Jul 16 14:06:34.850: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:34.850: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:34.850: INFO: 
Jul 16 14:06:34.850: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 14:06:35.854: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:35.854: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:35.854: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:35.854: INFO: 
Jul 16 14:06:35.854: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:36.858: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:36.858: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:36.858: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:36.858: INFO: 
Jul 16 14:06:36.858: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:37.863: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:37.863: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:37.863: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:37.863: INFO: 
Jul 16 14:06:37.863: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:38.867: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:38.867: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:38.867: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:38.867: INFO: 
Jul 16 14:06:38.867: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:39.872: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:39.872: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:39.872: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:39.872: INFO: 
Jul 16 14:06:39.872: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:40.877: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:40.877: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:40.877: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:40.877: INFO: 
Jul 16 14:06:40.877: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 14:06:41.881: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Jul 16 14:06:41.881: INFO: ss-1  i-7znxt5so  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:41.881: INFO: ss-2  i-taxl6ow1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:06:00 +0000 UTC  }]
Jul 16 14:06:41.881: INFO: 
Jul 16 14:06:41.882: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-7bcvm
Jul 16 14:06:42.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:06:43.064: INFO: rc: 1
Jul 16 14:06:43.064: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001b229f0 exit status 1 <nil> <nil> true [0xc00177a3e8 0xc00177a400 0xc00177a418] [0xc00177a3e8 0xc00177a400 0xc00177a418] [0xc00177a3f8 0xc00177a410] [0x92f8e0 0x92f8e0] 0xc00161eb40 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jul 16 14:06:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:06:53.166: INFO: rc: 1
Jul 16 14:06:53.166: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00199bfb0 exit status 1 <nil> <nil> true [0xc000cd8560 0xc000cd8578 0xc000cd85c8] [0xc000cd8560 0xc000cd8578 0xc000cd85c8] [0xc000cd8570 0xc000cd85b0] [0x92f8e0 0x92f8e0] 0xc0015cccc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:03.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:03.272: INFO: rc: 1
Jul 16 14:07:03.273: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000d94d20 exit status 1 <nil> <nil> true [0xc0003179e8 0xc000317a00 0xc000317a48] [0xc0003179e8 0xc000317a00 0xc000317a48] [0xc0003179f8 0xc000317a28] [0x92f8e0 0x92f8e0] 0xc0017cecc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:13.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:13.381: INFO: rc: 1
Jul 16 14:07:13.381: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000d95110 exit status 1 <nil> <nil> true [0xc000317a50 0xc000317a70 0xc000317ac8] [0xc000317a50 0xc000317a70 0xc000317ac8] [0xc000317a60 0xc000317aa8] [0x92f8e0 0x92f8e0] 0xc0017cf7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:23.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:23.472: INFO: rc: 1
Jul 16 14:07:23.472: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001b22de0 exit status 1 <nil> <nil> true [0xc00177a420 0xc00177a438 0xc00177a450] [0xc00177a420 0xc00177a438 0xc00177a450] [0xc00177a430 0xc00177a448] [0x92f8e0 0x92f8e0] 0xc00161f860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:33.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:33.567: INFO: rc: 1
Jul 16 14:07:33.567: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001b23470 exit status 1 <nil> <nil> true [0xc00177a458 0xc00177a470 0xc00177a488] [0xc00177a458 0xc00177a470 0xc00177a488] [0xc00177a468 0xc00177a480] [0x92f8e0 0x92f8e0] 0xc0000a6840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:43.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:43.670: INFO: rc: 1
Jul 16 14:07:43.670: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002326690 exit status 1 <nil> <nil> true [0xc00000e180 0xc00000e780 0xc00000e9e8] [0xc00000e180 0xc00000e780 0xc00000e9e8] [0xc00000e6f0 0xc00000e880] [0x92f8e0 0x92f8e0] 0xc001034240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:07:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:07:53.773: INFO: rc: 1
Jul 16 14:07:53.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fc4b0 exit status 1 <nil> <nil> true [0xc00038f0e0 0xc00038f150 0xc00038f2b8] [0xc00038f0e0 0xc00038f150 0xc00038f2b8] [0xc00038f130 0xc00038f1e8] [0x92f8e0 0x92f8e0] 0xc00161e120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:03.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:03.868: INFO: rc: 1
Jul 16 14:08:03.868: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002326a50 exit status 1 <nil> <nil> true [0xc00000ea28 0xc00000f270 0xc00000f448] [0xc00000ea28 0xc00000f270 0xc00000f448] [0xc00000f190 0xc00000f3d0] [0x92f8e0 0x92f8e0] 0xc001034540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:13.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:13.961: INFO: rc: 1
Jul 16 14:08:13.961: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fc8a0 exit status 1 <nil> <nil> true [0xc00038f2c8 0xc00038f348 0xc00038f3b0] [0xc00038f2c8 0xc00038f348 0xc00038f3b0] [0xc00038f328 0xc00038f380] [0x92f8e0 0x92f8e0] 0xc00161eb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:23.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:24.066: INFO: rc: 1
Jul 16 14:08:24.067: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00199a3f0 exit status 1 <nil> <nil> true [0xc000cd8000 0xc000cd8030 0xc000cd8048] [0xc000cd8000 0xc000cd8030 0xc000cd8048] [0xc000cd8028 0xc000cd8040] [0x92f8e0 0x92f8e0] 0xc001e16240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:34.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:34.176: INFO: rc: 1
Jul 16 14:08:34.176: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002326ed0 exit status 1 <nil> <nil> true [0xc00000f4b8 0xc00000f4f0 0xc00000f5e0] [0xc00000f4b8 0xc00000f4f0 0xc00000f5e0] [0xc00000f4e8 0xc00000f5b0] [0x92f8e0 0x92f8e0] 0xc0010348a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:44.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:44.268: INFO: rc: 1
Jul 16 14:08:44.268: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fcfc0 exit status 1 <nil> <nil> true [0xc00038f3d0 0xc00038f488 0xc00038f668] [0xc00038f3d0 0xc00038f488 0xc00038f668] [0xc00038f470 0xc00038f618] [0x92f8e0 0x92f8e0] 0xc00161f860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:08:54.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:08:54.361: INFO: rc: 1
Jul 16 14:08:54.362: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00199a810 exit status 1 <nil> <nil> true [0xc000cd8050 0xc000cd8090 0xc000cd80b8] [0xc000cd8050 0xc000cd8090 0xc000cd80b8] [0xc000cd8088 0xc000cd80b0] [0x92f8e0 0x92f8e0] 0xc001e16600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:04.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:04.453: INFO: rc: 1
Jul 16 14:09:04.453: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00199ac30 exit status 1 <nil> <nil> true [0xc000cd80c0 0xc000cd80d8 0xc000cd80f0] [0xc000cd80c0 0xc000cd80d8 0xc000cd80f0] [0xc000cd80d0 0xc000cd80e8] [0x92f8e0 0x92f8e0] 0xc001e16900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:14.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:14.542: INFO: rc: 1
Jul 16 14:09:14.542: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00199b080 exit status 1 <nil> <nil> true [0xc000cd80f8 0xc000cd8110 0xc000cd8128] [0xc000cd80f8 0xc000cd8110 0xc000cd8128] [0xc000cd8108 0xc000cd8120] [0x92f8e0 0x92f8e0] 0xc001e16c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:24.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:24.640: INFO: rc: 1
Jul 16 14:09:24.640: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0013e24b0 exit status 1 <nil> <nil> true [0xc00177a000 0xc00177a018 0xc00177a058] [0xc00177a000 0xc00177a018 0xc00177a058] [0xc00177a010 0xc00177a038] [0x92f8e0 0x92f8e0] 0xc0021b6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:34.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:34.753: INFO: rc: 1
Jul 16 14:09:34.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023272f0 exit status 1 <nil> <nil> true [0xc00000f630 0xc00000f720 0xc00000f760] [0xc00000f630 0xc00000f720 0xc00000f760] [0xc00000f670 0xc00000f740] [0x92f8e0 0x92f8e0] 0xc001034ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:44.841: INFO: rc: 1
Jul 16 14:09:44.841: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0013e2480 exit status 1 <nil> <nil> true [0xc00177a000 0xc00177a018 0xc00177a058] [0xc00177a000 0xc00177a018 0xc00177a058] [0xc00177a010 0xc00177a038] [0x92f8e0 0x92f8e0] 0xc0021b6180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:09:54.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:09:54.924: INFO: rc: 1
Jul 16 14:09:54.924: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fc480 exit status 1 <nil> <nil> true [0xc000cd8000 0xc000cd8030 0xc000cd8048] [0xc000cd8000 0xc000cd8030 0xc000cd8048] [0xc000cd8028 0xc000cd8040] [0x92f8e0 0x92f8e0] 0xc001e16240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:04.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:05.019: INFO: rc: 1
Jul 16 14:10:05.020: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fc870 exit status 1 <nil> <nil> true [0xc000cd8050 0xc000cd8090 0xc000cd80b8] [0xc000cd8050 0xc000cd8090 0xc000cd80b8] [0xc000cd8088 0xc000cd80b0] [0x92f8e0 0x92f8e0] 0xc001e16600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:15.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:15.133: INFO: rc: 1
Jul 16 14:10:15.133: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023266c0 exit status 1 <nil> <nil> true [0xc00038f0e0 0xc00038f150 0xc00038f2b8] [0xc00038f0e0 0xc00038f150 0xc00038f2b8] [0xc00038f130 0xc00038f1e8] [0x92f8e0 0x92f8e0] 0xc00161e5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:25.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:25.240: INFO: rc: 1
Jul 16 14:10:25.240: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0013e2a50 exit status 1 <nil> <nil> true [0xc00177a078 0xc00177a0a0 0xc00177a0b8] [0xc00177a078 0xc00177a0a0 0xc00177a0b8] [0xc00177a098 0xc00177a0b0] [0x92f8e0 0x92f8e0] 0xc0021b6cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:35.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:35.339: INFO: rc: 1
Jul 16 14:10:35.339: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fcff0 exit status 1 <nil> <nil> true [0xc000cd80c0 0xc000cd80d8 0xc000cd80f0] [0xc000cd80c0 0xc000cd80d8 0xc000cd80f0] [0xc000cd80d0 0xc000cd80e8] [0x92f8e0 0x92f8e0] 0xc001e16900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:45.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:45.429: INFO: rc: 1
Jul 16 14:10:45.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0013e3020 exit status 1 <nil> <nil> true [0xc00177a0c0 0xc00177a0d8 0xc00177a108] [0xc00177a0c0 0xc00177a0d8 0xc00177a108] [0xc00177a0d0 0xc00177a0f0] [0x92f8e0 0x92f8e0] 0xc0021b79e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:10:55.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:10:55.543: INFO: rc: 1
Jul 16 14:10:55.543: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002326ae0 exit status 1 <nil> <nil> true [0xc00038f2c8 0xc00038f348 0xc00038f3b0] [0xc00038f2c8 0xc00038f348 0xc00038f3b0] [0xc00038f328 0xc00038f380] [0x92f8e0 0x92f8e0] 0xc00161f3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:11:05.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:11:05.644: INFO: rc: 1
Jul 16 14:11:05.644: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002326fc0 exit status 1 <nil> <nil> true [0xc00038f3d0 0xc00038f488 0xc00038f668] [0xc00038f3d0 0xc00038f488 0xc00038f668] [0xc00038f470 0xc00038f618] [0x92f8e0 0x92f8e0] 0xc001034000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:11:15.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:11:15.752: INFO: rc: 1
Jul 16 14:11:15.752: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0013e34d0 exit status 1 <nil> <nil> true [0xc00177a118 0xc00177a170 0xc00177a188] [0xc00177a118 0xc00177a170 0xc00177a188] [0xc00177a158 0xc00177a180] [0x92f8e0 0x92f8e0] 0xc000726360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:11:25.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:11:25.837: INFO: rc: 1
Jul 16 14:11:25.837: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002327440 exit status 1 <nil> <nil> true [0xc00038f7d0 0xc00038fc70 0xc00038fdf0] [0xc00038f7d0 0xc00038fc70 0xc00038fdf0] [0xc00038fb68 0xc00038fdd8] [0x92f8e0 0x92f8e0] 0xc001034300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:11:35.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:11:35.952: INFO: rc: 1
Jul 16 14:11:35.953: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014fd410 exit status 1 <nil> <nil> true [0xc000cd80f8 0xc000cd8110 0xc000cd8128] [0xc000cd80f8 0xc000cd8110 0xc000cd8128] [0xc000cd8108 0xc000cd8120] [0x92f8e0 0x92f8e0] 0xc001e16c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jul 16 14:11:45.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-7bcvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:11:46.037: INFO: rc: 1
Jul 16 14:11:46.037: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jul 16 14:11:46.037: INFO: Scaling statefulset ss to 0
Jul 16 14:11:46.046: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 16 14:11:46.052: INFO: Deleting all statefulset in ns e2e-tests-statefulset-7bcvm
Jul 16 14:11:46.054: INFO: Scaling statefulset ss to 0
Jul 16 14:11:46.063: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:11:46.066: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:11:46.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-7bcvm" for this suite.
Jul 16 14:11:52.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:11:54.317: INFO: namespace: e2e-tests-statefulset-7bcvm, resource: bindings, ignored listing per whitelist
Jul 16 14:11:54.417: INFO: namespace e2e-tests-statefulset-7bcvm deletion completed in 8.330122664s

• [SLOW TEST:374.170 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:11:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:11:54.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-ddg7j" to be "success or failure"
Jul 16 14:11:54.545: INFO: Pod "downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.320601ms
Jul 16 14:11:56.549: INFO: Pod "downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010520106s
Jul 16 14:11:58.553: INFO: Pod "downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014277343s
STEP: Saw pod success
Jul 16 14:11:58.553: INFO: Pod "downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:11:58.556: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:11:58.587: INFO: Waiting for pod downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:11:58.590: INFO: Pod downwardapi-volume-aa181498-a7d3-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:11:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ddg7j" for this suite.
Jul 16 14:12:04.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:12:05.589: INFO: namespace: e2e-tests-downward-api-ddg7j, resource: bindings, ignored listing per whitelist
Jul 16 14:12:06.939: INFO: namespace e2e-tests-downward-api-ddg7j deletion completed in 8.335410605s

• [SLOW TEST:12.522 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:12:06.939: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jul 16 14:12:07.025: INFO: Waiting up to 5m0s for pod "downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-g78ls" to be "success or failure"
Jul 16 14:12:07.029: INFO: Pod "downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00205ms
Jul 16 14:12:09.036: INFO: Pod "downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011181389s
Jul 16 14:12:11.040: INFO: Pod "downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01450773s
STEP: Saw pod success
Jul 16 14:12:11.040: INFO: Pod "downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:12:11.042: INFO: Trying to get logs from node i-taxl6ow1 pod downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:12:11.066: INFO: Waiting for pod downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:12:11.069: INFO: Pod downward-api-b18c0cf2-a7d3-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:12:11.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-g78ls" for this suite.
Jul 16 14:12:17.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:12:19.061: INFO: namespace: e2e-tests-downward-api-g78ls, resource: bindings, ignored listing per whitelist
Jul 16 14:12:19.412: INFO: namespace e2e-tests-downward-api-g78ls deletion completed in 8.33585494s

• [SLOW TEST:12.472 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:12:19.412: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Jul 16 14:12:20.098: INFO: created pod pod-service-account-defaultsa
Jul 16 14:12:20.098: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 16 14:12:20.112: INFO: created pod pod-service-account-mountsa
Jul 16 14:12:20.113: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 16 14:12:20.118: INFO: created pod pod-service-account-nomountsa
Jul 16 14:12:20.118: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 16 14:12:20.126: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 16 14:12:20.126: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 16 14:12:20.140: INFO: created pod pod-service-account-mountsa-mountspec
Jul 16 14:12:20.140: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 16 14:12:20.165: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 16 14:12:20.165: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 16 14:12:20.178: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 16 14:12:20.179: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 16 14:12:20.184: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 16 14:12:20.184: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 16 14:12:20.190: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 16 14:12:20.190: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:12:20.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-mdkjj" for this suite.
Jul 16 14:12:44.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:12:44.280: INFO: namespace: e2e-tests-svcaccounts-mdkjj, resource: bindings, ignored listing per whitelist
Jul 16 14:12:46.528: INFO: namespace e2e-tests-svcaccounts-mdkjj deletion completed in 26.326663148s

• [SLOW TEST:27.116 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:12:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:12:48.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-rb98c" for this suite.
Jul 16 14:13:28.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:13:28.907: INFO: namespace: e2e-tests-kubelet-test-rb98c, resource: bindings, ignored listing per whitelist
Jul 16 14:13:30.957: INFO: namespace e2e-tests-kubelet-test-rb98c deletion completed in 42.325665965s

• [SLOW TEST:44.428 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:13:30.958: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-r6zww
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 14:13:31.041: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 14:13:55.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.100.78:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-r6zww PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:13:55.110: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 14:13:55.389: INFO: Found all expected endpoints: [netserver-0]
Jul 16 14:13:55.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.83.243:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-r6zww PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:13:55.392: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
Jul 16 14:13:55.615: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:13:55.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-r6zww" for this suite.
Jul 16 14:14:19.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:14:19.802: INFO: namespace: e2e-tests-pod-network-test-r6zww, resource: bindings, ignored listing per whitelist
Jul 16 14:14:21.952: INFO: namespace e2e-tests-pod-network-test-r6zww deletion completed in 26.327888708s

• [SLOW TEST:50.995 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:14:21.953: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jul 16 14:14:22.041: INFO: namespace e2e-tests-kubectl-zqvs7
Jul 16 14:14:22.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-zqvs7'
Jul 16 14:14:22.624: INFO: stderr: ""
Jul 16 14:14:22.624: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 14:14:23.628: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:14:23.628: INFO: Found 0 / 1
Jul 16 14:14:24.628: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:14:24.628: INFO: Found 0 / 1
Jul 16 14:14:25.629: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:14:25.630: INFO: Found 1 / 1
Jul 16 14:14:25.630: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 14:14:25.633: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:14:25.633: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 14:14:25.633: INFO: wait on redis-master startup in e2e-tests-kubectl-zqvs7 
Jul 16 14:14:25.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 logs redis-master-qp2f9 redis-master --namespace=e2e-tests-kubectl-zqvs7'
Jul 16 14:14:25.794: INFO: stderr: ""
Jul 16 14:14:25.794: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 14:14:24.147 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 14:14:24.148 # Server started, Redis version 3.2.12\n1:M 16 Jul 14:14:24.148 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 14:14:24.148 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 16 14:14:25.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-zqvs7'
Jul 16 14:14:25.919: INFO: stderr: ""
Jul 16 14:14:25.919: INFO: stdout: "service/rm2 exposed\n"
Jul 16 14:14:25.922: INFO: Service rm2 in namespace e2e-tests-kubectl-zqvs7 found.
STEP: exposing service
Jul 16 14:14:27.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-zqvs7'
Jul 16 14:14:28.090: INFO: stderr: ""
Jul 16 14:14:28.090: INFO: stdout: "service/rm3 exposed\n"
Jul 16 14:14:28.093: INFO: Service rm3 in namespace e2e-tests-kubectl-zqvs7 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:14:30.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zqvs7" for this suite.
Jul 16 14:14:54.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:14:56.256: INFO: namespace: e2e-tests-kubectl-zqvs7, resource: bindings, ignored listing per whitelist
Jul 16 14:14:56.457: INFO: namespace e2e-tests-kubectl-zqvs7 deletion completed in 26.339478034s

• [SLOW TEST:34.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:14:56.458: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jul 16 14:15:01.082: INFO: Successfully updated pod "labelsupdate16986874-a7d4-11e9-9e69-7a63f556cc5d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:15:03.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6hkcv" for this suite.
Jul 16 14:15:25.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:15:25.171: INFO: namespace: e2e-tests-downward-api-6hkcv, resource: bindings, ignored listing per whitelist
Jul 16 14:15:27.475: INFO: namespace e2e-tests-downward-api-6hkcv deletion completed in 24.332006385s

• [SLOW TEST:31.018 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:15:27.475: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 16 14:15:27.598: INFO: Waiting up to 5m0s for pod "pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-lpn7f" to be "success or failure"
Jul 16 14:15:27.601: INFO: Pod "pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.473192ms
Jul 16 14:15:29.607: INFO: Pod "pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009137612s
STEP: Saw pod success
Jul 16 14:15:29.607: INFO: Pod "pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:15:29.611: INFO: Trying to get logs from node i-taxl6ow1 pod pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 14:15:29.639: INFO: Waiting for pod pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:15:29.643: INFO: Pod pod-29188c04-a7d4-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:15:29.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lpn7f" for this suite.
Jul 16 14:15:35.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:15:36.561: INFO: namespace: e2e-tests-emptydir-lpn7f, resource: bindings, ignored listing per whitelist
Jul 16 14:15:38.011: INFO: namespace e2e-tests-emptydir-lpn7f deletion completed in 8.354541428s

• [SLOW TEST:10.536 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:15:38.011: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-2f60ca69-a7d4-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:15:38.140: INFO: Waiting up to 5m0s for pod "pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-8mp5s" to be "success or failure"
Jul 16 14:15:38.146: INFO: Pod "pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720685ms
Jul 16 14:15:40.157: INFO: Pod "pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01698337s
STEP: Saw pod success
Jul 16 14:15:40.162: INFO: Pod "pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:15:40.166: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:15:40.196: INFO: Waiting for pod pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:15:40.199: INFO: Pod pod-secrets-2f61646f-a7d4-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:15:40.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8mp5s" for this suite.
Jul 16 14:15:46.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:15:48.404: INFO: namespace: e2e-tests-secrets-8mp5s, resource: bindings, ignored listing per whitelist
Jul 16 14:15:48.553: INFO: namespace e2e-tests-secrets-8mp5s deletion completed in 8.347010197s

• [SLOW TEST:10.542 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:15:48.554: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-35a56cb6-a7d4-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:15:48.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-5r8tl" to be "success or failure"
Jul 16 14:15:48.660: INFO: Pod "pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818143ms
Jul 16 14:15:50.664: INFO: Pod "pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007513741s
STEP: Saw pod success
Jul 16 14:15:50.664: INFO: Pod "pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:15:50.667: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:15:50.711: INFO: Waiting for pod pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:15:50.715: INFO: Pod pod-projected-secrets-35a6068a-a7d4-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:15:50.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5r8tl" for this suite.
Jul 16 14:15:56.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:15:56.916: INFO: namespace: e2e-tests-projected-5r8tl, resource: bindings, ignored listing per whitelist
Jul 16 14:15:59.066: INFO: namespace e2e-tests-projected-5r8tl deletion completed in 8.343383379s

• [SLOW TEST:10.511 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:15:59.066: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 16 14:15:59.166: INFO: Waiting up to 5m0s for pod "pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-emptydir-cf6kx" to be "success or failure"
Jul 16 14:15:59.170: INFO: Pod "pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.833799ms
Jul 16 14:16:01.174: INFO: Pod "pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007453556s
Jul 16 14:16:03.179: INFO: Pod "pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012952032s
STEP: Saw pod success
Jul 16 14:16:03.180: INFO: Pod "pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:16:03.183: INFO: Trying to get logs from node i-taxl6ow1 pod pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 14:16:03.203: INFO: Waiting for pod pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:16:03.207: INFO: Pod pod-3bea2876-a7d4-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:16:03.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cf6kx" for this suite.
Jul 16 14:16:09.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:16:09.244: INFO: namespace: e2e-tests-emptydir-cf6kx, resource: bindings, ignored listing per whitelist
Jul 16 14:16:11.547: INFO: namespace e2e-tests-emptydir-cf6kx deletion completed in 8.333524256s

• [SLOW TEST:12.481 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:16:11.548: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:16:11.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-cqnnt" to be "success or failure"
Jul 16 14:16:11.659: INFO: Pod "downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853381ms
Jul 16 14:16:13.663: INFO: Pod "downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007260093s
Jul 16 14:16:15.667: INFO: Pod "downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011062805s
STEP: Saw pod success
Jul 16 14:16:15.667: INFO: Pod "downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:16:15.669: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:16:15.700: INFO: Waiting for pod downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:16:15.702: INFO: Pod downwardapi-volume-435b8df9-a7d4-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:16:15.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cqnnt" for this suite.
Jul 16 14:16:21.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:16:21.747: INFO: namespace: e2e-tests-projected-cqnnt, resource: bindings, ignored listing per whitelist
Jul 16 14:16:24.034: INFO: namespace e2e-tests-projected-cqnnt deletion completed in 8.326857763s

• [SLOW TEST:12.487 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:16:24.037: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jul 16 14:16:24.117: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:16:27.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-nt7ng" for this suite.
Jul 16 14:16:33.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:16:33.888: INFO: namespace: e2e-tests-init-container-nt7ng, resource: bindings, ignored listing per whitelist
Jul 16 14:16:36.038: INFO: namespace e2e-tests-init-container-nt7ng deletion completed in 8.330716274s

• [SLOW TEST:12.002 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:16:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0716 14:16:37.180961      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 14:16:37.181: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:16:37.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-r7tn8" for this suite.
Jul 16 14:16:43.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:16:44.362: INFO: namespace: e2e-tests-gc-r7tn8, resource: bindings, ignored listing per whitelist
Jul 16 14:16:45.513: INFO: namespace e2e-tests-gc-r7tn8 deletion completed in 8.327583214s

• [SLOW TEST:9.475 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:16:45.515: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:16:45.810: INFO: Pod name wrapped-volume-race-57b5d744-a7d4-11e9-9e69-7a63f556cc5d: Found 0 pods out of 5
Jul 16 14:16:50.830: INFO: Pod name wrapped-volume-race-57b5d744-a7d4-11e9-9e69-7a63f556cc5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-57b5d744-a7d4-11e9-9e69-7a63f556cc5d in namespace e2e-tests-emptydir-wrapper-69zfz, will wait for the garbage collector to delete the pods
Jul 16 14:18:52.937: INFO: Deleting ReplicationController wrapped-volume-race-57b5d744-a7d4-11e9-9e69-7a63f556cc5d took: 8.203871ms
Jul 16 14:18:53.037: INFO: Terminating ReplicationController wrapped-volume-race-57b5d744-a7d4-11e9-9e69-7a63f556cc5d pods took: 100.243952ms
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:19:36.160: INFO: Pod name wrapped-volume-race-bd3e24f0-a7d4-11e9-9e69-7a63f556cc5d: Found 0 pods out of 5
Jul 16 14:19:41.167: INFO: Pod name wrapped-volume-race-bd3e24f0-a7d4-11e9-9e69-7a63f556cc5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bd3e24f0-a7d4-11e9-9e69-7a63f556cc5d in namespace e2e-tests-emptydir-wrapper-69zfz, will wait for the garbage collector to delete the pods
Jul 16 14:22:07.252: INFO: Deleting ReplicationController wrapped-volume-race-bd3e24f0-a7d4-11e9-9e69-7a63f556cc5d took: 8.134661ms
Jul 16 14:22:07.352: INFO: Terminating ReplicationController wrapped-volume-race-bd3e24f0-a7d4-11e9-9e69-7a63f556cc5d pods took: 100.305275ms
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:23:01.977: INFO: Pod name wrapped-volume-race-37eb16e4-a7d5-11e9-9e69-7a63f556cc5d: Found 0 pods out of 5
Jul 16 14:23:06.985: INFO: Pod name wrapped-volume-race-37eb16e4-a7d5-11e9-9e69-7a63f556cc5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-37eb16e4-a7d5-11e9-9e69-7a63f556cc5d in namespace e2e-tests-emptydir-wrapper-69zfz, will wait for the garbage collector to delete the pods
Jul 16 14:26:19.061: INFO: Deleting ReplicationController wrapped-volume-race-37eb16e4-a7d5-11e9-9e69-7a63f556cc5d took: 6.972528ms
Jul 16 14:26:19.161: INFO: Terminating ReplicationController wrapped-volume-race-37eb16e4-a7d5-11e9-9e69-7a63f556cc5d pods took: 100.316949ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:27:17.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-69zfz" for this suite.
Jul 16 14:27:23.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:27:24.032: INFO: namespace: e2e-tests-emptydir-wrapper-69zfz, resource: bindings, ignored listing per whitelist
Jul 16 14:27:26.133: INFO: namespace e2e-tests-emptydir-wrapper-69zfz deletion completed in 8.339922502s

• [SLOW TEST:640.618 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:27:26.133: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-d56f53c9-a7d5-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:27:26.240: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-wwl87" to be "success or failure"
Jul 16 14:27:26.242: INFO: Pod "pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.708134ms
Jul 16 14:27:28.246: INFO: Pod "pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006407262s
STEP: Saw pod success
Jul 16 14:27:28.246: INFO: Pod "pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:27:28.249: INFO: Trying to get logs from node i-taxl6ow1 pod pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:27:28.268: INFO: Waiting for pod pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:27:28.271: INFO: Pod pod-projected-secrets-d56ffddb-a7d5-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:27:28.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wwl87" for this suite.
Jul 16 14:27:34.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:27:34.423: INFO: namespace: e2e-tests-projected-wwl87, resource: bindings, ignored listing per whitelist
Jul 16 14:27:36.606: INFO: namespace e2e-tests-projected-wwl87 deletion completed in 8.325824387s

• [SLOW TEST:10.473 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:27:36.606: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Jul 16 14:27:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:37.141: INFO: stderr: ""
Jul 16 14:27:37.141: INFO: stdout: "pod/pause created\n"
Jul 16 14:27:37.141: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 16 14:27:37.142: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-qgssc" to be "running and ready"
Jul 16 14:27:37.146: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.267249ms
Jul 16 14:27:39.156: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014660526s
Jul 16 14:27:39.156: INFO: Pod "pause" satisfied condition "running and ready"
Jul 16 14:27:39.156: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 16 14:27:39.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.286: INFO: stderr: ""
Jul 16 14:27:39.286: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 16 14:27:39.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pod pause -L testing-label --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.398: INFO: stderr: ""
Jul 16 14:27:39.398: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 16 14:27:39.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 label pods pause testing-label- --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.521: INFO: stderr: ""
Jul 16 14:27:39.521: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 16 14:27:39.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pod pause -L testing-label --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.618: INFO: stderr: ""
Jul 16 14:27:39.618: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Jul 16 14:27:39.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 14:27:39.719: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 16 14:27:39.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-qgssc'
Jul 16 14:27:39.829: INFO: stderr: "No resources found.\n"
Jul 16 14:27:39.829: INFO: stdout: ""
Jul 16 14:27:39.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -l name=pause --namespace=e2e-tests-kubectl-qgssc -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 14:27:39.930: INFO: stderr: ""
Jul 16 14:27:39.930: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:27:39.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qgssc" for this suite.
Jul 16 14:27:45.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:27:47.063: INFO: namespace: e2e-tests-kubectl-qgssc, resource: bindings, ignored listing per whitelist
Jul 16 14:27:48.263: INFO: namespace e2e-tests-kubectl-qgssc deletion completed in 8.329077192s

• [SLOW TEST:11.657 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:27:48.264: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-e2a59180-a7d5-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:27:48.402: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-8vvkv" to be "success or failure"
Jul 16 14:27:48.415: INFO: Pod "pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.336593ms
Jul 16 14:27:50.419: INFO: Pod "pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016382165s
Jul 16 14:27:52.423: INFO: Pod "pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020080042s
STEP: Saw pod success
Jul 16 14:27:52.423: INFO: Pod "pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:27:52.426: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:27:52.450: INFO: Waiting for pod pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:27:52.454: INFO: Pod pod-configmaps-e2a66da5-a7d5-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:27:52.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8vvkv" for this suite.
Jul 16 14:27:58.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:27:58.574: INFO: namespace: e2e-tests-configmap-8vvkv, resource: bindings, ignored listing per whitelist
Jul 16 14:28:00.785: INFO: namespace e2e-tests-configmap-8vvkv deletion completed in 8.32576317s

• [SLOW TEST:12.522 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:28:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Jul 16 14:28:00.872: INFO: Waiting up to 5m0s for pod "client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-containers-2tq58" to be "success or failure"
Jul 16 14:28:00.875: INFO: Pod "client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.539385ms
Jul 16 14:28:02.879: INFO: Pod "client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006723928s
Jul 16 14:28:04.883: INFO: Pod "client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010654882s
STEP: Saw pod success
Jul 16 14:28:04.883: INFO: Pod "client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:28:04.891: INFO: Trying to get logs from node i-taxl6ow1 pod client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d container test-container: <nil>
STEP: delete the pod
Jul 16 14:28:04.914: INFO: Waiting for pod client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:28:04.918: INFO: Pod client-containers-ea157fca-a7d5-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:28:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2tq58" for this suite.
Jul 16 14:28:10.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:28:11.804: INFO: namespace: e2e-tests-containers-2tq58, resource: bindings, ignored listing per whitelist
Jul 16 14:28:13.254: INFO: namespace e2e-tests-containers-2tq58 deletion completed in 8.332171366s

• [SLOW TEST:12.468 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:28:13.255: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jul 16 14:28:13.331: INFO: PodSpec: initContainers in spec.initContainers
Jul 16 14:29:00.204: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f18382c5-a7d5-11e9-9e69-7a63f556cc5d", GenerateName:"", Namespace:"e2e-tests-init-container-xqps7", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-xqps7/pods/pod-init-f18382c5-a7d5-11e9-9e69-7a63f556cc5d", UID:"f183e28d-a7d5-11e9-a7d7-5254228aab6e", ResourceVersion:"28705", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63698884093, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"331123835"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-skjrb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001ea4200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skjrb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skjrb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skjrb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00224d388), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"i-taxl6ow1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0016340c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00224d580)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00224d5a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00224d5a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00224d5ac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698884093, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698884093, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698884093, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698884093, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.7", PodIP:"10.233.100.109", StartTime:(*v1.Time)(0xc00094b5e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001139490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001139500)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://fad0ac8509f2ea1f1a72069cba44fe5c8dbed0e405aef910976a72c0f6b386b8"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00094b660), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00094b620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:29:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xqps7" for this suite.
Jul 16 14:29:22.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:29:24.403: INFO: namespace: e2e-tests-init-container-xqps7, resource: bindings, ignored listing per whitelist
Jul 16 14:29:24.553: INFO: namespace e2e-tests-init-container-xqps7 deletion completed in 24.340722893s

• [SLOW TEST:71.298 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:29:24.554: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 14:29:24.658: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 16 14:29:24.665: INFO: Number of nodes with available pods: 0
Jul 16 14:29:24.665: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 16 14:29:24.685: INFO: Number of nodes with available pods: 0
Jul 16 14:29:24.685: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:25.690: INFO: Number of nodes with available pods: 0
Jul 16 14:29:25.690: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:26.689: INFO: Number of nodes with available pods: 0
Jul 16 14:29:26.689: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:27.689: INFO: Number of nodes with available pods: 1
Jul 16 14:29:27.689: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 16 14:29:27.703: INFO: Number of nodes with available pods: 1
Jul 16 14:29:27.703: INFO: Number of running nodes: 0, number of available pods: 1
Jul 16 14:29:28.707: INFO: Number of nodes with available pods: 0
Jul 16 14:29:28.707: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 16 14:29:28.716: INFO: Number of nodes with available pods: 0
Jul 16 14:29:28.716: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:29.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:29.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:30.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:30.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:31.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:31.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:32.721: INFO: Number of nodes with available pods: 0
Jul 16 14:29:32.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:33.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:33.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:34.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:34.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:35.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:35.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:36.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:36.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:37.722: INFO: Number of nodes with available pods: 0
Jul 16 14:29:37.722: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:38.721: INFO: Number of nodes with available pods: 0
Jul 16 14:29:38.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:39.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:39.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:40.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:40.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:41.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:41.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:42.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:42.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:43.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:43.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:44.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:44.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:45.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:45.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:46.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:46.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:47.721: INFO: Number of nodes with available pods: 0
Jul 16 14:29:47.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:48.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:48.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:49.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:49.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:50.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:50.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:51.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:51.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:52.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:52.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:53.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:53.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:54.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:54.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:55.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:55.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:56.721: INFO: Number of nodes with available pods: 0
Jul 16 14:29:56.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:57.725: INFO: Number of nodes with available pods: 0
Jul 16 14:29:57.725: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:58.720: INFO: Number of nodes with available pods: 0
Jul 16 14:29:58.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:29:59.719: INFO: Number of nodes with available pods: 0
Jul 16 14:29:59.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:30:00.721: INFO: Number of nodes with available pods: 0
Jul 16 14:30:00.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:30:01.723: INFO: Number of nodes with available pods: 0
Jul 16 14:30:01.723: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:30:02.721: INFO: Number of nodes with available pods: 0
Jul 16 14:30:02.721: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:30:03.720: INFO: Number of nodes with available pods: 0
Jul 16 14:30:03.720: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:30:04.720: INFO: Number of nodes with available pods: 1
Jul 16 14:30:04.720: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-mlbzj, will wait for the garbage collector to delete the pods
Jul 16 14:30:04.788: INFO: Deleting DaemonSet.extensions daemon-set took: 10.04347ms
Jul 16 14:30:04.889: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.97424ms
Jul 16 14:30:45.792: INFO: Number of nodes with available pods: 0
Jul 16 14:30:45.793: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 14:30:45.795: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mlbzj/daemonsets","resourceVersion":"28949"},"items":null}

Jul 16 14:30:45.797: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mlbzj/pods","resourceVersion":"28949"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:30:45.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mlbzj" for this suite.
Jul 16 14:30:51.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:30:51.879: INFO: namespace: e2e-tests-daemonsets-mlbzj, resource: bindings, ignored listing per whitelist
Jul 16 14:30:54.158: INFO: namespace e2e-tests-daemonsets-mlbzj deletion completed in 8.336582413s

• [SLOW TEST:89.605 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:30:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:30:54.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-fbwsx" to be "success or failure"
Jul 16 14:30:54.271: INFO: Pod "downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996143ms
Jul 16 14:30:56.275: INFO: Pod "downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009298704s
STEP: Saw pod success
Jul 16 14:30:56.275: INFO: Pod "downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:30:56.281: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:30:56.304: INFO: Waiting for pod downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:30:56.307: INFO: Pod downwardapi-volume-516ec5ab-a7d6-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:30:56.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fbwsx" for this suite.
Jul 16 14:31:02.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:02.422: INFO: namespace: e2e-tests-projected-fbwsx, resource: bindings, ignored listing per whitelist
Jul 16 14:31:04.653: INFO: namespace e2e-tests-projected-fbwsx deletion completed in 8.341782282s

• [SLOW TEST:10.494 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:31:04.655: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:31:04.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-f4tl4" for this suite.
Jul 16 14:31:10.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:10.917: INFO: namespace: e2e-tests-kubelet-test-f4tl4, resource: bindings, ignored listing per whitelist
Jul 16 14:31:13.092: INFO: namespace e2e-tests-kubelet-test-f4tl4 deletion completed in 8.331353669s

• [SLOW TEST:8.438 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:31:13.093: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Jul 16 14:31:13.191: INFO: Waiting up to 5m0s for pod "var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-var-expansion-qc987" to be "success or failure"
Jul 16 14:31:13.194: INFO: Pod "var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116609ms
Jul 16 14:31:15.198: INFO: Pod "var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007150251s
Jul 16 14:31:17.203: INFO: Pod "var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011663666s
STEP: Saw pod success
Jul 16 14:31:17.203: INFO: Pod "var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:31:17.214: INFO: Trying to get logs from node i-taxl6ow1 pod var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:31:17.248: INFO: Waiting for pod var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:31:17.251: INFO: Pod var-expansion-5cb6cdf6-a7d6-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:31:17.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qc987" for this suite.
Jul 16 14:31:23.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:24.641: INFO: namespace: e2e-tests-var-expansion-qc987, resource: bindings, ignored listing per whitelist
Jul 16 14:31:25.587: INFO: namespace e2e-tests-var-expansion-qc987 deletion completed in 8.329262252s

• [SLOW TEST:12.495 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:31:25.588: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tg69x in namespace e2e-tests-proxy-pfgb8
I0716 14:31:25.692946      18 runners.go:184] Created replication controller with name: proxy-service-tg69x, namespace: e2e-tests-proxy-pfgb8, replica count: 1
I0716 14:31:26.743584      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 14:31:27.743822      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 14:31:28.744045      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 14:31:29.744292      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 14:31:30.744864      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:31.745136      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:32.745429      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:33.745731      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:34.746141      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:35.746816      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 14:31:36.747242      18 runners.go:184] proxy-service-tg69x Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 16 14:31:36.750: INFO: setup took 11.069767609s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 16 14:31:36.758: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 6.988983ms)
Jul 16 14:31:36.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 9.121177ms)
Jul 16 14:31:36.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.164891ms)
Jul 16 14:31:36.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 9.383318ms)
Jul 16 14:31:36.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 9.997321ms)
Jul 16 14:31:36.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 10.262681ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 26.855389ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 27.085468ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 26.945694ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 26.852806ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 27.077137ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 27.081988ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 26.859417ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 27.280513ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 26.667631ms)
Jul 16 14:31:36.778: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 27.283574ms)
Jul 16 14:31:36.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 7.588136ms)
Jul 16 14:31:36.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 7.665373ms)
Jul 16 14:31:36.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 7.343286ms)
Jul 16 14:31:36.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 7.390582ms)
Jul 16 14:31:36.787: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 8.314768ms)
Jul 16 14:31:36.787: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.515741ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 13.038851ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 13.072988ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 13.197286ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 13.088993ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 13.287447ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 13.219742ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 13.525163ms)
Jul 16 14:31:36.792: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 13.262712ms)
Jul 16 14:31:36.795: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 15.665426ms)
Jul 16 14:31:36.795: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 16.127243ms)
Jul 16 14:31:36.808: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 12.734034ms)
Jul 16 14:31:36.811: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 15.591166ms)
Jul 16 14:31:36.811: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 15.954446ms)
Jul 16 14:31:36.819: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 22.738314ms)
Jul 16 14:31:36.819: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 22.863975ms)
Jul 16 14:31:36.819: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 23.401505ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 23.900283ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 24.051935ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 24.15442ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 23.906188ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 24.068193ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 23.6777ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 24.072175ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 24.152742ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 24.075439ms)
Jul 16 14:31:36.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 24.206276ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 12.71686ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 12.262742ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 12.342759ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 12.28163ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 12.300206ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 12.356623ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 12.276685ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 12.292985ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 12.676256ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 12.413927ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 12.403873ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 12.839571ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 12.406332ms)
Jul 16 14:31:36.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 12.438513ms)
Jul 16 14:31:36.834: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 12.388362ms)
Jul 16 14:31:36.834: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 12.427301ms)
Jul 16 14:31:36.838: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 4.222088ms)
Jul 16 14:31:36.838: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 3.886604ms)
Jul 16 14:31:36.839: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 4.767626ms)
Jul 16 14:31:36.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 7.896329ms)
Jul 16 14:31:36.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 7.920211ms)
Jul 16 14:31:36.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 7.998554ms)
Jul 16 14:31:36.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 8.386333ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 8.483633ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 8.448414ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 8.545893ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 8.874431ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 8.825039ms)
Jul 16 14:31:36.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 8.759232ms)
Jul 16 14:31:36.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 9.895191ms)
Jul 16 14:31:36.845: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 10.318515ms)
Jul 16 14:31:36.846: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 11.96899ms)
Jul 16 14:31:36.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 6.806405ms)
Jul 16 14:31:36.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 6.627294ms)
Jul 16 14:31:36.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 6.551725ms)
Jul 16 14:31:36.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 6.491384ms)
Jul 16 14:31:36.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 4.816938ms)
Jul 16 14:31:36.855: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 8.278838ms)
Jul 16 14:31:36.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 7.833693ms)
Jul 16 14:31:36.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 9.923442ms)
Jul 16 14:31:36.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 8.769711ms)
Jul 16 14:31:36.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 8.699581ms)
Jul 16 14:31:36.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 8.233195ms)
Jul 16 14:31:36.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 7.890318ms)
Jul 16 14:31:36.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 8.7711ms)
Jul 16 14:31:36.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 11.804092ms)
Jul 16 14:31:36.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 10.11755ms)
Jul 16 14:31:36.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 10.078862ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 6.271281ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 6.380106ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 6.280289ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 6.256531ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 6.384206ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 6.816486ms)
Jul 16 14:31:36.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 6.615308ms)
Jul 16 14:31:36.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 6.398273ms)
Jul 16 14:31:36.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 6.768955ms)
Jul 16 14:31:36.867: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 8.57455ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 8.639036ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.81952ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 9.245781ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 9.278216ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 9.072741ms)
Jul 16 14:31:36.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 9.092097ms)
Jul 16 14:31:36.877: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 8.987228ms)
Jul 16 14:31:36.877: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 8.717832ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.010336ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 9.439609ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 8.242583ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.317407ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.043547ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 9.260554ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 9.603094ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 9.260425ms)
Jul 16 14:31:36.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 9.337447ms)
Jul 16 14:31:36.880: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 11.051308ms)
Jul 16 14:31:36.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 20.97426ms)
Jul 16 14:31:36.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 21.664477ms)
Jul 16 14:31:36.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 20.807481ms)
Jul 16 14:31:36.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 21.54176ms)
Jul 16 14:31:36.896: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 5.568312ms)
Jul 16 14:31:36.897: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 6.464132ms)
Jul 16 14:31:36.897: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 6.892597ms)
Jul 16 14:31:36.898: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 7.075858ms)
Jul 16 14:31:36.898: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 7.21612ms)
Jul 16 14:31:36.900: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 8.648543ms)
Jul 16 14:31:36.900: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 8.682809ms)
Jul 16 14:31:36.900: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 10.120413ms)
Jul 16 14:31:36.901: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 9.33626ms)
Jul 16 14:31:36.901: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 9.542899ms)
Jul 16 14:31:36.901: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.922462ms)
Jul 16 14:31:36.902: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 10.90763ms)
Jul 16 14:31:36.902: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 11.167771ms)
Jul 16 14:31:36.902: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 11.819809ms)
Jul 16 14:31:36.902: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 11.622213ms)
Jul 16 14:31:36.903: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 12.187518ms)
Jul 16 14:31:36.912: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.073553ms)
Jul 16 14:31:36.912: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 9.299514ms)
Jul 16 14:31:36.913: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 9.3692ms)
Jul 16 14:31:36.913: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.558507ms)
Jul 16 14:31:36.913: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 9.474809ms)
Jul 16 14:31:36.913: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 9.519886ms)
Jul 16 14:31:36.918: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 15.024468ms)
Jul 16 14:31:36.918: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 15.129556ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 15.259929ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 15.350946ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 15.359118ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 15.382411ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 15.391331ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 15.106077ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 15.826282ms)
Jul 16 14:31:36.919: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 15.927995ms)
Jul 16 14:31:36.923: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 3.769076ms)
Jul 16 14:31:36.925: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 5.372482ms)
Jul 16 14:31:36.925: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 5.952832ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 11.59456ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 11.608308ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 11.757714ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 11.687231ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 11.801553ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 11.715086ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 11.870191ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 11.779239ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 11.784016ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 11.849607ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 11.850135ms)
Jul 16 14:31:36.931: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 11.916344ms)
Jul 16 14:31:36.933: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 13.177351ms)
Jul 16 14:31:36.939: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 6.499406ms)
Jul 16 14:31:36.939: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 6.437459ms)
Jul 16 14:31:36.939: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 6.616806ms)
Jul 16 14:31:36.939: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 6.46656ms)
Jul 16 14:31:36.939: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 6.617125ms)
Jul 16 14:31:36.941: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 8.521614ms)
Jul 16 14:31:36.942: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 8.974161ms)
Jul 16 14:31:36.942: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 9.408997ms)
Jul 16 14:31:36.945: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 11.911574ms)
Jul 16 14:31:36.945: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 12.154748ms)
Jul 16 14:31:36.945: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 11.955359ms)
Jul 16 14:31:36.945: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 12.049079ms)
Jul 16 14:31:36.945: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 12.106941ms)
Jul 16 14:31:36.946: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 12.810551ms)
Jul 16 14:31:36.946: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 13.167514ms)
Jul 16 14:31:36.946: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 13.189424ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 13.520788ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 13.477719ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 13.461075ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 13.904472ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 14.075391ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 13.990607ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 14.228459ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 14.173066ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 14.187158ms)
Jul 16 14:31:36.960: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 14.197452ms)
Jul 16 14:31:36.961: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 14.2593ms)
Jul 16 14:31:36.961: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 14.454272ms)
Jul 16 14:31:36.962: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 16.145397ms)
Jul 16 14:31:36.962: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 16.198265ms)
Jul 16 14:31:36.962: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 16.20696ms)
Jul 16 14:31:36.962: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 16.30324ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 5.998916ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 6.074866ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 6.210969ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 6.144176ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 6.287325ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 6.255621ms)
Jul 16 14:31:36.969: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 6.173027ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.271212ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 8.529166ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 8.697803ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 8.825833ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 8.491616ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 8.634713ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 8.856904ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 8.517803ms)
Jul 16 14:31:36.972: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.581314ms)
Jul 16 14:31:36.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 7.402587ms)
Jul 16 14:31:36.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 7.543237ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 7.469966ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 8.732899ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 8.330472ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 7.49046ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 7.445536ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 8.254004ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 8.184587ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 7.712057ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 8.639193ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 8.13246ms)
Jul 16 14:31:36.981: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 8.31429ms)
Jul 16 14:31:36.983: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 10.465991ms)
Jul 16 14:31:36.983: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 10.960174ms)
Jul 16 14:31:36.983: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 10.711417ms)
Jul 16 14:31:36.986: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 2.754861ms)
Jul 16 14:31:36.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.490953ms)
Jul 16 14:31:36.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 9.247023ms)
Jul 16 14:31:36.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 9.599827ms)
Jul 16 14:31:36.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 9.259347ms)
Jul 16 14:31:36.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 10.166287ms)
Jul 16 14:31:36.995: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 9.444189ms)
Jul 16 14:31:36.995: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 9.998548ms)
Jul 16 14:31:36.995: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 10.000593ms)
Jul 16 14:31:36.995: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 11.04396ms)
Jul 16 14:31:36.997: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 12.861458ms)
Jul 16 14:31:36.997: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 12.404365ms)
Jul 16 14:31:36.997: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 13.503317ms)
Jul 16 14:31:36.997: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 13.422945ms)
Jul 16 14:31:36.997: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 13.097822ms)
Jul 16 14:31:36.998: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 13.252057ms)
Jul 16 14:31:37.002: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 4.300014ms)
Jul 16 14:31:37.004: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 5.858807ms)
Jul 16 14:31:37.007: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 8.739943ms)
Jul 16 14:31:37.007: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 8.799659ms)
Jul 16 14:31:37.007: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 8.852018ms)
Jul 16 14:31:37.011: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 13.117839ms)
Jul 16 14:31:37.013: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 14.961483ms)
Jul 16 14:31:37.013: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 15.218905ms)
Jul 16 14:31:37.014: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 16.120101ms)
Jul 16 14:31:37.025: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 26.465028ms)
Jul 16 14:31:37.026: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 27.955063ms)
Jul 16 14:31:37.026: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 27.973527ms)
Jul 16 14:31:37.026: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 27.953267ms)
Jul 16 14:31:37.026: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 28.000782ms)
Jul 16 14:31:37.026: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 28.109952ms)
Jul 16 14:31:37.027: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 28.188553ms)
Jul 16 14:31:37.033: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 6.028219ms)
Jul 16 14:31:37.033: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 6.261132ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 12.691032ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 12.799628ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 13.010338ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 13.049164ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 12.925504ms)
Jul 16 14:31:37.040: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 13.428212ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 13.576701ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 13.422776ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 13.506914ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 13.828365ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 13.538892ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 13.648149ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 13.717172ms)
Jul 16 14:31:37.041: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 13.568177ms)
Jul 16 14:31:37.047: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 6.313294ms)
Jul 16 14:31:37.060: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 18.725422ms)
Jul 16 14:31:37.061: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 19.490567ms)
Jul 16 14:31:37.061: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 19.033588ms)
Jul 16 14:31:37.061: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 18.820184ms)
Jul 16 14:31:37.061: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 19.508662ms)
Jul 16 14:31:37.061: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 18.851355ms)
Jul 16 14:31:37.064: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 22.015135ms)
Jul 16 14:31:37.064: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 22.193717ms)
Jul 16 14:31:37.064: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 21.875236ms)
Jul 16 14:31:37.065: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 22.653587ms)
Jul 16 14:31:37.073: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 31.575963ms)
Jul 16 14:31:37.073: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 31.844026ms)
Jul 16 14:31:37.073: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 31.53344ms)
Jul 16 14:31:37.074: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 32.090244ms)
Jul 16 14:31:37.078: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 36.33939ms)
Jul 16 14:31:37.085: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:443/proxy/... (200; 7.306487ms)
Jul 16 14:31:37.086: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx/proxy/rewriteme"... (200; 7.524126ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname2/proxy/: bar (200; 51.567617ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/proxy-service-tg69x:portname1/proxy/: foo (200; 50.974646ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname2/proxy/: bar (200; 50.459089ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname2/proxy/: tls qux (200; 50.299983ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/http:proxy-service-tg69x:portname1/proxy/: foo (200; 51.742431ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/services/https:proxy-service-tg69x:tlsportname1/proxy/: tls baz (200; 50.971308ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:462/proxy/: tls qux (200; 50.630982ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:162/proxy/: bar (200; 50.95488ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:1080/proxy/rewri... (200; 50.377132ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:1080/proxy/... (200; 51.551996ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:160/proxy/: foo (200; 50.476649ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/proxy-service-tg69x-hggcx:160/proxy/: foo (200; 51.718554ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/http:proxy-service-tg69x-hggcx:162/proxy/: bar (200; 51.052776ms)
Jul 16 14:31:37.130: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-pfgb8/pods/https:proxy-service-tg69x-hggcx:460/proxy/: tls baz (200; 51.545544ms)
STEP: deleting ReplicationController proxy-service-tg69x in namespace e2e-tests-proxy-pfgb8, will wait for the garbage collector to delete the pods
Jul 16 14:31:37.195: INFO: Deleting ReplicationController proxy-service-tg69x took: 9.415274ms
Jul 16 14:31:37.295: INFO: Terminating ReplicationController proxy-service-tg69x pods took: 100.289599ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:31:45.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-pfgb8" for this suite.
Jul 16 14:31:51.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:51.743: INFO: namespace: e2e-tests-proxy-pfgb8, resource: bindings, ignored listing per whitelist
Jul 16 14:31:51.966: INFO: namespace e2e-tests-proxy-pfgb8 deletion completed in 6.264512392s

• [SLOW TEST:26.379 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:31:51.967: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:31:52.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:31:52.188: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 14:31:52.188: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 16 14:31:52.197: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 16 14:31:52.209: INFO: scanned /root for discovery docs: <nil>
Jul 16 14:31:52.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:05.093: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 16 14:32:05.093: INFO: stdout: "Created e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb\nScaling up e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 16 14:32:05.093: INFO: stdout: "Created e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb\nScaling up e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 16 14:32:05.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:05.232: INFO: stderr: ""
Jul 16 14:32:05.232: INFO: stdout: "e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb-47hq4 e2e-test-nginx-rc-hbg6h "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jul 16 14:32:10.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:10.345: INFO: stderr: ""
Jul 16 14:32:10.345: INFO: stdout: "e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb-47hq4 "
Jul 16 14:32:10.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb-47hq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:10.453: INFO: stderr: ""
Jul 16 14:32:10.453: INFO: stdout: "true"
Jul 16 14:32:10.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb-47hq4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:10.563: INFO: stderr: ""
Jul 16 14:32:10.563: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 16 14:32:10.563: INFO: e2e-test-nginx-rc-2074a0106cb77311749470bb517de7bb-47hq4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Jul 16 14:32:10.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-kxfqr'
Jul 16 14:32:10.699: INFO: stderr: ""
Jul 16 14:32:10.699: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:32:10.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kxfqr" for this suite.
Jul 16 14:32:16.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:32:16.754: INFO: namespace: e2e-tests-kubectl-kxfqr, resource: bindings, ignored listing per whitelist
Jul 16 14:32:19.034: INFO: namespace e2e-tests-kubectl-kxfqr deletion completed in 8.326241759s

• [SLOW TEST:27.067 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:32:19.036: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-b22k7
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-b22k7
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-b22k7
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-b22k7
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-b22k7
Jul 16 14:32:23.160: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b22k7, name: ss-0, uid: 857a546b-a7d6-11e9-a7d7-5254228aab6e, status phase: Pending. Waiting for statefulset controller to delete.
Jul 16 14:32:24.160: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b22k7, name: ss-0, uid: 857a546b-a7d6-11e9-a7d7-5254228aab6e, status phase: Failed. Waiting for statefulset controller to delete.
Jul 16 14:32:24.169: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b22k7, name: ss-0, uid: 857a546b-a7d6-11e9-a7d7-5254228aab6e, status phase: Failed. Waiting for statefulset controller to delete.
Jul 16 14:32:24.169: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-b22k7
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-b22k7
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-b22k7 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 16 14:32:28.189: INFO: Deleting all statefulset in ns e2e-tests-statefulset-b22k7
Jul 16 14:32:28.192: INFO: Scaling statefulset ss to 0
Jul 16 14:32:38.214: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:32:38.218: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:32:38.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-b22k7" for this suite.
Jul 16 14:32:44.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:32:44.384: INFO: namespace: e2e-tests-statefulset-b22k7, resource: bindings, ignored listing per whitelist
Jul 16 14:32:46.596: INFO: namespace e2e-tests-statefulset-b22k7 deletion completed in 8.351791539s

• [SLOW TEST:27.560 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:32:46.596: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-hprgd/secret-test-94725f99-a7d6-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:32:46.699: INFO: Waiting up to 5m0s for pod "pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-hprgd" to be "success or failure"
Jul 16 14:32:46.704: INFO: Pod "pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233954ms
Jul 16 14:32:48.708: INFO: Pod "pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00890031s
Jul 16 14:32:50.712: INFO: Pod "pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012704341s
STEP: Saw pod success
Jul 16 14:32:50.712: INFO: Pod "pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:32:50.715: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d container env-test: <nil>
STEP: delete the pod
Jul 16 14:32:50.744: INFO: Waiting for pod pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:32:50.747: INFO: Pod pod-configmaps-9472d32d-a7d6-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:32:50.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hprgd" for this suite.
Jul 16 14:32:56.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:32:56.878: INFO: namespace: e2e-tests-secrets-hprgd, resource: bindings, ignored listing per whitelist
Jul 16 14:32:59.094: INFO: namespace e2e-tests-secrets-hprgd deletion completed in 8.341881321s

• [SLOW TEST:12.498 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:32:59.094: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d
Jul 16 14:32:59.227: INFO: Pod name my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d: Found 0 pods out of 1
Jul 16 14:33:04.237: INFO: Pod name my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d: Found 1 pods out of 1
Jul 16 14:33:04.237: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d" are running
Jul 16 14:33:04.240: INFO: Pod "my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d-r5lg7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 14:32:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 14:33:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 14:33:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 14:32:59 +0000 UTC Reason: Message:}])
Jul 16 14:33:04.240: INFO: Trying to dial the pod
Jul 16 14:33:09.252: INFO: Controller my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d: Got expected result from replica 1 [my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d-r5lg7]: "my-hostname-basic-9be9c66e-a7d6-11e9-9e69-7a63f556cc5d-r5lg7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:33:09.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-v6j5f" for this suite.
Jul 16 14:33:15.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:33:15.340: INFO: namespace: e2e-tests-replication-controller-v6j5f, resource: bindings, ignored listing per whitelist
Jul 16 14:33:17.587: INFO: namespace e2e-tests-replication-controller-v6j5f deletion completed in 8.329296159s

• [SLOW TEST:18.493 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:33:17.587: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:33:17.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-nwmhg'
Jul 16 14:33:17.840: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 14:33:17.840: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Jul 16 14:33:17.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-nwmhg'
Jul 16 14:33:17.977: INFO: stderr: ""
Jul 16 14:33:17.977: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:33:17.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nwmhg" for this suite.
Jul 16 14:33:24.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:33:24.084: INFO: namespace: e2e-tests-kubectl-nwmhg, resource: bindings, ignored listing per whitelist
Jul 16 14:33:26.316: INFO: namespace e2e-tests-kubectl-nwmhg deletion completed in 8.33054143s

• [SLOW TEST:8.729 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:33:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-rd67x
I0716 14:33:26.396669      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-rd67x, replica count: 1
I0716 14:33:27.447143      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 14:33:28.447584      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 16 14:33:28.573: INFO: Created: latency-svc-nztd5
Jul 16 14:33:28.589: INFO: Got endpoints: latency-svc-nztd5 [41.14638ms]
Jul 16 14:33:28.598: INFO: Created: latency-svc-8rzf7
Jul 16 14:33:28.602: INFO: Created: latency-svc-ksb49
Jul 16 14:33:28.604: INFO: Created: latency-svc-5bkrg
Jul 16 14:33:28.607: INFO: Got endpoints: latency-svc-5bkrg [17.786736ms]
Jul 16 14:33:28.607: INFO: Got endpoints: latency-svc-ksb49 [17.780943ms]
Jul 16 14:33:28.607: INFO: Got endpoints: latency-svc-8rzf7 [18.310598ms]
Jul 16 14:33:28.609: INFO: Created: latency-svc-jr2bm
Jul 16 14:33:28.612: INFO: Got endpoints: latency-svc-jr2bm [22.843088ms]
Jul 16 14:33:28.613: INFO: Created: latency-svc-vzrww
Jul 16 14:33:28.618: INFO: Got endpoints: latency-svc-vzrww [29.240099ms]
Jul 16 14:33:28.622: INFO: Created: latency-svc-g2fwk
Jul 16 14:33:28.622: INFO: Got endpoints: latency-svc-g2fwk [32.765214ms]
Jul 16 14:33:28.622: INFO: Created: latency-svc-vdcjb
Jul 16 14:33:28.631: INFO: Created: latency-svc-l595d
Jul 16 14:33:28.633: INFO: Created: latency-svc-2gd95
Jul 16 14:33:28.633: INFO: Got endpoints: latency-svc-vdcjb [43.623823ms]
Jul 16 14:33:28.635: INFO: Got endpoints: latency-svc-l595d [45.354909ms]
Jul 16 14:33:28.638: INFO: Got endpoints: latency-svc-2gd95 [48.206637ms]
Jul 16 14:33:28.640: INFO: Created: latency-svc-bs5vs
Jul 16 14:33:28.643: INFO: Created: latency-svc-wxdlw
Jul 16 14:33:28.643: INFO: Got endpoints: latency-svc-wxdlw [53.344773ms]
Jul 16 14:33:28.646: INFO: Got endpoints: latency-svc-bs5vs [56.927187ms]
Jul 16 14:33:28.652: INFO: Created: latency-svc-9qxdc
Jul 16 14:33:28.652: INFO: Created: latency-svc-7df2n
Jul 16 14:33:28.652: INFO: Got endpoints: latency-svc-9qxdc [62.309186ms]
Jul 16 14:33:28.652: INFO: Created: latency-svc-jjz4x
Jul 16 14:33:28.652: INFO: Created: latency-svc-lk9vw
Jul 16 14:33:28.653: INFO: Got endpoints: latency-svc-7df2n [64.194512ms]
Jul 16 14:33:28.654: INFO: Got endpoints: latency-svc-lk9vw [64.506584ms]
Jul 16 14:33:28.655: INFO: Got endpoints: latency-svc-jjz4x [65.238501ms]
Jul 16 14:33:28.658: INFO: Created: latency-svc-grfbf
Jul 16 14:33:28.659: INFO: Created: latency-svc-zfkjg
Jul 16 14:33:28.660: INFO: Got endpoints: latency-svc-grfbf [52.991018ms]
Jul 16 14:33:28.663: INFO: Got endpoints: latency-svc-zfkjg [56.086672ms]
Jul 16 14:33:28.665: INFO: Created: latency-svc-j5fs5
Jul 16 14:33:28.668: INFO: Created: latency-svc-zn96n
Jul 16 14:33:28.669: INFO: Got endpoints: latency-svc-j5fs5 [62.061165ms]
Jul 16 14:33:28.672: INFO: Got endpoints: latency-svc-zn96n [59.970726ms]
Jul 16 14:33:28.674: INFO: Created: latency-svc-9hnpc
Jul 16 14:33:28.682: INFO: Created: latency-svc-96zgx
Jul 16 14:33:28.682: INFO: Got endpoints: latency-svc-9hnpc [62.471665ms]
Jul 16 14:33:28.685: INFO: Created: latency-svc-wcsmv
Jul 16 14:33:28.688: INFO: Got endpoints: latency-svc-wcsmv [53.221285ms]
Jul 16 14:33:28.704: INFO: Created: latency-svc-hb4pv
Jul 16 14:33:28.747: INFO: Created: latency-svc-rb5gr
Jul 16 14:33:28.747: INFO: Created: latency-svc-q2l44
Jul 16 14:33:28.747: INFO: Created: latency-svc-xjh8v
Jul 16 14:33:28.747: INFO: Created: latency-svc-qsm77
Jul 16 14:33:28.747: INFO: Created: latency-svc-67ssj
Jul 16 14:33:28.747: INFO: Created: latency-svc-xqsgw
Jul 16 14:33:28.747: INFO: Created: latency-svc-hsjl9
Jul 16 14:33:28.747: INFO: Created: latency-svc-wkw42
Jul 16 14:33:28.747: INFO: Created: latency-svc-hrmfx
Jul 16 14:33:28.747: INFO: Created: latency-svc-cwn66
Jul 16 14:33:28.747: INFO: Created: latency-svc-wsmxm
Jul 16 14:33:28.747: INFO: Created: latency-svc-k8956
Jul 16 14:33:28.747: INFO: Created: latency-svc-49594
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-hsjl9 [84.402377ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-qsm77 [114.49299ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-49594 [87.836633ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-xqsgw [96.101216ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-xjh8v [101.907783ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-hb4pv [110.48556ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-hrmfx [93.516392ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-wkw42 [94.994329ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-q2l44 [105.506831ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-96zgx [126.309463ms]
Jul 16 14:33:28.748: INFO: Got endpoints: latency-svc-k8956 [94.896589ms]
Jul 16 14:33:28.758: INFO: Created: latency-svc-xc28c
Jul 16 14:33:28.764: INFO: Created: latency-svc-7jlgb
Jul 16 14:33:28.773: INFO: Created: latency-svc-c9hfn
Jul 16 14:33:28.777: INFO: Got endpoints: latency-svc-67ssj [107.872305ms]
Jul 16 14:33:28.777: INFO: Created: latency-svc-ftn9h
Jul 16 14:33:28.781: INFO: Created: latency-svc-lh2kw
Jul 16 14:33:28.787: INFO: Created: latency-svc-mstbw
Jul 16 14:33:28.790: INFO: Created: latency-svc-s4lzf
Jul 16 14:33:28.794: INFO: Created: latency-svc-t7wsq
Jul 16 14:33:28.797: INFO: Created: latency-svc-jvszl
Jul 16 14:33:28.799: INFO: Created: latency-svc-g4mml
Jul 16 14:33:28.805: INFO: Created: latency-svc-cqlt5
Jul 16 14:33:28.810: INFO: Created: latency-svc-9q5b7
Jul 16 14:33:28.824: INFO: Got endpoints: latency-svc-cwn66 [151.958489ms]
Jul 16 14:33:28.832: INFO: Created: latency-svc-fq5x4
Jul 16 14:33:28.874: INFO: Got endpoints: latency-svc-wsmxm [191.714486ms]
Jul 16 14:33:28.879: INFO: Created: latency-svc-qkz9p
Jul 16 14:33:28.925: INFO: Got endpoints: latency-svc-rb5gr [236.679953ms]
Jul 16 14:33:28.931: INFO: Created: latency-svc-dzvmv
Jul 16 14:33:28.976: INFO: Got endpoints: latency-svc-xc28c [227.426778ms]
Jul 16 14:33:28.982: INFO: Created: latency-svc-fkvkw
Jul 16 14:33:29.025: INFO: Got endpoints: latency-svc-7jlgb [276.547655ms]
Jul 16 14:33:29.033: INFO: Created: latency-svc-njblf
Jul 16 14:33:29.075: INFO: Got endpoints: latency-svc-c9hfn [327.008197ms]
Jul 16 14:33:29.082: INFO: Created: latency-svc-8862s
Jul 16 14:33:29.124: INFO: Got endpoints: latency-svc-ftn9h [375.91167ms]
Jul 16 14:33:29.132: INFO: Created: latency-svc-slvf2
Jul 16 14:33:29.174: INFO: Got endpoints: latency-svc-lh2kw [425.876874ms]
Jul 16 14:33:29.191: INFO: Created: latency-svc-xl6jb
Jul 16 14:33:29.227: INFO: Got endpoints: latency-svc-mstbw [478.866815ms]
Jul 16 14:33:29.235: INFO: Created: latency-svc-p5rl7
Jul 16 14:33:29.274: INFO: Got endpoints: latency-svc-s4lzf [524.980699ms]
Jul 16 14:33:29.283: INFO: Created: latency-svc-msx6f
Jul 16 14:33:29.324: INFO: Got endpoints: latency-svc-t7wsq [575.564253ms]
Jul 16 14:33:29.329: INFO: Created: latency-svc-xgbgl
Jul 16 14:33:29.374: INFO: Got endpoints: latency-svc-jvszl [625.798316ms]
Jul 16 14:33:29.383: INFO: Created: latency-svc-hfwpv
Jul 16 14:33:29.424: INFO: Got endpoints: latency-svc-g4mml [676.46123ms]
Jul 16 14:33:29.432: INFO: Created: latency-svc-rch7f
Jul 16 14:33:29.482: INFO: Got endpoints: latency-svc-cqlt5 [734.174561ms]
Jul 16 14:33:29.490: INFO: Created: latency-svc-knw5s
Jul 16 14:33:29.528: INFO: Got endpoints: latency-svc-9q5b7 [751.044827ms]
Jul 16 14:33:29.545: INFO: Created: latency-svc-7bt8z
Jul 16 14:33:29.574: INFO: Got endpoints: latency-svc-fq5x4 [750.063254ms]
Jul 16 14:33:29.588: INFO: Created: latency-svc-t2qbq
Jul 16 14:33:29.624: INFO: Got endpoints: latency-svc-qkz9p [750.741729ms]
Jul 16 14:33:29.636: INFO: Created: latency-svc-kzdkg
Jul 16 14:33:29.675: INFO: Got endpoints: latency-svc-dzvmv [750.204856ms]
Jul 16 14:33:29.687: INFO: Created: latency-svc-sm6tj
Jul 16 14:33:29.723: INFO: Got endpoints: latency-svc-fkvkw [747.886165ms]
Jul 16 14:33:29.730: INFO: Created: latency-svc-n79lq
Jul 16 14:33:29.774: INFO: Got endpoints: latency-svc-njblf [749.762003ms]
Jul 16 14:33:29.782: INFO: Created: latency-svc-q2zxd
Jul 16 14:33:29.822: INFO: Got endpoints: latency-svc-8862s [746.894347ms]
Jul 16 14:33:29.829: INFO: Created: latency-svc-mxs2t
Jul 16 14:33:29.880: INFO: Got endpoints: latency-svc-slvf2 [755.479086ms]
Jul 16 14:33:29.888: INFO: Created: latency-svc-p4tsm
Jul 16 14:33:29.924: INFO: Got endpoints: latency-svc-xl6jb [749.893969ms]
Jul 16 14:33:29.931: INFO: Created: latency-svc-mvhzx
Jul 16 14:33:29.979: INFO: Got endpoints: latency-svc-p5rl7 [751.997828ms]
Jul 16 14:33:29.989: INFO: Created: latency-svc-tm948
Jul 16 14:33:30.030: INFO: Got endpoints: latency-svc-msx6f [756.182341ms]
Jul 16 14:33:30.045: INFO: Created: latency-svc-sdmhq
Jul 16 14:33:30.074: INFO: Got endpoints: latency-svc-xgbgl [750.414545ms]
Jul 16 14:33:30.083: INFO: Created: latency-svc-nc7s8
Jul 16 14:33:30.126: INFO: Got endpoints: latency-svc-hfwpv [752.010141ms]
Jul 16 14:33:30.140: INFO: Created: latency-svc-w2vnq
Jul 16 14:33:30.180: INFO: Got endpoints: latency-svc-rch7f [754.645543ms]
Jul 16 14:33:30.199: INFO: Created: latency-svc-rdfjm
Jul 16 14:33:30.227: INFO: Got endpoints: latency-svc-knw5s [744.344016ms]
Jul 16 14:33:30.258: INFO: Created: latency-svc-w5q26
Jul 16 14:33:30.276: INFO: Got endpoints: latency-svc-7bt8z [747.477879ms]
Jul 16 14:33:30.284: INFO: Created: latency-svc-8w27n
Jul 16 14:33:30.327: INFO: Got endpoints: latency-svc-t2qbq [752.55672ms]
Jul 16 14:33:30.336: INFO: Created: latency-svc-c8xq4
Jul 16 14:33:30.375: INFO: Got endpoints: latency-svc-kzdkg [750.503991ms]
Jul 16 14:33:30.382: INFO: Created: latency-svc-7tfxd
Jul 16 14:33:30.424: INFO: Got endpoints: latency-svc-sm6tj [748.738726ms]
Jul 16 14:33:30.434: INFO: Created: latency-svc-kx7sv
Jul 16 14:33:30.487: INFO: Got endpoints: latency-svc-n79lq [763.329396ms]
Jul 16 14:33:30.507: INFO: Created: latency-svc-l29db
Jul 16 14:33:30.525: INFO: Got endpoints: latency-svc-q2zxd [749.984605ms]
Jul 16 14:33:30.539: INFO: Created: latency-svc-d5c7r
Jul 16 14:33:30.575: INFO: Got endpoints: latency-svc-mxs2t [753.20077ms]
Jul 16 14:33:30.586: INFO: Created: latency-svc-z69hn
Jul 16 14:33:30.625: INFO: Got endpoints: latency-svc-p4tsm [744.726086ms]
Jul 16 14:33:30.642: INFO: Created: latency-svc-bv4wl
Jul 16 14:33:30.675: INFO: Got endpoints: latency-svc-mvhzx [751.11414ms]
Jul 16 14:33:30.685: INFO: Created: latency-svc-pjmhk
Jul 16 14:33:30.724: INFO: Got endpoints: latency-svc-tm948 [745.057655ms]
Jul 16 14:33:30.732: INFO: Created: latency-svc-z6v2p
Jul 16 14:33:30.776: INFO: Got endpoints: latency-svc-sdmhq [746.066547ms]
Jul 16 14:33:30.793: INFO: Created: latency-svc-hnhlg
Jul 16 14:33:30.832: INFO: Got endpoints: latency-svc-nc7s8 [758.034241ms]
Jul 16 14:33:30.843: INFO: Created: latency-svc-9ndsx
Jul 16 14:33:30.879: INFO: Got endpoints: latency-svc-w2vnq [752.616936ms]
Jul 16 14:33:30.890: INFO: Created: latency-svc-6jcx6
Jul 16 14:33:30.926: INFO: Got endpoints: latency-svc-rdfjm [746.279773ms]
Jul 16 14:33:30.936: INFO: Created: latency-svc-qf9n8
Jul 16 14:33:30.977: INFO: Got endpoints: latency-svc-w5q26 [749.886327ms]
Jul 16 14:33:30.994: INFO: Created: latency-svc-qkzch
Jul 16 14:33:31.024: INFO: Got endpoints: latency-svc-8w27n [748.013282ms]
Jul 16 14:33:31.030: INFO: Created: latency-svc-sbxcj
Jul 16 14:33:31.074: INFO: Got endpoints: latency-svc-c8xq4 [747.319908ms]
Jul 16 14:33:31.080: INFO: Created: latency-svc-6mvs6
Jul 16 14:33:31.123: INFO: Got endpoints: latency-svc-7tfxd [748.356729ms]
Jul 16 14:33:31.132: INFO: Created: latency-svc-tpgmf
Jul 16 14:33:31.174: INFO: Got endpoints: latency-svc-kx7sv [750.302895ms]
Jul 16 14:33:31.186: INFO: Created: latency-svc-lt45w
Jul 16 14:33:31.223: INFO: Got endpoints: latency-svc-l29db [736.528465ms]
Jul 16 14:33:31.233: INFO: Created: latency-svc-nk4bz
Jul 16 14:33:31.273: INFO: Got endpoints: latency-svc-d5c7r [748.897918ms]
Jul 16 14:33:31.282: INFO: Created: latency-svc-p6rqd
Jul 16 14:33:31.324: INFO: Got endpoints: latency-svc-z69hn [748.061183ms]
Jul 16 14:33:31.339: INFO: Created: latency-svc-lxvsf
Jul 16 14:33:31.374: INFO: Got endpoints: latency-svc-bv4wl [749.427157ms]
Jul 16 14:33:31.392: INFO: Created: latency-svc-78qpd
Jul 16 14:33:31.424: INFO: Got endpoints: latency-svc-pjmhk [748.256963ms]
Jul 16 14:33:31.432: INFO: Created: latency-svc-qfxbh
Jul 16 14:33:31.474: INFO: Got endpoints: latency-svc-z6v2p [749.947948ms]
Jul 16 14:33:31.482: INFO: Created: latency-svc-ktmnn
Jul 16 14:33:31.524: INFO: Got endpoints: latency-svc-hnhlg [747.640444ms]
Jul 16 14:33:31.532: INFO: Created: latency-svc-5xqhl
Jul 16 14:33:31.575: INFO: Got endpoints: latency-svc-9ndsx [742.445103ms]
Jul 16 14:33:31.584: INFO: Created: latency-svc-d5mdd
Jul 16 14:33:31.624: INFO: Got endpoints: latency-svc-6jcx6 [745.632116ms]
Jul 16 14:33:31.631: INFO: Created: latency-svc-2d7mc
Jul 16 14:33:31.674: INFO: Got endpoints: latency-svc-qf9n8 [747.837556ms]
Jul 16 14:33:31.681: INFO: Created: latency-svc-vr6q7
Jul 16 14:33:31.724: INFO: Got endpoints: latency-svc-qkzch [747.749061ms]
Jul 16 14:33:31.733: INFO: Created: latency-svc-tftd2
Jul 16 14:33:31.774: INFO: Got endpoints: latency-svc-sbxcj [750.128734ms]
Jul 16 14:33:31.783: INFO: Created: latency-svc-7c8cs
Jul 16 14:33:31.824: INFO: Got endpoints: latency-svc-6mvs6 [749.672711ms]
Jul 16 14:33:31.833: INFO: Created: latency-svc-srcpw
Jul 16 14:33:31.874: INFO: Got endpoints: latency-svc-tpgmf [750.604473ms]
Jul 16 14:33:31.892: INFO: Created: latency-svc-8n6d7
Jul 16 14:33:31.925: INFO: Got endpoints: latency-svc-lt45w [750.735671ms]
Jul 16 14:33:31.936: INFO: Created: latency-svc-tbs6j
Jul 16 14:33:31.975: INFO: Got endpoints: latency-svc-nk4bz [751.455062ms]
Jul 16 14:33:31.984: INFO: Created: latency-svc-grl4b
Jul 16 14:33:32.026: INFO: Got endpoints: latency-svc-p6rqd [752.892196ms]
Jul 16 14:33:32.037: INFO: Created: latency-svc-965f8
Jul 16 14:33:32.075: INFO: Got endpoints: latency-svc-lxvsf [751.279779ms]
Jul 16 14:33:32.087: INFO: Created: latency-svc-qbl8t
Jul 16 14:33:32.124: INFO: Got endpoints: latency-svc-78qpd [749.613326ms]
Jul 16 14:33:32.136: INFO: Created: latency-svc-dkvwr
Jul 16 14:33:32.176: INFO: Got endpoints: latency-svc-qfxbh [751.721585ms]
Jul 16 14:33:32.183: INFO: Created: latency-svc-fnt56
Jul 16 14:33:32.241: INFO: Got endpoints: latency-svc-ktmnn [766.988338ms]
Jul 16 14:33:32.262: INFO: Created: latency-svc-v8ngn
Jul 16 14:33:32.280: INFO: Got endpoints: latency-svc-5xqhl [756.09998ms]
Jul 16 14:33:32.313: INFO: Created: latency-svc-4n4ss
Jul 16 14:33:32.324: INFO: Got endpoints: latency-svc-d5mdd [749.291791ms]
Jul 16 14:33:32.332: INFO: Created: latency-svc-69tc9
Jul 16 14:33:32.374: INFO: Got endpoints: latency-svc-2d7mc [749.9346ms]
Jul 16 14:33:32.383: INFO: Created: latency-svc-twg9l
Jul 16 14:33:32.424: INFO: Got endpoints: latency-svc-vr6q7 [750.04869ms]
Jul 16 14:33:32.436: INFO: Created: latency-svc-qxd6z
Jul 16 14:33:32.474: INFO: Got endpoints: latency-svc-tftd2 [749.205885ms]
Jul 16 14:33:32.483: INFO: Created: latency-svc-khjg8
Jul 16 14:33:32.524: INFO: Got endpoints: latency-svc-7c8cs [749.67182ms]
Jul 16 14:33:32.531: INFO: Created: latency-svc-jwrlf
Jul 16 14:33:32.574: INFO: Got endpoints: latency-svc-srcpw [749.847727ms]
Jul 16 14:33:32.582: INFO: Created: latency-svc-4s9z9
Jul 16 14:33:32.625: INFO: Got endpoints: latency-svc-8n6d7 [750.415612ms]
Jul 16 14:33:32.638: INFO: Created: latency-svc-nbfcl
Jul 16 14:33:32.674: INFO: Got endpoints: latency-svc-tbs6j [748.767687ms]
Jul 16 14:33:32.682: INFO: Created: latency-svc-tbr96
Jul 16 14:33:32.724: INFO: Got endpoints: latency-svc-grl4b [748.675801ms]
Jul 16 14:33:32.730: INFO: Created: latency-svc-77vrg
Jul 16 14:33:32.774: INFO: Got endpoints: latency-svc-965f8 [747.663675ms]
Jul 16 14:33:32.784: INFO: Created: latency-svc-z95bs
Jul 16 14:33:32.825: INFO: Got endpoints: latency-svc-qbl8t [750.439915ms]
Jul 16 14:33:32.833: INFO: Created: latency-svc-cltt5
Jul 16 14:33:32.874: INFO: Got endpoints: latency-svc-dkvwr [750.426792ms]
Jul 16 14:33:32.893: INFO: Created: latency-svc-djqwr
Jul 16 14:33:32.925: INFO: Got endpoints: latency-svc-fnt56 [748.917024ms]
Jul 16 14:33:32.932: INFO: Created: latency-svc-ndn2b
Jul 16 14:33:32.974: INFO: Got endpoints: latency-svc-v8ngn [732.228652ms]
Jul 16 14:33:32.982: INFO: Created: latency-svc-dnzww
Jul 16 14:33:33.024: INFO: Got endpoints: latency-svc-4n4ss [744.086569ms]
Jul 16 14:33:33.035: INFO: Created: latency-svc-9b5v4
Jul 16 14:33:33.076: INFO: Got endpoints: latency-svc-69tc9 [751.558736ms]
Jul 16 14:33:33.086: INFO: Created: latency-svc-8jk7r
Jul 16 14:33:33.125: INFO: Got endpoints: latency-svc-twg9l [750.94526ms]
Jul 16 14:33:33.138: INFO: Created: latency-svc-5g2tr
Jul 16 14:33:33.176: INFO: Got endpoints: latency-svc-qxd6z [751.212902ms]
Jul 16 14:33:33.185: INFO: Created: latency-svc-rg5g8
Jul 16 14:33:33.224: INFO: Got endpoints: latency-svc-khjg8 [750.662494ms]
Jul 16 14:33:33.232: INFO: Created: latency-svc-8tbjt
Jul 16 14:33:33.275: INFO: Got endpoints: latency-svc-jwrlf [750.347678ms]
Jul 16 14:33:33.284: INFO: Created: latency-svc-p99tg
Jul 16 14:33:33.329: INFO: Got endpoints: latency-svc-4s9z9 [755.364084ms]
Jul 16 14:33:33.339: INFO: Created: latency-svc-h85gk
Jul 16 14:33:33.376: INFO: Got endpoints: latency-svc-nbfcl [750.873988ms]
Jul 16 14:33:33.390: INFO: Created: latency-svc-86tvz
Jul 16 14:33:33.426: INFO: Got endpoints: latency-svc-tbr96 [752.389194ms]
Jul 16 14:33:33.436: INFO: Created: latency-svc-59pnj
Jul 16 14:33:33.479: INFO: Got endpoints: latency-svc-77vrg [755.221979ms]
Jul 16 14:33:33.488: INFO: Created: latency-svc-gn4dk
Jul 16 14:33:33.525: INFO: Got endpoints: latency-svc-z95bs [750.459304ms]
Jul 16 14:33:33.531: INFO: Created: latency-svc-49kxz
Jul 16 14:33:33.575: INFO: Got endpoints: latency-svc-cltt5 [749.909997ms]
Jul 16 14:33:33.582: INFO: Created: latency-svc-76d8h
Jul 16 14:33:33.624: INFO: Got endpoints: latency-svc-djqwr [749.506009ms]
Jul 16 14:33:33.631: INFO: Created: latency-svc-t7z26
Jul 16 14:33:33.674: INFO: Got endpoints: latency-svc-ndn2b [748.849236ms]
Jul 16 14:33:33.686: INFO: Created: latency-svc-f57jw
Jul 16 14:33:33.724: INFO: Got endpoints: latency-svc-dnzww [750.522525ms]
Jul 16 14:33:33.733: INFO: Created: latency-svc-v9m54
Jul 16 14:33:33.776: INFO: Got endpoints: latency-svc-9b5v4 [751.685604ms]
Jul 16 14:33:33.786: INFO: Created: latency-svc-b2bt8
Jul 16 14:33:33.824: INFO: Got endpoints: latency-svc-8jk7r [748.390827ms]
Jul 16 14:33:33.835: INFO: Created: latency-svc-27mz4
Jul 16 14:33:33.874: INFO: Got endpoints: latency-svc-5g2tr [748.954772ms]
Jul 16 14:33:33.882: INFO: Created: latency-svc-8788s
Jul 16 14:33:33.924: INFO: Got endpoints: latency-svc-rg5g8 [747.893734ms]
Jul 16 14:33:33.939: INFO: Created: latency-svc-4hgz4
Jul 16 14:33:33.975: INFO: Got endpoints: latency-svc-8tbjt [750.293424ms]
Jul 16 14:33:33.980: INFO: Created: latency-svc-v9rjv
Jul 16 14:33:34.024: INFO: Got endpoints: latency-svc-p99tg [749.291817ms]
Jul 16 14:33:34.033: INFO: Created: latency-svc-hb7h2
Jul 16 14:33:34.074: INFO: Got endpoints: latency-svc-h85gk [744.203328ms]
Jul 16 14:33:34.081: INFO: Created: latency-svc-qw4qr
Jul 16 14:33:34.124: INFO: Got endpoints: latency-svc-86tvz [748.536746ms]
Jul 16 14:33:34.132: INFO: Created: latency-svc-pnz6b
Jul 16 14:33:34.174: INFO: Got endpoints: latency-svc-59pnj [747.054725ms]
Jul 16 14:33:34.181: INFO: Created: latency-svc-wmvqp
Jul 16 14:33:34.224: INFO: Got endpoints: latency-svc-gn4dk [744.432094ms]
Jul 16 14:33:34.232: INFO: Created: latency-svc-pk4mg
Jul 16 14:33:34.275: INFO: Got endpoints: latency-svc-49kxz [749.609857ms]
Jul 16 14:33:34.282: INFO: Created: latency-svc-dm49f
Jul 16 14:33:34.326: INFO: Got endpoints: latency-svc-76d8h [750.545681ms]
Jul 16 14:33:34.335: INFO: Created: latency-svc-2lgm6
Jul 16 14:33:34.380: INFO: Got endpoints: latency-svc-t7z26 [755.755508ms]
Jul 16 14:33:34.395: INFO: Created: latency-svc-t4rbm
Jul 16 14:33:34.426: INFO: Got endpoints: latency-svc-f57jw [752.857916ms]
Jul 16 14:33:34.435: INFO: Created: latency-svc-wln8z
Jul 16 14:33:34.474: INFO: Got endpoints: latency-svc-v9m54 [749.904692ms]
Jul 16 14:33:34.487: INFO: Created: latency-svc-thcht
Jul 16 14:33:34.524: INFO: Got endpoints: latency-svc-b2bt8 [748.227088ms]
Jul 16 14:33:34.533: INFO: Created: latency-svc-89wkv
Jul 16 14:33:34.574: INFO: Got endpoints: latency-svc-27mz4 [749.735336ms]
Jul 16 14:33:34.582: INFO: Created: latency-svc-pp6qf
Jul 16 14:33:34.624: INFO: Got endpoints: latency-svc-8788s [749.814982ms]
Jul 16 14:33:34.632: INFO: Created: latency-svc-2xn2b
Jul 16 14:33:34.674: INFO: Got endpoints: latency-svc-4hgz4 [750.000483ms]
Jul 16 14:33:34.681: INFO: Created: latency-svc-58znp
Jul 16 14:33:34.724: INFO: Got endpoints: latency-svc-v9rjv [748.963236ms]
Jul 16 14:33:34.733: INFO: Created: latency-svc-94s2d
Jul 16 14:33:34.774: INFO: Got endpoints: latency-svc-hb7h2 [749.858077ms]
Jul 16 14:33:34.782: INFO: Created: latency-svc-vtt58
Jul 16 14:33:34.824: INFO: Got endpoints: latency-svc-qw4qr [750.358373ms]
Jul 16 14:33:34.833: INFO: Created: latency-svc-wg724
Jul 16 14:33:34.875: INFO: Got endpoints: latency-svc-pnz6b [750.51269ms]
Jul 16 14:33:34.882: INFO: Created: latency-svc-ppxgh
Jul 16 14:33:34.924: INFO: Got endpoints: latency-svc-wmvqp [750.745153ms]
Jul 16 14:33:34.932: INFO: Created: latency-svc-pm477
Jul 16 14:33:34.979: INFO: Got endpoints: latency-svc-pk4mg [755.476055ms]
Jul 16 14:33:34.986: INFO: Created: latency-svc-qjkvs
Jul 16 14:33:35.024: INFO: Got endpoints: latency-svc-dm49f [749.429951ms]
Jul 16 14:33:35.031: INFO: Created: latency-svc-lsjmr
Jul 16 14:33:35.074: INFO: Got endpoints: latency-svc-2lgm6 [747.682158ms]
Jul 16 14:33:35.083: INFO: Created: latency-svc-lwmb6
Jul 16 14:33:35.124: INFO: Got endpoints: latency-svc-t4rbm [744.111071ms]
Jul 16 14:33:35.132: INFO: Created: latency-svc-2rlwk
Jul 16 14:33:35.174: INFO: Got endpoints: latency-svc-wln8z [747.654987ms]
Jul 16 14:33:35.182: INFO: Created: latency-svc-8z5dj
Jul 16 14:33:35.224: INFO: Got endpoints: latency-svc-thcht [749.942731ms]
Jul 16 14:33:35.234: INFO: Created: latency-svc-7vwmf
Jul 16 14:33:35.274: INFO: Got endpoints: latency-svc-89wkv [750.060203ms]
Jul 16 14:33:35.281: INFO: Created: latency-svc-8p2vq
Jul 16 14:33:35.324: INFO: Got endpoints: latency-svc-pp6qf [749.299486ms]
Jul 16 14:33:35.331: INFO: Created: latency-svc-7wkcr
Jul 16 14:33:35.374: INFO: Got endpoints: latency-svc-2xn2b [749.75215ms]
Jul 16 14:33:35.382: INFO: Created: latency-svc-7mxmh
Jul 16 14:33:35.425: INFO: Got endpoints: latency-svc-58znp [750.694308ms]
Jul 16 14:33:35.433: INFO: Created: latency-svc-jsrq4
Jul 16 14:33:35.475: INFO: Got endpoints: latency-svc-94s2d [751.569365ms]
Jul 16 14:33:35.484: INFO: Created: latency-svc-8hfc4
Jul 16 14:33:35.526: INFO: Got endpoints: latency-svc-vtt58 [751.976938ms]
Jul 16 14:33:35.533: INFO: Created: latency-svc-dxc54
Jul 16 14:33:35.574: INFO: Got endpoints: latency-svc-wg724 [750.288466ms]
Jul 16 14:33:35.581: INFO: Created: latency-svc-75srq
Jul 16 14:33:35.626: INFO: Got endpoints: latency-svc-ppxgh [750.516739ms]
Jul 16 14:33:35.635: INFO: Created: latency-svc-tjqkc
Jul 16 14:33:35.674: INFO: Got endpoints: latency-svc-pm477 [749.665357ms]
Jul 16 14:33:35.682: INFO: Created: latency-svc-lgf47
Jul 16 14:33:35.724: INFO: Got endpoints: latency-svc-qjkvs [744.360621ms]
Jul 16 14:33:35.732: INFO: Created: latency-svc-p9npw
Jul 16 14:33:35.775: INFO: Got endpoints: latency-svc-lsjmr [751.117831ms]
Jul 16 14:33:35.783: INFO: Created: latency-svc-b4l72
Jul 16 14:33:35.824: INFO: Got endpoints: latency-svc-lwmb6 [750.523954ms]
Jul 16 14:33:35.832: INFO: Created: latency-svc-84tsq
Jul 16 14:33:35.875: INFO: Got endpoints: latency-svc-2rlwk [750.482706ms]
Jul 16 14:33:35.883: INFO: Created: latency-svc-2w7ws
Jul 16 14:33:35.924: INFO: Got endpoints: latency-svc-8z5dj [749.655214ms]
Jul 16 14:33:35.932: INFO: Created: latency-svc-9n9lh
Jul 16 14:33:35.974: INFO: Got endpoints: latency-svc-7vwmf [749.624238ms]
Jul 16 14:33:35.982: INFO: Created: latency-svc-8gk7j
Jul 16 14:33:36.025: INFO: Got endpoints: latency-svc-8p2vq [750.76493ms]
Jul 16 14:33:36.033: INFO: Created: latency-svc-hshtm
Jul 16 14:33:36.074: INFO: Got endpoints: latency-svc-7wkcr [750.533013ms]
Jul 16 14:33:36.082: INFO: Created: latency-svc-vw889
Jul 16 14:33:36.128: INFO: Got endpoints: latency-svc-7mxmh [754.233046ms]
Jul 16 14:33:36.144: INFO: Created: latency-svc-445fh
Jul 16 14:33:36.174: INFO: Got endpoints: latency-svc-jsrq4 [748.443736ms]
Jul 16 14:33:36.182: INFO: Created: latency-svc-tqn9b
Jul 16 14:33:36.224: INFO: Got endpoints: latency-svc-8hfc4 [748.735844ms]
Jul 16 14:33:36.233: INFO: Created: latency-svc-4vss8
Jul 16 14:33:36.274: INFO: Got endpoints: latency-svc-dxc54 [748.30781ms]
Jul 16 14:33:36.282: INFO: Created: latency-svc-nwv4n
Jul 16 14:33:36.323: INFO: Got endpoints: latency-svc-75srq [748.867139ms]
Jul 16 14:33:36.333: INFO: Created: latency-svc-rggdx
Jul 16 14:33:36.374: INFO: Got endpoints: latency-svc-tjqkc [747.994223ms]
Jul 16 14:33:36.380: INFO: Created: latency-svc-7z94p
Jul 16 14:33:36.424: INFO: Got endpoints: latency-svc-lgf47 [749.701609ms]
Jul 16 14:33:36.474: INFO: Got endpoints: latency-svc-p9npw [750.129087ms]
Jul 16 14:33:36.524: INFO: Got endpoints: latency-svc-b4l72 [748.437474ms]
Jul 16 14:33:36.575: INFO: Got endpoints: latency-svc-84tsq [750.621359ms]
Jul 16 14:33:36.624: INFO: Got endpoints: latency-svc-2w7ws [749.495623ms]
Jul 16 14:33:36.674: INFO: Got endpoints: latency-svc-9n9lh [749.420856ms]
Jul 16 14:33:36.724: INFO: Got endpoints: latency-svc-8gk7j [750.358503ms]
Jul 16 14:33:36.774: INFO: Got endpoints: latency-svc-hshtm [748.989545ms]
Jul 16 14:33:36.824: INFO: Got endpoints: latency-svc-vw889 [749.295326ms]
Jul 16 14:33:36.875: INFO: Got endpoints: latency-svc-445fh [746.125199ms]
Jul 16 14:33:36.924: INFO: Got endpoints: latency-svc-tqn9b [750.357789ms]
Jul 16 14:33:36.975: INFO: Got endpoints: latency-svc-4vss8 [751.156186ms]
Jul 16 14:33:37.025: INFO: Got endpoints: latency-svc-nwv4n [750.224447ms]
Jul 16 14:33:37.075: INFO: Got endpoints: latency-svc-rggdx [751.218895ms]
Jul 16 14:33:37.129: INFO: Got endpoints: latency-svc-7z94p [755.303456ms]
Jul 16 14:33:37.129: INFO: Latencies: [17.780943ms 17.786736ms 18.310598ms 22.843088ms 29.240099ms 32.765214ms 43.623823ms 45.354909ms 48.206637ms 52.991018ms 53.221285ms 53.344773ms 56.086672ms 56.927187ms 59.970726ms 62.061165ms 62.309186ms 62.471665ms 64.194512ms 64.506584ms 65.238501ms 84.402377ms 87.836633ms 93.516392ms 94.896589ms 94.994329ms 96.101216ms 101.907783ms 105.506831ms 107.872305ms 110.48556ms 114.49299ms 126.309463ms 151.958489ms 191.714486ms 227.426778ms 236.679953ms 276.547655ms 327.008197ms 375.91167ms 425.876874ms 478.866815ms 524.980699ms 575.564253ms 625.798316ms 676.46123ms 732.228652ms 734.174561ms 736.528465ms 742.445103ms 744.086569ms 744.111071ms 744.203328ms 744.344016ms 744.360621ms 744.432094ms 744.726086ms 745.057655ms 745.632116ms 746.066547ms 746.125199ms 746.279773ms 746.894347ms 747.054725ms 747.319908ms 747.477879ms 747.640444ms 747.654987ms 747.663675ms 747.682158ms 747.749061ms 747.837556ms 747.886165ms 747.893734ms 747.994223ms 748.013282ms 748.061183ms 748.227088ms 748.256963ms 748.30781ms 748.356729ms 748.390827ms 748.437474ms 748.443736ms 748.536746ms 748.675801ms 748.735844ms 748.738726ms 748.767687ms 748.849236ms 748.867139ms 748.897918ms 748.917024ms 748.954772ms 748.963236ms 748.989545ms 749.205885ms 749.291791ms 749.291817ms 749.295326ms 749.299486ms 749.420856ms 749.427157ms 749.429951ms 749.495623ms 749.506009ms 749.609857ms 749.613326ms 749.624238ms 749.655214ms 749.665357ms 749.67182ms 749.672711ms 749.701609ms 749.735336ms 749.75215ms 749.762003ms 749.814982ms 749.847727ms 749.858077ms 749.886327ms 749.893969ms 749.904692ms 749.909997ms 749.9346ms 749.942731ms 749.947948ms 749.984605ms 750.000483ms 750.04869ms 750.060203ms 750.063254ms 750.128734ms 750.129087ms 750.204856ms 750.224447ms 750.288466ms 750.293424ms 750.302895ms 750.347678ms 750.357789ms 750.358373ms 750.358503ms 750.414545ms 750.415612ms 750.426792ms 750.439915ms 750.459304ms 750.482706ms 750.503991ms 750.51269ms 750.516739ms 750.522525ms 750.523954ms 750.533013ms 750.545681ms 750.604473ms 750.621359ms 750.662494ms 750.694308ms 750.735671ms 750.741729ms 750.745153ms 750.76493ms 750.873988ms 750.94526ms 751.044827ms 751.11414ms 751.117831ms 751.156186ms 751.212902ms 751.218895ms 751.279779ms 751.455062ms 751.558736ms 751.569365ms 751.685604ms 751.721585ms 751.976938ms 751.997828ms 752.010141ms 752.389194ms 752.55672ms 752.616936ms 752.857916ms 752.892196ms 753.20077ms 754.233046ms 754.645543ms 755.221979ms 755.303456ms 755.364084ms 755.476055ms 755.479086ms 755.755508ms 756.09998ms 756.182341ms 758.034241ms 763.329396ms 766.988338ms]
Jul 16 14:33:37.130: INFO: 50 %ile: 749.299486ms
Jul 16 14:33:37.130: INFO: 90 %ile: 752.010141ms
Jul 16 14:33:37.130: INFO: 99 %ile: 763.329396ms
Jul 16 14:33:37.130: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:33:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-rd67x" for this suite.
Jul 16 14:34:01.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:34:01.260: INFO: namespace: e2e-tests-svc-latency-rd67x, resource: bindings, ignored listing per whitelist
Jul 16 14:34:03.487: INFO: namespace e2e-tests-svc-latency-rd67x deletion completed in 26.348632149s

• [SLOW TEST:37.170 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:34:03.487: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Jul 16 14:34:03.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 cluster-info'
Jul 16 14:34:03.687: INFO: stderr: ""
Jul 16 14:34:03.687: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:34:03.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mx9cl" for this suite.
Jul 16 14:34:09.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:34:09.925: INFO: namespace: e2e-tests-kubectl-mx9cl, resource: bindings, ignored listing per whitelist
Jul 16 14:34:12.025: INFO: namespace e2e-tests-kubectl-mx9cl deletion completed in 8.333317106s

• [SLOW TEST:8.538 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:34:12.026: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:34:12.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-sf9mx" to be "success or failure"
Jul 16 14:34:12.151: INFO: Pod "downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303274ms
Jul 16 14:34:14.154: INFO: Pod "downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009171547s
Jul 16 14:34:16.158: INFO: Pod "downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013857287s
STEP: Saw pod success
Jul 16 14:34:16.158: INFO: Pod "downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:34:16.161: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:34:16.182: INFO: Waiting for pod downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:34:16.185: INFO: Pod downwardapi-volume-c7600621-a7d6-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:34:16.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sf9mx" for this suite.
Jul 16 14:34:22.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:34:22.975: INFO: namespace: e2e-tests-downward-api-sf9mx, resource: bindings, ignored listing per whitelist
Jul 16 14:34:24.525: INFO: namespace e2e-tests-downward-api-sf9mx deletion completed in 8.33355223s

• [SLOW TEST:12.499 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:34:24.525: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Jul 16 14:34:24.610: INFO: Waiting up to 5m0s for pod "var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-var-expansion-w78gw" to be "success or failure"
Jul 16 14:34:24.614: INFO: Pod "var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684883ms
Jul 16 14:34:26.617: INFO: Pod "var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007251112s
STEP: Saw pod success
Jul 16 14:34:26.618: INFO: Pod "var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:34:26.620: INFO: Trying to get logs from node i-taxl6ow1 pod var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:34:26.639: INFO: Waiting for pod var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:34:26.642: INFO: Pod var-expansion-cecf871d-a7d6-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:34:26.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-w78gw" for this suite.
Jul 16 14:34:32.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:34:32.718: INFO: namespace: e2e-tests-var-expansion-w78gw, resource: bindings, ignored listing per whitelist
Jul 16 14:34:34.989: INFO: namespace e2e-tests-var-expansion-w78gw deletion completed in 8.341378215s

• [SLOW TEST:10.465 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:34:34.991: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:34:39.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-5m4jz" for this suite.
Jul 16 14:35:27.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:35:29.362: INFO: namespace: e2e-tests-kubelet-test-5m4jz, resource: bindings, ignored listing per whitelist
Jul 16 14:35:29.459: INFO: namespace e2e-tests-kubelet-test-5m4jz deletion completed in 50.332458138s

• [SLOW TEST:54.468 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:35:29.460: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 14:35:29.574: INFO: (0) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.131132ms)
Jul 16 14:35:29.585: INFO: (1) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.037705ms)
Jul 16 14:35:29.591: INFO: (2) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.639743ms)
Jul 16 14:35:29.596: INFO: (3) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.859232ms)
Jul 16 14:35:29.603: INFO: (4) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.476527ms)
Jul 16 14:35:29.609: INFO: (5) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.053939ms)
Jul 16 14:35:29.619: INFO: (6) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.320331ms)
Jul 16 14:35:29.624: INFO: (7) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.195253ms)
Jul 16 14:35:29.629: INFO: (8) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.434499ms)
Jul 16 14:35:29.635: INFO: (9) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.022592ms)
Jul 16 14:35:29.640: INFO: (10) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.861855ms)
Jul 16 14:35:29.646: INFO: (11) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.234403ms)
Jul 16 14:35:29.651: INFO: (12) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.55864ms)
Jul 16 14:35:29.656: INFO: (13) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.295972ms)
Jul 16 14:35:29.661: INFO: (14) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.763016ms)
Jul 16 14:35:29.665: INFO: (15) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.023346ms)
Jul 16 14:35:29.672: INFO: (16) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.39063ms)
Jul 16 14:35:29.682: INFO: (17) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.726996ms)
Jul 16 14:35:29.700: INFO: (18) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.976956ms)
Jul 16 14:35:29.711: INFO: (19) /api/v1/nodes/i-7znxt5so:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.146745ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:35:29.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zkqvk" for this suite.
Jul 16 14:35:35.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:35:35.845: INFO: namespace: e2e-tests-proxy-zkqvk, resource: bindings, ignored listing per whitelist
Jul 16 14:35:35.938: INFO: namespace e2e-tests-proxy-zkqvk deletion completed in 6.22292564s

• [SLOW TEST:6.479 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:35:35.939: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 16 14:35:36.054: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31352,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 14:35:36.054: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31353,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 16 14:35:36.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31354,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 16 14:35:46.081: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31371,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 14:35:46.082: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31372,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 16 14:35:46.082: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-dfjg2,SelfLink:/api/v1/namespaces/e2e-tests-watch-dfjg2/configmaps/e2e-watch-test-label-changed,UID:f961bc2a-a7d6-11e9-a7d7-5254228aab6e,ResourceVersion:31373,Generation:0,CreationTimestamp:2019-07-16 14:35:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:35:46.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-dfjg2" for this suite.
Jul 16 14:35:52.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:35:54.217: INFO: namespace: e2e-tests-watch-dfjg2, resource: bindings, ignored listing per whitelist
Jul 16 14:35:54.418: INFO: namespace e2e-tests-watch-dfjg2 deletion completed in 8.32871832s

• [SLOW TEST:18.479 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:35:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:36:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-k2w9l" for this suite.
Jul 16 14:36:06.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:08.550: INFO: namespace: e2e-tests-namespaces-k2w9l, resource: bindings, ignored listing per whitelist
Jul 16 14:36:08.949: INFO: namespace e2e-tests-namespaces-k2w9l deletion completed in 8.34240779s
STEP: Destroying namespace "e2e-tests-nsdeletetest-lkvzs" for this suite.
Jul 16 14:36:08.953: INFO: Namespace e2e-tests-nsdeletetest-lkvzs was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-98c49" for this suite.
Jul 16 14:36:14.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:15.061: INFO: namespace: e2e-tests-nsdeletetest-98c49, resource: bindings, ignored listing per whitelist
Jul 16 14:36:17.286: INFO: namespace e2e-tests-nsdeletetest-98c49 deletion completed in 8.333668058s

• [SLOW TEST:22.868 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:36:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Jul 16 14:36:17.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-tdwrv'
Jul 16 14:36:17.723: INFO: stderr: ""
Jul 16 14:36:17.723: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Jul 16 14:36:18.726: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:36:18.726: INFO: Found 0 / 1
Jul 16 14:36:19.727: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:36:19.728: INFO: Found 1 / 1
Jul 16 14:36:19.728: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 14:36:19.732: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:36:19.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 16 14:36:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 logs redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv'
Jul 16 14:36:19.856: INFO: stderr: ""
Jul 16 14:36:19.856: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 14:36:19.209 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 14:36:19.209 # Server started, Redis version 3.2.12\n1:M 16 Jul 14:36:19.209 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 14:36:19.209 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 16 14:36:19.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 log redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv --tail=1'
Jul 16 14:36:19.969: INFO: stderr: ""
Jul 16 14:36:19.969: INFO: stdout: "1:M 16 Jul 14:36:19.209 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 16 14:36:19.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 log redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv --limit-bytes=1'
Jul 16 14:36:20.106: INFO: stderr: ""
Jul 16 14:36:20.106: INFO: stdout: " "
STEP: exposing timestamps
Jul 16 14:36:20.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 log redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv --tail=1 --timestamps'
Jul 16 14:36:20.282: INFO: stderr: ""
Jul 16 14:36:20.282: INFO: stdout: "2019-07-16T14:36:19.210191127Z 1:M 16 Jul 14:36:19.209 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 16 14:36:22.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 log redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv --since=1s'
Jul 16 14:36:22.903: INFO: stderr: ""
Jul 16 14:36:22.903: INFO: stdout: ""
Jul 16 14:36:22.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 log redis-master-8j9rp redis-master --namespace=e2e-tests-kubectl-tdwrv --since=24h'
Jul 16 14:36:23.034: INFO: stderr: ""
Jul 16 14:36:23.034: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 14:36:19.209 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 14:36:19.209 # Server started, Redis version 3.2.12\n1:M 16 Jul 14:36:19.209 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 14:36:19.209 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Jul 16 14:36:23.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tdwrv'
Jul 16 14:36:23.136: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 14:36:23.136: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 16 14:36:23.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-tdwrv'
Jul 16 14:36:23.262: INFO: stderr: "No resources found.\n"
Jul 16 14:36:23.262: INFO: stdout: ""
Jul 16 14:36:23.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 get pods -l name=nginx --namespace=e2e-tests-kubectl-tdwrv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 14:36:23.358: INFO: stderr: ""
Jul 16 14:36:23.358: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:36:23.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tdwrv" for this suite.
Jul 16 14:36:29.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:29.480: INFO: namespace: e2e-tests-kubectl-tdwrv, resource: bindings, ignored listing per whitelist
Jul 16 14:36:31.687: INFO: namespace e2e-tests-kubectl-tdwrv deletion completed in 8.324438976s

• [SLOW TEST:14.400 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:36:31.688: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0716 14:36:41.799495      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 14:36:41.799: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:36:41.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-q5hct" for this suite.
Jul 16 14:36:47.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:47.935: INFO: namespace: e2e-tests-gc-q5hct, resource: bindings, ignored listing per whitelist
Jul 16 14:36:50.137: INFO: namespace e2e-tests-gc-q5hct deletion completed in 8.334075268s

• [SLOW TEST:18.450 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:36:50.140: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-259ecf51-a7d7-11e9-9e69-7a63f556cc5d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:36:52.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rzgdr" for this suite.
Jul 16 14:37:16.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:37:16.346: INFO: namespace: e2e-tests-configmap-rzgdr, resource: bindings, ignored listing per whitelist
Jul 16 14:37:18.621: INFO: namespace e2e-tests-configmap-rzgdr deletion completed in 26.324686846s

• [SLOW TEST:28.481 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:37:18.621: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-f9jn6
Jul 16 14:37:22.722: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-f9jn6
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 14:37:22.724: INFO: Initial restart count of pod liveness-http is 0
Jul 16 14:37:42.761: INFO: Restart count of pod e2e-tests-container-probe-f9jn6/liveness-http is now 1 (20.036820315s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:37:42.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-f9jn6" for this suite.
Jul 16 14:37:48.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:37:48.835: INFO: namespace: e2e-tests-container-probe-f9jn6, resource: bindings, ignored listing per whitelist
Jul 16 14:37:51.101: INFO: namespace e2e-tests-container-probe-f9jn6 deletion completed in 8.326986684s

• [SLOW TEST:32.480 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:37:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 16 14:37:51.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-48w8z,SelfLink:/api/v1/namespaces/e2e-tests-watch-48w8z/configmaps/e2e-watch-test-resource-version,UID:49f2cd13-a7d7-11e9-a7d7-5254228aab6e,ResourceVersion:31832,Generation:0,CreationTimestamp:2019-07-16 14:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 14:37:51.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-48w8z,SelfLink:/api/v1/namespaces/e2e-tests-watch-48w8z/configmaps/e2e-watch-test-resource-version,UID:49f2cd13-a7d7-11e9-a7d7-5254228aab6e,ResourceVersion:31833,Generation:0,CreationTimestamp:2019-07-16 14:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:37:51.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-48w8z" for this suite.
Jul 16 14:37:57.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:37:57.321: INFO: namespace: e2e-tests-watch-48w8z, resource: bindings, ignored listing per whitelist
Jul 16 14:37:59.564: INFO: namespace e2e-tests-watch-48w8z deletion completed in 8.337983629s

• [SLOW TEST:8.463 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:37:59.565: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hlbgr
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-hlbgr
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-hlbgr
Jul 16 14:37:59.653: INFO: Found 0 stateful pods, waiting for 1
Jul 16 14:38:09.657: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 16 14:38:09.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:38:09.955: INFO: stderr: ""
Jul 16 14:38:09.955: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:38:09.955: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:38:09.964: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 16 14:38:19.971: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:38:19.971: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:38:19.987: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999928s
Jul 16 14:38:20.990: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996769032s
Jul 16 14:38:21.994: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993102826s
Jul 16 14:38:23.006: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989214763s
Jul 16 14:38:24.010: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977758347s
Jul 16 14:38:25.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973773104s
Jul 16 14:38:26.019: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969374357s
Jul 16 14:38:27.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964912876s
Jul 16 14:38:28.030: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.956699505s
Jul 16 14:38:29.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 953.513273ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-hlbgr
Jul 16 14:38:30.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:38:30.360: INFO: stderr: ""
Jul 16 14:38:30.360: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:38:30.360: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:38:30.364: INFO: Found 1 stateful pods, waiting for 3
Jul 16 14:38:40.369: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:38:40.369: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:38:40.369: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 16 14:38:40.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:38:40.714: INFO: stderr: ""
Jul 16 14:38:40.714: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:38:40.714: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:38:40.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:38:41.100: INFO: stderr: ""
Jul 16 14:38:41.100: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:38:41.100: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:38:41.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:38:41.476: INFO: stderr: ""
Jul 16 14:38:41.476: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:38:41.476: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:38:41.476: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:38:41.479: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 16 14:38:51.496: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:38:51.496: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:38:51.496: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:38:51.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999153s
Jul 16 14:38:52.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99558089s
Jul 16 14:38:53.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989325349s
Jul 16 14:38:54.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984374459s
Jul 16 14:38:55.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980012587s
Jul 16 14:38:56.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974161649s
Jul 16 14:38:57.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969869472s
Jul 16 14:38:58.544: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963959533s
Jul 16 14:38:59.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959364981s
Jul 16 14:39:00.555: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.538629ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-hlbgr
Jul 16 14:39:01.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:39:02.014: INFO: stderr: ""
Jul 16 14:39:02.015: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:39:02.015: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:39:02.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:39:02.530: INFO: stderr: ""
Jul 16 14:39:02.530: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:39:02.530: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:39:02.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 exec --namespace=e2e-tests-statefulset-hlbgr ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:39:02.949: INFO: stderr: ""
Jul 16 14:39:02.949: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:39:02.949: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:39:02.949: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 16 14:39:22.965: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hlbgr
Jul 16 14:39:22.968: INFO: Scaling statefulset ss to 0
Jul 16 14:39:22.977: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:39:22.979: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:39:22.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hlbgr" for this suite.
Jul 16 14:39:29.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:39:30.483: INFO: namespace: e2e-tests-statefulset-hlbgr, resource: bindings, ignored listing per whitelist
Jul 16 14:39:31.331: INFO: namespace e2e-tests-statefulset-hlbgr deletion completed in 8.329483519s

• [SLOW TEST:91.766 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:39:31.333: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:39:31.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-zpgxz" to be "success or failure"
Jul 16 14:39:31.454: INFO: Pod "downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029562ms
Jul 16 14:39:33.458: INFO: Pod "downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00743475s
Jul 16 14:39:35.463: INFO: Pod "downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011979987s
STEP: Saw pod success
Jul 16 14:39:35.463: INFO: Pod "downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:39:35.466: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:39:35.487: INFO: Waiting for pod downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:39:35.490: INFO: Pod downwardapi-volume-85b3184e-a7d7-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:39:35.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zpgxz" for this suite.
Jul 16 14:39:41.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:39:41.972: INFO: namespace: e2e-tests-downward-api-zpgxz, resource: bindings, ignored listing per whitelist
Jul 16 14:39:43.822: INFO: namespace e2e-tests-downward-api-zpgxz deletion completed in 8.32698425s

• [SLOW TEST:12.490 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:39:43.823: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:39:43.956: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-downward-api-kztbt" to be "success or failure"
Jul 16 14:39:43.959: INFO: Pod "downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886541ms
Jul 16 14:39:45.962: INFO: Pod "downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006140127s
Jul 16 14:39:47.967: INFO: Pod "downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01090606s
STEP: Saw pod success
Jul 16 14:39:47.967: INFO: Pod "downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:39:47.969: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:39:47.994: INFO: Waiting for pod downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:39:47.996: INFO: Pod downwardapi-volume-8d26f649-a7d7-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:39:47.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kztbt" for this suite.
Jul 16 14:39:54.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:39:54.040: INFO: namespace: e2e-tests-downward-api-kztbt, resource: bindings, ignored listing per whitelist
Jul 16 14:39:56.330: INFO: namespace e2e-tests-downward-api-kztbt deletion completed in 8.329999308s

• [SLOW TEST:12.508 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:39:56.330: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-9496b5e7-a7d7-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:39:56.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-7q7bx" to be "success or failure"
Jul 16 14:39:56.444: INFO: Pod "pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295279ms
Jul 16 14:39:58.448: INFO: Pod "pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011302659s
STEP: Saw pod success
Jul 16 14:39:58.448: INFO: Pod "pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:39:58.451: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:39:58.472: INFO: Waiting for pod pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:39:58.475: INFO: Pod pod-configmaps-949775e7-a7d7-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:39:58.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7q7bx" for this suite.
Jul 16 14:40:04.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:40:04.566: INFO: namespace: e2e-tests-configmap-7q7bx, resource: bindings, ignored listing per whitelist
Jul 16 14:40:06.826: INFO: namespace e2e-tests-configmap-7q7bx deletion completed in 8.346943178s

• [SLOW TEST:10.495 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:40:06.826: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qqbjb
Jul 16 14:40:08.937: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qqbjb
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 14:40:08.939: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:44:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qqbjb" for this suite.
Jul 16 14:44:15.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:16.788: INFO: namespace: e2e-tests-container-probe-qqbjb, resource: bindings, ignored listing per whitelist
Jul 16 14:44:17.837: INFO: namespace e2e-tests-container-probe-qqbjb deletion completed in 8.334375951s

• [SLOW TEST:251.011 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:44:17.837: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jul 16 14:44:17.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 create -f - --namespace=e2e-tests-kubectl-j2n6b'
Jul 16 14:44:18.388: INFO: stderr: ""
Jul 16 14:44:18.388: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 14:44:19.392: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:44:19.392: INFO: Found 0 / 1
Jul 16 14:44:20.392: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:44:20.392: INFO: Found 0 / 1
Jul 16 14:44:21.392: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:44:21.393: INFO: Found 1 / 1
Jul 16 14:44:21.393: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 16 14:44:21.396: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:44:21.396: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 14:44:21.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 patch pod redis-master-skxpv --namespace=e2e-tests-kubectl-j2n6b -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 16 14:44:21.577: INFO: stderr: ""
Jul 16 14:44:21.577: INFO: stdout: "pod/redis-master-skxpv patched\n"
STEP: checking annotations
Jul 16 14:44:21.587: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:44:21.587: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:44:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j2n6b" for this suite.
Jul 16 14:44:43.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:43.774: INFO: namespace: e2e-tests-kubectl-j2n6b, resource: bindings, ignored listing per whitelist
Jul 16 14:44:45.924: INFO: namespace e2e-tests-kubectl-j2n6b deletion completed in 24.3314858s

• [SLOW TEST:28.087 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:44:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jul 16 14:44:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453034999 version'
Jul 16 14:44:46.112: INFO: stderr: ""
Jul 16 14:44:46.112: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.5\", GitCommit:\"2166946f41b36dea2c4626f90a77706f426cdea2\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:19:22Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:44:46.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w678d" for this suite.
Jul 16 14:44:52.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:52.229: INFO: namespace: e2e-tests-kubectl-w678d, resource: bindings, ignored listing per whitelist
Jul 16 14:44:54.444: INFO: namespace e2e-tests-kubectl-w678d deletion completed in 8.326870079s

• [SLOW TEST:8.520 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:44:54.446: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 16 14:44:54.567: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32912,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 14:44:54.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32912,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 16 14:45:04.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32928,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 16 14:45:04.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32928,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 16 14:45:14.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32947,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 14:45:14.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32947,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 16 14:45:24.596: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32962,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 14:45:24.596: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-a,UID:464b7dcf-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32962,Generation:0,CreationTimestamp:2019-07-16 14:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 16 14:45:34.610: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-b,UID:5e2853d3-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32977,Generation:0,CreationTimestamp:2019-07-16 14:45:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 14:45:34.611: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-b,UID:5e2853d3-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32977,Generation:0,CreationTimestamp:2019-07-16 14:45:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 16 14:45:44.616: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-b,UID:5e2853d3-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32992,Generation:0,CreationTimestamp:2019-07-16 14:45:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 14:45:44.616: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-c8qbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c8qbp/configmaps/e2e-watch-test-configmap-b,UID:5e2853d3-a7d8-11e9-a7d7-5254228aab6e,ResourceVersion:32992,Generation:0,CreationTimestamp:2019-07-16 14:45:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:45:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-c8qbp" for this suite.
Jul 16 14:46:00.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:46:02.757: INFO: namespace: e2e-tests-watch-c8qbp, resource: bindings, ignored listing per whitelist
Jul 16 14:46:02.957: INFO: namespace e2e-tests-watch-c8qbp deletion completed in 8.334786597s

• [SLOW TEST:68.512 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:46:02.958: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 14:46:03.088: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:03.091: INFO: Number of nodes with available pods: 0
Jul 16 14:46:03.091: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:04.095: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:04.098: INFO: Number of nodes with available pods: 0
Jul 16 14:46:04.098: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:05.099: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:05.105: INFO: Number of nodes with available pods: 1
Jul 16 14:46:05.105: INFO: Node i-taxl6ow1 is running more than one daemon pod
Jul 16 14:46:06.097: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:06.100: INFO: Number of nodes with available pods: 2
Jul 16 14:46:06.100: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 16 14:46:06.117: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:06.121: INFO: Number of nodes with available pods: 1
Jul 16 14:46:06.121: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:07.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:07.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:07.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:08.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:08.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:08.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:09.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:09.128: INFO: Number of nodes with available pods: 1
Jul 16 14:46:09.129: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:10.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:10.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:10.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:11.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:11.129: INFO: Number of nodes with available pods: 1
Jul 16 14:46:11.129: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:12.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:12.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:12.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:13.130: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:13.134: INFO: Number of nodes with available pods: 1
Jul 16 14:46:13.134: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:14.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:14.134: INFO: Number of nodes with available pods: 1
Jul 16 14:46:14.134: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:15.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:15.129: INFO: Number of nodes with available pods: 1
Jul 16 14:46:15.129: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:16.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:16.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:16.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:17.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:17.132: INFO: Number of nodes with available pods: 1
Jul 16 14:46:17.132: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:18.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:18.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:18.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:19.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:19.133: INFO: Number of nodes with available pods: 1
Jul 16 14:46:19.133: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:20.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:20.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:20.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:21.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:21.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:21.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:22.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:22.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:22.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:23.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:23.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:23.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:24.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:24.148: INFO: Number of nodes with available pods: 1
Jul 16 14:46:24.148: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:25.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:25.134: INFO: Number of nodes with available pods: 1
Jul 16 14:46:25.134: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:26.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:26.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:26.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:27.130: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:27.135: INFO: Number of nodes with available pods: 1
Jul 16 14:46:27.135: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:28.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:28.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:28.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:29.129: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:29.134: INFO: Number of nodes with available pods: 1
Jul 16 14:46:29.134: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:30.132: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:30.140: INFO: Number of nodes with available pods: 1
Jul 16 14:46:30.140: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:31.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:31.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:31.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:32.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:32.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:32.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:33.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:33.131: INFO: Number of nodes with available pods: 1
Jul 16 14:46:33.131: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:34.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:34.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:34.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:35.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:35.132: INFO: Number of nodes with available pods: 1
Jul 16 14:46:35.132: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:36.126: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:36.129: INFO: Number of nodes with available pods: 1
Jul 16 14:46:36.129: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:37.129: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:37.133: INFO: Number of nodes with available pods: 1
Jul 16 14:46:37.133: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:38.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:38.132: INFO: Number of nodes with available pods: 1
Jul 16 14:46:38.132: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:39.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:39.134: INFO: Number of nodes with available pods: 1
Jul 16 14:46:39.134: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:40.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:40.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:40.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:41.127: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:41.130: INFO: Number of nodes with available pods: 1
Jul 16 14:46:41.130: INFO: Node i-7znxt5so is running more than one daemon pod
Jul 16 14:46:42.128: INFO: DaemonSet pods can't tolerate node i-auk2ptwc with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 16 14:46:42.132: INFO: Number of nodes with available pods: 2
Jul 16 14:46:42.132: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-lv6st, will wait for the garbage collector to delete the pods
Jul 16 14:46:42.194: INFO: Deleting DaemonSet.extensions daemon-set took: 5.771428ms
Jul 16 14:46:42.295: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.343002ms
Jul 16 14:47:25.798: INFO: Number of nodes with available pods: 0
Jul 16 14:47:25.798: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 14:47:25.800: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-lv6st/daemonsets","resourceVersion":"33228"},"items":null}

Jul 16 14:47:25.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-lv6st/pods","resourceVersion":"33228"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:47:25.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-lv6st" for this suite.
Jul 16 14:47:31.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:47:32.088: INFO: namespace: e2e-tests-daemonsets-lv6st, resource: bindings, ignored listing per whitelist
Jul 16 14:47:34.261: INFO: namespace e2e-tests-daemonsets-lv6st deletion completed in 8.41387429s

• [SLOW TEST:91.303 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:47:34.261: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jul 16 14:47:36.873: INFO: Successfully updated pod "labelsupdatea586ef9e-a7d8-11e9-9e69-7a63f556cc5d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:47:38.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tqxms" for this suite.
Jul 16 14:48:00.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:48:00.943: INFO: namespace: e2e-tests-projected-tqxms, resource: bindings, ignored listing per whitelist
Jul 16 14:48:03.238: INFO: namespace e2e-tests-projected-tqxms deletion completed in 24.332598321s

• [SLOW TEST:28.978 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:48:03.239: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 16 14:48:09.383: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:09.386: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:11.386: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:11.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:13.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:13.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:15.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:15.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:17.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:17.391: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:19.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:19.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:21.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:21.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:23.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:23.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:25.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:25.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:27.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:27.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:29.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:29.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:31.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:31.390: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:33.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:33.393: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:35.390: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:35.394: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:48:37.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:48:37.390: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:48:37.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-tkrv8" for this suite.
Jul 16 14:48:59.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:48:59.505: INFO: namespace: e2e-tests-container-lifecycle-hook-tkrv8, resource: bindings, ignored listing per whitelist
Jul 16 14:49:01.742: INFO: namespace e2e-tests-container-lifecycle-hook-tkrv8 deletion completed in 24.333477035s

• [SLOW TEST:58.503 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:49:01.742: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-d9c0f14a-a7d8-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume configMaps
Jul 16 14:49:02.004: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-configmap-mmpbl" to be "success or failure"
Jul 16 14:49:02.009: INFO: Pod "pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583546ms
Jul 16 14:49:04.013: INFO: Pod "pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008997151s
Jul 16 14:49:06.017: INFO: Pod "pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01246003s
STEP: Saw pod success
Jul 16 14:49:06.017: INFO: Pod "pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:49:06.019: INFO: Trying to get logs from node i-taxl6ow1 pod pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:49:06.036: INFO: Waiting for pod pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:49:06.038: INFO: Pod pod-configmaps-d9c1a49b-a7d8-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:49:06.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mmpbl" for this suite.
Jul 16 14:49:12.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:12.111: INFO: namespace: e2e-tests-configmap-mmpbl, resource: bindings, ignored listing per whitelist
Jul 16 14:49:14.375: INFO: namespace e2e-tests-configmap-mmpbl deletion completed in 8.333248006s

• [SLOW TEST:12.633 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:49:14.375: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-e135c1a7-a7d8-11e9-9e69-7a63f556cc5d
STEP: Creating secret with name secret-projected-all-test-volume-e135c18d-a7d8-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 16 14:49:14.490: INFO: Waiting up to 5m0s for pod "projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-kl6th" to be "success or failure"
Jul 16 14:49:14.497: INFO: Pod "projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462823ms
Jul 16 14:49:16.500: INFO: Pod "projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010256264s
Jul 16 14:49:18.504: INFO: Pod "projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013435568s
STEP: Saw pod success
Jul 16 14:49:18.504: INFO: Pod "projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:49:18.506: INFO: Trying to get logs from node i-taxl6ow1 pod projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 16 14:49:18.522: INFO: Waiting for pod projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:49:18.524: INFO: Pod projected-volume-e135c033-a7d8-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:49:18.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kl6th" for this suite.
Jul 16 14:49:24.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:24.620: INFO: namespace: e2e-tests-projected-kl6th, resource: bindings, ignored listing per whitelist
Jul 16 14:49:26.858: INFO: namespace e2e-tests-projected-kl6th deletion completed in 8.32882176s

• [SLOW TEST:12.483 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:49:26.860: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 16 14:49:26.950: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 14:49:26.965: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 14:49:26.968: INFO: 
Logging pods the kubelet thinks is on node i-7znxt5so before test
Jul 16 14:49:27.007: INFO: alerting-executor-78764b5795-fkxjn from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 14:49:27.007: INFO: tiller-deploy-6f9697dfd9-j9hch from kube-system started at 2019-07-16 12:18:13 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container tiller ready: true, restart count 0
Jul 16 14:49:27.007: INFO: istio-init-crd-11-7plk7 from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container istio-init-crd-11 ready: false, restart count 0
Jul 16 14:49:27.007: INFO: logging-fluentbit-operator-555dd47d6d-8pnkc from kubesphere-logging-system started at 2019-07-16 12:39:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container fluentbit-operator ready: true, restart count 0
Jul 16 14:49:27.007: INFO: openpitrix-job-db-ctrl-job-rhhwx from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container openpitrix-job-db-ctrl ready: false, restart count 5
Jul 16 14:49:27.007: INFO: uc-jenkins-update-center-598f8d6549-4nmnj from kubesphere-devops-system started at 2019-07-16 12:42:29 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container jenkins-update-center ready: true, restart count 0
Jul 16 14:49:27.007: INFO: istio-pilot-9685dfc4b-xxlch from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container discovery ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.007: INFO: kube-state-metrics-7f9c44c88-ljpct from kubesphere-monitoring-system started at 2019-07-16 12:40:14 +0000 UTC (4 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 16 14:49:27.007: INFO: notification-deployment-79974b56c9-ppqc2 from kubesphere-alerting-system started at 2019-07-16 12:40:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container notification ready: true, restart count 0
Jul 16 14:49:27.007: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-6968x from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 14:49:27.007: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 14:49:27.007: INFO: csi-qingcloud-controller-0 from kube-system started at 2019-07-16 12:37:54 +0000 UTC (3 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:49:27.007: INFO: openpitrix-runtime-manager-deployment-5fcbb6f447-vjw7t from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container openpitrix-runtime-manager ready: true, restart count 0
Jul 16 14:49:27.007: INFO: notification-db-init-job-5zrd7 from kubesphere-alerting-system started at 2019-07-16 12:40:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container notification-db-init ready: false, restart count 0
Jul 16 14:49:27.007: INFO: alerting-client-64b7bb9d99-mhx29 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container alerting-client ready: true, restart count 0
Jul 16 14:49:27.007: INFO: istio-policy-b87497cf4-n4kkt from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.007: INFO: 	Container mixer ready: true, restart count 2
Jul 16 14:49:27.007: INFO: ks-jenkins-7f67d57746-k87dc from kubesphere-devops-system started at 2019-07-16 12:43:04 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container ks-jenkins ready: true, restart count 0
Jul 16 14:49:27.007: INFO: alerting-manager-5cf779dc56-47pg6 from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container alerting-manager ready: true, restart count 0
Jul 16 14:49:27.007: INFO: coredns-bbbb94784-9rtpg from kube-system started at 2019-07-16 12:18:09 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container coredns ready: true, restart count 0
Jul 16 14:49:27.007: INFO: istio-galley-689b548d98-jm8l2 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.007: INFO: 	Container galley ready: true, restart count 0
Jul 16 14:49:27.007: INFO: alerting-db-init-job-zgqlj from kubesphere-alerting-system started at 2019-07-16 12:38:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container alerting-db-init ready: false, restart count 0
Jul 16 14:49:27.008: INFO: alerting-watcher-5867dc5ffc-6c22k from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container alerting-watcher ready: true, restart count 0
Jul 16 14:49:27.008: INFO: istio-ingressgateway-5477b7ffd9-6zjgw from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.008: INFO: csi-qingcloud-node-mfxls from kube-system started at 2019-07-16 12:37:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 14:49:27.008: INFO: openpitrix-iam-db-ctrl-job-6zwht from openpitrix-system started at 2019-07-16 12:40:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container openpitrix-iam-db-ctrl ready: false, restart count 3
Jul 16 14:49:27.008: INFO: kubectl-admin-d784b7777-zl544 from kubesphere-controls-system started at 2019-07-16 12:41:39 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container kubectl ready: true, restart count 0
Jul 16 14:49:27.008: INFO: kube-proxy-q6k4p from kube-system started at 2019-07-16 12:19:15 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:49:27.008: INFO: fluent-bit-4bldx from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 14:49:27.008: INFO: alerting-db-ctrl-job-pxdbk from kubesphere-alerting-system started at 2019-07-16 12:41:01 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container alerting-db-ctrl ready: false, restart count 0
Jul 16 14:49:27.008: INFO: openpitrix-task-db-ctrl-job-f5jjj from openpitrix-system started at 2019-07-16 12:40:25 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container openpitrix-task-db-ctrl ready: false, restart count 4
Jul 16 14:49:27.008: INFO: istio-telemetry-758d9c786f-xtfm2 from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container mixer ready: true, restart count 3
Jul 16 14:49:27.008: INFO: calico-node-r5c8r from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:49:27.008: INFO: openpitrix-repo-manager-deployment-84fd5b5fdf-fbr58 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container openpitrix-repo-manager ready: true, restart count 0
Jul 16 14:49:27.008: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:31 +0000 UTC (3 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:49:27.008: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:49:27.008: INFO: metrics-server-85d786dd4d-448wj from kube-system started at 2019-07-16 12:38:51 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container metrics-server ready: true, restart count 0
Jul 16 14:49:27.008: INFO: openpitrix-app-db-ctrl-job-4gk2k from openpitrix-system started at 2019-07-16 12:40:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container openpitrix-app-db-ctrl ready: false, restart count 4
Jul 16 14:49:27.008: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:49:27.008: INFO: openpitrix-iam-service-deployment-864df9fb6f-kbvs8 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container openpitrix-iam-service ready: true, restart count 0
Jul 16 14:49:27.008: INFO: istio-init-crd-10-c6klc from istio-system started at 2019-07-16 12:38:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container istio-init-crd-10 ready: false, restart count 0
Jul 16 14:49:27.008: INFO: ks-docs-77c4796dc9-q4bdj from kubesphere-system started at 2019-07-16 12:39:46 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container ks-docs ready: true, restart count 0
Jul 16 14:49:27.008: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:49:27.008: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:49:27.008: INFO: prometheus-operator-6f5d67dcd4-sc6ft from kubesphere-monitoring-system started at 2019-07-16 12:39:26 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 16 14:49:27.008: INFO: node-exporter-92tpb from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:49:27.008: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:49:27.008: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2019-07-16 12:49:11 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:49:27.008: INFO: notification-db-ctrl-job-c82wv from kubesphere-alerting-system started at 2019-07-16 12:40:37 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.008: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Jul 16 14:49:27.008: INFO: 
Logging pods the kubelet thinks is on node i-taxl6ow1 before test
Jul 16 14:49:27.047: INFO: istio-galley-689b548d98-26dvv from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container galley ready: true, restart count 0
Jul 16 14:49:27.047: INFO: csi-qingcloud-node-vzj9n from kube-system started at 2019-07-16 12:37:58 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jul 16 14:49:27.047: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 16 14:49:27.047: INFO: s2ioperator-0 from kubesphere-devops-system started at 2019-07-16 12:39:09 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:49:27.047: INFO: 	Container manager ready: true, restart count 0
Jul 16 14:49:27.047: INFO: ks-sonarqube-postgresql-5d58f5574b-5r955 from kubesphere-devops-system started at 2019-07-16 12:38:58 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container ks-sonarqube-postgresql ready: true, restart count 0
Jul 16 14:49:27.047: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 12:54:10 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 16 14:49:27.047: INFO: calico-kube-controllers-767548c9d9-mqjxp from kube-system started at 2019-07-16 12:17:28 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 16 14:49:27.047: INFO: jaeger-collector-f579b55fb-ljwcp from istio-system started at 2019-07-16 12:47:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container jaeger-collector ready: true, restart count 3
Jul 16 14:49:27.047: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:36 +0000 UTC (3 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:49:27.047: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:49:27.047: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:49:27.047: INFO: istio-pilot-9685dfc4b-f8qhv from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container discovery ready: true, restart count 0
Jul 16 14:49:27.047: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.047: INFO: openpitrix-category-manager-deployment-7968d789d6-lljbl from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container openpitrix-category-manager ready: true, restart count 0
Jul 16 14:49:27.047: INFO: openpitrix-task-manager-deployment-59578dc9d6-rghp8 from openpitrix-system started at 2019-07-16 12:38:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container openpitrix-task-manager ready: true, restart count 0
Jul 16 14:49:27.047: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-07-16 12:40:41 +0000 UTC (3 container statuses recorded)
Jul 16 14:49:27.047: INFO: 	Container prometheus ready: true, restart count 1
Jul 16 14:49:27.047: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 16 14:49:27.047: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 16 14:49:27.048: INFO: istio-telemetry-758d9c786f-zkkh6 from istio-system started at 2019-07-16 12:42:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container mixer ready: true, restart count 4
Jul 16 14:49:27.048: INFO: ks-devops-db-init-job-h9xvn from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container devops-db-init ready: false, restart count 0
Jul 16 14:49:27.048: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2019-07-16 12:40:31 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container elasticsearch ready: true, restart count 0
Jul 16 14:49:27.048: INFO: istio-policy-b87497cf4-nstpl from istio-system started at 2019-07-16 12:42:54 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container mixer ready: true, restart count 2
Jul 16 14:49:27.048: INFO: istio-ingressgateway-5477b7ffd9-xt7sk from istio-system started at 2019-07-16 13:47:12 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-cluster-manager-deployment-6dcd96b68b-ptl6w from openpitrix-system started at 2019-07-16 12:38:20 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-cluster-manager ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-repo-indexer-deployment-5f4c895b54-vqj26 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-repo-indexer ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-repo-db-ctrl-job-69hdm from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-repo-db-ctrl ready: false, restart count 5
Jul 16 14:49:27.048: INFO: jaeger-operator-8cb967c86-pcznr from istio-system started at 2019-07-16 12:42:40 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container jaeger-operator ready: true, restart count 0
Jul 16 14:49:27.048: INFO: kube-proxy-xn8lr from kube-system started at 2019-07-16 12:18:35 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-job-manager-deployment-588858bcb9-vrlv2 from openpitrix-system started at 2019-07-16 12:38:21 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-job-manager ready: true, restart count 0
Jul 16 14:49:27.048: INFO: ks-devops-db-ctrl-job-4dwlz from kubesphere-devops-system started at 2019-07-16 12:40:30 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container devops-db-ctrl ready: false, restart count 0
Jul 16 14:49:27.048: INFO: sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-dpktl from heptio-sonobuoy started at 2019-07-16 12:54:16 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 14:49:27.048: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 14:49:27.048: INFO: openpitrix-cluster-db-ctrl-job-mqp4p from openpitrix-system started at 2019-07-16 12:40:22 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-cluster-db-ctrl ready: false, restart count 2
Jul 16 14:49:27.048: INFO: openpitrix-db-init-job-4g8dq from openpitrix-system started at 2019-07-16 12:40:26 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-db-init ready: false, restart count 0
Jul 16 14:49:27.048: INFO: default-http-backend-96d94689d-sl8zp from kubesphere-controls-system started at 2019-07-16 12:38:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-etcd-deployment-54bc9bb948-s6wmk from openpitrix-system started at 2019-07-16 12:39:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-etcd ready: true, restart count 0
Jul 16 14:49:27.048: INFO: istio-citadel-5f886dc9b4-hxx8p from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container citadel ready: true, restart count 0
Jul 16 14:49:27.048: INFO: istio-sidecar-injector-74666b458c-v8r9r from istio-system started at 2019-07-16 12:42:38 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
Jul 16 14:49:27.048: INFO: node-exporter-hkg6g from kubesphere-monitoring-system started at 2019-07-16 12:39:27 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:49:27.048: INFO: alerting-executor-78764b5795-v7tvg from kubesphere-alerting-system started at 2019-07-16 12:40:57 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container alerting-adapter ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container alerting-executor ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-app-manager-deployment-595dcd76f-58qvb from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-app-manager ready: true, restart count 0
Jul 16 14:49:27.048: INFO: istio-ingressgateway-5477b7ffd9-mqd6p from istio-system started at 2019-07-16 12:42:54 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container istio-proxy ready: true, restart count 0
Jul 16 14:49:27.048: INFO: jaeger-query-6b5d8f7bcd-pxqtq from istio-system started at 2019-07-16 12:47:02 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container jaeger-agent ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container jaeger-query ready: true, restart count 1
Jul 16 14:49:27.048: INFO: openpitrix-minio-deployment-84d5f9c94b-v4gkn from openpitrix-system started at 2019-07-16 12:39:12 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-minio ready: true, restart count 0
Jul 16 14:49:27.048: INFO: fluent-bit-r8xb4 from kubesphere-logging-system started at 2019-07-16 12:39:38 +0000 UTC (2 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container config-reloader ready: true, restart count 0
Jul 16 14:49:27.048: INFO: 	Container fluent-bit ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-api-gateway-deployment-587cc46874-gwm9j from openpitrix-system started at 2019-07-16 12:38:19 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-api-gateway ready: true, restart count 0
Jul 16 14:49:27.048: INFO: openpitrix-runtime-db-ctrl-job-gcr9h from openpitrix-system started at 2019-07-16 12:40:24 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container openpitrix-runtime-db-ctrl ready: false, restart count 5
Jul 16 14:49:27.048: INFO: dns-autoscaler-89c7bbd57-7x9kr from kube-system started at 2019-07-16 12:18:07 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container autoscaler ready: true, restart count 0
Jul 16 14:49:27.048: INFO: calico-node-js6px from kube-system started at 2019-07-16 12:17:23 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:49:27.048: INFO: ks-sonarqube-sonarqube-9c5876cd8-jvl46 from kubesphere-devops-system started at 2019-07-16 12:38:03 +0000 UTC (1 container statuses recorded)
Jul 16 14:49:27.048: INFO: 	Container sonarqube ready: true, restart count 4
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node i-7znxt5so
STEP: verifying the node has the label node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod sonobuoy requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-6968x requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod sonobuoy-systemd-logs-daemon-set-81f60fd3753c4bc0-dpktl requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-citadel-5f886dc9b4-hxx8p requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-galley-689b548d98-26dvv requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-galley-689b548d98-jm8l2 requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod istio-ingressgateway-5477b7ffd9-6zjgw requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod istio-ingressgateway-5477b7ffd9-mqd6p requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-ingressgateway-5477b7ffd9-xt7sk requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-pilot-9685dfc4b-f8qhv requesting resource cpu=600m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-pilot-9685dfc4b-xxlch requesting resource cpu=600m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod istio-policy-b87497cf4-n4kkt requesting resource cpu=110m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod istio-policy-b87497cf4-nstpl requesting resource cpu=110m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-sidecar-injector-74666b458c-v8r9r requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod istio-telemetry-758d9c786f-xtfm2 requesting resource cpu=600m on Node i-7znxt5so
Jul 16 14:49:27.124: INFO: Pod istio-telemetry-758d9c786f-zkkh6 requesting resource cpu=600m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod jaeger-collector-f579b55fb-ljwcp requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod jaeger-operator-8cb967c86-pcznr requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod jaeger-query-6b5d8f7bcd-pxqtq requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.124: INFO: Pod calico-kube-controllers-767548c9d9-mqjxp requesting resource cpu=30m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod calico-node-js6px requesting resource cpu=150m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod calico-node-r5c8r requesting resource cpu=150m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod coredns-bbbb94784-9rtpg requesting resource cpu=100m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod csi-qingcloud-controller-0 requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod csi-qingcloud-node-mfxls requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod csi-qingcloud-node-vzj9n requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod dns-autoscaler-89c7bbd57-7x9kr requesting resource cpu=20m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod kube-proxy-q6k4p requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod kube-proxy-xn8lr requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod metrics-server-85d786dd4d-448wj requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod tiller-deploy-6f9697dfd9-j9hch requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod alerting-client-64b7bb9d99-mhx29 requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod alerting-executor-78764b5795-fkxjn requesting resource cpu=20m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod alerting-executor-78764b5795-v7tvg requesting resource cpu=20m on Node i-taxl6ow1
Jul 16 14:49:27.125: INFO: Pod alerting-manager-5cf779dc56-47pg6 requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod alerting-watcher-5867dc5ffc-6c22k requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.125: INFO: Pod notification-deployment-79974b56c9-ppqc2 requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.128: INFO: Pod default-http-backend-96d94689d-sl8zp requesting resource cpu=10m on Node i-taxl6ow1
Jul 16 14:49:27.131: INFO: Pod kubectl-admin-d784b7777-zl544 requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod ks-jenkins-7f67d57746-k87dc requesting resource cpu=200m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod ks-sonarqube-postgresql-5d58f5574b-5r955 requesting resource cpu=100m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod ks-sonarqube-sonarqube-9c5876cd8-jvl46 requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod s2ioperator-0 requesting resource cpu=25m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod uc-jenkins-update-center-598f8d6549-4nmnj requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod elasticsearch-logging-data-0 requesting resource cpu=25m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod elasticsearch-logging-data-1 requesting resource cpu=25m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod elasticsearch-logging-discovery-0 requesting resource cpu=25m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod fluent-bit-4bldx requesting resource cpu=0m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod fluent-bit-r8xb4 requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod logging-fluentbit-operator-555dd47d6d-8pnkc requesting resource cpu=25m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod kube-state-metrics-7f9c44c88-ljpct requesting resource cpu=136m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod node-exporter-92tpb requesting resource cpu=112m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod node-exporter-hkg6g requesting resource cpu=112m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod prometheus-k8s-0 requesting resource cpu=155m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod prometheus-k8s-1 requesting resource cpu=155m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod prometheus-k8s-system-0 requesting resource cpu=135m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod prometheus-k8s-system-1 requesting resource cpu=135m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod prometheus-operator-6f5d67dcd4-sc6ft requesting resource cpu=80m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod ks-docs-77c4796dc9-q4bdj requesting resource cpu=10m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod openpitrix-api-gateway-deployment-587cc46874-gwm9j requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-app-manager-deployment-595dcd76f-58qvb requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-category-manager-deployment-7968d789d6-lljbl requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-cluster-manager-deployment-6dcd96b68b-ptl6w requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-etcd-deployment-54bc9bb948-s6wmk requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-iam-service-deployment-864df9fb6f-kbvs8 requesting resource cpu=50m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod openpitrix-job-manager-deployment-588858bcb9-vrlv2 requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-minio-deployment-84d5f9c94b-v4gkn requesting resource cpu=0m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-repo-indexer-deployment-5f4c895b54-vqj26 requesting resource cpu=50m on Node i-taxl6ow1
Jul 16 14:49:27.132: INFO: Pod openpitrix-repo-manager-deployment-84fd5b5fdf-fbr58 requesting resource cpu=50m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod openpitrix-runtime-manager-deployment-5fcbb6f447-vjw7t requesting resource cpu=50m on Node i-7znxt5so
Jul 16 14:49:27.132: INFO: Pod openpitrix-task-manager-deployment-59578dc9d6-rghp8 requesting resource cpu=50m on Node i-taxl6ow1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c242e8-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab4eaf2256], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-84f5b/filler-pod-e8c242e8-a7d8-11e9-9e69-7a63f556cc5d to i-7znxt5so]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c242e8-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab9630a268], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c242e8-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab99df522e], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c242e8-a7d8-11e9-9e69-7a63f556cc5d.15b1eaabaa8f9f2a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c4544a-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab4ed67d32], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-84f5b/filler-pod-e8c4544a-a7d8-11e9-9e69-7a63f556cc5d to i-taxl6ow1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c4544a-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab938db066], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c4544a-a7d8-11e9-9e69-7a63f556cc5d.15b1eaab97a059e7], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e8c4544a-a7d8-11e9-9e69-7a63f556cc5d.15b1eaaba6457c12], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b1eaac3eb86744], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node i-7znxt5so
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node i-taxl6ow1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:49:32.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-84f5b" for this suite.
Jul 16 14:49:38.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:38.825: INFO: namespace: e2e-tests-sched-pred-84f5b, resource: bindings, ignored listing per whitelist
Jul 16 14:49:40.575: INFO: namespace e2e-tests-sched-pred-84f5b deletion completed in 8.341758777s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:13.715 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:49:40.575: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f0d124db-a7d8-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:49:40.715: INFO: Waiting up to 5m0s for pod "pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-j4z6l" to be "success or failure"
Jul 16 14:49:40.718: INFO: Pod "pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592228ms
Jul 16 14:49:42.721: INFO: Pod "pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00603699s
STEP: Saw pod success
Jul 16 14:49:42.721: INFO: Pod "pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:49:42.726: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:49:42.753: INFO: Waiting for pod pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:49:42.755: INFO: Pod pod-secrets-f0d9b9d2-a7d8-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:49:42.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j4z6l" for this suite.
Jul 16 14:49:48.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:50.493: INFO: namespace: e2e-tests-secrets-j4z6l, resource: bindings, ignored listing per whitelist
Jul 16 14:49:51.091: INFO: namespace e2e-tests-secrets-j4z6l deletion completed in 8.330961825s
STEP: Destroying namespace "e2e-tests-secret-namespace-kwngq" for this suite.
Jul 16 14:49:57.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:57.321: INFO: namespace: e2e-tests-secret-namespace-kwngq, resource: bindings, ignored listing per whitelist
Jul 16 14:49:59.426: INFO: namespace e2e-tests-secret-namespace-kwngq deletion completed in 8.334666861s

• [SLOW TEST:18.851 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:49:59.426: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:49:59.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-qp54f" for this suite.
Jul 16 14:50:05.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:50:05.616: INFO: namespace: e2e-tests-services-qp54f, resource: bindings, ignored listing per whitelist
Jul 16 14:50:07.861: INFO: namespace e2e-tests-services-qp54f deletion completed in 8.330574343s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:8.436 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:50:07.862: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:50:11.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-chlgb" for this suite.
Jul 16 14:50:18.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:50:19.627: INFO: namespace: e2e-tests-kubelet-test-chlgb, resource: bindings, ignored listing per whitelist
Jul 16 14:50:20.327: INFO: namespace e2e-tests-kubelet-test-chlgb deletion completed in 8.329284827s

• [SLOW TEST:12.465 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:50:20.327: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:50:20.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-projected-h8n57" to be "success or failure"
Jul 16 14:50:20.424: INFO: Pod "downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.348223ms
Jul 16 14:50:22.428: INFO: Pod "downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007211964s
Jul 16 14:50:24.432: INFO: Pod "downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011498388s
STEP: Saw pod success
Jul 16 14:50:24.432: INFO: Pod "downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:50:24.435: INFO: Trying to get logs from node i-taxl6ow1 pod downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d container client-container: <nil>
STEP: delete the pod
Jul 16 14:50:24.467: INFO: Waiting for pod downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:50:24.472: INFO: Pod downwardapi-volume-08847774-a7d9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:50:24.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h8n57" for this suite.
Jul 16 14:50:30.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:50:30.527: INFO: namespace: e2e-tests-projected-h8n57, resource: bindings, ignored listing per whitelist
Jul 16 14:50:32.814: INFO: namespace e2e-tests-projected-h8n57 deletion completed in 8.338087632s

• [SLOW TEST:12.487 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jul 16 14:50:32.815: INFO: >>> kubeConfig: /tmp/kubeconfig-453034999
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0ff81460-a7d9-11e9-9e69-7a63f556cc5d
STEP: Creating a pod to test consume secrets
Jul 16 14:50:32.925: INFO: Waiting up to 5m0s for pod "pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d" in namespace "e2e-tests-secrets-49h2s" to be "success or failure"
Jul 16 14:50:32.928: INFO: Pod "pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012004ms
Jul 16 14:50:34.931: INFO: Pod "pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00545164s
Jul 16 14:50:36.935: INFO: Pod "pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009297616s
STEP: Saw pod success
Jul 16 14:50:36.935: INFO: Pod "pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d" satisfied condition "success or failure"
Jul 16 14:50:36.938: INFO: Trying to get logs from node i-taxl6ow1 pod pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:50:36.959: INFO: Waiting for pod pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d to disappear
Jul 16 14:50:36.962: INFO: Pod pod-secrets-0ff8955c-a7d9-11e9-9e69-7a63f556cc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jul 16 14:50:36.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-49h2s" for this suite.
Jul 16 14:50:42.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:50:43.067: INFO: namespace: e2e-tests-secrets-49h2s, resource: bindings, ignored listing per whitelist
Jul 16 14:50:45.300: INFO: namespace e2e-tests-secrets-49h2s deletion completed in 8.333798746s

• [SLOW TEST:12.485 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSJul 16 14:50:45.301: INFO: Running AfterSuite actions on all nodes
Jul 16 14:50:45.301: INFO: Running AfterSuite actions on node 1
Jul 16 14:50:45.301: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6924.503 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h55m25.642739895s
Test Suite Passed
