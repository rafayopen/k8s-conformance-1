I0529 04:37:03.201472      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-762096317
I0529 04:37:03.202232      15 e2e.go:224] Starting e2e run "67496f8e-81cb-11e9-b4f9-d20c9d8615e3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1559104622 - Will randomize all specs
Will run 201 of 1946 specs

May 29 04:37:03.322: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 04:37:03.324: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 29 04:37:03.350: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 29 04:37:03.378: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 29 04:37:03.378: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 29 04:37:03.378: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 29 04:37:03.385: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-cni-networkmonitor' (0 seconds elapsed)
May 29 04:37:03.385: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
May 29 04:37:03.385: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'blobfuse-flexvol-installer' (0 seconds elapsed)
May 29 04:37:03.385: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'keyvault-flexvolume' (0 seconds elapsed)
May 29 04:37:03.385: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 29 04:37:03.385: INFO: e2e test version: v1.13.0
May 29 04:37:03.386: INFO: kube-apiserver version: v1.13.6
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:37:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
May 29 04:37:03.470: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-68009cf9-81cb-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 04:37:03.480: INFO: Waiting up to 5m0s for pod "pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-jdgb9" to be "success or failure"
May 29 04:37:03.484: INFO: Pod "pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174271ms
May 29 04:37:05.487: INFO: Pod "pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006953143s
May 29 04:37:07.490: INFO: Pod "pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009806058s
STEP: Saw pod success
May 29 04:37:07.490: INFO: Pod "pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:37:07.492: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 04:37:07.533: INFO: Waiting for pod pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:37:07.535: INFO: Pod pod-secrets-6800fe10-81cb-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:37:07.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jdgb9" for this suite.
May 29 04:37:13.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:37:13.620: INFO: namespace: e2e-tests-secrets-jdgb9, resource: bindings, ignored listing per whitelist
May 29 04:37:13.626: INFO: namespace e2e-tests-secrets-jdgb9 deletion completed in 6.087035243s

â€¢ [SLOW TEST:10.240 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:37:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-c9295
I0529 04:37:13.717948      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-c9295, replica count: 1
I0529 04:37:14.768414      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 04:37:15.768675      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 04:37:16.768971      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 29 04:37:16.885: INFO: Created: latency-svc-6gq7c
May 29 04:37:16.892: INFO: Got endpoints: latency-svc-6gq7c [22.987939ms]
May 29 04:37:16.929: INFO: Created: latency-svc-6zms8
May 29 04:37:16.933: INFO: Got endpoints: latency-svc-6zms8 [41.154913ms]
May 29 04:37:16.950: INFO: Created: latency-svc-4tn97
May 29 04:37:16.953: INFO: Got endpoints: latency-svc-4tn97 [61.322772ms]
May 29 04:37:16.973: INFO: Created: latency-svc-mms2f
May 29 04:37:16.978: INFO: Got endpoints: latency-svc-mms2f [85.9348ms]
May 29 04:37:16.978: INFO: Created: latency-svc-7mc65
May 29 04:37:16.992: INFO: Got endpoints: latency-svc-7mc65 [100.2079ms]
May 29 04:37:16.995: INFO: Created: latency-svc-b8br9
May 29 04:37:17.012: INFO: Got endpoints: latency-svc-b8br9 [119.923262ms]
May 29 04:37:17.020: INFO: Created: latency-svc-5fft4
May 29 04:37:17.030: INFO: Got endpoints: latency-svc-5fft4 [137.859737ms]
May 29 04:37:17.030: INFO: Created: latency-svc-zvjqm
May 29 04:37:17.044: INFO: Got endpoints: latency-svc-zvjqm [151.629341ms]
May 29 04:37:17.048: INFO: Created: latency-svc-2kphs
May 29 04:37:17.055: INFO: Got endpoints: latency-svc-2kphs [162.806563ms]
May 29 04:37:17.063: INFO: Created: latency-svc-glc9v
May 29 04:37:17.066: INFO: Got endpoints: latency-svc-glc9v [173.839686ms]
May 29 04:37:17.084: INFO: Created: latency-svc-9d6bt
May 29 04:37:17.092: INFO: Created: latency-svc-txcf4
May 29 04:37:17.095: INFO: Got endpoints: latency-svc-9d6bt [202.420087ms]
May 29 04:37:17.111: INFO: Got endpoints: latency-svc-txcf4 [218.363876ms]
May 29 04:37:17.113: INFO: Created: latency-svc-q4gk9
May 29 04:37:17.122: INFO: Created: latency-svc-nctkg
May 29 04:37:17.122: INFO: Got endpoints: latency-svc-q4gk9 [229.902596ms]
May 29 04:37:17.137: INFO: Got endpoints: latency-svc-nctkg [244.541094ms]
May 29 04:37:17.158: INFO: Created: latency-svc-sqtxm
May 29 04:37:17.172: INFO: Got endpoints: latency-svc-sqtxm [279.823347ms]
May 29 04:37:17.174: INFO: Created: latency-svc-bthzs
May 29 04:37:17.193: INFO: Got endpoints: latency-svc-bthzs [300.8923ms]
May 29 04:37:17.195: INFO: Created: latency-svc-dxn8h
May 29 04:37:17.208: INFO: Got endpoints: latency-svc-dxn8h [270.423913ms]
May 29 04:37:17.211: INFO: Created: latency-svc-kqf7h
May 29 04:37:17.226: INFO: Got endpoints: latency-svc-kqf7h [272.875896ms]
May 29 04:37:17.228: INFO: Created: latency-svc-79hpl
May 29 04:37:17.237: INFO: Got endpoints: latency-svc-79hpl [259.108192ms]
May 29 04:37:17.240: INFO: Created: latency-svc-s4lnn
May 29 04:37:17.245: INFO: Got endpoints: latency-svc-s4lnn [249.585659ms]
May 29 04:37:17.260: INFO: Created: latency-svc-j6smd
May 29 04:37:17.267: INFO: Got endpoints: latency-svc-j6smd [255.16102ms]
May 29 04:37:17.281: INFO: Created: latency-svc-zsjfl
May 29 04:37:17.288: INFO: Created: latency-svc-b7fp9
May 29 04:37:17.290: INFO: Got endpoints: latency-svc-zsjfl [260.129486ms]
May 29 04:37:17.296: INFO: Got endpoints: latency-svc-b7fp9 [252.213341ms]
May 29 04:37:17.307: INFO: Created: latency-svc-6x7r9
May 29 04:37:17.318: INFO: Got endpoints: latency-svc-6x7r9 [263.369163ms]
May 29 04:37:17.321: INFO: Created: latency-svc-svld8
May 29 04:37:17.332: INFO: Got endpoints: latency-svc-svld8 [266.003344ms]
May 29 04:37:17.336: INFO: Created: latency-svc-6f9xw
May 29 04:37:17.349: INFO: Created: latency-svc-qv29q
May 29 04:37:17.353: INFO: Got endpoints: latency-svc-6f9xw [258.713895ms]
May 29 04:37:17.360: INFO: Got endpoints: latency-svc-qv29q [249.236361ms]
May 29 04:37:17.384: INFO: Created: latency-svc-gzxcx
May 29 04:37:17.394: INFO: Got endpoints: latency-svc-gzxcx [271.474306ms]
May 29 04:37:17.400: INFO: Created: latency-svc-cphlf
May 29 04:37:17.407: INFO: Created: latency-svc-g8cxm
May 29 04:37:17.412: INFO: Got endpoints: latency-svc-cphlf [272.236601ms]
May 29 04:37:17.420: INFO: Got endpoints: latency-svc-g8cxm [247.649773ms]
May 29 04:37:17.434: INFO: Created: latency-svc-9tchf
May 29 04:37:17.441: INFO: Created: latency-svc-jgq56
May 29 04:37:17.443: INFO: Got endpoints: latency-svc-9tchf [248.350267ms]
May 29 04:37:17.457: INFO: Created: latency-svc-xc9kb
May 29 04:37:17.457: INFO: Got endpoints: latency-svc-jgq56 [248.332168ms]
May 29 04:37:17.486: INFO: Got endpoints: latency-svc-xc9kb [260.178885ms]
May 29 04:37:17.534: INFO: Created: latency-svc-48wbd
May 29 04:37:17.551: INFO: Created: latency-svc-l5mpr
May 29 04:37:17.553: INFO: Got endpoints: latency-svc-48wbd [316.320194ms]
May 29 04:37:17.570: INFO: Got endpoints: latency-svc-l5mpr [325.232231ms]
May 29 04:37:17.576: INFO: Created: latency-svc-s9g2n
May 29 04:37:17.589: INFO: Got endpoints: latency-svc-s9g2n [321.860355ms]
May 29 04:37:17.589: INFO: Created: latency-svc-zpj98
May 29 04:37:17.613: INFO: Got endpoints: latency-svc-zpj98 [323.208946ms]
May 29 04:37:17.619: INFO: Created: latency-svc-wn6gr
May 29 04:37:17.638: INFO: Got endpoints: latency-svc-wn6gr [342.234112ms]
May 29 04:37:17.640: INFO: Created: latency-svc-mkl64
May 29 04:37:17.663: INFO: Created: latency-svc-9hn4m
May 29 04:37:17.664: INFO: Got endpoints: latency-svc-mkl64 [345.893187ms]
May 29 04:37:17.679: INFO: Created: latency-svc-pvssl
May 29 04:37:17.692: INFO: Got endpoints: latency-svc-pvssl [336.107556ms]
May 29 04:37:17.692: INFO: Got endpoints: latency-svc-9hn4m [360.357687ms]
May 29 04:37:17.716: INFO: Created: latency-svc-qsq8q
May 29 04:37:17.724: INFO: Got endpoints: latency-svc-qsq8q [363.738163ms]
May 29 04:37:17.732: INFO: Created: latency-svc-9tlbv
May 29 04:37:17.738: INFO: Created: latency-svc-qmc9t
May 29 04:37:17.739: INFO: Got endpoints: latency-svc-9tlbv [345.40059ms]
May 29 04:37:17.757: INFO: Got endpoints: latency-svc-qmc9t [344.989794ms]
May 29 04:37:17.783: INFO: Created: latency-svc-m7z5f
May 29 04:37:17.798: INFO: Created: latency-svc-wnkjn
May 29 04:37:17.802: INFO: Got endpoints: latency-svc-m7z5f [382.114934ms]
May 29 04:37:17.818: INFO: Created: latency-svc-pnbtw
May 29 04:37:17.821: INFO: Got endpoints: latency-svc-wnkjn [377.309768ms]
May 29 04:37:17.849: INFO: Created: latency-svc-xh7hx
May 29 04:37:17.854: INFO: Got endpoints: latency-svc-pnbtw [396.296535ms]
May 29 04:37:17.863: INFO: Created: latency-svc-gd24v
May 29 04:37:17.866: INFO: Got endpoints: latency-svc-xh7hx [379.514353ms]
May 29 04:37:17.872: INFO: Got endpoints: latency-svc-gd24v [316.283393ms]
May 29 04:37:17.887: INFO: Created: latency-svc-6cc76
May 29 04:37:17.894: INFO: Got endpoints: latency-svc-6cc76 [323.585743ms]
May 29 04:37:17.897: INFO: Created: latency-svc-bxv4g
May 29 04:37:17.914: INFO: Created: latency-svc-ndmxq
May 29 04:37:17.940: INFO: Created: latency-svc-p2znl
May 29 04:37:17.978: INFO: Created: latency-svc-99s9z
May 29 04:37:17.980: INFO: Got endpoints: latency-svc-bxv4g [390.880974ms]
May 29 04:37:17.988: INFO: Created: latency-svc-m6l8h
May 29 04:37:17.993: INFO: Got endpoints: latency-svc-ndmxq [379.531553ms]
May 29 04:37:18.011: INFO: Created: latency-svc-7lqsw
May 29 04:37:18.029: INFO: Created: latency-svc-xcq2t
May 29 04:37:18.047: INFO: Created: latency-svc-sjtxf
May 29 04:37:18.050: INFO: Got endpoints: latency-svc-p2znl [411.922427ms]
May 29 04:37:18.076: INFO: Created: latency-svc-rtbnc
May 29 04:37:18.088: INFO: Created: latency-svc-mlzdq
May 29 04:37:18.100: INFO: Got endpoints: latency-svc-99s9z [432.152587ms]
May 29 04:37:18.105: INFO: Created: latency-svc-597qr
May 29 04:37:18.127: INFO: Created: latency-svc-ktqzw
May 29 04:37:18.140: INFO: Got endpoints: latency-svc-m6l8h [442.253416ms]
May 29 04:37:18.141: INFO: Created: latency-svc-zbgks
May 29 04:37:18.160: INFO: Created: latency-svc-8smgm
May 29 04:37:18.180: INFO: Created: latency-svc-8m4wq
May 29 04:37:18.194: INFO: Got endpoints: latency-svc-7lqsw [501.201106ms]
May 29 04:37:18.197: INFO: Created: latency-svc-dxvsh
May 29 04:37:18.220: INFO: Created: latency-svc-g6xzk
May 29 04:37:18.236: INFO: Created: latency-svc-mh8mt
May 29 04:37:18.251: INFO: Got endpoints: latency-svc-xcq2t [527.222024ms]
May 29 04:37:18.252: INFO: Created: latency-svc-tcjcx
May 29 04:37:18.272: INFO: Created: latency-svc-2vk86
May 29 04:37:18.296: INFO: Got endpoints: latency-svc-sjtxf [556.65632ms]
May 29 04:37:18.300: INFO: Created: latency-svc-djhj6
May 29 04:37:18.311: INFO: Created: latency-svc-bgdld
May 29 04:37:18.334: INFO: Created: latency-svc-zxzg7
May 29 04:37:18.339: INFO: Got endpoints: latency-svc-rtbnc [580.418954ms]
May 29 04:37:18.355: INFO: Created: latency-svc-vn768
May 29 04:37:18.387: INFO: Got endpoints: latency-svc-mlzdq [582.747839ms]
May 29 04:37:18.422: INFO: Created: latency-svc-lcg59
May 29 04:37:18.447: INFO: Got endpoints: latency-svc-597qr [625.117343ms]
May 29 04:37:18.479: INFO: Created: latency-svc-6nc74
May 29 04:37:18.489: INFO: Got endpoints: latency-svc-ktqzw [635.451372ms]
May 29 04:37:18.506: INFO: Created: latency-svc-2szwp
May 29 04:37:18.548: INFO: Got endpoints: latency-svc-zbgks [682.174246ms]
May 29 04:37:18.564: INFO: Created: latency-svc-zz57v
May 29 04:37:18.586: INFO: Got endpoints: latency-svc-8smgm [713.849526ms]
May 29 04:37:18.602: INFO: Created: latency-svc-2x6mv
May 29 04:37:18.643: INFO: Got endpoints: latency-svc-8m4wq [748.979081ms]
May 29 04:37:18.662: INFO: Created: latency-svc-t98gt
May 29 04:37:18.697: INFO: Got endpoints: latency-svc-dxvsh [716.840305ms]
May 29 04:37:18.717: INFO: Created: latency-svc-jq6nk
May 29 04:37:18.747: INFO: Got endpoints: latency-svc-g6xzk [753.521051ms]
May 29 04:37:18.765: INFO: Created: latency-svc-hcvxn
May 29 04:37:18.787: INFO: Got endpoints: latency-svc-mh8mt [736.603569ms]
May 29 04:37:18.805: INFO: Created: latency-svc-2g4bl
May 29 04:37:18.842: INFO: Got endpoints: latency-svc-tcjcx [739.734346ms]
May 29 04:37:18.862: INFO: Created: latency-svc-6kbvj
May 29 04:37:18.887: INFO: Got endpoints: latency-svc-2vk86 [747.178095ms]
May 29 04:37:18.901: INFO: Created: latency-svc-gckl7
May 29 04:37:18.938: INFO: Got endpoints: latency-svc-djhj6 [743.892817ms]
May 29 04:37:18.952: INFO: Created: latency-svc-sgtzn
May 29 04:37:18.993: INFO: Got endpoints: latency-svc-bgdld [741.106137ms]
May 29 04:37:19.006: INFO: Created: latency-svc-6lnx2
May 29 04:37:19.038: INFO: Got endpoints: latency-svc-zxzg7 [741.929931ms]
May 29 04:37:19.054: INFO: Created: latency-svc-gnwzj
May 29 04:37:19.090: INFO: Got endpoints: latency-svc-vn768 [750.280474ms]
May 29 04:37:19.107: INFO: Created: latency-svc-rjxpm
May 29 04:37:19.137: INFO: Got endpoints: latency-svc-lcg59 [749.888077ms]
May 29 04:37:19.156: INFO: Created: latency-svc-wwr54
May 29 04:37:19.187: INFO: Got endpoints: latency-svc-6nc74 [740.84444ms]
May 29 04:37:19.224: INFO: Created: latency-svc-v2wjv
May 29 04:37:19.241: INFO: Got endpoints: latency-svc-2szwp [751.851864ms]
May 29 04:37:19.258: INFO: Created: latency-svc-cbkml
May 29 04:37:19.287: INFO: Got endpoints: latency-svc-zz57v [738.589857ms]
May 29 04:37:19.305: INFO: Created: latency-svc-vxmbl
May 29 04:37:19.338: INFO: Got endpoints: latency-svc-2x6mv [752.144463ms]
May 29 04:37:19.361: INFO: Created: latency-svc-nbcnf
May 29 04:37:19.387: INFO: Got endpoints: latency-svc-t98gt [738.35576ms]
May 29 04:37:19.412: INFO: Created: latency-svc-kllm4
May 29 04:37:19.438: INFO: Got endpoints: latency-svc-jq6nk [735.272882ms]
May 29 04:37:19.458: INFO: Created: latency-svc-7fgxq
May 29 04:37:19.487: INFO: Got endpoints: latency-svc-hcvxn [737.531166ms]
May 29 04:37:19.500: INFO: Created: latency-svc-csfp8
May 29 04:37:19.548: INFO: Got endpoints: latency-svc-2g4bl [760.382308ms]
May 29 04:37:19.570: INFO: Created: latency-svc-tdm52
May 29 04:37:19.587: INFO: Got endpoints: latency-svc-6kbvj [744.967415ms]
May 29 04:37:19.604: INFO: Created: latency-svc-m7xdc
May 29 04:37:19.637: INFO: Got endpoints: latency-svc-gckl7 [749.401084ms]
May 29 04:37:19.661: INFO: Created: latency-svc-r7tnr
May 29 04:37:19.698: INFO: Got endpoints: latency-svc-sgtzn [760.108711ms]
May 29 04:37:19.713: INFO: Created: latency-svc-ktvn8
May 29 04:37:19.737: INFO: Got endpoints: latency-svc-6lnx2 [743.420327ms]
May 29 04:37:19.752: INFO: Created: latency-svc-d9qsl
May 29 04:37:19.786: INFO: Got endpoints: latency-svc-gnwzj [748.560092ms]
May 29 04:37:19.803: INFO: Created: latency-svc-mqnbj
May 29 04:37:19.836: INFO: Got endpoints: latency-svc-rjxpm [746.660605ms]
May 29 04:37:19.851: INFO: Created: latency-svc-5czfr
May 29 04:37:19.892: INFO: Got endpoints: latency-svc-wwr54 [754.258852ms]
May 29 04:37:19.914: INFO: Created: latency-svc-92csh
May 29 04:37:19.936: INFO: Got endpoints: latency-svc-v2wjv [748.86549ms]
May 29 04:37:19.960: INFO: Created: latency-svc-cltm6
May 29 04:37:19.986: INFO: Got endpoints: latency-svc-cbkml [745.576812ms]
May 29 04:37:20.015: INFO: Created: latency-svc-lddbk
May 29 04:37:20.052: INFO: Got endpoints: latency-svc-vxmbl [762.446696ms]
May 29 04:37:20.073: INFO: Created: latency-svc-s5ghz
May 29 04:37:20.086: INFO: Got endpoints: latency-svc-nbcnf [747.870997ms]
May 29 04:37:20.100: INFO: Created: latency-svc-nzndr
May 29 04:37:20.137: INFO: Got endpoints: latency-svc-kllm4 [750.013582ms]
May 29 04:37:20.154: INFO: Created: latency-svc-hwddx
May 29 04:37:20.192: INFO: Got endpoints: latency-svc-7fgxq [753.655258ms]
May 29 04:37:20.208: INFO: Created: latency-svc-lqwcj
May 29 04:37:20.237: INFO: Got endpoints: latency-svc-csfp8 [749.369988ms]
May 29 04:37:20.253: INFO: Created: latency-svc-8kg49
May 29 04:37:20.287: INFO: Got endpoints: latency-svc-tdm52 [731.95451ms]
May 29 04:37:20.303: INFO: Created: latency-svc-zcvjr
May 29 04:37:20.348: INFO: Got endpoints: latency-svc-m7xdc [759.414719ms]
May 29 04:37:20.363: INFO: Created: latency-svc-grz89
May 29 04:37:20.388: INFO: Got endpoints: latency-svc-r7tnr [750.560481ms]
May 29 04:37:20.407: INFO: Created: latency-svc-p5k2x
May 29 04:37:20.437: INFO: Got endpoints: latency-svc-ktvn8 [736.798577ms]
May 29 04:37:20.457: INFO: Created: latency-svc-gcdkr
May 29 04:37:20.486: INFO: Got endpoints: latency-svc-d9qsl [749.146892ms]
May 29 04:37:20.504: INFO: Created: latency-svc-w4mqs
May 29 04:37:20.537: INFO: Got endpoints: latency-svc-mqnbj [749.705188ms]
May 29 04:37:20.560: INFO: Created: latency-svc-ztv2d
May 29 04:37:20.593: INFO: Got endpoints: latency-svc-5czfr [756.832739ms]
May 29 04:37:20.612: INFO: Created: latency-svc-cvmp5
May 29 04:37:20.636: INFO: Got endpoints: latency-svc-92csh [737.337075ms]
May 29 04:37:20.649: INFO: Created: latency-svc-rs42x
May 29 04:37:20.698: INFO: Got endpoints: latency-svc-cltm6 [761.327709ms]
May 29 04:37:20.710: INFO: Created: latency-svc-jd582
May 29 04:37:20.739: INFO: Got endpoints: latency-svc-lddbk [750.260086ms]
May 29 04:37:20.760: INFO: Created: latency-svc-pcjwn
May 29 04:37:20.805: INFO: Got endpoints: latency-svc-s5ghz [753.455364ms]
May 29 04:37:20.842: INFO: Got endpoints: latency-svc-nzndr [755.354351ms]
May 29 04:37:20.847: INFO: Created: latency-svc-fjxh8
May 29 04:37:20.862: INFO: Created: latency-svc-f8xzg
May 29 04:37:20.887: INFO: Got endpoints: latency-svc-hwddx [750.164486ms]
May 29 04:37:20.922: INFO: Created: latency-svc-f5nf5
May 29 04:37:20.939: INFO: Got endpoints: latency-svc-lqwcj [744.757724ms]
May 29 04:37:20.953: INFO: Created: latency-svc-g99k6
May 29 04:37:20.988: INFO: Got endpoints: latency-svc-8kg49 [751.266879ms]
May 29 04:37:21.008: INFO: Created: latency-svc-2594l
May 29 04:37:21.039: INFO: Got endpoints: latency-svc-zcvjr [752.62117ms]
May 29 04:37:21.060: INFO: Created: latency-svc-zs8ng
May 29 04:37:21.094: INFO: Got endpoints: latency-svc-grz89 [746.213915ms]
May 29 04:37:21.110: INFO: Created: latency-svc-f92f2
May 29 04:37:21.154: INFO: Got endpoints: latency-svc-p5k2x [766.331576ms]
May 29 04:37:21.172: INFO: Created: latency-svc-9jn5l
May 29 04:37:21.195: INFO: Got endpoints: latency-svc-gcdkr [755.67895ms]
May 29 04:37:21.212: INFO: Created: latency-svc-ct6f8
May 29 04:37:21.240: INFO: Got endpoints: latency-svc-w4mqs [753.530565ms]
May 29 04:37:21.272: INFO: Created: latency-svc-nldz7
May 29 04:37:21.286: INFO: Got endpoints: latency-svc-ztv2d [746.595414ms]
May 29 04:37:21.301: INFO: Created: latency-svc-72rbw
May 29 04:37:21.343: INFO: Got endpoints: latency-svc-cvmp5 [747.015511ms]
May 29 04:37:21.363: INFO: Created: latency-svc-2scd9
May 29 04:37:21.386: INFO: Got endpoints: latency-svc-rs42x [750.20359ms]
May 29 04:37:21.402: INFO: Created: latency-svc-4654q
May 29 04:37:21.437: INFO: Got endpoints: latency-svc-jd582 [738.990568ms]
May 29 04:37:21.455: INFO: Created: latency-svc-wj9p4
May 29 04:37:21.499: INFO: Got endpoints: latency-svc-pcjwn [756.712046ms]
May 29 04:37:21.513: INFO: Created: latency-svc-l62kc
May 29 04:37:21.537: INFO: Got endpoints: latency-svc-fjxh8 [731.68242ms]
May 29 04:37:21.552: INFO: Created: latency-svc-7qdhn
May 29 04:37:21.589: INFO: Got endpoints: latency-svc-f8xzg [746.873514ms]
May 29 04:37:21.610: INFO: Created: latency-svc-2zkld
May 29 04:37:21.644: INFO: Got endpoints: latency-svc-f5nf5 [755.144557ms]
May 29 04:37:21.662: INFO: Created: latency-svc-xvghh
May 29 04:37:21.693: INFO: Got endpoints: latency-svc-g99k6 [754.096765ms]
May 29 04:37:21.709: INFO: Created: latency-svc-9k4r5
May 29 04:37:21.754: INFO: Got endpoints: latency-svc-2594l [765.716685ms]
May 29 04:37:21.773: INFO: Created: latency-svc-5k6xd
May 29 04:37:21.787: INFO: Got endpoints: latency-svc-zs8ng [745.289427ms]
May 29 04:37:21.804: INFO: Created: latency-svc-svnsc
May 29 04:37:21.843: INFO: Got endpoints: latency-svc-f92f2 [746.516018ms]
May 29 04:37:21.865: INFO: Created: latency-svc-cvqcx
May 29 04:37:21.895: INFO: Got endpoints: latency-svc-9jn5l [741.020856ms]
May 29 04:37:21.912: INFO: Created: latency-svc-ktw6g
May 29 04:37:21.948: INFO: Got endpoints: latency-svc-ct6f8 [752.505577ms]
May 29 04:37:21.964: INFO: Created: latency-svc-59gnj
May 29 04:37:21.987: INFO: Got endpoints: latency-svc-nldz7 [747.209614ms]
May 29 04:37:22.006: INFO: Created: latency-svc-76s79
May 29 04:37:22.038: INFO: Got endpoints: latency-svc-72rbw [751.021688ms]
May 29 04:37:22.065: INFO: Created: latency-svc-5cqfj
May 29 04:37:22.087: INFO: Got endpoints: latency-svc-2scd9 [740.899858ms]
May 29 04:37:22.107: INFO: Created: latency-svc-xflg4
May 29 04:37:22.137: INFO: Got endpoints: latency-svc-4654q [750.601891ms]
May 29 04:37:22.157: INFO: Created: latency-svc-w8sq4
May 29 04:37:22.186: INFO: Got endpoints: latency-svc-wj9p4 [748.660005ms]
May 29 04:37:22.203: INFO: Created: latency-svc-s64kz
May 29 04:37:22.244: INFO: Got endpoints: latency-svc-l62kc [743.153644ms]
May 29 04:37:22.280: INFO: Created: latency-svc-l6rvf
May 29 04:37:22.296: INFO: Got endpoints: latency-svc-7qdhn [756.784349ms]
May 29 04:37:22.311: INFO: Created: latency-svc-lkztk
May 29 04:37:22.338: INFO: Got endpoints: latency-svc-2zkld [749.698899ms]
May 29 04:37:22.356: INFO: Created: latency-svc-4lbnb
May 29 04:37:22.393: INFO: Got endpoints: latency-svc-xvghh [745.962926ms]
May 29 04:37:22.429: INFO: Created: latency-svc-hqcth
May 29 04:37:22.437: INFO: Got endpoints: latency-svc-9k4r5 [741.00356ms]
May 29 04:37:22.458: INFO: Created: latency-svc-nh6l8
May 29 04:37:22.487: INFO: Got endpoints: latency-svc-5k6xd [730.768132ms]
May 29 04:37:22.511: INFO: Created: latency-svc-7bkjg
May 29 04:37:22.541: INFO: Got endpoints: latency-svc-svnsc [754.09237ms]
May 29 04:37:22.559: INFO: Created: latency-svc-lxj9n
May 29 04:37:22.590: INFO: Got endpoints: latency-svc-cvqcx [744.750635ms]
May 29 04:37:22.616: INFO: Created: latency-svc-6n2xh
May 29 04:37:22.649: INFO: Got endpoints: latency-svc-ktw6g [752.003386ms]
May 29 04:37:22.679: INFO: Created: latency-svc-jhlrn
May 29 04:37:22.694: INFO: Got endpoints: latency-svc-59gnj [743.746444ms]
May 29 04:37:22.716: INFO: Created: latency-svc-nphsg
May 29 04:37:22.746: INFO: Got endpoints: latency-svc-76s79 [756.949752ms]
May 29 04:37:22.793: INFO: Got endpoints: latency-svc-5cqfj [754.112872ms]
May 29 04:37:22.798: INFO: Created: latency-svc-25vjz
May 29 04:37:22.821: INFO: Created: latency-svc-674kz
May 29 04:37:22.836: INFO: Got endpoints: latency-svc-xflg4 [749.419305ms]
May 29 04:37:22.853: INFO: Created: latency-svc-lm4wb
May 29 04:37:22.889: INFO: Got endpoints: latency-svc-w8sq4 [752.100186ms]
May 29 04:37:22.905: INFO: Created: latency-svc-4sb5d
May 29 04:37:22.946: INFO: Got endpoints: latency-svc-s64kz [759.863932ms]
May 29 04:37:22.973: INFO: Created: latency-svc-7zfvt
May 29 04:37:22.994: INFO: Got endpoints: latency-svc-l6rvf [747.892515ms]
May 29 04:37:23.026: INFO: Created: latency-svc-5t4k4
May 29 04:37:23.037: INFO: Got endpoints: latency-svc-lkztk [741.278161ms]
May 29 04:37:23.052: INFO: Created: latency-svc-4xjmv
May 29 04:37:23.119: INFO: Got endpoints: latency-svc-4lbnb [779.105799ms]
May 29 04:37:23.159: INFO: Created: latency-svc-pstp7
May 29 04:37:23.164: INFO: Got endpoints: latency-svc-hqcth [771.044056ms]
May 29 04:37:23.190: INFO: Got endpoints: latency-svc-nh6l8 [750.960296ms]
May 29 04:37:23.192: INFO: Created: latency-svc-6shnt
May 29 04:37:23.219: INFO: Created: latency-svc-mfmkl
May 29 04:37:23.265: INFO: Got endpoints: latency-svc-7bkjg [776.39902ms]
May 29 04:37:23.288: INFO: Created: latency-svc-pv6jd
May 29 04:37:23.296: INFO: Got endpoints: latency-svc-lxj9n [753.099482ms]
May 29 04:37:23.355: INFO: Created: latency-svc-58nk9
May 29 04:37:23.357: INFO: Got endpoints: latency-svc-6n2xh [764.9031ms]
May 29 04:37:23.390: INFO: Got endpoints: latency-svc-jhlrn [738.498983ms]
May 29 04:37:23.394: INFO: Created: latency-svc-fcr6r
May 29 04:37:23.412: INFO: Created: latency-svc-cpwmk
May 29 04:37:23.437: INFO: Got endpoints: latency-svc-nphsg [740.115373ms]
May 29 04:37:23.473: INFO: Created: latency-svc-sxgxm
May 29 04:37:23.488: INFO: Got endpoints: latency-svc-25vjz [742.047059ms]
May 29 04:37:23.505: INFO: Created: latency-svc-m5th9
May 29 04:37:23.537: INFO: Got endpoints: latency-svc-674kz [743.601249ms]
May 29 04:37:23.555: INFO: Created: latency-svc-7pdk2
May 29 04:37:23.586: INFO: Got endpoints: latency-svc-lm4wb [749.771607ms]
May 29 04:37:23.609: INFO: Created: latency-svc-p2zg2
May 29 04:37:23.637: INFO: Got endpoints: latency-svc-4sb5d [748.050019ms]
May 29 04:37:23.654: INFO: Created: latency-svc-gcm8l
May 29 04:37:23.687: INFO: Got endpoints: latency-svc-7zfvt [736.187702ms]
May 29 04:37:23.719: INFO: Created: latency-svc-6cgvn
May 29 04:37:23.743: INFO: Got endpoints: latency-svc-5t4k4 [748.357618ms]
May 29 04:37:23.759: INFO: Created: latency-svc-kjfwc
May 29 04:37:23.787: INFO: Got endpoints: latency-svc-4xjmv [749.171613ms]
May 29 04:37:23.822: INFO: Created: latency-svc-jxpj2
May 29 04:37:23.841: INFO: Got endpoints: latency-svc-pstp7 [721.892502ms]
May 29 04:37:23.854: INFO: Created: latency-svc-jjfjp
May 29 04:37:23.888: INFO: Got endpoints: latency-svc-6shnt [722.600197ms]
May 29 04:37:23.914: INFO: Created: latency-svc-jvlht
May 29 04:37:23.955: INFO: Got endpoints: latency-svc-mfmkl [762.957117ms]
May 29 04:37:23.971: INFO: Created: latency-svc-z64gn
May 29 04:37:23.986: INFO: Got endpoints: latency-svc-pv6jd [720.601911ms]
May 29 04:37:24.014: INFO: Created: latency-svc-2hv8q
May 29 04:37:24.050: INFO: Got endpoints: latency-svc-58nk9 [751.869894ms]
May 29 04:37:24.072: INFO: Created: latency-svc-dftl7
May 29 04:37:24.086: INFO: Got endpoints: latency-svc-fcr6r [726.775468ms]
May 29 04:37:24.123: INFO: Created: latency-svc-w9gr5
May 29 04:37:24.140: INFO: Got endpoints: latency-svc-cpwmk [746.527832ms]
May 29 04:37:24.187: INFO: Created: latency-svc-l5zfm
May 29 04:37:24.192: INFO: Got endpoints: latency-svc-sxgxm [753.352185ms]
May 29 04:37:24.207: INFO: Created: latency-svc-46ccs
May 29 04:37:24.237: INFO: Got endpoints: latency-svc-m5th9 [748.39872ms]
May 29 04:37:24.255: INFO: Created: latency-svc-cvxjt
May 29 04:37:24.289: INFO: Got endpoints: latency-svc-7pdk2 [751.554299ms]
May 29 04:37:24.309: INFO: Created: latency-svc-fmfm8
May 29 04:37:24.336: INFO: Got endpoints: latency-svc-p2zg2 [749.970509ms]
May 29 04:37:24.358: INFO: Created: latency-svc-6p2zh
May 29 04:37:24.392: INFO: Got endpoints: latency-svc-gcm8l [754.515278ms]
May 29 04:37:24.413: INFO: Created: latency-svc-pc7j6
May 29 04:37:24.437: INFO: Got endpoints: latency-svc-6cgvn [750.628105ms]
May 29 04:37:24.466: INFO: Created: latency-svc-mp5mq
May 29 04:37:24.503: INFO: Got endpoints: latency-svc-kjfwc [759.216747ms]
May 29 04:37:24.523: INFO: Created: latency-svc-6jhpc
May 29 04:37:24.538: INFO: Got endpoints: latency-svc-jxpj2 [751.029403ms]
May 29 04:37:24.557: INFO: Created: latency-svc-6q4wc
May 29 04:37:24.587: INFO: Got endpoints: latency-svc-jjfjp [745.373743ms]
May 29 04:37:24.623: INFO: Created: latency-svc-dkrrb
May 29 04:37:24.637: INFO: Got endpoints: latency-svc-jvlht [748.155625ms]
May 29 04:37:24.679: INFO: Created: latency-svc-ssp5z
May 29 04:37:24.689: INFO: Got endpoints: latency-svc-z64gn [730.970144ms]
May 29 04:37:24.717: INFO: Created: latency-svc-k9bwv
May 29 04:37:24.748: INFO: Got endpoints: latency-svc-2hv8q [759.338048ms]
May 29 04:37:24.788: INFO: Got endpoints: latency-svc-dftl7 [734.970617ms]
May 29 04:37:24.837: INFO: Got endpoints: latency-svc-w9gr5 [748.686922ms]
May 29 04:37:24.892: INFO: Got endpoints: latency-svc-l5zfm [750.875607ms]
May 29 04:37:24.945: INFO: Got endpoints: latency-svc-46ccs [753.26769ms]
May 29 04:37:24.992: INFO: Got endpoints: latency-svc-cvxjt [754.427382ms]
May 29 04:37:25.059: INFO: Got endpoints: latency-svc-fmfm8 [769.651478ms]
May 29 04:37:25.090: INFO: Got endpoints: latency-svc-6p2zh [753.41319ms]
May 29 04:37:25.158: INFO: Got endpoints: latency-svc-pc7j6 [766.3712ms]
May 29 04:37:25.187: INFO: Got endpoints: latency-svc-mp5mq [750.019414ms]
May 29 04:37:25.254: INFO: Got endpoints: latency-svc-6jhpc [750.236713ms]
May 29 04:37:25.287: INFO: Got endpoints: latency-svc-6q4wc [749.443119ms]
May 29 04:37:25.338: INFO: Got endpoints: latency-svc-dkrrb [750.73561ms]
May 29 04:37:25.387: INFO: Got endpoints: latency-svc-ssp5z [749.822017ms]
May 29 04:37:25.452: INFO: Got endpoints: latency-svc-k9bwv [760.455644ms]
May 29 04:37:25.455: INFO: Latencies: [41.154913ms 61.322772ms 85.9348ms 100.2079ms 119.923262ms 137.859737ms 151.629341ms 162.806563ms 173.839686ms 202.420087ms 218.363876ms 229.902596ms 244.541094ms 247.649773ms 248.332168ms 248.350267ms 249.236361ms 249.585659ms 252.213341ms 255.16102ms 258.713895ms 259.108192ms 260.129486ms 260.178885ms 263.369163ms 266.003344ms 270.423913ms 271.474306ms 272.236601ms 272.875896ms 279.823347ms 300.8923ms 316.283393ms 316.320194ms 321.860355ms 323.208946ms 323.585743ms 325.232231ms 336.107556ms 342.234112ms 344.989794ms 345.40059ms 345.893187ms 360.357687ms 363.738163ms 377.309768ms 379.514353ms 379.531553ms 382.114934ms 390.880974ms 396.296535ms 411.922427ms 432.152587ms 442.253416ms 501.201106ms 527.222024ms 556.65632ms 580.418954ms 582.747839ms 625.117343ms 635.451372ms 682.174246ms 713.849526ms 716.840305ms 720.601911ms 721.892502ms 722.600197ms 726.775468ms 730.768132ms 730.970144ms 731.68242ms 731.95451ms 734.970617ms 735.272882ms 736.187702ms 736.603569ms 736.798577ms 737.337075ms 737.531166ms 738.35576ms 738.498983ms 738.589857ms 738.990568ms 739.734346ms 740.115373ms 740.84444ms 740.899858ms 741.00356ms 741.020856ms 741.106137ms 741.278161ms 741.929931ms 742.047059ms 743.153644ms 743.420327ms 743.601249ms 743.746444ms 743.892817ms 744.750635ms 744.757724ms 744.967415ms 745.289427ms 745.373743ms 745.576812ms 745.962926ms 746.213915ms 746.516018ms 746.527832ms 746.595414ms 746.660605ms 746.873514ms 747.015511ms 747.178095ms 747.209614ms 747.870997ms 747.892515ms 748.050019ms 748.155625ms 748.357618ms 748.39872ms 748.560092ms 748.660005ms 748.686922ms 748.86549ms 748.979081ms 749.146892ms 749.171613ms 749.369988ms 749.401084ms 749.419305ms 749.443119ms 749.698899ms 749.705188ms 749.771607ms 749.822017ms 749.888077ms 749.970509ms 750.013582ms 750.019414ms 750.164486ms 750.20359ms 750.236713ms 750.260086ms 750.280474ms 750.560481ms 750.601891ms 750.628105ms 750.73561ms 750.875607ms 750.960296ms 751.021688ms 751.029403ms 751.266879ms 751.554299ms 751.851864ms 751.869894ms 752.003386ms 752.100186ms 752.144463ms 752.505577ms 752.62117ms 753.099482ms 753.26769ms 753.352185ms 753.41319ms 753.455364ms 753.521051ms 753.530565ms 753.655258ms 754.09237ms 754.096765ms 754.112872ms 754.258852ms 754.427382ms 754.515278ms 755.144557ms 755.354351ms 755.67895ms 756.712046ms 756.784349ms 756.832739ms 756.949752ms 759.216747ms 759.338048ms 759.414719ms 759.863932ms 760.108711ms 760.382308ms 760.455644ms 761.327709ms 762.446696ms 762.957117ms 764.9031ms 765.716685ms 766.331576ms 766.3712ms 769.651478ms 771.044056ms 776.39902ms 779.105799ms]
May 29 04:37:25.457: INFO: 50 %ile: 744.967415ms
May 29 04:37:25.460: INFO: 90 %ile: 756.832739ms
May 29 04:37:25.463: INFO: 99 %ile: 776.39902ms
May 29 04:37:25.464: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:37:25.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-c9295" for this suite.
May 29 04:37:39.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:37:39.587: INFO: namespace: e2e-tests-svc-latency-c9295, resource: bindings, ignored listing per whitelist
May 29 04:37:39.595: INFO: namespace e2e-tests-svc-latency-c9295 deletion completed in 14.117073159s

â€¢ [SLOW TEST:25.968 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:37:39.596: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 04:37:39.685: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 29 04:37:44.688: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 04:37:48.693: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 04:37:48.716: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-jqvdk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jqvdk/deployments/test-cleanup-deployment,UID:82f53547-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3491,Generation:1,CreationTimestamp:2019-05-29 04:37:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 29 04:37:48.726: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 29 04:37:48.727: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 29 04:37:48.727: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-jqvdk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jqvdk/replicasets/test-cleanup-controller,UID:7d952c8f-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3492,Generation:1,CreationTimestamp:2019-05-29 04:37:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 82f53547-81cb-11e9-85ba-000d3a6e4ecc 0xc00236312f 0xc002363140}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 04:37:48.737: INFO: Pod "test-cleanup-controller-cm2gt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-cm2gt,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-jqvdk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jqvdk/pods/test-cleanup-controller-cm2gt,UID:7d96e69f-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3486,Generation:0,CreationTimestamp:2019-05-29 04:37:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 7d952c8f-81cb-11e9-85ba-000d3a6e4ecc 0xc0023637ff 0xc002363810}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p6ggf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p6ggf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p6ggf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002363870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002363890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:37:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:37:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:37:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:37:39 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.40,StartTime:2019-05-29 04:37:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 04:37:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d1e98ee1d18a6b6e4922815e7d96e120c3ad68eddaef1bac38e36d42070c297a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:37:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jqvdk" for this suite.
May 29 04:37:54.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:37:54.830: INFO: namespace: e2e-tests-deployment-jqvdk, resource: bindings, ignored listing per whitelist
May 29 04:37:54.842: INFO: namespace e2e-tests-deployment-jqvdk deletion completed in 6.090654004s

â€¢ [SLOW TEST:15.246 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:37:54.843: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 04:37:54.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-5h5kv" to be "success or failure"
May 29 04:37:54.938: INFO: Pod "downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194634ms
May 29 04:37:56.941: INFO: Pod "downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006140312s
May 29 04:37:58.944: INFO: Pod "downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009314365s
STEP: Saw pod success
May 29 04:37:58.944: INFO: Pod "downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:37:58.946: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 04:37:58.990: INFO: Waiting for pod downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:37:58.996: INFO: Pod downwardapi-volume-86ac6b64-81cb-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:37:58.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5h5kv" for this suite.
May 29 04:38:05.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:38:05.030: INFO: namespace: e2e-tests-projected-5h5kv, resource: bindings, ignored listing per whitelist
May 29 04:38:05.085: INFO: namespace e2e-tests-projected-5h5kv deletion completed in 6.086095621s

â€¢ [SLOW TEST:10.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:38:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8cc65706-81cb-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:38:05.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-l5v4d" to be "success or failure"
May 29 04:38:05.189: INFO: Pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.198604ms
May 29 04:38:07.193: INFO: Pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018422062s
May 29 04:38:09.197: INFO: Pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022475058s
May 29 04:38:11.200: INFO: Pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025779413s
STEP: Saw pod success
May 29 04:38:11.200: INFO: Pod "pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:38:11.203: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 04:38:11.244: INFO: Waiting for pod pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:38:11.247: INFO: Pod pod-projected-configmaps-8cc6bf09-81cb-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:38:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l5v4d" for this suite.
May 29 04:38:17.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:38:17.313: INFO: namespace: e2e-tests-projected-l5v4d, resource: bindings, ignored listing per whitelist
May 29 04:38:17.339: INFO: namespace e2e-tests-projected-l5v4d deletion completed in 6.089926831s

â€¢ [SLOW TEST:12.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:38:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 04:38:17.431: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 29 04:38:22.434: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 04:38:24.439: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 29 04:38:26.442: INFO: Creating deployment "test-rollover-deployment"
May 29 04:38:26.456: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 29 04:38:28.464: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 29 04:38:28.469: INFO: Ensure that both replica sets have 1 created replica
May 29 04:38:28.474: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 29 04:38:28.479: INFO: Updating deployment test-rollover-deployment
May 29 04:38:28.479: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 29 04:38:30.484: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 29 04:38:30.488: INFO: Make sure deployment "test-rollover-deployment" is complete
May 29 04:38:30.492: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:30.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701508, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:32.497: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:32.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701508, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:34.497: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:34.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701513, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:36.498: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:36.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701513, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:38.498: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:38.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701513, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:40.498: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:40.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701513, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:42.497: INFO: all replica sets need to contain the pod-template-hash label
May 29 04:38:42.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701513, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694701506, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:38:44.497: INFO: 
May 29 04:38:44.497: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 04:38:44.503: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-4lchc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4lchc/deployments/test-rollover-deployment,UID:9974dab2-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3745,Generation:2,CreationTimestamp:2019-05-29 04:38:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-29 04:38:26 +0000 UTC 2019-05-29 04:38:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-29 04:38:43 +0000 UTC 2019-05-29 04:38:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 29 04:38:44.505: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-4lchc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4lchc/replicasets/test-rollover-deployment-6b7f9d6597,UID:9aab9446-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3736,Generation:2,CreationTimestamp:2019-05-29 04:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9974dab2-81cb-11e9-85ba-000d3a6e4ecc 0xc002310a37 0xc002310a38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 04:38:44.505: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 29 04:38:44.505: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-4lchc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4lchc/replicasets/test-rollover-controller,UID:9413ef42-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3744,Generation:2,CreationTimestamp:2019-05-29 04:38:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9974dab2-81cb-11e9-85ba-000d3a6e4ecc 0xc00231089f 0xc0023108b0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 04:38:44.505: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-4lchc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4lchc/replicasets/test-rollover-deployment-6586df867b,UID:9977ad06-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3705,Generation:2,CreationTimestamp:2019-05-29 04:38:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9974dab2-81cb-11e9-85ba-000d3a6e4ecc 0xc002310967 0xc002310968}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 04:38:44.507: INFO: Pod "test-rollover-deployment-6b7f9d6597-zq95k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-zq95k,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-4lchc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4lchc/pods/test-rollover-deployment-6b7f9d6597-zq95k,UID:9ab11ce7-81cb-11e9-85ba-000d3a6e4ecc,ResourceVersion:3718,Generation:0,CreationTimestamp:2019-05-29 04:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 9aab9446-81cb-11e9-85ba-000d3a6e4ecc 0xc0023115d7 0xc0023115d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7nd68 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7nd68,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7nd68 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002311640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002311660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:38:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:38:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:38:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:38:28 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.43,StartTime:2019-05-29 04:38:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-29 04:38:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6988dd8ec58c9d8de8908bddd8ee75fed3631a1beb2a56e29e58e13df9118463}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:38:44.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4lchc" for this suite.
May 29 04:38:50.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:38:50.572: INFO: namespace: e2e-tests-deployment-4lchc, resource: bindings, ignored listing per whitelist
May 29 04:38:50.595: INFO: namespace e2e-tests-deployment-4lchc deletion completed in 6.084232291s

â€¢ [SLOW TEST:33.255 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:38:50.595: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-a7e54605-81cb-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 04:38:50.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-d2xwb" to be "success or failure"
May 29 04:38:50.686: INFO: Pod "pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.816606ms
May 29 04:38:52.689: INFO: Pod "pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008781935s
May 29 04:38:54.692: INFO: Pod "pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011609512s
STEP: Saw pod success
May 29 04:38:54.692: INFO: Pod "pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:38:54.694: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 04:38:54.715: INFO: Waiting for pod pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:38:54.717: INFO: Pod pod-projected-secrets-a7e68bb6-81cb-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:38:54.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d2xwb" for this suite.
May 29 04:39:00.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:39:00.760: INFO: namespace: e2e-tests-projected-d2xwb, resource: bindings, ignored listing per whitelist
May 29 04:39:00.806: INFO: namespace e2e-tests-projected-d2xwb deletion completed in 6.086494264s

â€¢ [SLOW TEST:10.212 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:39:00.807: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 29 04:39:00.908: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:00.913: INFO: Number of nodes with available pods: 0
May 29 04:39:00.913: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:01.918: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:01.920: INFO: Number of nodes with available pods: 0
May 29 04:39:01.920: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:02.917: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:02.920: INFO: Number of nodes with available pods: 0
May 29 04:39:02.920: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:03.919: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:03.922: INFO: Number of nodes with available pods: 0
May 29 04:39:03.922: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:04.917: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:04.920: INFO: Number of nodes with available pods: 3
May 29 04:39:04.920: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 29 04:39:04.939: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:04.942: INFO: Number of nodes with available pods: 2
May 29 04:39:04.942: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:05.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:05.949: INFO: Number of nodes with available pods: 2
May 29 04:39:05.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:06.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:06.949: INFO: Number of nodes with available pods: 2
May 29 04:39:06.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:07.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:07.948: INFO: Number of nodes with available pods: 2
May 29 04:39:07.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:08.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:08.948: INFO: Number of nodes with available pods: 2
May 29 04:39:08.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:09.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:09.948: INFO: Number of nodes with available pods: 2
May 29 04:39:09.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:10.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:10.949: INFO: Number of nodes with available pods: 2
May 29 04:39:10.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:11.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:11.948: INFO: Number of nodes with available pods: 2
May 29 04:39:11.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:12.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:12.948: INFO: Number of nodes with available pods: 2
May 29 04:39:12.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:13.947: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:13.949: INFO: Number of nodes with available pods: 2
May 29 04:39:13.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:14.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:14.948: INFO: Number of nodes with available pods: 2
May 29 04:39:14.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:15.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:15.948: INFO: Number of nodes with available pods: 2
May 29 04:39:15.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:16.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:16.948: INFO: Number of nodes with available pods: 2
May 29 04:39:16.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:17.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:17.949: INFO: Number of nodes with available pods: 2
May 29 04:39:17.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:18.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:18.948: INFO: Number of nodes with available pods: 2
May 29 04:39:18.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:19.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:19.948: INFO: Number of nodes with available pods: 2
May 29 04:39:19.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:20.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:20.948: INFO: Number of nodes with available pods: 2
May 29 04:39:20.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:21.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:21.948: INFO: Number of nodes with available pods: 2
May 29 04:39:21.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:22.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:22.948: INFO: Number of nodes with available pods: 2
May 29 04:39:22.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:23.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:23.948: INFO: Number of nodes with available pods: 2
May 29 04:39:23.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:24.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:24.948: INFO: Number of nodes with available pods: 2
May 29 04:39:24.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:25.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:25.949: INFO: Number of nodes with available pods: 2
May 29 04:39:25.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:26.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:26.949: INFO: Number of nodes with available pods: 2
May 29 04:39:26.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:27.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:27.948: INFO: Number of nodes with available pods: 2
May 29 04:39:27.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:28.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:28.948: INFO: Number of nodes with available pods: 2
May 29 04:39:28.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:29.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:29.951: INFO: Number of nodes with available pods: 2
May 29 04:39:29.951: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:30.947: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:30.949: INFO: Number of nodes with available pods: 2
May 29 04:39:30.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:31.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:31.948: INFO: Number of nodes with available pods: 2
May 29 04:39:31.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:32.952: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:32.955: INFO: Number of nodes with available pods: 2
May 29 04:39:32.955: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:33.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:33.949: INFO: Number of nodes with available pods: 2
May 29 04:39:33.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:34.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:34.949: INFO: Number of nodes with available pods: 2
May 29 04:39:34.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:35.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:35.948: INFO: Number of nodes with available pods: 2
May 29 04:39:35.948: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:36.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:36.949: INFO: Number of nodes with available pods: 2
May 29 04:39:36.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:37.952: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:37.955: INFO: Number of nodes with available pods: 2
May 29 04:39:37.955: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:38.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:38.949: INFO: Number of nodes with available pods: 2
May 29 04:39:38.949: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:39.945: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:39.947: INFO: Number of nodes with available pods: 2
May 29 04:39:39.947: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 04:39:40.946: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 04:39:40.948: INFO: Number of nodes with available pods: 3
May 29 04:39:40.948: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-j8npr, will wait for the garbage collector to delete the pods
May 29 04:39:41.007: INFO: Deleting DaemonSet.extensions daemon-set took: 4.44174ms
May 29 04:39:41.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172646ms
May 29 04:40:22.210: INFO: Number of nodes with available pods: 0
May 29 04:40:22.210: INFO: Number of running nodes: 0, number of available pods: 0
May 29 04:40:22.214: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-j8npr/daemonsets","resourceVersion":"4040"},"items":null}

May 29 04:40:22.215: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-j8npr/pods","resourceVersion":"4040"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:40:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-j8npr" for this suite.
May 29 04:40:28.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:40:28.302: INFO: namespace: e2e-tests-daemonsets-j8npr, resource: bindings, ignored listing per whitelist
May 29 04:40:28.322: INFO: namespace e2e-tests-daemonsets-j8npr deletion completed in 6.095527159s

â€¢ [SLOW TEST:87.516 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:40:28.322: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e226a6a1-81cb-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 04:40:28.412: INFO: Waiting up to 5m0s for pod "pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-hnpbz" to be "success or failure"
May 29 04:40:28.424: INFO: Pod "pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.347253ms
May 29 04:40:30.428: INFO: Pod "pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015755523s
May 29 04:40:32.431: INFO: Pod "pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018827311s
STEP: Saw pod success
May 29 04:40:32.431: INFO: Pod "pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:40:32.433: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 04:40:32.453: INFO: Waiting for pod pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:40:32.456: INFO: Pod pod-secrets-e226ff84-81cb-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:40:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hnpbz" for this suite.
May 29 04:40:38.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:40:38.515: INFO: namespace: e2e-tests-secrets-hnpbz, resource: bindings, ignored listing per whitelist
May 29 04:40:38.543: INFO: namespace e2e-tests-secrets-hnpbz deletion completed in 6.083957071s

â€¢ [SLOW TEST:10.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:40:38.544: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 04:40:38.624: INFO: (0) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.588847ms)
May 29 04:40:38.628: INFO: (1) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.801856ms)
May 29 04:40:38.631: INFO: (2) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.036265ms)
May 29 04:40:38.634: INFO: (3) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.071064ms)
May 29 04:40:38.638: INFO: (4) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.807556ms)
May 29 04:40:38.642: INFO: (5) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.179851ms)
May 29 04:40:38.645: INFO: (6) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.927566ms)
May 29 04:40:38.652: INFO: (7) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.627711ms)
May 29 04:40:38.656: INFO: (8) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.911355ms)
May 29 04:40:38.662: INFO: (9) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.16844ms)
May 29 04:40:38.675: INFO: (10) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.97105ms)
May 29 04:40:38.678: INFO: (11) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.082864ms)
May 29 04:40:38.681: INFO: (12) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.947465ms)
May 29 04:40:38.684: INFO: (13) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.906466ms)
May 29 04:40:38.687: INFO: (14) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.088264ms)
May 29 04:40:38.691: INFO: (15) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.940554ms)
May 29 04:40:38.694: INFO: (16) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.005665ms)
May 29 04:40:38.697: INFO: (17) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.082064ms)
May 29 04:40:38.701: INFO: (18) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.720057ms)
May 29 04:40:38.704: INFO: (19) /api/v1/nodes/k8s-pool1-29361026-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.075265ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:40:38.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-29xnr" for this suite.
May 29 04:40:44.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:40:44.745: INFO: namespace: e2e-tests-proxy-29xnr, resource: bindings, ignored listing per whitelist
May 29 04:40:44.788: INFO: namespace e2e-tests-proxy-29xnr deletion completed in 6.080259728s

â€¢ [SLOW TEST:6.244 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:40:44.789: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-fn4d8
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
May 29 04:40:44.889: INFO: Found 0 stateful pods, waiting for 3
May 29 04:40:54.893: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:40:54.893: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:40:54.893: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 29 04:41:04.893: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:41:04.893: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:41:04.893: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:41:04.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-fn4d8 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:41:05.128: INFO: stderr: ""
May 29 04:41:05.128: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:41:05.128: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 29 04:41:15.156: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 29 04:41:25.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-fn4d8 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:41:25.378: INFO: stderr: ""
May 29 04:41:25.378: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 04:41:25.378: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 04:41:35.396: INFO: Waiting for StatefulSet e2e-tests-statefulset-fn4d8/ss2 to complete update
May 29 04:41:35.396: INFO: Waiting for Pod e2e-tests-statefulset-fn4d8/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 04:41:35.396: INFO: Waiting for Pod e2e-tests-statefulset-fn4d8/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 04:41:45.402: INFO: Waiting for StatefulSet e2e-tests-statefulset-fn4d8/ss2 to complete update
May 29 04:41:45.402: INFO: Waiting for Pod e2e-tests-statefulset-fn4d8/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 04:41:55.401: INFO: Waiting for StatefulSet e2e-tests-statefulset-fn4d8/ss2 to complete update
May 29 04:41:55.401: INFO: Waiting for Pod e2e-tests-statefulset-fn4d8/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
May 29 04:42:05.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-fn4d8 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:42:05.661: INFO: stderr: ""
May 29 04:42:05.661: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:42:05.661: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 04:42:15.687: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 29 04:42:25.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-fn4d8 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:42:25.900: INFO: stderr: ""
May 29 04:42:25.900: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 04:42:25.900: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 04:42:45.914: INFO: Waiting for StatefulSet e2e-tests-statefulset-fn4d8/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 04:42:55.925: INFO: Deleting all statefulset in ns e2e-tests-statefulset-fn4d8
May 29 04:42:55.927: INFO: Scaling statefulset ss2 to 0
May 29 04:43:15.939: INFO: Waiting for statefulset status.replicas updated to 0
May 29 04:43:15.941: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:43:15.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-fn4d8" for this suite.
May 29 04:43:21.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:43:22.006: INFO: namespace: e2e-tests-statefulset-fn4d8, resource: bindings, ignored listing per whitelist
May 29 04:43:22.048: INFO: namespace e2e-tests-statefulset-fn4d8 deletion completed in 6.091719743s

â€¢ [SLOW TEST:157.260 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:43:22.049: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 29 04:43:22.146: INFO: Pod name pod-release: Found 0 pods out of 1
May 29 04:43:27.149: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:43:28.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-crm88" for this suite.
May 29 04:43:34.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:43:34.195: INFO: namespace: e2e-tests-replication-controller-crm88, resource: bindings, ignored listing per whitelist
May 29 04:43:34.250: INFO: namespace e2e-tests-replication-controller-crm88 deletion completed in 6.087496619s

â€¢ [SLOW TEST:12.202 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:43:34.251: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-s56dz
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
May 29 04:43:34.359: INFO: Found 0 stateful pods, waiting for 3
May 29 04:43:44.362: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:43:44.362: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:43:44.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 29 04:43:44.386: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 29 04:43:54.416: INFO: Updating stateful set ss2
May 29 04:43:54.422: INFO: Waiting for Pod e2e-tests-statefulset-s56dz/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 29 04:44:04.472: INFO: Found 1 stateful pods, waiting for 3
May 29 04:44:14.476: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:44:14.476: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:44:14.476: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 29 04:44:14.496: INFO: Updating stateful set ss2
May 29 04:44:14.509: INFO: Waiting for Pod e2e-tests-statefulset-s56dz/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 04:44:24.531: INFO: Updating stateful set ss2
May 29 04:44:24.537: INFO: Waiting for StatefulSet e2e-tests-statefulset-s56dz/ss2 to complete update
May 29 04:44:24.537: INFO: Waiting for Pod e2e-tests-statefulset-s56dz/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 04:44:34.544: INFO: Waiting for StatefulSet e2e-tests-statefulset-s56dz/ss2 to complete update
May 29 04:44:34.544: INFO: Waiting for Pod e2e-tests-statefulset-s56dz/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 04:44:44.544: INFO: Deleting all statefulset in ns e2e-tests-statefulset-s56dz
May 29 04:44:44.546: INFO: Scaling statefulset ss2 to 0
May 29 04:45:04.560: INFO: Waiting for statefulset status.replicas updated to 0
May 29 04:45:04.562: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:45:04.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-s56dz" for this suite.
May 29 04:45:10.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:45:10.630: INFO: namespace: e2e-tests-statefulset-s56dz, resource: bindings, ignored listing per whitelist
May 29 04:45:10.675: INFO: namespace e2e-tests-statefulset-s56dz deletion completed in 6.094492501s

â€¢ [SLOW TEST:96.424 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:45:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 04:45:10.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-mjk2c" to be "success or failure"
May 29 04:45:10.768: INFO: Pod "downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.712517ms
May 29 04:45:12.771: INFO: Pod "downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013353516s
May 29 04:45:14.775: INFO: Pod "downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016620131s
STEP: Saw pod success
May 29 04:45:14.775: INFO: Pod "downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:45:14.777: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 04:45:14.798: INFO: Waiting for pod downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:45:14.801: INFO: Pod downwardapi-volume-8a71e75d-81cc-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:45:14.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mjk2c" for this suite.
May 29 04:45:20.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:45:20.882: INFO: namespace: e2e-tests-downward-api-mjk2c, resource: bindings, ignored listing per whitelist
May 29 04:45:20.893: INFO: namespace e2e-tests-downward-api-mjk2c deletion completed in 6.088810543s

â€¢ [SLOW TEST:10.218 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:45:20.894: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-txb4c
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-txb4c
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-txb4c
May 29 04:45:20.993: INFO: Found 0 stateful pods, waiting for 1
May 29 04:45:30.997: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 29 04:45:30.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:45:31.213: INFO: stderr: ""
May 29 04:45:31.213: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:45:31.213: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 04:45:31.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 29 04:45:41.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 04:45:41.220: INFO: Waiting for statefulset status.replicas updated to 0
May 29 04:45:41.236: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:45:41.236: INFO: ss-0  k8s-pool1-29361026-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  }]
May 29 04:45:41.236: INFO: ss-1                                 Pending         []
May 29 04:45:41.236: INFO: 
May 29 04:45:41.236: INFO: StatefulSet ss has not reached scale 3, at 2
May 29 04:45:42.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992039069s
May 29 04:45:43.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988359298s
May 29 04:45:44.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984815025s
May 29 04:45:45.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981281948s
May 29 04:45:46.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97761097s
May 29 04:45:47.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973448395s
May 29 04:45:48.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966887337s
May 29 04:45:49.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962954054s
May 29 04:45:50.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.282875ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-txb4c
May 29 04:45:51.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:45:51.486: INFO: stderr: ""
May 29 04:45:51.486: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 04:45:51.486: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 04:45:51.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:45:51.699: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 29 04:45:51.699: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 04:45:51.699: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 04:45:51.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:45:51.913: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 29 04:45:51.913: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 04:45:51.913: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 04:45:51.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 29 04:46:01.921: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:46:01.921: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 04:46:01.921: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 29 04:46:01.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:46:02.144: INFO: stderr: ""
May 29 04:46:02.144: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:46:02.144: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 04:46:02.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:46:02.438: INFO: stderr: ""
May 29 04:46:02.438: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:46:02.438: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 04:46:02.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 04:46:02.707: INFO: stderr: ""
May 29 04:46:02.707: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 04:46:02.707: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 04:46:02.707: INFO: Waiting for statefulset status.replicas updated to 0
May 29 04:46:02.710: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 29 04:46:12.717: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 04:46:12.717: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 29 04:46:12.717: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 29 04:46:12.732: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:12.732: INFO: ss-0  k8s-pool1-29361026-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  }]
May 29 04:46:12.732: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:12.732: INFO: ss-2  k8s-pool1-29361026-vmss000002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:12.732: INFO: 
May 29 04:46:12.732: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 04:46:13.736: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:13.736: INFO: ss-0  k8s-pool1-29361026-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:21 +0000 UTC  }]
May 29 04:46:13.736: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:13.736: INFO: ss-2  k8s-pool1-29361026-vmss000002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:13.736: INFO: 
May 29 04:46:13.736: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 04:46:14.739: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:14.739: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:14.739: INFO: 
May 29 04:46:14.739: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:15.743: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:15.743: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:15.743: INFO: 
May 29 04:46:15.743: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:16.746: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:16.746: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:16.746: INFO: 
May 29 04:46:16.746: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:17.750: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:17.750: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:17.750: INFO: 
May 29 04:46:17.750: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:18.753: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:18.754: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:18.754: INFO: 
May 29 04:46:18.754: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:19.758: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:19.758: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:19.758: INFO: 
May 29 04:46:19.759: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:20.762: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:20.762: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:20.762: INFO: 
May 29 04:46:20.762: INFO: StatefulSet ss has not reached scale 0, at 1
May 29 04:46:21.766: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 04:46:21.766: INFO: ss-1  k8s-pool1-29361026-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:46:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:45:41 +0000 UTC  }]
May 29 04:46:21.766: INFO: 
May 29 04:46:21.766: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-txb4c
May 29 04:46:22.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:46:22.890: INFO: rc: 1
May 29 04:46:22.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0020ff6e0 exit status 1 <nil> <nil> true [0xc0028e73f0 0xc0028e7408 0xc0028e7420] [0xc0028e73f0 0xc0028e7408 0xc0028e7420] [0xc0028e7400 0xc0028e7418] [0x92f8e0 0x92f8e0] 0xc000f2df80 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 29 04:46:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:46:32.958: INFO: rc: 1
May 29 04:46:32.958: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020ffad0 exit status 1 <nil> <nil> true [0xc0028e7428 0xc0028e7440 0xc0028e7458] [0xc0028e7428 0xc0028e7440 0xc0028e7458] [0xc0028e7438 0xc0028e7450] [0x92f8e0 0x92f8e0] 0xc0022aa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:46:42.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:46:43.032: INFO: rc: 1
May 29 04:46:43.032: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000da6030 exit status 1 <nil> <nil> true [0xc000942880 0xc0009428b0 0xc0009428e0] [0xc000942880 0xc0009428b0 0xc0009428e0] [0xc000942898 0xc0009428c8] [0x92f8e0 0x92f8e0] 0xc0019aa4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:46:53.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:46:53.102: INFO: rc: 1
May 29 04:46:53.102: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020ffec0 exit status 1 <nil> <nil> true [0xc0028e7460 0xc0028e7478 0xc0028e7490] [0xc0028e7460 0xc0028e7478 0xc0028e7490] [0xc0028e7470 0xc0028e7488] [0x92f8e0 0x92f8e0] 0xc0022aa5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:03.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:03.190: INFO: rc: 1
May 29 04:47:03.190: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edc3c0 exit status 1 <nil> <nil> true [0xc00000fd68 0xc00000fe60 0xc0000c20f0] [0xc00000fd68 0xc00000fe60 0xc0000c20f0] [0xc00000fe58 0xc00000ff90] [0x92f8e0 0x92f8e0] 0xc000f2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:13.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:13.268: INFO: rc: 1
May 29 04:47:13.268: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cae420 exit status 1 <nil> <nil> true [0xc0005220c0 0xc0005226d8 0xc0005227b0] [0xc0005220c0 0xc0005226d8 0xc0005227b0] [0xc0005222f0 0xc0005227a8] [0x92f8e0 0x92f8e0] 0xc000a2a240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:23.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:23.337: INFO: rc: 1
May 29 04:47:23.337: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edc8a0 exit status 1 <nil> <nil> true [0xc0000c2120 0xc0000c22a8 0xc0000c2550] [0xc0000c2120 0xc0000c22a8 0xc0000c2550] [0xc0000c21a8 0xc0000c2458] [0x92f8e0 0x92f8e0] 0xc000f2c6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:33.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:33.405: INFO: rc: 1
May 29 04:47:33.405: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edccc0 exit status 1 <nil> <nil> true [0xc0000c25a8 0xc0000c26d0 0xc0000c2778] [0xc0000c25a8 0xc0000c26d0 0xc0000c2778] [0xc0000c2698 0xc0000c2770] [0x92f8e0 0x92f8e0] 0xc000f2c9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:43.476: INFO: rc: 1
May 29 04:47:43.476: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cae870 exit status 1 <nil> <nil> true [0xc0005228a8 0xc000522b88 0xc000522ce0] [0xc0005228a8 0xc000522b88 0xc000522ce0] [0xc000522ae8 0xc000522c50] [0x92f8e0 0x92f8e0] 0xc000a2a540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:47:53.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:47:53.543: INFO: rc: 1
May 29 04:47:53.543: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caecc0 exit status 1 <nil> <nil> true [0xc000522ce8 0xc000522e18 0xc000522fa0] [0xc000522ce8 0xc000522e18 0xc000522fa0] [0xc000522dd0 0xc000522f98] [0x92f8e0 0x92f8e0] 0xc000a2aae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:03.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:03.618: INFO: rc: 1
May 29 04:48:03.618: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd110 exit status 1 <nil> <nil> true [0xc0000c27c8 0xc0000c28b8 0xc0000c29d0] [0xc0000c27c8 0xc0000c28b8 0xc0000c29d0] [0xc0000c28a0 0xc0000c29a8] [0x92f8e0 0x92f8e0] 0xc000f2ccc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:13.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:13.689: INFO: rc: 1
May 29 04:48:13.689: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caf140 exit status 1 <nil> <nil> true [0xc000522ff8 0xc0005231c8 0xc000523218] [0xc000522ff8 0xc0005231c8 0xc000523218] [0xc000523198 0xc000523200] [0x92f8e0 0x92f8e0] 0xc000a2b0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:23.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:23.762: INFO: rc: 1
May 29 04:48:23.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd500 exit status 1 <nil> <nil> true [0xc0000c29d8 0xc0000c2f60 0xc0000c2ff8] [0xc0000c29d8 0xc0000c2f60 0xc0000c2ff8] [0xc0000c29f8 0xc0000c2fa8] [0x92f8e0 0x92f8e0] 0xc000f2cfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:33.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:33.835: INFO: rc: 1
May 29 04:48:33.835: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caf590 exit status 1 <nil> <nil> true [0xc0005232e0 0xc000523400 0xc000523550] [0xc0005232e0 0xc000523400 0xc000523550] [0xc000523348 0xc0005234f8] [0x92f8e0 0x92f8e0] 0xc000a2b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:43.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:43.905: INFO: rc: 1
May 29 04:48:43.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd9e0 exit status 1 <nil> <nil> true [0xc0000c3070 0xc001626000 0xc001626018] [0xc0000c3070 0xc001626000 0xc001626018] [0xc0000c30d8 0xc001626010] [0x92f8e0 0x92f8e0] 0xc000f2d320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:48:53.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:48:53.980: INFO: rc: 1
May 29 04:48:53.980: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cafc20 exit status 1 <nil> <nil> true [0xc0005235a8 0xc000523688 0xc000523700] [0xc0005235a8 0xc000523688 0xc000523700] [0xc000523638 0xc0005236c0] [0x92f8e0 0x92f8e0] 0xc000a2b9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:03.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:04.051: INFO: rc: 1
May 29 04:49:04.051: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cae3f0 exit status 1 <nil> <nil> true [0xc0000c2120 0xc0000c22a8 0xc0000c2550] [0xc0000c2120 0xc0000c22a8 0xc0000c2550] [0xc0000c21a8 0xc0000c2458] [0x92f8e0 0x92f8e0] 0xc000a2a240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:14.123: INFO: rc: 1
May 29 04:49:14.123: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cae8a0 exit status 1 <nil> <nil> true [0xc0000c25a8 0xc0000c26d0 0xc0000c2778] [0xc0000c25a8 0xc0000c26d0 0xc0000c2778] [0xc0000c2698 0xc0000c2770] [0x92f8e0 0x92f8e0] 0xc000a2a540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:24.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:24.202: INFO: rc: 1
May 29 04:49:24.202: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caed20 exit status 1 <nil> <nil> true [0xc0000c27c8 0xc0000c28b8 0xc0000c29d0] [0xc0000c27c8 0xc0000c28b8 0xc0000c29d0] [0xc0000c28a0 0xc0000c29a8] [0x92f8e0 0x92f8e0] 0xc000a2aae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:34.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:34.271: INFO: rc: 1
May 29 04:49:34.271: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caf170 exit status 1 <nil> <nil> true [0xc0000c29d8 0xc0000c2f60 0xc0000c2ff8] [0xc0000c29d8 0xc0000c2f60 0xc0000c2ff8] [0xc0000c29f8 0xc0000c2fa8] [0x92f8e0 0x92f8e0] 0xc000a2b0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:44.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:44.348: INFO: rc: 1
May 29 04:49:44.349: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000caf620 exit status 1 <nil> <nil> true [0xc0000c3070 0xc00000e010 0xc00000fe58] [0xc0000c3070 0xc00000e010 0xc00000fe58] [0xc0000c30d8 0xc00000fdf0] [0x92f8e0 0x92f8e0] 0xc000a2b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:49:54.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:49:54.422: INFO: rc: 1
May 29 04:49:54.422: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cafcb0 exit status 1 <nil> <nil> true [0xc00000fe60 0xc0005220c0 0xc0005226d8] [0xc00000fe60 0xc0005220c0 0xc0005226d8] [0xc00000ff90 0xc0005222f0] [0x92f8e0 0x92f8e0] 0xc000a2b9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:04.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:04.491: INFO: rc: 1
May 29 04:50:04.491: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edc3f0 exit status 1 <nil> <nil> true [0xc001626000 0xc001626018 0xc001626030] [0xc001626000 0xc001626018 0xc001626030] [0xc001626010 0xc001626028] [0x92f8e0 0x92f8e0] 0xc000f2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:14.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:14.571: INFO: rc: 1
May 29 04:50:14.572: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edc840 exit status 1 <nil> <nil> true [0xc001626038 0xc001626050 0xc001626068] [0xc001626038 0xc001626050 0xc001626068] [0xc001626048 0xc001626060] [0x92f8e0 0x92f8e0] 0xc000f2c6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:24.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:24.643: INFO: rc: 1
May 29 04:50:24.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edcc90 exit status 1 <nil> <nil> true [0xc001626070 0xc001626088 0xc0016260a0] [0xc001626070 0xc001626088 0xc0016260a0] [0xc001626080 0xc001626098] [0x92f8e0 0x92f8e0] 0xc000f2c9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:34.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:34.709: INFO: rc: 1
May 29 04:50:34.709: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd0b0 exit status 1 <nil> <nil> true [0xc0016260a8 0xc0016260c0 0xc0016260d8] [0xc0016260a8 0xc0016260c0 0xc0016260d8] [0xc0016260b8 0xc0016260d0] [0x92f8e0 0x92f8e0] 0xc000f2ccc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:44.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:44.779: INFO: rc: 1
May 29 04:50:44.779: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd4d0 exit status 1 <nil> <nil> true [0xc0016260e0 0xc0016260f8 0xc001626110] [0xc0016260e0 0xc0016260f8 0xc001626110] [0xc0016260f0 0xc001626108] [0x92f8e0 0x92f8e0] 0xc000f2cfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:50:54.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:50:54.858: INFO: rc: 1
May 29 04:50:54.858: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edd9b0 exit status 1 <nil> <nil> true [0xc001626118 0xc001626130 0xc001626148] [0xc001626118 0xc001626130 0xc001626148] [0xc001626128 0xc001626140] [0x92f8e0 0x92f8e0] 0xc000f2d320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:51:04.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:51:04.942: INFO: rc: 1
May 29 04:51:04.942: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edddd0 exit status 1 <nil> <nil> true [0xc001626158 0xc001626170 0xc001626188] [0xc001626158 0xc001626170 0xc001626188] [0xc001626168 0xc001626180] [0x92f8e0 0x92f8e0] 0xc000f2d6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:51:14.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:51:15.012: INFO: rc: 1
May 29 04:51:15.012: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001edc3c0 exit status 1 <nil> <nil> true [0xc00000e010 0xc00000fe58 0xc00000ff90] [0xc00000e010 0xc00000fe58 0xc00000ff90] [0xc00000fdf0 0xc00000fe78] [0x92f8e0 0x92f8e0] 0xc000f2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 29 04:51:25.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-txb4c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 04:51:25.089: INFO: rc: 1
May 29 04:51:25.089: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
May 29 04:51:25.089: INFO: Scaling statefulset ss to 0
May 29 04:51:25.098: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 04:51:25.100: INFO: Deleting all statefulset in ns e2e-tests-statefulset-txb4c
May 29 04:51:25.103: INFO: Scaling statefulset ss to 0
May 29 04:51:25.109: INFO: Waiting for statefulset status.replicas updated to 0
May 29 04:51:25.111: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:51:25.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-txb4c" for this suite.
May 29 04:51:31.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:51:31.164: INFO: namespace: e2e-tests-statefulset-txb4c, resource: bindings, ignored listing per whitelist
May 29 04:51:31.210: INFO: namespace e2e-tests-statefulset-txb4c deletion completed in 6.085613954s

â€¢ [SLOW TEST:370.316 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:51:31.210: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-6d42bc92-81cd-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:51:31.300: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-pj8m6" to be "success or failure"
May 29 04:51:31.309: INFO: Pod "pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.471187ms
May 29 04:51:33.312: INFO: Pod "pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011750628s
May 29 04:51:35.319: INFO: Pod "pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018672776s
STEP: Saw pod success
May 29 04:51:35.319: INFO: Pod "pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:51:35.321: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 04:51:35.338: INFO: Waiting for pod pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:51:35.347: INFO: Pod pod-configmaps-6d43e460-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:51:35.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pj8m6" for this suite.
May 29 04:51:41.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:51:41.417: INFO: namespace: e2e-tests-configmap-pj8m6, resource: bindings, ignored listing per whitelist
May 29 04:51:41.435: INFO: namespace e2e-tests-configmap-pj8m6 deletion completed in 6.084699836s

â€¢ [SLOW TEST:10.225 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:51:41.435: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 29 04:51:41.535: INFO: Waiting up to 5m0s for pod "pod-735db086-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-tzr7z" to be "success or failure"
May 29 04:51:41.539: INFO: Pod "pod-735db086-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574939ms
May 29 04:51:43.542: INFO: Pod "pod-735db086-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007760063s
May 29 04:51:45.551: INFO: Pod "pod-735db086-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016170972s
STEP: Saw pod success
May 29 04:51:45.551: INFO: Pod "pod-735db086-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:51:45.553: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-735db086-81cd-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 04:51:45.587: INFO: Waiting for pod pod-735db086-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:51:45.589: INFO: Pod pod-735db086-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:51:45.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tzr7z" for this suite.
May 29 04:51:51.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:51:51.648: INFO: namespace: e2e-tests-emptydir-tzr7z, resource: bindings, ignored listing per whitelist
May 29 04:51:51.676: INFO: namespace e2e-tests-emptydir-tzr7z deletion completed in 6.083848283s

â€¢ [SLOW TEST:10.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:51:51.676: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 04:51:51.752: INFO: Creating ReplicaSet my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3
May 29 04:51:51.758: INFO: Pod name my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3: Found 0 pods out of 1
May 29 04:51:56.762: INFO: Pod name my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3: Found 1 pods out of 1
May 29 04:51:56.762: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3" is running
May 29 04:51:56.764: INFO: Pod "my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3-7kr4x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 04:51:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 04:51:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 04:51:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 04:51:51 +0000 UTC Reason: Message:}])
May 29 04:51:56.764: INFO: Trying to dial the pod
May 29 04:52:01.778: INFO: Controller my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3: Got expected result from replica 1 [my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3-7kr4x]: "my-hostname-basic-79755451-81cd-11e9-b4f9-d20c9d8615e3-7kr4x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:52:01.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-76dr5" for this suite.
May 29 04:52:07.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:52:07.814: INFO: namespace: e2e-tests-replicaset-76dr5, resource: bindings, ignored listing per whitelist
May 29 04:52:07.864: INFO: namespace e2e-tests-replicaset-76dr5 deletion completed in 6.082968049s

â€¢ [SLOW TEST:16.188 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:52:07.865: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:52:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gl5zt" for this suite.
May 29 04:52:45.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:52:46.010: INFO: namespace: e2e-tests-pods-gl5zt, resource: bindings, ignored listing per whitelist
May 29 04:52:46.044: INFO: namespace e2e-tests-pods-gl5zt deletion completed in 38.08610615s

â€¢ [SLOW TEST:38.180 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:52:46.044: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0529 04:52:56.196481      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 04:52:56.196: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:52:56.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6kbh5" for this suite.
May 29 04:53:02.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:02.244: INFO: namespace: e2e-tests-gc-6kbh5, resource: bindings, ignored listing per whitelist
May 29 04:53:02.316: INFO: namespace e2e-tests-gc-6kbh5 deletion completed in 6.11716841s

â€¢ [SLOW TEST:16.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:53:02.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-r5dtq" for this suite.
May 29 04:53:08.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:08.497: INFO: namespace: e2e-tests-kubelet-test-r5dtq, resource: bindings, ignored listing per whitelist
May 29 04:53:08.531: INFO: namespace e2e-tests-kubelet-test-r5dtq deletion completed in 6.087961044s

â€¢ [SLOW TEST:6.213 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:08.532: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 04:53:08.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-dpj52" to be "success or failure"
May 29 04:53:08.624: INFO: Pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.547156ms
May 29 04:53:10.626: INFO: Pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006214178s
May 29 04:53:12.630: INFO: Pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009427131s
May 29 04:53:14.634: INFO: Pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013509712s
STEP: Saw pod success
May 29 04:53:14.634: INFO: Pod "downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:53:14.636: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 04:53:14.657: INFO: Waiting for pod downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:53:14.659: INFO: Pod downwardapi-volume-a745dfa8-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:53:14.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dpj52" for this suite.
May 29 04:53:20.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:20.720: INFO: namespace: e2e-tests-downward-api-dpj52, resource: bindings, ignored listing per whitelist
May 29 04:53:20.741: INFO: namespace e2e-tests-downward-api-dpj52 deletion completed in 6.078645963s

â€¢ [SLOW TEST:12.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:20.741: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 29 04:53:20.823: INFO: Waiting up to 5m0s for pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-zx4j4" to be "success or failure"
May 29 04:53:20.833: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.548385ms
May 29 04:53:22.836: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012742928s
May 29 04:53:24.839: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016024507s
May 29 04:53:26.842: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01873213s
May 29 04:53:28.846: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022245479s
STEP: Saw pod success
May 29 04:53:28.846: INFO: Pod "downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:53:28.849: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 04:53:28.866: INFO: Waiting for pod downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:53:28.882: INFO: Pod downward-api-ae8bd7a2-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:53:28.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zx4j4" for this suite.
May 29 04:53:34.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:34.950: INFO: namespace: e2e-tests-downward-api-zx4j4, resource: bindings, ignored listing per whitelist
May 29 04:53:34.977: INFO: namespace e2e-tests-downward-api-zx4j4 deletion completed in 6.090712999s

â€¢ [SLOW TEST:14.236 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:34.979: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 29 04:53:35.063: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7v6gw,SelfLink:/api/v1/namespaces/e2e-tests-watch-7v6gw/configmaps/e2e-watch-test-watch-closed,UID:b7080766-81cd-11e9-85ba-000d3a6e4ecc,ResourceVersion:6529,Generation:0,CreationTimestamp:2019-05-29 04:53:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 04:53:35.063: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7v6gw,SelfLink:/api/v1/namespaces/e2e-tests-watch-7v6gw/configmaps/e2e-watch-test-watch-closed,UID:b7080766-81cd-11e9-85ba-000d3a6e4ecc,ResourceVersion:6530,Generation:0,CreationTimestamp:2019-05-29 04:53:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 29 04:53:35.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7v6gw,SelfLink:/api/v1/namespaces/e2e-tests-watch-7v6gw/configmaps/e2e-watch-test-watch-closed,UID:b7080766-81cd-11e9-85ba-000d3a6e4ecc,ResourceVersion:6531,Generation:0,CreationTimestamp:2019-05-29 04:53:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 04:53:35.072: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7v6gw,SelfLink:/api/v1/namespaces/e2e-tests-watch-7v6gw/configmaps/e2e-watch-test-watch-closed,UID:b7080766-81cd-11e9-85ba-000d3a6e4ecc,ResourceVersion:6532,Generation:0,CreationTimestamp:2019-05-29 04:53:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:53:35.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-7v6gw" for this suite.
May 29 04:53:41.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:41.153: INFO: namespace: e2e-tests-watch-7v6gw, resource: bindings, ignored listing per whitelist
May 29 04:53:41.184: INFO: namespace e2e-tests-watch-7v6gw deletion completed in 6.108289118s

â€¢ [SLOW TEST:6.206 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:41.185: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 29 04:53:41.266: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:53:51.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-br4wx" for this suite.
May 29 04:53:57.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:53:57.191: INFO: namespace: e2e-tests-init-container-br4wx, resource: bindings, ignored listing per whitelist
May 29 04:53:57.194: INFO: namespace e2e-tests-init-container-br4wx deletion completed in 6.100854917s

â€¢ [SLOW TEST:16.010 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:53:57.195: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 29 04:54:03.804: INFO: Successfully updated pod "pod-update-c4477753-81cd-11e9-b4f9-d20c9d8615e3"
STEP: verifying the updated pod is in kubernetes
May 29 04:54:03.816: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:54:03.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mz4k4" for this suite.
May 29 04:54:25.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:54:25.923: INFO: namespace: e2e-tests-pods-mz4k4, resource: bindings, ignored listing per whitelist
May 29 04:54:25.933: INFO: namespace e2e-tests-pods-mz4k4 deletion completed in 22.113690292s

â€¢ [SLOW TEST:28.739 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:54:25.934: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-ccm7t/configmap-test-d56744aa-81cd-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:54:26.024: INFO: Waiting up to 5m0s for pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-ccm7t" to be "success or failure"
May 29 04:54:26.034: INFO: Pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.788086ms
May 29 04:54:28.037: INFO: Pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013162881s
May 29 04:54:30.041: INFO: Pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016363706s
May 29 04:54:32.044: INFO: Pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019342861s
STEP: Saw pod success
May 29 04:54:32.044: INFO: Pod "pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:54:32.045: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3 container env-test: <nil>
STEP: delete the pod
May 29 04:54:32.059: INFO: Waiting for pod pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:54:32.061: INFO: Pod pod-configmaps-d568b6f7-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:54:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ccm7t" for this suite.
May 29 04:54:38.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:54:38.118: INFO: namespace: e2e-tests-configmap-ccm7t, resource: bindings, ignored listing per whitelist
May 29 04:54:38.153: INFO: namespace e2e-tests-configmap-ccm7t deletion completed in 6.088848114s

â€¢ [SLOW TEST:12.219 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:54:38.154: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
May 29 04:54:38.231: INFO: Waiting up to 5m0s for pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-containers-xxdg7" to be "success or failure"
May 29 04:54:38.242: INFO: Pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.632967ms
May 29 04:54:40.246: INFO: Pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014949031s
May 29 04:54:42.249: INFO: Pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018262422s
May 29 04:54:44.252: INFO: Pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021088846s
STEP: Saw pod success
May 29 04:54:44.252: INFO: Pod "client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:54:44.254: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 04:54:44.271: INFO: Waiting for pod client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:54:44.273: INFO: Pod client-containers-dcaf3063-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:54:44.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-xxdg7" for this suite.
May 29 04:54:50.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:54:50.316: INFO: namespace: e2e-tests-containers-xxdg7, resource: bindings, ignored listing per whitelist
May 29 04:54:50.360: INFO: namespace e2e-tests-containers-xxdg7 deletion completed in 6.082251988s

â€¢ [SLOW TEST:12.207 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:54:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e3f616eb-81cd-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 04:54:50.443: INFO: Waiting up to 5m0s for pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-sknsm" to be "success or failure"
May 29 04:54:50.453: INFO: Pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.422692ms
May 29 04:54:52.456: INFO: Pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012398822s
May 29 04:54:54.458: INFO: Pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015227078s
May 29 04:54:56.461: INFO: Pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01810236s
STEP: Saw pod success
May 29 04:54:56.461: INFO: Pod "pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:54:56.463: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 04:54:56.480: INFO: Waiting for pod pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:54:56.489: INFO: Pod pod-secrets-e3f6787c-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:54:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sknsm" for this suite.
May 29 04:55:02.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:55:02.553: INFO: namespace: e2e-tests-secrets-sknsm, resource: bindings, ignored listing per whitelist
May 29 04:55:02.608: INFO: namespace e2e-tests-secrets-sknsm deletion completed in 6.115693082s

â€¢ [SLOW TEST:12.248 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:55:02.610: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-eb43e998-81cd-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:55:02.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-8sz7d" to be "success or failure"
May 29 04:55:02.699: INFO: Pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.335851ms
May 29 04:55:04.702: INFO: Pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007063937s
May 29 04:55:06.705: INFO: Pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009766548s
May 29 04:55:08.713: INFO: Pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017467827s
STEP: Saw pod success
May 29 04:55:08.713: INFO: Pod "pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:55:08.716: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 04:55:08.748: INFO: Waiting for pod pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:55:08.750: INFO: Pod pod-configmaps-eb443c8d-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:55:08.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8sz7d" for this suite.
May 29 04:55:14.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:55:14.831: INFO: namespace: e2e-tests-configmap-8sz7d, resource: bindings, ignored listing per whitelist
May 29 04:55:14.842: INFO: namespace e2e-tests-configmap-8sz7d deletion completed in 6.088475945s

â€¢ [SLOW TEST:12.232 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:55:14.843: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 29 04:55:14.925: INFO: Waiting up to 5m0s for pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-hpvk2" to be "success or failure"
May 29 04:55:14.928: INFO: Pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278063ms
May 29 04:55:16.932: INFO: Pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006574689s
May 29 04:55:18.935: INFO: Pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009542443s
May 29 04:55:20.939: INFO: Pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01333231s
STEP: Saw pod success
May 29 04:55:20.939: INFO: Pod "pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:55:20.943: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 04:55:20.966: INFO: Waiting for pod pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:55:20.968: INFO: Pod pod-f28e5868-81cd-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:55:20.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hpvk2" for this suite.
May 29 04:55:26.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:55:26.995: INFO: namespace: e2e-tests-emptydir-hpvk2, resource: bindings, ignored listing per whitelist
May 29 04:55:27.059: INFO: namespace e2e-tests-emptydir-hpvk2 deletion completed in 6.087906583s

â€¢ [SLOW TEST:12.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:55:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:55:33.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-d8m8p" for this suite.
May 29 04:55:39.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:55:39.251: INFO: namespace: e2e-tests-emptydir-wrapper-d8m8p, resource: bindings, ignored listing per whitelist
May 29 04:55:39.299: INFO: namespace e2e-tests-emptydir-wrapper-d8m8p deletion completed in 6.104611508s

â€¢ [SLOW TEST:12.240 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:55:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:55:45.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-r2lfn" for this suite.
May 29 04:56:27.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:56:27.436: INFO: namespace: e2e-tests-kubelet-test-r2lfn, resource: bindings, ignored listing per whitelist
May 29 04:56:27.488: INFO: namespace e2e-tests-kubelet-test-r2lfn deletion completed in 42.087455161s

â€¢ [SLOW TEST:48.188 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:56:27.489: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 04:56:27.569: INFO: Creating deployment "test-recreate-deployment"
May 29 04:56:27.576: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 29 04:56:27.585: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 29 04:56:29.590: INFO: Waiting deployment "test-recreate-deployment" to complete
May 29 04:56:29.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:56:31.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694702587, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 04:56:33.595: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 29 04:56:33.601: INFO: Updating deployment test-recreate-deployment
May 29 04:56:33.601: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 04:56:33.674: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-q97xl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q97xl/deployments/test-recreate-deployment,UID:1ddb8a3d-81ce-11e9-85ba-000d3a6e4ecc,ResourceVersion:7104,Generation:2,CreationTimestamp:2019-05-29 04:56:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-29 04:56:33 +0000 UTC 2019-05-29 04:56:33 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-29 04:56:33 +0000 UTC 2019-05-29 04:56:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 29 04:56:33.676: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-q97xl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q97xl/replicasets/test-recreate-deployment-697fbf54bf,UID:21786690-81ce-11e9-85ba-000d3a6e4ecc,ResourceVersion:7102,Generation:1,CreationTimestamp:2019-05-29 04:56:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1ddb8a3d-81ce-11e9-85ba-000d3a6e4ecc 0xc000b3e037 0xc000b3e038}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 04:56:33.677: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 29 04:56:33.677: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-q97xl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q97xl/replicasets/test-recreate-deployment-5dfdcc846d,UID:1ddd03e6-81ce-11e9-85ba-000d3a6e4ecc,ResourceVersion:7094,Generation:2,CreationTimestamp:2019-05-29 04:56:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1ddb8a3d-81ce-11e9-85ba-000d3a6e4ecc 0xc000869f87 0xc000869f88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 04:56:33.680: INFO: Pod "test-recreate-deployment-697fbf54bf-vgqjn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-vgqjn,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-q97xl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-q97xl/pods/test-recreate-deployment-697fbf54bf-vgqjn,UID:2178c645-81ce-11e9-85ba-000d3a6e4ecc,ResourceVersion:7100,Generation:0,CreationTimestamp:2019-05-29 04:56:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 21786690-81ce-11e9-85ba-000d3a6e4ecc 0xc000b3e8a7 0xc000b3e8a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wpkt2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wpkt2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wpkt2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b3e910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b3e930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 04:56:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:56:33.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-q97xl" for this suite.
May 29 04:56:39.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:56:39.758: INFO: namespace: e2e-tests-deployment-q97xl, resource: bindings, ignored listing per whitelist
May 29 04:56:39.772: INFO: namespace e2e-tests-deployment-q97xl deletion completed in 6.088614557s

â€¢ [SLOW TEST:12.283 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:56:39.774: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-252e08e1-81ce-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:56:39.865: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-vxxzt" to be "success or failure"
May 29 04:56:39.870: INFO: Pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882547ms
May 29 04:56:41.874: INFO: Pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008918911s
May 29 04:56:43.877: INFO: Pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011873004s
May 29 04:56:45.881: INFO: Pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015447207s
STEP: Saw pod success
May 29 04:56:45.881: INFO: Pod "pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:56:45.883: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 04:56:45.902: INFO: Waiting for pod pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:56:45.906: INFO: Pod pod-projected-configmaps-252e68c4-81ce-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:56:45.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vxxzt" for this suite.
May 29 04:56:51.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:56:51.950: INFO: namespace: e2e-tests-projected-vxxzt, resource: bindings, ignored listing per whitelist
May 29 04:56:51.999: INFO: namespace e2e-tests-projected-vxxzt deletion completed in 6.089616954s

â€¢ [SLOW TEST:12.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:56:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 04:56:52.081: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-xwlpg" to be "success or failure"
May 29 04:56:52.090: INFO: Pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.921503ms
May 29 04:56:54.093: INFO: Pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012158376s
May 29 04:56:56.100: INFO: Pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018739029s
May 29 04:56:58.103: INFO: Pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021836936s
STEP: Saw pod success
May 29 04:56:58.103: INFO: Pod "downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:56:58.105: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 04:56:58.126: INFO: Waiting for pod downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:56:58.129: INFO: Pod downwardapi-volume-2c773bb6-81ce-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:56:58.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xwlpg" for this suite.
May 29 04:57:04.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:57:04.215: INFO: namespace: e2e-tests-projected-xwlpg, resource: bindings, ignored listing per whitelist
May 29 04:57:04.221: INFO: namespace e2e-tests-projected-xwlpg deletion completed in 6.089213254s

â€¢ [SLOW TEST:12.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:57:04.221: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-33c09256-81ce-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 04:57:04.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-4lll5" to be "success or failure"
May 29 04:57:04.327: INFO: Pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.999171ms
May 29 04:57:06.330: INFO: Pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014961142s
May 29 04:57:08.333: INFO: Pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017968429s
May 29 04:57:10.336: INFO: Pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021495825s
STEP: Saw pod success
May 29 04:57:10.336: INFO: Pod "pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:57:10.339: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 04:57:10.354: INFO: Waiting for pod pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:57:10.356: INFO: Pod pod-projected-configmaps-33c1150d-81ce-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:57:10.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4lll5" for this suite.
May 29 04:57:16.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:57:16.385: INFO: namespace: e2e-tests-projected-4lll5, resource: bindings, ignored listing per whitelist
May 29 04:57:16.444: INFO: namespace e2e-tests-projected-4lll5 deletion completed in 6.084849183s

â€¢ [SLOW TEST:12.222 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:57:16.449: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-3b09b296-81ce-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 04:57:16.531: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-x4rns" to be "success or failure"
May 29 04:57:16.534: INFO: Pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.040368ms
May 29 04:57:18.537: INFO: Pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006159229s
May 29 04:57:20.540: INFO: Pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009067007s
May 29 04:57:22.542: INFO: Pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011340205s
STEP: Saw pod success
May 29 04:57:22.542: INFO: Pod "pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:57:22.545: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 04:57:22.565: INFO: Waiting for pod pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:57:22.569: INFO: Pod pod-projected-secrets-3b0a1e16-81ce-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:57:22.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x4rns" for this suite.
May 29 04:57:28.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:57:28.603: INFO: namespace: e2e-tests-projected-x4rns, resource: bindings, ignored listing per whitelist
May 29 04:57:28.652: INFO: namespace e2e-tests-projected-x4rns deletion completed in 6.079663906s

â€¢ [SLOW TEST:12.202 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:57:28.652: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 29 04:57:35.259: INFO: Successfully updated pod "pod-update-activedeadlineseconds-424f74b8-81ce-11e9-b4f9-d20c9d8615e3"
May 29 04:57:35.259: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-424f74b8-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-pods-m9g6w" to be "terminated due to deadline exceeded"
May 29 04:57:35.263: INFO: Pod "pod-update-activedeadlineseconds-424f74b8-81ce-11e9-b4f9-d20c9d8615e3": Phase="Running", Reason="", readiness=true. Elapsed: 4.086057ms
May 29 04:57:37.265: INFO: Pod "pod-update-activedeadlineseconds-424f74b8-81ce-11e9-b4f9-d20c9d8615e3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006654355s
May 29 04:57:37.265: INFO: Pod "pod-update-activedeadlineseconds-424f74b8-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:57:37.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-m9g6w" for this suite.
May 29 04:57:43.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:57:43.303: INFO: namespace: e2e-tests-pods-m9g6w, resource: bindings, ignored listing per whitelist
May 29 04:57:43.354: INFO: namespace e2e-tests-pods-m9g6w deletion completed in 6.084977854s

â€¢ [SLOW TEST:14.703 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:57:43.355: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-s2n2
STEP: Creating a pod to test atomic-volume-subpath
May 29 04:57:43.453: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s2n2" in namespace "e2e-tests-subpath-dm4qr" to be "success or failure"
May 29 04:57:43.456: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.023168ms
May 29 04:57:45.459: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006014716s
May 29 04:57:47.462: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008269985s
May 29 04:57:49.465: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011273959s
May 29 04:57:51.468: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014623642s
May 29 04:57:53.471: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 10.017437544s
May 29 04:57:55.474: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 12.020252458s
May 29 04:57:57.476: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 14.023097684s
May 29 04:57:59.480: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 16.026580917s
May 29 04:58:01.483: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 18.029893963s
May 29 04:58:03.487: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 20.033602517s
May 29 04:58:05.490: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 22.036440093s
May 29 04:58:07.492: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 24.039063383s
May 29 04:58:09.495: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 26.041695785s
May 29 04:58:11.498: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Running", Reason="", readiness=false. Elapsed: 28.045026792s
May 29 04:58:13.502: INFO: Pod "pod-subpath-test-configmap-s2n2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.048178712s
STEP: Saw pod success
May 29 04:58:13.502: INFO: Pod "pod-subpath-test-configmap-s2n2" satisfied condition "success or failure"
May 29 04:58:13.504: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-subpath-test-configmap-s2n2 container test-container-subpath-configmap-s2n2: <nil>
STEP: delete the pod
May 29 04:58:13.525: INFO: Waiting for pod pod-subpath-test-configmap-s2n2 to disappear
May 29 04:58:13.527: INFO: Pod pod-subpath-test-configmap-s2n2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s2n2
May 29 04:58:13.527: INFO: Deleting pod "pod-subpath-test-configmap-s2n2" in namespace "e2e-tests-subpath-dm4qr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:58:13.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-dm4qr" for this suite.
May 29 04:58:19.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:58:19.551: INFO: namespace: e2e-tests-subpath-dm4qr, resource: bindings, ignored listing per whitelist
May 29 04:58:19.622: INFO: namespace e2e-tests-subpath-dm4qr deletion completed in 6.090301982s

â€¢ [SLOW TEST:36.267 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:58:19.622: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:58:25.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-msg7n" for this suite.
May 29 04:59:07.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:59:07.785: INFO: namespace: e2e-tests-kubelet-test-msg7n, resource: bindings, ignored listing per whitelist
May 29 04:59:07.826: INFO: namespace e2e-tests-kubelet-test-msg7n deletion completed in 42.084340556s

â€¢ [SLOW TEST:48.203 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:59:07.826: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 29 04:59:14.433: INFO: Successfully updated pod "annotationupdate7d6caa78-81ce-11e9-b4f9-d20c9d8615e3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:59:16.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2ssr2" for this suite.
May 29 04:59:38.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:59:38.514: INFO: namespace: e2e-tests-downward-api-2ssr2, resource: bindings, ignored listing per whitelist
May 29 04:59:38.530: INFO: namespace e2e-tests-downward-api-2ssr2 deletion completed in 22.079590047s

â€¢ [SLOW TEST:30.704 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:59:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:59:38.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-vqsfs" for this suite.
May 29 04:59:44.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:59:44.684: INFO: namespace: e2e-tests-services-vqsfs, resource: bindings, ignored listing per whitelist
May 29 04:59:44.715: INFO: namespace e2e-tests-services-vqsfs deletion completed in 6.095876995s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.184 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:59:44.715: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 04:59:44.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-z55ff" to be "success or failure"
May 29 04:59:44.802: INFO: Pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.529281ms
May 29 04:59:46.805: INFO: Pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014280964s
May 29 04:59:48.808: INFO: Pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017201253s
May 29 04:59:50.811: INFO: Pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020491046s
STEP: Saw pod success
May 29 04:59:50.811: INFO: Pod "downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 04:59:50.813: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 04:59:50.826: INFO: Waiting for pod downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3 to disappear
May 29 04:59:50.829: INFO: Pod downwardapi-volume-93688f2a-81ce-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 04:59:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z55ff" for this suite.
May 29 04:59:56.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 04:59:56.900: INFO: namespace: e2e-tests-downward-api-z55ff, resource: bindings, ignored listing per whitelist
May 29 04:59:56.922: INFO: namespace e2e-tests-downward-api-z55ff deletion completed in 6.089840506s

â€¢ [SLOW TEST:12.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 04:59:56.924: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
May 29 04:59:57.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-4dq7d'
May 29 04:59:58.829: INFO: stderr: ""
May 29 04:59:58.829: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 04:59:58.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4dq7d'
May 29 04:59:58.915: INFO: stderr: ""
May 29 04:59:58.915: INFO: stdout: "update-demo-nautilus-4trw5 update-demo-nautilus-qrf7c "
May 29 04:59:58.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-4trw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 04:59:58.996: INFO: stderr: ""
May 29 04:59:58.996: INFO: stdout: ""
May 29 04:59:58.996: INFO: update-demo-nautilus-4trw5 is created but not running
May 29 05:00:03.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:04.095: INFO: stderr: ""
May 29 05:00:04.095: INFO: stdout: "update-demo-nautilus-4trw5 update-demo-nautilus-qrf7c "
May 29 05:00:04.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-4trw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:04.180: INFO: stderr: ""
May 29 05:00:04.180: INFO: stdout: ""
May 29 05:00:04.180: INFO: update-demo-nautilus-4trw5 is created but not running
May 29 05:00:09.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:09.257: INFO: stderr: ""
May 29 05:00:09.257: INFO: stdout: "update-demo-nautilus-4trw5 update-demo-nautilus-qrf7c "
May 29 05:00:09.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-4trw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:09.335: INFO: stderr: ""
May 29 05:00:09.335: INFO: stdout: "true"
May 29 05:00:09.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-4trw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:09.406: INFO: stderr: ""
May 29 05:00:09.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:00:09.406: INFO: validating pod update-demo-nautilus-4trw5
May 29 05:00:09.412: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:00:09.412: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:00:09.412: INFO: update-demo-nautilus-4trw5 is verified up and running
May 29 05:00:09.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-qrf7c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:09.484: INFO: stderr: ""
May 29 05:00:09.484: INFO: stdout: "true"
May 29 05:00:09.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-qrf7c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:09.554: INFO: stderr: ""
May 29 05:00:09.554: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:00:09.554: INFO: validating pod update-demo-nautilus-qrf7c
May 29 05:00:09.560: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:00:09.560: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:00:09.560: INFO: update-demo-nautilus-qrf7c is verified up and running
STEP: rolling-update to new replication controller
May 29 05:00:09.561: INFO: scanned /root for discovery docs: <nil>
May 29 05:00:09.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.125: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 29 05:00:37.125: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 05:00:37.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.221: INFO: stderr: ""
May 29 05:00:37.221: INFO: stdout: "update-demo-kitten-54vv5 update-demo-kitten-xh7nv "
May 29 05:00:37.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-kitten-54vv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.300: INFO: stderr: ""
May 29 05:00:37.300: INFO: stdout: "true"
May 29 05:00:37.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-kitten-54vv5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.376: INFO: stderr: ""
May 29 05:00:37.376: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 29 05:00:37.376: INFO: validating pod update-demo-kitten-54vv5
May 29 05:00:37.384: INFO: got data: {
  "image": "kitten.jpg"
}

May 29 05:00:37.384: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 29 05:00:37.384: INFO: update-demo-kitten-54vv5 is verified up and running
May 29 05:00:37.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-kitten-xh7nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.459: INFO: stderr: ""
May 29 05:00:37.459: INFO: stdout: "true"
May 29 05:00:37.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-kitten-xh7nv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4dq7d'
May 29 05:00:37.543: INFO: stderr: ""
May 29 05:00:37.543: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 29 05:00:37.543: INFO: validating pod update-demo-kitten-xh7nv
May 29 05:00:37.549: INFO: got data: {
  "image": "kitten.jpg"
}

May 29 05:00:37.549: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 29 05:00:37.549: INFO: update-demo-kitten-xh7nv is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:00:37.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4dq7d" for this suite.
May 29 05:00:59.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:00:59.605: INFO: namespace: e2e-tests-kubectl-4dq7d, resource: bindings, ignored listing per whitelist
May 29 05:00:59.659: INFO: namespace e2e-tests-kubectl-4dq7d deletion completed in 22.106363526s

â€¢ [SLOW TEST:62.735 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:00:59.659: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:01:29.781: INFO: Container started at 2019-05-29 05:01:04 +0000 UTC, pod became ready at 2019-05-29 05:01:28 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:01:29.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-blf6m" for this suite.
May 29 05:01:51.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:01:51.830: INFO: namespace: e2e-tests-container-probe-blf6m, resource: bindings, ignored listing per whitelist
May 29 05:01:51.877: INFO: namespace e2e-tests-container-probe-blf6m deletion completed in 22.093095019s

â€¢ [SLOW TEST:52.218 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:01:51.877: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 29 05:02:03.997: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:04.008: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:06.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:06.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:08.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:08.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:10.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:10.012: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:12.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:12.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:14.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:14.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:16.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:16.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:18.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:18.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:20.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:20.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:22.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:22.014: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:24.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:24.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 05:02:26.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 05:02:26.011: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:02:26.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-27c9s" for this suite.
May 29 05:02:48.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:02:48.066: INFO: namespace: e2e-tests-container-lifecycle-hook-27c9s, resource: bindings, ignored listing per whitelist
May 29 05:02:48.096: INFO: namespace e2e-tests-container-lifecycle-hook-27c9s deletion completed in 22.082148475s

â€¢ [SLOW TEST:56.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:02:48.097: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-00b7bb83-81cf-11e9-b4f9-d20c9d8615e3
STEP: Creating configMap with name cm-test-opt-upd-00b7bbb2-81cf-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-00b7bb83-81cf-11e9-b4f9-d20c9d8615e3
STEP: Updating configmap cm-test-opt-upd-00b7bbb2-81cf-11e9-b4f9-d20c9d8615e3
STEP: Creating configMap with name cm-test-opt-create-00b7bbcb-81cf-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:02:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cdq8q" for this suite.
May 29 05:03:20.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:03:20.306: INFO: namespace: e2e-tests-configmap-cdq8q, resource: bindings, ignored listing per whitelist
May 29 05:03:20.358: INFO: namespace e2e-tests-configmap-cdq8q deletion completed in 22.09024445s

â€¢ [SLOW TEST:32.261 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:03:20.358: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:03:20.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-gcd2t" to be "success or failure"
May 29 05:03:20.495: INFO: Pod "downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.709535ms
May 29 05:03:22.497: INFO: Pod "downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011268453s
May 29 05:03:24.500: INFO: Pod "downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013527039s
STEP: Saw pod success
May 29 05:03:24.500: INFO: Pod "downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:03:24.502: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:03:24.517: INFO: Waiting for pod downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:03:24.519: INFO: Pod downwardapi-volume-13f5e379-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:03:24.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gcd2t" for this suite.
May 29 05:03:30.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:03:30.554: INFO: namespace: e2e-tests-projected-gcd2t, resource: bindings, ignored listing per whitelist
May 29 05:03:30.602: INFO: namespace e2e-tests-projected-gcd2t deletion completed in 6.079590574s

â€¢ [SLOW TEST:10.244 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:03:30.603: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:03:30.676: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 29 05:03:30.681: INFO: Pod name sample-pod: Found 0 pods out of 1
May 29 05:03:35.684: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 05:03:35.684: INFO: Creating deployment "test-rolling-update-deployment"
May 29 05:03:35.687: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 29 05:03:35.701: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 29 05:03:37.706: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 29 05:03:37.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 05:03:39.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703015, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 05:03:41.712: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 05:03:41.717: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-tbdjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tbdjf/deployments/test-rolling-update-deployment,UID:1d0858f0-81cf-11e9-85ba-000d3a6e4ecc,ResourceVersion:8369,Generation:1,CreationTimestamp:2019-05-29 05:03:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-29 05:03:35 +0000 UTC 2019-05-29 05:03:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-29 05:03:40 +0000 UTC 2019-05-29 05:03:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 29 05:03:41.720: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-tbdjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tbdjf/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:1d0b98a2-81cf-11e9-85ba-000d3a6e4ecc,ResourceVersion:8360,Generation:1,CreationTimestamp:2019-05-29 05:03:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1d0858f0-81cf-11e9-85ba-000d3a6e4ecc 0xc001511507 0xc001511508}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 05:03:41.720: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 29 05:03:41.720: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-tbdjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tbdjf/replicasets/test-rolling-update-controller,UID:1a0c2bc9-81cf-11e9-85ba-000d3a6e4ecc,ResourceVersion:8368,Generation:2,CreationTimestamp:2019-05-29 05:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1d0858f0-81cf-11e9-85ba-000d3a6e4ecc 0xc00151143f 0xc001511450}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 05:03:41.722: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-d2jkc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-d2jkc,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-tbdjf,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tbdjf/pods/test-rolling-update-deployment-68b55d7bc6-d2jkc,UID:1d0c2093-81cf-11e9-85ba-000d3a6e4ecc,ResourceVersion:8359,Generation:0,CreationTimestamp:2019-05-29 05:03:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 1d0b98a2-81cf-11e9-85ba-000d3a6e4ecc 0xc001511db7 0xc001511db8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4cwmk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4cwmk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-4cwmk true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001511e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001511e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:03:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:03:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:03:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:03:35 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.67,StartTime:2019-05-29 05:03:35 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-29 05:03:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://02bc7e74d57cbf9baa768f180141c2230c845760a40d151fb2b2b3550d74a0c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:03:41.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tbdjf" for this suite.
May 29 05:03:47.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:03:47.784: INFO: namespace: e2e-tests-deployment-tbdjf, resource: bindings, ignored listing per whitelist
May 29 05:03:47.823: INFO: namespace e2e-tests-deployment-tbdjf deletion completed in 6.097278851s

â€¢ [SLOW TEST:17.220 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:03:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:03:54.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-lj5cq" for this suite.
May 29 05:04:00.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:04:00.091: INFO: namespace: e2e-tests-namespaces-lj5cq, resource: bindings, ignored listing per whitelist
May 29 05:04:00.096: INFO: namespace e2e-tests-namespaces-lj5cq deletion completed in 6.085194788s
STEP: Destroying namespace "e2e-tests-nsdeletetest-nl2xg" for this suite.
May 29 05:04:00.099: INFO: Namespace e2e-tests-nsdeletetest-nl2xg was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-q96tt" for this suite.
May 29 05:04:06.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:04:06.162: INFO: namespace: e2e-tests-nsdeletetest-q96tt, resource: bindings, ignored listing per whitelist
May 29 05:04:06.182: INFO: namespace e2e-tests-nsdeletetest-q96tt deletion completed in 6.08341316s

â€¢ [SLOW TEST:18.359 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:04:06.182: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:04:06.276: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 29 05:04:06.289: INFO: Number of nodes with available pods: 0
May 29 05:04:06.289: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 29 05:04:06.312: INFO: Number of nodes with available pods: 0
May 29 05:04:06.312: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:07.315: INFO: Number of nodes with available pods: 0
May 29 05:04:07.315: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:08.316: INFO: Number of nodes with available pods: 0
May 29 05:04:08.316: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:09.315: INFO: Number of nodes with available pods: 0
May 29 05:04:09.315: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:10.314: INFO: Number of nodes with available pods: 0
May 29 05:04:10.314: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:11.315: INFO: Number of nodes with available pods: 1
May 29 05:04:11.315: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 29 05:04:11.331: INFO: Number of nodes with available pods: 1
May 29 05:04:11.331: INFO: Number of running nodes: 0, number of available pods: 1
May 29 05:04:12.333: INFO: Number of nodes with available pods: 0
May 29 05:04:12.333: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 29 05:04:12.343: INFO: Number of nodes with available pods: 0
May 29 05:04:12.343: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:13.345: INFO: Number of nodes with available pods: 0
May 29 05:04:13.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:14.345: INFO: Number of nodes with available pods: 0
May 29 05:04:14.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:15.345: INFO: Number of nodes with available pods: 0
May 29 05:04:15.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:16.346: INFO: Number of nodes with available pods: 0
May 29 05:04:16.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:17.346: INFO: Number of nodes with available pods: 0
May 29 05:04:17.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:18.345: INFO: Number of nodes with available pods: 0
May 29 05:04:18.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:19.346: INFO: Number of nodes with available pods: 0
May 29 05:04:19.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:20.346: INFO: Number of nodes with available pods: 0
May 29 05:04:20.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:21.346: INFO: Number of nodes with available pods: 0
May 29 05:04:21.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:22.346: INFO: Number of nodes with available pods: 0
May 29 05:04:22.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:23.349: INFO: Number of nodes with available pods: 0
May 29 05:04:23.349: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:24.345: INFO: Number of nodes with available pods: 0
May 29 05:04:24.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:25.346: INFO: Number of nodes with available pods: 0
May 29 05:04:25.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:26.346: INFO: Number of nodes with available pods: 0
May 29 05:04:26.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:27.346: INFO: Number of nodes with available pods: 0
May 29 05:04:27.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:28.346: INFO: Number of nodes with available pods: 0
May 29 05:04:28.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:29.346: INFO: Number of nodes with available pods: 0
May 29 05:04:29.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:30.345: INFO: Number of nodes with available pods: 0
May 29 05:04:30.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:31.346: INFO: Number of nodes with available pods: 0
May 29 05:04:31.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:32.345: INFO: Number of nodes with available pods: 0
May 29 05:04:32.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:33.346: INFO: Number of nodes with available pods: 0
May 29 05:04:33.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:34.346: INFO: Number of nodes with available pods: 0
May 29 05:04:34.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:35.345: INFO: Number of nodes with available pods: 0
May 29 05:04:35.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:36.347: INFO: Number of nodes with available pods: 0
May 29 05:04:36.347: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:37.346: INFO: Number of nodes with available pods: 0
May 29 05:04:37.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:38.346: INFO: Number of nodes with available pods: 0
May 29 05:04:38.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:39.346: INFO: Number of nodes with available pods: 0
May 29 05:04:39.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:40.346: INFO: Number of nodes with available pods: 0
May 29 05:04:40.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:41.346: INFO: Number of nodes with available pods: 0
May 29 05:04:41.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:42.345: INFO: Number of nodes with available pods: 0
May 29 05:04:42.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:43.346: INFO: Number of nodes with available pods: 0
May 29 05:04:43.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:44.345: INFO: Number of nodes with available pods: 0
May 29 05:04:44.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:45.345: INFO: Number of nodes with available pods: 0
May 29 05:04:45.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:46.352: INFO: Number of nodes with available pods: 0
May 29 05:04:46.352: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:47.346: INFO: Number of nodes with available pods: 0
May 29 05:04:47.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:48.347: INFO: Number of nodes with available pods: 0
May 29 05:04:48.347: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:49.346: INFO: Number of nodes with available pods: 0
May 29 05:04:49.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:50.350: INFO: Number of nodes with available pods: 0
May 29 05:04:50.350: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:51.345: INFO: Number of nodes with available pods: 0
May 29 05:04:51.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:52.345: INFO: Number of nodes with available pods: 0
May 29 05:04:52.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:53.346: INFO: Number of nodes with available pods: 0
May 29 05:04:53.346: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:54.345: INFO: Number of nodes with available pods: 0
May 29 05:04:54.345: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:04:55.345: INFO: Number of nodes with available pods: 1
May 29 05:04:55.345: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-8t98v, will wait for the garbage collector to delete the pods
May 29 05:04:55.405: INFO: Deleting DaemonSet.extensions daemon-set took: 4.389723ms
May 29 05:04:55.506: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.173045ms
May 29 05:05:40.308: INFO: Number of nodes with available pods: 0
May 29 05:05:40.308: INFO: Number of running nodes: 0, number of available pods: 0
May 29 05:05:40.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8t98v/daemonsets","resourceVersion":"8663"},"items":null}

May 29 05:05:40.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8t98v/pods","resourceVersion":"8663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:05:40.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8t98v" for this suite.
May 29 05:05:46.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:05:46.432: INFO: namespace: e2e-tests-daemonsets-8t98v, resource: bindings, ignored listing per whitelist
May 29 05:05:46.439: INFO: namespace e2e-tests-daemonsets-8t98v deletion completed in 6.095950398s

â€¢ [SLOW TEST:100.256 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:05:46.439: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 29 05:05:46.529: INFO: Waiting up to 5m0s for pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-tkcn7" to be "success or failure"
May 29 05:05:46.537: INFO: Pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.574556ms
May 29 05:05:48.541: INFO: Pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879272s
May 29 05:05:50.543: INFO: Pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014718044s
May 29 05:05:52.546: INFO: Pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017536863s
STEP: Saw pod success
May 29 05:05:52.546: INFO: Pod "downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:05:52.548: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 05:05:52.570: INFO: Waiting for pod downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:05:52.573: INFO: Pod downward-api-6b0551e9-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:05:52.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tkcn7" for this suite.
May 29 05:05:58.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:05:58.661: INFO: namespace: e2e-tests-downward-api-tkcn7, resource: bindings, ignored listing per whitelist
May 29 05:05:58.665: INFO: namespace e2e-tests-downward-api-tkcn7 deletion completed in 6.088763198s

â€¢ [SLOW TEST:12.226 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:05:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 29 05:05:58.758: INFO: Waiting up to 5m0s for pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-c6l7t" to be "success or failure"
May 29 05:05:58.768: INFO: Pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060232ms
May 29 05:06:00.771: INFO: Pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013601231s
May 29 05:06:02.774: INFO: Pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016187593s
May 29 05:06:04.777: INFO: Pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019642586s
STEP: Saw pod success
May 29 05:06:04.777: INFO: Pod "pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:06:04.779: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:06:04.799: INFO: Waiting for pod pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:06:04.802: INFO: Pod pod-724f25cc-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:06:04.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-c6l7t" for this suite.
May 29 05:06:10.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:06:10.855: INFO: namespace: e2e-tests-emptydir-c6l7t, resource: bindings, ignored listing per whitelist
May 29 05:06:10.891: INFO: namespace e2e-tests-emptydir-c6l7t deletion completed in 6.085968502s

â€¢ [SLOW TEST:12.225 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:06:10.891: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
May 29 05:06:10.978: INFO: Waiting up to 5m0s for pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-xdxg2" to be "success or failure"
May 29 05:06:10.981: INFO: Pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399644ms
May 29 05:06:12.984: INFO: Pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006492231s
May 29 05:06:14.987: INFO: Pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009284067s
May 29 05:06:16.990: INFO: Pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012343045s
STEP: Saw pod success
May 29 05:06:16.990: INFO: Pod "pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:06:16.992: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:06:17.006: INFO: Waiting for pod pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:06:17.009: INFO: Pod pod-79983e1e-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:06:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xdxg2" for this suite.
May 29 05:06:23.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:06:23.069: INFO: namespace: e2e-tests-emptydir-xdxg2, resource: bindings, ignored listing per whitelist
May 29 05:06:23.108: INFO: namespace e2e-tests-emptydir-xdxg2 deletion completed in 6.09683136s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:06:23.108: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0529 05:06:33.208264      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 05:06:33.208: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:06:33.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-25n72" for this suite.
May 29 05:06:39.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:06:39.243: INFO: namespace: e2e-tests-gc-25n72, resource: bindings, ignored listing per whitelist
May 29 05:06:39.301: INFO: namespace e2e-tests-gc-25n72 deletion completed in 6.08963146s

â€¢ [SLOW TEST:16.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:06:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 05:06:39.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-d4vx6'
May 29 05:06:39.461: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 05:06:39.461: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
May 29 05:06:39.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-d4vx6'
May 29 05:06:39.542: INFO: stderr: ""
May 29 05:06:39.542: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:06:39.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d4vx6" for this suite.
May 29 05:06:45.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:06:45.624: INFO: namespace: e2e-tests-kubectl-d4vx6, resource: bindings, ignored listing per whitelist
May 29 05:06:45.635: INFO: namespace e2e-tests-kubectl-d4vx6 deletion completed in 6.088868784s

â€¢ [SLOW TEST:6.334 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:06:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-9n4nx
May 29 05:06:51.825: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-9n4nx
STEP: checking the pod's current state and verifying that restartCount is present
May 29 05:06:51.828: INFO: Initial restart count of pod liveness-exec is 0
May 29 05:07:45.920: INFO: Restart count of pod e2e-tests-container-probe-9n4nx/liveness-exec is now 1 (54.091997595s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:07:45.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9n4nx" for this suite.
May 29 05:07:51.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:07:51.985: INFO: namespace: e2e-tests-container-probe-9n4nx, resource: bindings, ignored listing per whitelist
May 29 05:07:52.012: INFO: namespace e2e-tests-container-probe-9n4nx deletion completed in 6.082379019s

â€¢ [SLOW TEST:66.377 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:07:52.013: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
May 29 05:07:52.602: INFO: Waiting up to 5m0s for pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm" in namespace "e2e-tests-svcaccounts-z2fxs" to be "success or failure"
May 29 05:07:52.612: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.108259ms
May 29 05:07:54.615: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01227103s
May 29 05:07:56.618: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015236942s
May 29 05:07:58.621: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018392688s
May 29 05:08:00.624: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021835766s
STEP: Saw pod success
May 29 05:08:00.624: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm" satisfied condition "success or failure"
May 29 05:08:00.626: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm container token-test: <nil>
STEP: delete the pod
May 29 05:08:00.650: INFO: Waiting for pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm to disappear
May 29 05:08:00.652: INFO: Pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-lxsxm no longer exists
STEP: Creating a pod to test consume service account root CA
May 29 05:08:00.656: INFO: Waiting up to 5m0s for pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm" in namespace "e2e-tests-svcaccounts-z2fxs" to be "success or failure"
May 29 05:08:00.668: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.302509ms
May 29 05:08:02.671: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015207734s
May 29 05:08:04.675: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018738884s
May 29 05:08:06.678: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021864878s
May 29 05:08:08.681: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0255205s
STEP: Saw pod success
May 29 05:08:08.681: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm" satisfied condition "success or failure"
May 29 05:08:08.684: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm container root-ca-test: <nil>
STEP: delete the pod
May 29 05:08:08.706: INFO: Waiting for pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm to disappear
May 29 05:08:08.709: INFO: Pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-g4bjm no longer exists
STEP: Creating a pod to test consume service account namespace
May 29 05:08:08.713: INFO: Waiting up to 5m0s for pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9" in namespace "e2e-tests-svcaccounts-z2fxs" to be "success or failure"
May 29 05:08:08.720: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.937993ms
May 29 05:08:10.723: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010189158s
May 29 05:08:12.726: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013370159s
May 29 05:08:14.728: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015813108s
May 29 05:08:16.732: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01906618s
STEP: Saw pod success
May 29 05:08:16.732: INFO: Pod "pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9" satisfied condition "success or failure"
May 29 05:08:16.734: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9 container namespace-test: <nil>
STEP: delete the pod
May 29 05:08:16.755: INFO: Waiting for pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9 to disappear
May 29 05:08:16.757: INFO: Pod pod-service-account-b62abd47-81cf-11e9-b4f9-d20c9d8615e3-s59v9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:08:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-z2fxs" for this suite.
May 29 05:08:22.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:08:22.799: INFO: namespace: e2e-tests-svcaccounts-z2fxs, resource: bindings, ignored listing per whitelist
May 29 05:08:22.852: INFO: namespace e2e-tests-svcaccounts-z2fxs deletion completed in 6.085124679s

â€¢ [SLOW TEST:30.840 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:08:22.853: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:08:22.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-fcf26" to be "success or failure"
May 29 05:08:22.949: INFO: Pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489432ms
May 29 05:08:24.953: INFO: Pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007829448s
May 29 05:08:26.955: INFO: Pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01037231s
May 29 05:08:28.960: INFO: Pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015189673s
STEP: Saw pod success
May 29 05:08:28.960: INFO: Pod "downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:08:28.962: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:08:28.982: INFO: Waiting for pod downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:08:28.985: INFO: Pod downwardapi-volume-c83f2aad-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:08:28.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fcf26" for this suite.
May 29 05:08:34.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:08:35.035: INFO: namespace: e2e-tests-downward-api-fcf26, resource: bindings, ignored listing per whitelist
May 29 05:08:35.110: INFO: namespace e2e-tests-downward-api-fcf26 deletion completed in 6.121704372s

â€¢ [SLOW TEST:12.257 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:08:35.110: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-rxrpq/configmap-test-cf902812-81cf-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 05:08:35.215: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-rxrpq" to be "success or failure"
May 29 05:08:35.217: INFO: Pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474063ms
May 29 05:08:37.220: INFO: Pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005562594s
May 29 05:08:39.224: INFO: Pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009132053s
May 29 05:08:41.226: INFO: Pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011528363s
STEP: Saw pod success
May 29 05:08:41.226: INFO: Pod "pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:08:41.228: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3 container env-test: <nil>
STEP: delete the pod
May 29 05:08:41.246: INFO: Waiting for pod pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:08:41.248: INFO: Pod pod-configmaps-cf90b4b8-81cf-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:08:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rxrpq" for this suite.
May 29 05:08:47.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:08:47.269: INFO: namespace: e2e-tests-configmap-rxrpq, resource: bindings, ignored listing per whitelist
May 29 05:08:47.341: INFO: namespace e2e-tests-configmap-rxrpq deletion completed in 6.089639992s

â€¢ [SLOW TEST:12.230 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:08:47.342: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 29 05:09:03.454: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:03.454: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:03.576: INFO: Exec stderr: ""
May 29 05:09:03.576: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:03.688: INFO: Exec stderr: ""
May 29 05:09:03.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:03.804: INFO: Exec stderr: ""
May 29 05:09:03.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:03.805: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:03.935: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 29 05:09:03.935: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:03.935: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.057: INFO: Exec stderr: ""
May 29 05:09:04.057: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:04.057: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.168: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 29 05:09:04.168: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:04.168: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.307: INFO: Exec stderr: ""
May 29 05:09:04.307: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:04.307: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.421: INFO: Exec stderr: ""
May 29 05:09:04.421: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.556: INFO: Exec stderr: ""
May 29 05:09:04.556: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-r894g PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:09:04.556: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:09:04.676: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:09:04.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-r894g" for this suite.
May 29 05:09:54.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:09:54.740: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-r894g, resource: bindings, ignored listing per whitelist
May 29 05:09:54.780: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-r894g deletion completed in 50.10118104s

â€¢ [SLOW TEST:67.438 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:09:54.781: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
May 29 05:10:00.957: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:10:25.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-zqsvn" for this suite.
May 29 05:10:31.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:10:31.068: INFO: namespace: e2e-tests-namespaces-zqsvn, resource: bindings, ignored listing per whitelist
May 29 05:10:31.097: INFO: namespace e2e-tests-namespaces-zqsvn deletion completed in 6.086753531s
STEP: Destroying namespace "e2e-tests-nsdeletetest-jj866" for this suite.
May 29 05:10:31.099: INFO: Namespace e2e-tests-nsdeletetest-jj866 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-582r5" for this suite.
May 29 05:10:37.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:10:37.152: INFO: namespace: e2e-tests-nsdeletetest-582r5, resource: bindings, ignored listing per whitelist
May 29 05:10:37.186: INFO: namespace e2e-tests-nsdeletetest-582r5 deletion completed in 6.087298675s

â€¢ [SLOW TEST:42.405 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:10:37.187: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 29 05:10:37.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-thq6g'
May 29 05:10:39.481: INFO: stderr: ""
May 29 05:10:39.481: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 05:10:40.484: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:40.484: INFO: Found 0 / 1
May 29 05:10:41.484: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:41.484: INFO: Found 0 / 1
May 29 05:10:42.485: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:42.485: INFO: Found 0 / 1
May 29 05:10:43.484: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:43.484: INFO: Found 0 / 1
May 29 05:10:44.484: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:44.484: INFO: Found 1 / 1
May 29 05:10:44.484: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 29 05:10:44.486: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:44.486: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 05:10:44.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 patch pod redis-master-gpk6z --namespace=e2e-tests-kubectl-thq6g -p {"metadata":{"annotations":{"x":"y"}}}'
May 29 05:10:44.571: INFO: stderr: ""
May 29 05:10:44.571: INFO: stdout: "pod/redis-master-gpk6z patched\n"
STEP: checking annotations
May 29 05:10:44.573: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:10:44.573: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:10:44.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-thq6g" for this suite.
May 29 05:11:06.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:11:06.606: INFO: namespace: e2e-tests-kubectl-thq6g, resource: bindings, ignored listing per whitelist
May 29 05:11:06.676: INFO: namespace e2e-tests-kubectl-thq6g deletion completed in 22.100026223s

â€¢ [SLOW TEST:29.489 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:11:06.677: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
May 29 05:11:12.772: INFO: Pod pod-hostip-29e47453-81d0-11e9-b4f9-d20c9d8615e3 has hostIP: 10.240.0.66
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:11:12.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-x6cvl" for this suite.
May 29 05:11:34.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:11:34.833: INFO: namespace: e2e-tests-pods-x6cvl, resource: bindings, ignored listing per whitelist
May 29 05:11:34.859: INFO: namespace e2e-tests-pods-x6cvl deletion completed in 22.083422789s

â€¢ [SLOW TEST:28.183 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:11:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-3ab1b2c6-81d0-11e9-b4f9-d20c9d8615e3
STEP: Creating secret with name s-test-opt-upd-3ab1b30d-81d0-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3ab1b2c6-81d0-11e9-b4f9-d20c9d8615e3
STEP: Updating secret s-test-opt-upd-3ab1b30d-81d0-11e9-b4f9-d20c9d8615e3
STEP: Creating secret with name s-test-opt-create-3ab1b326-81d0-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:13:11.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-h9sms" for this suite.
May 29 05:13:33.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:13:33.476: INFO: namespace: e2e-tests-secrets-h9sms, resource: bindings, ignored listing per whitelist
May 29 05:13:33.501: INFO: namespace e2e-tests-secrets-h9sms deletion completed in 22.097552462s

â€¢ [SLOW TEST:118.641 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:13:33.501: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4nxnc in namespace e2e-tests-proxy-68z9b
I0529 05:13:33.606407      15 runners.go:184] Created replication controller with name: proxy-service-4nxnc, namespace: e2e-tests-proxy-68z9b, replica count: 1
I0529 05:13:34.656851      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:35.657090      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:36.657353      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:37.657549      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:38.657873      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:39.658179      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 05:13:40.658370      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:41.658670      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:42.658896      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:43.659148      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:44.659394      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:45.659636      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:46.659908      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 05:13:47.660205      15 runners.go:184] proxy-service-4nxnc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 29 05:13:47.663: INFO: setup took 14.081515144s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 29 05:13:47.674: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.764559ms)
May 29 05:13:47.674: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 10.63026ms)
May 29 05:13:47.675: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 11.197153ms)
May 29 05:13:47.676: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 12.266739ms)
May 29 05:13:47.680: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 15.512996ms)
May 29 05:13:47.680: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 16.74338ms)
May 29 05:13:47.681: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 17.43607ms)
May 29 05:13:47.682: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 17.275272ms)
May 29 05:13:47.682: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 17.915164ms)
May 29 05:13:47.682: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 18.404258ms)
May 29 05:13:47.683: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 18.262759ms)
May 29 05:13:47.683: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 19.675941ms)
May 29 05:13:47.683: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 20.047736ms)
May 29 05:13:47.684: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 20.445431ms)
May 29 05:13:47.684: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 20.50673ms)
May 29 05:13:47.686: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 21.949011ms)
May 29 05:13:47.691: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 4.721338ms)
May 29 05:13:47.693: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 6.404015ms)
May 29 05:13:47.693: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 6.888009ms)
May 29 05:13:47.694: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 7.265504ms)
May 29 05:13:47.694: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 7.753598ms)
May 29 05:13:47.695: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 8.052894ms)
May 29 05:13:47.695: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 8.30159ms)
May 29 05:13:47.696: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 9.486975ms)
May 29 05:13:47.696: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 9.635973ms)
May 29 05:13:47.696: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 9.244278ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 10.882757ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 11.944142ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.918857ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 11.010455ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 11.231352ms)
May 29 05:13:47.698: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 11.050454ms)
May 29 05:13:47.710: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 11.007855ms)
May 29 05:13:47.710: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 10.512862ms)
May 29 05:13:47.710: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 10.861457ms)
May 29 05:13:47.710: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 11.38325ms)
May 29 05:13:47.711: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 12.771431ms)
May 29 05:13:47.711: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 12.555635ms)
May 29 05:13:47.711: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 12.449836ms)
May 29 05:13:47.711: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 12.522935ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 11.955142ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 12.638233ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 12.93123ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 12.353437ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 12.496636ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 13.452823ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 12.92783ms)
May 29 05:13:47.712: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 12.628334ms)
May 29 05:13:47.718: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 5.35473ms)
May 29 05:13:47.718: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 5.763324ms)
May 29 05:13:47.718: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 5.940722ms)
May 29 05:13:47.718: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 6.12602ms)
May 29 05:13:47.719: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 6.399515ms)
May 29 05:13:47.719: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 5.966322ms)
May 29 05:13:47.719: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 5.922922ms)
May 29 05:13:47.721: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 8.149592ms)
May 29 05:13:47.721: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 8.34219ms)
May 29 05:13:47.722: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 9.435975ms)
May 29 05:13:47.723: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 9.963869ms)
May 29 05:13:47.723: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.736258ms)
May 29 05:13:47.723: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 10.380063ms)
May 29 05:13:47.723: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 10.930056ms)
May 29 05:13:47.724: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.66066ms)
May 29 05:13:47.724: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 10.862257ms)
May 29 05:13:47.732: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 7.710498ms)
May 29 05:13:47.733: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 8.986381ms)
May 29 05:13:47.733: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 8.941682ms)
May 29 05:13:47.733: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 9.195479ms)
May 29 05:13:47.733: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 8.942982ms)
May 29 05:13:47.733: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 9.401376ms)
May 29 05:13:47.734: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.020768ms)
May 29 05:13:47.734: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 9.86677ms)
May 29 05:13:47.734: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 9.946969ms)
May 29 05:13:47.734: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 10.212866ms)
May 29 05:13:47.737: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 13.050429ms)
May 29 05:13:47.738: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 13.65302ms)
May 29 05:13:47.738: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 13.70422ms)
May 29 05:13:47.738: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 13.717319ms)
May 29 05:13:47.738: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 13.823218ms)
May 29 05:13:47.738: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 13.726019ms)
May 29 05:13:47.747: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 8.719585ms)
May 29 05:13:47.750: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 12.12304ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 12.393337ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 12.444136ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 12.564734ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 12.475936ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 12.514336ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 13.006628ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 13.020328ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 12.783032ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 12.749932ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 12.686933ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 12.964229ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 12.998829ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 13.170326ms)
May 29 05:13:47.751: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 12.971829ms)
May 29 05:13:47.757: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 5.646926ms)
May 29 05:13:47.757: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 5.789023ms)
May 29 05:13:47.757: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 5.814024ms)
May 29 05:13:47.758: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 6.121919ms)
May 29 05:13:47.758: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 6.214418ms)
May 29 05:13:47.758: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 6.597213ms)
May 29 05:13:47.759: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 7.380302ms)
May 29 05:13:47.760: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 7.6054ms)
May 29 05:13:47.760: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 7.5721ms)
May 29 05:13:47.760: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 7.875496ms)
May 29 05:13:47.761: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 9.782071ms)
May 29 05:13:47.762: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 10.154366ms)
May 29 05:13:47.763: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.804458ms)
May 29 05:13:47.765: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 12.497236ms)
May 29 05:13:47.765: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 12.558634ms)
May 29 05:13:47.765: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 12.682533ms)
May 29 05:13:47.772: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 6.990808ms)
May 29 05:13:47.775: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 8.945782ms)
May 29 05:13:47.775: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 9.375276ms)
May 29 05:13:47.776: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 11.002555ms)
May 29 05:13:47.776: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 10.972255ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 12.996329ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 13.356324ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 12.91393ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 13.293325ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 13.306625ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 13.64932ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 13.572521ms)
May 29 05:13:47.779: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 13.905417ms)
May 29 05:13:47.780: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 13.843318ms)
May 29 05:13:47.780: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 13.866218ms)
May 29 05:13:47.780: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 14.615207ms)
May 29 05:13:47.785: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 5.022134ms)
May 29 05:13:47.786: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 5.579826ms)
May 29 05:13:47.786: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 6.287217ms)
May 29 05:13:47.786: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 6.07582ms)
May 29 05:13:47.787: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 6.349216ms)
May 29 05:13:47.787: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 6.767411ms)
May 29 05:13:47.790: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 9.975969ms)
May 29 05:13:47.791: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.753459ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 11.677546ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 11.309951ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 11.563348ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 11.601147ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 11.559847ms)
May 29 05:13:47.792: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 11.989642ms)
May 29 05:13:47.794: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 14.010915ms)
May 29 05:13:47.795: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 14.330112ms)
May 29 05:13:47.803: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 7.733998ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 8.561787ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 8.808484ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 9.260778ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 8.669985ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 9.205179ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 8.878283ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 9.615373ms)
May 29 05:13:47.804: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 8.864983ms)
May 29 05:13:47.805: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 9.305777ms)
May 29 05:13:47.805: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 10.326464ms)
May 29 05:13:47.806: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.870457ms)
May 29 05:13:47.806: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 10.591461ms)
May 29 05:13:47.807: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 12.442636ms)
May 29 05:13:47.807: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 12.562335ms)
May 29 05:13:47.808: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 13.095028ms)
May 29 05:13:47.817: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 7.5622ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 13.382623ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 13.63962ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 13.733619ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 13.294325ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 13.156727ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 13.847718ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 12.96673ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 13.419324ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 13.64122ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 13.173027ms)
May 29 05:13:47.822: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 13.65192ms)
May 29 05:13:47.824: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 14.334011ms)
May 29 05:13:47.824: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 14.948203ms)
May 29 05:13:47.824: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 14.558808ms)
May 29 05:13:47.824: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 14.771705ms)
May 29 05:13:47.834: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 9.660473ms)
May 29 05:13:47.834: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 9.459576ms)
May 29 05:13:47.834: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 9.536675ms)
May 29 05:13:47.834: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 9.728972ms)
May 29 05:13:47.834: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 10.179366ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 10.074467ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 10.096067ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 10.487862ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.965355ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.899457ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.728558ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 10.998155ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 11.143354ms)
May 29 05:13:47.835: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 11.191252ms)
May 29 05:13:47.836: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 11.250652ms)
May 29 05:13:47.836: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 11.531548ms)
May 29 05:13:47.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 5.667326ms)
May 29 05:13:47.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 5.698725ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 6.422415ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 6.308717ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 6.376616ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 6.696012ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 7.029207ms)
May 29 05:13:47.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 7.004608ms)
May 29 05:13:47.844: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 7.646299ms)
May 29 05:13:47.844: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 7.684099ms)
May 29 05:13:47.846: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 9.899669ms)
May 29 05:13:47.847: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 10.906757ms)
May 29 05:13:47.847: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 10.941656ms)
May 29 05:13:47.847: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 10.863857ms)
May 29 05:13:47.847: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 10.851057ms)
May 29 05:13:47.848: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 11.556547ms)
May 29 05:13:47.855: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 6.777111ms)
May 29 05:13:47.858: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 9.808471ms)
May 29 05:13:47.858: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 9.514074ms)
May 29 05:13:47.858: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 9.596474ms)
May 29 05:13:47.858: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 9.546074ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 13.855318ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 13.523821ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 14.029115ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 14.148513ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 14.43441ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 14.320611ms)
May 29 05:13:47.862: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 14.280912ms)
May 29 05:13:47.863: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 14.675607ms)
May 29 05:13:47.863: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 14.598508ms)
May 29 05:13:47.863: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 14.522709ms)
May 29 05:13:47.864: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 15.534995ms)
May 29 05:13:47.870: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 6.273417ms)
May 29 05:13:47.873: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 8.871884ms)
May 29 05:13:47.874: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 9.821071ms)
May 29 05:13:47.874: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 9.802471ms)
May 29 05:13:47.874: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 10.181566ms)
May 29 05:13:47.876: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 11.953843ms)
May 29 05:13:47.878: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 14.109114ms)
May 29 05:13:47.878: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 14.235613ms)
May 29 05:13:47.878: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 14.37751ms)
May 29 05:13:47.879: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 14.830304ms)
May 29 05:13:47.879: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 14.599008ms)
May 29 05:13:47.879: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 14.999603ms)
May 29 05:13:47.879: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 14.793605ms)
May 29 05:13:47.879: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 14.915503ms)
May 29 05:13:47.880: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 15.657193ms)
May 29 05:13:47.880: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 15.96579ms)
May 29 05:13:47.885: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 5.365829ms)
May 29 05:13:47.885: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 5.466028ms)
May 29 05:13:47.887: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 6.582313ms)
May 29 05:13:47.888: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 7.928396ms)
May 29 05:13:47.890: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 9.463475ms)
May 29 05:13:47.893: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 13.214626ms)
May 29 05:13:47.894: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 13.865818ms)
May 29 05:13:47.894: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 13.855617ms)
May 29 05:13:47.895: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 15.087901ms)
May 29 05:13:47.895: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 15.1735ms)
May 29 05:13:47.895: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 15.340397ms)
May 29 05:13:47.896: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 15.733693ms)
May 29 05:13:47.896: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 15.839392ms)
May 29 05:13:47.896: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 15.981689ms)
May 29 05:13:47.896: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 16.062989ms)
May 29 05:13:47.896: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 16.137787ms)
May 29 05:13:47.918: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 21.792813ms)
May 29 05:13:47.919: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 22.305106ms)
May 29 05:13:47.919: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 22.7519ms)
May 29 05:13:47.919: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 22.853699ms)
May 29 05:13:47.923: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 27.064344ms)
May 29 05:13:47.924: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 27.839533ms)
May 29 05:13:47.926: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 29.62811ms)
May 29 05:13:47.933: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 36.957014ms)
May 29 05:13:47.933: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 36.834115ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 36.860814ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 37.24811ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 37.122811ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 37.20221ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 37.409707ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 37.032812ms)
May 29 05:13:47.934: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 37.866302ms)
May 29 05:13:47.947: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 12.368137ms)
May 29 05:13:47.948: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 13.62502ms)
May 29 05:13:47.948: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 13.682619ms)
May 29 05:13:47.949: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 14.43211ms)
May 29 05:13:47.950: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 15.489996ms)
May 29 05:13:47.952: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 17.195774ms)
May 29 05:13:47.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 18.158961ms)
May 29 05:13:47.960: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 25.447565ms)
May 29 05:13:47.960: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 25.175268ms)
May 29 05:13:47.960: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 25.697462ms)
May 29 05:13:47.961: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 25.912158ms)
May 29 05:13:47.963: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 27.759734ms)
May 29 05:13:47.964: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 28.754022ms)
May 29 05:13:47.964: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 28.671722ms)
May 29 05:13:47.964: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 28.891319ms)
May 29 05:13:47.965: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 29.425613ms)
May 29 05:13:47.973: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 8.024695ms)
May 29 05:13:47.982: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 16.997276ms)
May 29 05:13:47.984: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 18.870151ms)
May 29 05:13:47.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 19.547142ms)
May 29 05:13:47.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 19.525043ms)
May 29 05:13:47.986: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 20.570229ms)
May 29 05:13:47.987: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 21.618916ms)
May 29 05:13:47.987: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 21.898911ms)
May 29 05:13:47.987: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 22.305006ms)
May 29 05:13:47.987: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 22.519004ms)
May 29 05:13:47.990: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 24.831873ms)
May 29 05:13:47.991: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 25.643162ms)
May 29 05:13:47.992: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 26.934146ms)
May 29 05:13:47.992: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 26.869046ms)
May 29 05:13:47.993: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 27.662036ms)
May 29 05:13:47.993: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 27.818934ms)
May 29 05:13:48.013: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:1080/proxy/... (200; 20.200234ms)
May 29 05:13:48.021: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 27.873732ms)
May 29 05:13:48.021: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:1080/proxy/rewri... (200; 28.126329ms)
May 29 05:13:48.021: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/http:proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 27.909732ms)
May 29 05:13:48.022: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:160/proxy/: foo (200; 28.986118ms)
May 29 05:13:48.023: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:460/proxy/: tls baz (200; 29.59401ms)
May 29 05:13:48.023: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:443/proxy/... (200; 29.392813ms)
May 29 05:13:48.023: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6/proxy/rewriteme"... (200; 29.860507ms)
May 29 05:13:48.025: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname1/proxy/: foo (200; 31.16739ms)
May 29 05:13:48.026: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname1/proxy/: foo (200; 32.246476ms)
May 29 05:13:48.027: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/https:proxy-service-4nxnc-wkqz6:462/proxy/: tls qux (200; 33.334161ms)
May 29 05:13:48.027: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/pods/proxy-service-4nxnc-wkqz6:162/proxy/: bar (200; 33.688857ms)
May 29 05:13:48.028: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname1/proxy/: tls baz (200; 34.566745ms)
May 29 05:13:48.028: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/proxy-service-4nxnc:portname2/proxy/: bar (200; 34.91044ms)
May 29 05:13:48.028: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/http:proxy-service-4nxnc:portname2/proxy/: bar (200; 35.035039ms)
May 29 05:13:48.029: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-68z9b/services/https:proxy-service-4nxnc:tlsportname2/proxy/: tls qux (200; 35.194436ms)
STEP: deleting ReplicationController proxy-service-4nxnc in namespace e2e-tests-proxy-68z9b, will wait for the garbage collector to delete the pods
May 29 05:13:48.086: INFO: Deleting ReplicationController proxy-service-4nxnc took: 4.347443ms
May 29 05:13:48.186: INFO: Terminating ReplicationController proxy-service-4nxnc pods took: 100.173481ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:13:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-68z9b" for this suite.
May 29 05:14:01.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:14:01.839: INFO: namespace: e2e-tests-proxy-68z9b, resource: bindings, ignored listing per whitelist
May 29 05:14:01.872: INFO: namespace e2e-tests-proxy-68z9b deletion completed in 6.08006422s

â€¢ [SLOW TEST:28.371 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:14:01.873: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 29 05:14:01.947: INFO: PodSpec: initContainers in spec.initContainers
May 29 05:14:51.856: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-92511861-81d0-11e9-b4f9-d20c9d8615e3", GenerateName:"", Namespace:"e2e-tests-init-container-xgrll", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-xgrll/pods/pod-init-92511861-81d0-11e9-b4f9-d20c9d8615e3", UID:"925109bd-81d0-11e9-85ba-000d3a6e4ecc", ResourceVersion:"10140", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694703641, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"947969705"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9ch8k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001f65e00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9ch8k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9ch8k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9ch8k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a02748), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-pool1-29361026-vmss000002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0022fa4e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a027c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a027e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a027e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a027ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703641, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703641, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703641, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694703641, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.66", PodIP:"10.240.0.71", StartTime:(*v1.Time)(0xc001d3a400), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0012b3f10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0012b3f80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a9a65c479edd0ababd40c62fc8e516f5c1d1da7c28ee36163f8c76f3c100596b"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d3a440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d3a420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:14:51.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xgrll" for this suite.
May 29 05:15:13.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:15:13.967: INFO: namespace: e2e-tests-init-container-xgrll, resource: bindings, ignored listing per whitelist
May 29 05:15:13.994: INFO: namespace e2e-tests-init-container-xgrll deletion completed in 22.127722994s

â€¢ [SLOW TEST:72.121 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:15:13.994: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:15:14.084: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 29 05:15:14.090: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:14.092: INFO: Number of nodes with available pods: 0
May 29 05:15:14.092: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:15:15.096: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:15.099: INFO: Number of nodes with available pods: 0
May 29 05:15:15.099: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:15:16.097: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:16.100: INFO: Number of nodes with available pods: 0
May 29 05:15:16.100: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:15:17.096: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:17.099: INFO: Number of nodes with available pods: 0
May 29 05:15:17.099: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:15:18.097: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:18.099: INFO: Number of nodes with available pods: 0
May 29 05:15:18.099: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 05:15:19.098: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:19.100: INFO: Number of nodes with available pods: 3
May 29 05:15:19.100: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 29 05:15:19.117: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:19.117: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:19.117: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:19.120: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:20.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:20.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:20.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:20.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:21.127: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:21.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:21.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:21.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:22.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:22.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:22.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:22.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:23.126: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:23.126: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:23.126: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:23.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:24.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:24.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:24.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:24.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:25.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:25.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:25.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:25.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:26.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:26.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:26.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:26.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:27.126: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:27.126: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:27.126: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:27.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:28.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:28.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:28.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:28.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:29.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:29.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:29.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:29.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:30.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:30.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:30.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:30.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:31.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:31.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:31.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:31.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:32.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:32.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:32.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:32.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:33.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:33.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:33.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:33.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:34.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:34.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:34.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:34.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:35.127: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:35.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:35.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:35.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:36.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:36.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:36.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:36.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:37.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:37.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:37.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:37.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:38.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:38.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:38.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:38.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:39.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:39.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:39.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:39.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:40.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:40.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:40.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:40.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:41.129: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:41.129: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:41.129: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:41.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:42.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:42.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:42.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:42.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:43.127: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:43.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:43.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:43.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:44.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:44.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:44.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:44.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:45.127: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:45.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:45.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:45.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:46.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:46.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:46.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:46.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:47.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:47.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:47.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:47.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:48.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:48.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:48.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:48.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:49.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:49.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:49.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:49.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:50.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:50.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:50.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:50.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:51.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:51.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:51.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:51.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:52.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:52.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:52.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:52.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:53.131: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:53.131: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:53.131: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:53.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:54.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:54.124: INFO: Pod daemon-set-689hz is not available
May 29 05:15:54.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:54.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:54.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:55.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:55.124: INFO: Pod daemon-set-689hz is not available
May 29 05:15:55.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:55.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:55.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:56.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:56.125: INFO: Pod daemon-set-689hz is not available
May 29 05:15:56.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:56.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:56.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:57.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:57.124: INFO: Pod daemon-set-689hz is not available
May 29 05:15:57.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:57.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:57.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:58.124: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:58.124: INFO: Pod daemon-set-689hz is not available
May 29 05:15:58.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:58.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:58.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:15:59.127: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:59.127: INFO: Pod daemon-set-689hz is not available
May 29 05:15:59.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:59.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:15:59.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:00.125: INFO: Wrong image for pod: daemon-set-689hz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:00.125: INFO: Pod daemon-set-689hz is not available
May 29 05:16:00.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:00.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:00.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:01.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:01.124: INFO: Pod daemon-set-g889j is not available
May 29 05:16:01.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:01.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:02.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:02.124: INFO: Pod daemon-set-g889j is not available
May 29 05:16:02.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:02.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:03.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:03.127: INFO: Pod daemon-set-g889j is not available
May 29 05:16:03.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:03.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:04.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:04.124: INFO: Pod daemon-set-g889j is not available
May 29 05:16:04.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:04.126: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:05.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:05.125: INFO: Pod daemon-set-g889j is not available
May 29 05:16:05.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:05.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:06.130: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:06.130: INFO: Pod daemon-set-g889j is not available
May 29 05:16:06.130: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:06.133: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:07.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:07.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:07.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:08.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:08.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:08.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:09.135: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:09.135: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:09.144: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:10.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:10.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:10.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:11.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:11.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:11.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:12.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:12.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:12.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:13.129: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:13.129: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:13.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:14.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:14.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:14.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:15.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:15.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:15.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:16.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:16.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:16.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:17.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:17.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:17.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:18.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:18.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:18.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:19.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:19.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:19.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:20.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:20.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:20.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:21.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:21.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:21.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:22.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:22.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:22.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:23.127: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:23.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:23.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:24.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:24.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:24.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:25.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:25.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:25.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:26.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:26.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:26.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:27.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:27.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:27.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:28.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:28.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:28.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:29.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:29.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:29.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:30.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:30.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:30.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:31.128: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:31.128: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:31.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:32.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:32.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:32.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:33.125: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:33.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:33.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:34.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:34.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:34.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:35.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:35.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:35.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:36.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:36.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:36.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:37.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:37.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:37.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:38.124: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:38.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:38.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:39.131: INFO: Wrong image for pod: daemon-set-g5stf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:39.131: INFO: Pod daemon-set-g5stf is not available
May 29 05:16:39.131: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:39.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:40.123: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:40.123: INFO: Pod daemon-set-xq5xh is not available
May 29 05:16:40.126: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:41.128: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:41.128: INFO: Pod daemon-set-xq5xh is not available
May 29 05:16:41.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:42.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:42.124: INFO: Pod daemon-set-xq5xh is not available
May 29 05:16:42.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:43.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:43.125: INFO: Pod daemon-set-xq5xh is not available
May 29 05:16:43.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:44.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:44.125: INFO: Pod daemon-set-xq5xh is not available
May 29 05:16:44.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:45.131: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:45.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:46.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:46.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:47.132: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:47.135: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:48.127: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:48.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:49.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:49.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:50.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:50.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:51.129: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:51.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:52.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:52.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:53.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:53.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:54.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:54.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:55.123: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:55.130: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:56.123: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:56.126: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:57.130: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:57.133: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:58.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:58.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:16:59.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:16:59.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:00.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:00.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:01.130: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:01.140: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:02.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:02.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:03.126: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:03.132: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:04.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:04.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:05.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:05.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:06.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:06.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:07.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:07.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:08.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:08.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:09.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:09.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:10.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:10.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:11.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:11.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:12.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:12.127: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:13.126: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:13.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:14.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:14.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:15.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:15.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:16.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:16.126: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:17.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:17.125: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:17.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:18.126: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:18.126: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:18.129: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:19.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:19.124: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:19.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:20.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:20.125: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:20.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:21.125: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:21.125: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:21.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:22.124: INFO: Wrong image for pod: daemon-set-jmrdp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 29 05:17:22.124: INFO: Pod daemon-set-jmrdp is not available
May 29 05:17:22.128: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:23.126: INFO: Pod daemon-set-c5fmz is not available
May 29 05:17:23.131: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 29 05:17:23.134: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:23.136: INFO: Number of nodes with available pods: 2
May 29 05:17:23.136: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 05:17:24.141: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:24.143: INFO: Number of nodes with available pods: 2
May 29 05:17:24.143: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 05:17:25.142: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:25.144: INFO: Number of nodes with available pods: 2
May 29 05:17:25.144: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 05:17:26.140: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:26.143: INFO: Number of nodes with available pods: 2
May 29 05:17:26.143: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 05:17:27.142: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 05:17:27.144: INFO: Number of nodes with available pods: 3
May 29 05:17:27.144: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-hgnvk, will wait for the garbage collector to delete the pods
May 29 05:17:27.214: INFO: Deleting DaemonSet.extensions daemon-set took: 5.001739ms
May 29 05:17:27.314: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.186463ms
May 29 05:17:42.216: INFO: Number of nodes with available pods: 0
May 29 05:17:42.216: INFO: Number of running nodes: 0, number of available pods: 0
May 29 05:17:42.218: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-hgnvk/daemonsets","resourceVersion":"10537"},"items":null}

May 29 05:17:42.221: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-hgnvk/pods","resourceVersion":"10537"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:17:42.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-hgnvk" for this suite.
May 29 05:17:48.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:17:48.307: INFO: namespace: e2e-tests-daemonsets-hgnvk, resource: bindings, ignored listing per whitelist
May 29 05:17:48.328: INFO: namespace e2e-tests-daemonsets-hgnvk deletion completed in 6.095136681s

â€¢ [SLOW TEST:154.334 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:17:48.328: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 29 05:17:48.408: INFO: Waiting up to 5m0s for pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-9gntb" to be "success or failure"
May 29 05:17:48.417: INFO: Pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.239586ms
May 29 05:17:50.423: INFO: Pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014744259s
May 29 05:17:52.426: INFO: Pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017902873s
May 29 05:17:54.429: INFO: Pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020831501s
STEP: Saw pod success
May 29 05:17:54.429: INFO: Pod "pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:17:54.431: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:17:54.447: INFO: Waiting for pod pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:17:54.449: INFO: Pod pod-194ba4b9-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:17:54.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9gntb" for this suite.
May 29 05:18:00.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:18:00.493: INFO: namespace: e2e-tests-emptydir-9gntb, resource: bindings, ignored listing per whitelist
May 29 05:18:00.539: INFO: namespace e2e-tests-emptydir-9gntb deletion completed in 6.086944196s

â€¢ [SLOW TEST:12.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:18:00.539: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 29 05:18:00.650: INFO: Waiting up to 5m0s for pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-666cq" to be "success or failure"
May 29 05:18:00.654: INFO: Pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.363159ms
May 29 05:18:02.656: INFO: Pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005518043s
May 29 05:18:04.659: INFO: Pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008323231s
May 29 05:18:06.661: INFO: Pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010982832s
STEP: Saw pod success
May 29 05:18:06.661: INFO: Pod "pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:18:06.663: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:18:06.685: INFO: Waiting for pod pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:18:06.688: INFO: Pod pod-209743e1-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:18:06.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-666cq" for this suite.
May 29 05:18:12.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:18:12.780: INFO: namespace: e2e-tests-emptydir-666cq, resource: bindings, ignored listing per whitelist
May 29 05:18:12.780: INFO: namespace e2e-tests-emptydir-666cq deletion completed in 6.088659984s

â€¢ [SLOW TEST:12.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:18:12.780: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 29 05:18:12.858: INFO: Waiting up to 5m0s for pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-sm5lr" to be "success or failure"
May 29 05:18:12.861: INFO: Pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.036663ms
May 29 05:18:14.863: INFO: Pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005779908s
May 29 05:18:16.866: INFO: Pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008488465s
May 29 05:18:18.869: INFO: Pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011307531s
STEP: Saw pod success
May 29 05:18:18.869: INFO: Pod "downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:18:18.871: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 05:18:18.894: INFO: Waiting for pod downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:18:18.897: INFO: Pod downward-api-27dd6313-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:18:18.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sm5lr" for this suite.
May 29 05:18:24.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:18:24.964: INFO: namespace: e2e-tests-downward-api-sm5lr, resource: bindings, ignored listing per whitelist
May 29 05:18:24.988: INFO: namespace e2e-tests-downward-api-sm5lr deletion completed in 6.0877332s

â€¢ [SLOW TEST:12.209 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:18:24.989: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:18:25.060: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:18:26.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-vcc55" for this suite.
May 29 05:18:32.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:18:32.211: INFO: namespace: e2e-tests-custom-resource-definition-vcc55, resource: bindings, ignored listing per whitelist
May 29 05:18:32.224: INFO: namespace e2e-tests-custom-resource-definition-vcc55 deletion completed in 6.085284348s

â€¢ [SLOW TEST:7.235 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:18:32.224: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-lhmbl
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-lhmbl
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-lhmbl
May 29 05:18:32.314: INFO: Found 0 stateful pods, waiting for 1
May 29 05:18:42.318: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 29 05:18:42.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 05:18:42.546: INFO: stderr: ""
May 29 05:18:42.546: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 05:18:42.546: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 05:18:42.549: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 29 05:18:52.553: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 05:18:52.553: INFO: Waiting for statefulset status.replicas updated to 0
May 29 05:18:52.565: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
May 29 05:18:53.568: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996563548s
May 29 05:18:54.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993688287s
May 29 05:18:55.579: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985660285s
May 29 05:18:56.582: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982982817s
May 29 05:18:57.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979777152s
May 29 05:18:58.589: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976032891s
May 29 05:18:59.592: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972684123s
May 29 05:19:00.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969635649s
May 29 05:19:01.601: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.882868ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-lhmbl
May 29 05:19:02.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 05:19:02.804: INFO: stderr: ""
May 29 05:19:02.804: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 05:19:02.804: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 05:19:02.807: INFO: Found 1 stateful pods, waiting for 3
May 29 05:19:12.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 05:19:12.810: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 05:19:12.810: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 29 05:19:22.811: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 05:19:22.811: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 05:19:22.811: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 29 05:19:22.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 05:19:23.020: INFO: stderr: ""
May 29 05:19:23.020: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 05:19:23.020: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 05:19:23.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 05:19:23.557: INFO: stderr: ""
May 29 05:19:23.557: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 05:19:23.557: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 05:19:23.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 05:19:23.781: INFO: stderr: ""
May 29 05:19:23.781: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 05:19:23.781: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 05:19:23.781: INFO: Waiting for statefulset status.replicas updated to 0
May 29 05:19:23.784: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 29 05:19:33.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 05:19:33.790: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 29 05:19:33.790: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 29 05:19:33.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
May 29 05:19:34.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990160824s
May 29 05:19:35.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986325873s
May 29 05:19:36.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982367322s
May 29 05:19:37.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978707864s
May 29 05:19:38.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974637009s
May 29 05:19:39.829: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970833048s
May 29 05:19:40.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966812688s
May 29 05:19:41.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963391318s
May 29 05:19:42.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.685449ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-lhmbl
May 29 05:19:43.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 05:19:44.022: INFO: stderr: ""
May 29 05:19:44.022: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 05:19:44.022: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 05:19:44.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 05:19:44.217: INFO: stderr: ""
May 29 05:19:44.217: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 05:19:44.217: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 05:19:44.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 exec --namespace=e2e-tests-statefulset-lhmbl ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 05:19:44.421: INFO: stderr: ""
May 29 05:19:44.421: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 05:19:44.421: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 05:19:44.421: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 05:20:14.440: INFO: Deleting all statefulset in ns e2e-tests-statefulset-lhmbl
May 29 05:20:14.443: INFO: Scaling statefulset ss to 0
May 29 05:20:14.461: INFO: Waiting for statefulset status.replicas updated to 0
May 29 05:20:14.463: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:20:14.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-lhmbl" for this suite.
May 29 05:20:20.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:20:20.508: INFO: namespace: e2e-tests-statefulset-lhmbl, resource: bindings, ignored listing per whitelist
May 29 05:20:20.581: INFO: namespace e2e-tests-statefulset-lhmbl deletion completed in 6.105386297s

â€¢ [SLOW TEST:108.358 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:20:20.582: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-740b31ed-81d1-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 05:20:20.662: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-cr9xr" to be "success or failure"
May 29 05:20:20.672: INFO: Pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.998181ms
May 29 05:20:22.676: INFO: Pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014172633s
May 29 05:20:24.679: INFO: Pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017178108s
May 29 05:20:26.682: INFO: Pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020720185s
STEP: Saw pod success
May 29 05:20:26.682: INFO: Pod "pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:20:26.685: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 05:20:26.699: INFO: Waiting for pod pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:20:26.702: INFO: Pod pod-projected-configmaps-740b99a9-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:20:26.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cr9xr" for this suite.
May 29 05:20:32.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:20:32.789: INFO: namespace: e2e-tests-projected-cr9xr, resource: bindings, ignored listing per whitelist
May 29 05:20:32.799: INFO: namespace e2e-tests-projected-cr9xr deletion completed in 6.094551485s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:20:32.799: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 05:20:32.873: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 05:20:32.880: INFO: Waiting for terminating namespaces to be deleted...
May 29 05:20:32.882: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000000 before test
May 29 05:20:32.888: INFO: azure-cni-networkmonitor-nb4kr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:20:32.888: INFO: keyvault-flexvolume-5hhhq from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:20:32.888: INFO: azure-ip-masq-agent-28z5v from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:20:32.888: INFO: blobfuse-flexvol-installer-rr6q5 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:20:32.888: INFO: kube-proxy-hfc5q from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 05:20:32.888: INFO: sonobuoy-e2e-job-89123943a0f146da from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container e2e ready: true, restart count 0
May 29 05:20:32.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 05:20:32.888: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-99ztx from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:20:32.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 05:20:32.888: INFO: 	Container systemd-logs ready: true, restart count 0
May 29 05:20:32.888: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000001 before test
May 29 05:20:32.895: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-6w5ft from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 05:20:32.895: INFO: 	Container systemd-logs ready: true, restart count 0
May 29 05:20:32.895: INFO: azure-cni-networkmonitor-bg749 from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:20:32.895: INFO: blobfuse-flexvol-installer-stf4c from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:20:32.895: INFO: kube-proxy-dtmsl from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 05:20:32.895: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 04:35:39 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 05:20:32.895: INFO: keyvault-flexvolume-vn2lw from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:20:32.895: INFO: azure-ip-masq-agent-8pcj4 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.895: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:20:32.895: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000002 before test
May 29 05:20:32.900: INFO: metrics-server-69b44566d5-5fhmr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container metrics-server ready: true, restart count 1
May 29 05:20:32.900: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-7btfr from heptio-sonobuoy started at 2019-05-29 04:35:52 +0000 UTC (2 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 05:20:32.900: INFO: 	Container systemd-logs ready: true, restart count 0
May 29 05:20:32.900: INFO: azure-cni-networkmonitor-qrlgm from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:20:32.900: INFO: keyvault-flexvolume-w2dqp from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:20:32.900: INFO: blobfuse-flexvol-installer-lp79k from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:20:32.900: INFO: azure-ip-masq-agent-cpsxm from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:20:32.900: INFO: kube-proxy-fbq9l from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:20:32.900: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node k8s-pool1-29361026-vmss000000
STEP: verifying the node has the label node k8s-pool1-29361026-vmss000001
STEP: verifying the node has the label node k8s-pool1-29361026-vmss000002
May 29 05:20:32.974: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.974: INFO: Pod sonobuoy-e2e-job-89123943a0f146da requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.974: INFO: Pod sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-6w5ft requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.974: INFO: Pod sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-7btfr requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.974: INFO: Pod sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-99ztx requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.974: INFO: Pod azure-cni-networkmonitor-bg749 requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.974: INFO: Pod azure-cni-networkmonitor-nb4kr requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.974: INFO: Pod azure-cni-networkmonitor-qrlgm requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.974: INFO: Pod azure-ip-masq-agent-28z5v requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.974: INFO: Pod azure-ip-masq-agent-8pcj4 requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.974: INFO: Pod azure-ip-masq-agent-cpsxm requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.974: INFO: Pod blobfuse-flexvol-installer-lp79k requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.974: INFO: Pod blobfuse-flexvol-installer-rr6q5 requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.974: INFO: Pod blobfuse-flexvol-installer-stf4c requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.975: INFO: Pod keyvault-flexvolume-5hhhq requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.975: INFO: Pod keyvault-flexvolume-vn2lw requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.975: INFO: Pod keyvault-flexvolume-w2dqp requesting resource cpu=50m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.975: INFO: Pod kube-proxy-dtmsl requesting resource cpu=100m on Node k8s-pool1-29361026-vmss000001
May 29 05:20:32.975: INFO: Pod kube-proxy-fbq9l requesting resource cpu=100m on Node k8s-pool1-29361026-vmss000002
May 29 05:20:32.975: INFO: Pod kube-proxy-hfc5q requesting resource cpu=100m on Node k8s-pool1-29361026-vmss000000
May 29 05:20:32.975: INFO: Pod metrics-server-69b44566d5-5fhmr requesting resource cpu=0m on Node k8s-pool1-29361026-vmss000002
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b631c5b-81d1-11e9-b4f9-d20c9d8615e3.15a30fc4947d81d4], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-d4t9c/filler-pod-7b631c5b-81d1-11e9-b4f9-d20c9d8615e3 to k8s-pool1-29361026-vmss000000]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b631c5b-81d1-11e9-b4f9-d20c9d8615e3.15a30fc51744d1f6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b631c5b-81d1-11e9-b4f9-d20c9d8615e3.15a30fc5552801ba], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b631c5b-81d1-11e9-b4f9-d20c9d8615e3.15a30fc56566b108], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3.15a30fc49567764d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-d4t9c/filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3 to k8s-pool1-29361026-vmss000001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3.15a30fc5300e0565], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3.15a30fc55295538d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3.15a30fc59c7ffc1a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b63f171-81d1-11e9-b4f9-d20c9d8615e3.15a30fc5afb13795], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b6666e3-81d1-11e9-b4f9-d20c9d8615e3.15a30fc496380593], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-d4t9c/filler-pod-7b6666e3-81d1-11e9-b4f9-d20c9d8615e3 to k8s-pool1-29361026-vmss000002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b6666e3-81d1-11e9-b4f9-d20c9d8615e3.15a30fc51888ac66], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b6666e3-81d1-11e9-b4f9-d20c9d8615e3.15a30fc55a5bf84f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b6666e3-81d1-11e9-b4f9-d20c9d8615e3.15a30fc56b1a5d1f], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a30fc5fccc214d], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-pool1-29361026-vmss000000
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-pool1-29361026-vmss000001
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-pool1-29361026-vmss000002
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:20:40.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-d4t9c" for this suite.
May 29 05:20:46.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:20:46.184: INFO: namespace: e2e-tests-sched-pred-d4t9c, resource: bindings, ignored listing per whitelist
May 29 05:20:46.192: INFO: namespace e2e-tests-sched-pred-d4t9c deletion completed in 6.089039621s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:13.393 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:20:46.192: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 29 05:20:52.811: INFO: Successfully updated pod "labelsupdate8350dc30-81d1-11e9-b4f9-d20c9d8615e3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:20:54.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-74q4g" for this suite.
May 29 05:21:16.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:21:16.878: INFO: namespace: e2e-tests-downward-api-74q4g, resource: bindings, ignored listing per whitelist
May 29 05:21:16.908: INFO: namespace e2e-tests-downward-api-74q4g deletion completed in 22.080825265s

â€¢ [SLOW TEST:30.716 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:21:16.908: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:21:16.990: INFO: Waiting up to 5m0s for pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-swpz5" to be "success or failure"
May 29 05:21:17.003: INFO: Pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.78965ms
May 29 05:21:19.006: INFO: Pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01625334s
May 29 05:21:21.009: INFO: Pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019345042s
May 29 05:21:23.012: INFO: Pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022501851s
STEP: Saw pod success
May 29 05:21:23.012: INFO: Pod "downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:21:23.014: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:21:23.039: INFO: Waiting for pod downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:21:23.041: INFO: Pod downwardapi-volume-959dfd16-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:21:23.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-swpz5" for this suite.
May 29 05:21:29.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:21:29.111: INFO: namespace: e2e-tests-projected-swpz5, resource: bindings, ignored listing per whitelist
May 29 05:21:29.148: INFO: namespace e2e-tests-projected-swpz5 deletion completed in 6.103414869s

â€¢ [SLOW TEST:12.240 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:21:29.148: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0529 05:21:30.281213      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 05:21:30.281: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:21:30.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-txzzv" for this suite.
May 29 05:21:36.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:21:36.343: INFO: namespace: e2e-tests-gc-txzzv, resource: bindings, ignored listing per whitelist
May 29 05:21:36.381: INFO: namespace e2e-tests-gc-txzzv deletion completed in 6.097498021s

â€¢ [SLOW TEST:7.232 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:21:36.381: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 29 05:21:42.992: INFO: Successfully updated pod "annotationupdatea13a2fba-81d1-11e9-b4f9-d20c9d8615e3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:21:45.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z2qzb" for this suite.
May 29 05:22:17.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:22:17.074: INFO: namespace: e2e-tests-projected-z2qzb, resource: bindings, ignored listing per whitelist
May 29 05:22:17.104: INFO: namespace e2e-tests-projected-z2qzb deletion completed in 32.094853119s

â€¢ [SLOW TEST:40.723 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:22:17.105: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 29 05:22:17.193: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:22:27.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2h7f7" for this suite.
May 29 05:22:49.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:22:49.469: INFO: namespace: e2e-tests-init-container-2h7f7, resource: bindings, ignored listing per whitelist
May 29 05:22:49.519: INFO: namespace e2e-tests-init-container-2h7f7 deletion completed in 22.101514605s

â€¢ [SLOW TEST:32.414 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:22:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ccd0e1ca-81d1-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 05:22:49.602: INFO: Waiting up to 5m0s for pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-cxzch" to be "success or failure"
May 29 05:22:49.605: INFO: Pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272262ms
May 29 05:22:51.608: INFO: Pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006060087s
May 29 05:22:53.611: INFO: Pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008771618s
May 29 05:22:55.614: INFO: Pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011720653s
STEP: Saw pod success
May 29 05:22:55.614: INFO: Pod "pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:22:55.616: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 05:22:55.637: INFO: Waiting for pod pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:22:55.640: INFO: Pod pod-configmaps-ccd20a76-81d1-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:22:55.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cxzch" for this suite.
May 29 05:23:01.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:23:01.701: INFO: namespace: e2e-tests-configmap-cxzch, resource: bindings, ignored listing per whitelist
May 29 05:23:01.751: INFO: namespace e2e-tests-configmap-cxzch deletion completed in 6.107687996s

â€¢ [SLOW TEST:12.231 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:23:01.751: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
May 29 05:23:01.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 api-versions'
May 29 05:23:02.000: INFO: stderr: ""
May 29 05:23:02.000: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:23:02.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p8274" for this suite.
May 29 05:23:08.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:23:08.047: INFO: namespace: e2e-tests-kubectl-p8274, resource: bindings, ignored listing per whitelist
May 29 05:23:08.091: INFO: namespace e2e-tests-kubectl-p8274 deletion completed in 6.087599691s

â€¢ [SLOW TEST:6.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:23:08.091: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 29 05:23:08.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:10.719: INFO: stderr: ""
May 29 05:23:10.719: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 05:23:10.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:10.809: INFO: stderr: ""
May 29 05:23:10.809: INFO: stdout: "update-demo-nautilus-gwhqt update-demo-nautilus-j9xvd "
May 29 05:23:10.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-gwhqt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:10.885: INFO: stderr: ""
May 29 05:23:10.885: INFO: stdout: ""
May 29 05:23:10.885: INFO: update-demo-nautilus-gwhqt is created but not running
May 29 05:23:15.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:15.961: INFO: stderr: ""
May 29 05:23:15.961: INFO: stdout: "update-demo-nautilus-gwhqt update-demo-nautilus-j9xvd "
May 29 05:23:15.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-gwhqt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:16.036: INFO: stderr: ""
May 29 05:23:16.036: INFO: stdout: "true"
May 29 05:23:16.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-gwhqt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:16.116: INFO: stderr: ""
May 29 05:23:16.116: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:23:16.116: INFO: validating pod update-demo-nautilus-gwhqt
May 29 05:23:16.123: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:23:16.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:23:16.123: INFO: update-demo-nautilus-gwhqt is verified up and running
May 29 05:23:16.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:16.201: INFO: stderr: ""
May 29 05:23:16.201: INFO: stdout: "true"
May 29 05:23:16.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:16.277: INFO: stderr: ""
May 29 05:23:16.277: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:23:16.277: INFO: validating pod update-demo-nautilus-j9xvd
May 29 05:23:16.283: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:23:16.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:23:16.284: INFO: update-demo-nautilus-j9xvd is verified up and running
STEP: scaling down the replication controller
May 29 05:23:16.287: INFO: scanned /root for discovery docs: <nil>
May 29 05:23:16.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:17.390: INFO: stderr: ""
May 29 05:23:17.390: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 05:23:17.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:17.472: INFO: stderr: ""
May 29 05:23:17.472: INFO: stdout: "update-demo-nautilus-gwhqt update-demo-nautilus-j9xvd "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 29 05:23:22.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:22.552: INFO: stderr: ""
May 29 05:23:22.552: INFO: stdout: "update-demo-nautilus-gwhqt update-demo-nautilus-j9xvd "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 29 05:23:27.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:27.631: INFO: stderr: ""
May 29 05:23:27.631: INFO: stdout: "update-demo-nautilus-gwhqt update-demo-nautilus-j9xvd "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 29 05:23:32.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:32.709: INFO: stderr: ""
May 29 05:23:32.709: INFO: stdout: "update-demo-nautilus-j9xvd "
May 29 05:23:32.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:32.778: INFO: stderr: ""
May 29 05:23:32.778: INFO: stdout: "true"
May 29 05:23:32.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:32.855: INFO: stderr: ""
May 29 05:23:32.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:23:32.855: INFO: validating pod update-demo-nautilus-j9xvd
May 29 05:23:32.860: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:23:32.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:23:32.860: INFO: update-demo-nautilus-j9xvd is verified up and running
STEP: scaling up the replication controller
May 29 05:23:32.861: INFO: scanned /root for discovery docs: <nil>
May 29 05:23:32.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:33.959: INFO: stderr: ""
May 29 05:23:33.959: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 05:23:33.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:34.043: INFO: stderr: ""
May 29 05:23:34.043: INFO: stdout: "update-demo-nautilus-cgw82 update-demo-nautilus-j9xvd "
May 29 05:23:34.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-cgw82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:34.117: INFO: stderr: ""
May 29 05:23:34.117: INFO: stdout: ""
May 29 05:23:34.117: INFO: update-demo-nautilus-cgw82 is created but not running
May 29 05:23:39.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.198: INFO: stderr: ""
May 29 05:23:39.198: INFO: stdout: "update-demo-nautilus-cgw82 update-demo-nautilus-j9xvd "
May 29 05:23:39.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-cgw82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.270: INFO: stderr: ""
May 29 05:23:39.270: INFO: stdout: "true"
May 29 05:23:39.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-cgw82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.339: INFO: stderr: ""
May 29 05:23:39.339: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:23:39.339: INFO: validating pod update-demo-nautilus-cgw82
May 29 05:23:39.344: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:23:39.344: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:23:39.344: INFO: update-demo-nautilus-cgw82 is verified up and running
May 29 05:23:39.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.411: INFO: stderr: ""
May 29 05:23:39.411: INFO: stdout: "true"
May 29 05:23:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-j9xvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.479: INFO: stderr: ""
May 29 05:23:39.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 05:23:39.479: INFO: validating pod update-demo-nautilus-j9xvd
May 29 05:23:39.483: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 05:23:39.483: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 05:23:39.483: INFO: update-demo-nautilus-j9xvd is verified up and running
STEP: using delete to clean up resources
May 29 05:23:39.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.564: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 05:23:39.564: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 29 05:23:39.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-ldl95'
May 29 05:23:39.654: INFO: stderr: "No resources found.\n"
May 29 05:23:39.654: INFO: stdout: ""
May 29 05:23:39.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -l name=update-demo --namespace=e2e-tests-kubectl-ldl95 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 05:23:39.742: INFO: stderr: ""
May 29 05:23:39.742: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:23:39.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ldl95" for this suite.
May 29 05:24:01.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:24:01.820: INFO: namespace: e2e-tests-kubectl-ldl95, resource: bindings, ignored listing per whitelist
May 29 05:24:01.839: INFO: namespace e2e-tests-kubectl-ldl95 deletion completed in 22.093790943s

â€¢ [SLOW TEST:53.748 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:24:01.840: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-7h9c4;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7h9c4.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 44.110.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.110.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.110.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.110.44_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-7h9c4;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-7h9c4.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-7h9c4.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7h9c4.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 44.110.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.110.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.110.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.110.44_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 29 05:24:29.981: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:29.985: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:29.992: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:29.997: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.000: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.003: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.025: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.028: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.031: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.034: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.036: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.039: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.041: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.044: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:30.061: INFO: Lookups using e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4 wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-7h9c4 jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4 jessie_udp@dns-test-service.e2e-tests-dns-7h9c4.svc jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc]

May 29 05:24:35.066: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.068: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.073: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.079: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.119: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.122: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.127: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.130: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4 from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.133: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.136: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.138: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:35.159: INFO: Lookups using e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4 wheezy_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-7h9c4 jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4 jessie_udp@dns-test-service.e2e-tests-dns-7h9c4.svc jessie_tcp@dns-test-service.e2e-tests-dns-7h9c4.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc]

May 29 05:24:40.065: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:40.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:40.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc from pod e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3: the server could not find the requested resource (get pods dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3)
May 29 05:24:40.140: INFO: Lookups using e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3 failed for: [wheezy_udp@dns-test-service wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-7h9c4.svc]

May 29 05:24:45.232: INFO: DNS probes using e2e-tests-dns-7h9c4/dns-test-f7f06fdb-81d1-11e9-b4f9-d20c9d8615e3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:24:45.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-7h9c4" for this suite.
May 29 05:24:51.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:24:51.350: INFO: namespace: e2e-tests-dns-7h9c4, resource: bindings, ignored listing per whitelist
May 29 05:24:51.391: INFO: namespace e2e-tests-dns-7h9c4 deletion completed in 6.086749989s

â€¢ [SLOW TEST:49.551 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:24:51.392: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 05:24:51.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-w5htx'
May 29 05:24:51.539: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 05:24:51.539: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 29 05:24:51.555: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-f9gcg]
May 29 05:24:51.556: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-f9gcg" in namespace "e2e-tests-kubectl-w5htx" to be "running and ready"
May 29 05:24:51.558: INFO: Pod "e2e-test-nginx-rc-f9gcg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156176ms
May 29 05:24:53.561: INFO: Pod "e2e-test-nginx-rc-f9gcg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005381343s
May 29 05:24:55.564: INFO: Pod "e2e-test-nginx-rc-f9gcg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008596516s
May 29 05:24:57.567: INFO: Pod "e2e-test-nginx-rc-f9gcg": Phase="Running", Reason="", readiness=true. Elapsed: 6.011788394s
May 29 05:24:57.567: INFO: Pod "e2e-test-nginx-rc-f9gcg" satisfied condition "running and ready"
May 29 05:24:57.567: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-f9gcg]
May 29 05:24:57.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-w5htx'
May 29 05:24:57.697: INFO: stderr: ""
May 29 05:24:57.697: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
May 29 05:24:57.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-w5htx'
May 29 05:24:57.772: INFO: stderr: ""
May 29 05:24:57.772: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:24:57.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w5htx" for this suite.
May 29 05:25:19.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:25:19.836: INFO: namespace: e2e-tests-kubectl-w5htx, resource: bindings, ignored listing per whitelist
May 29 05:25:19.863: INFO: namespace e2e-tests-kubectl-w5htx deletion completed in 22.086511306s

â€¢ [SLOW TEST:28.472 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:25:19.864: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-kz8ml
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 05:25:19.938: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 05:25:48.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.37:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kz8ml PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:25:48.024: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:25:48.149: INFO: Found all expected endpoints: [netserver-0]
May 29 05:25:48.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.71:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kz8ml PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:25:48.152: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:25:48.263: INFO: Found all expected endpoints: [netserver-1]
May 29 05:25:48.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.23:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kz8ml PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:25:48.264: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:25:48.376: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:25:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-kz8ml" for this suite.
May 29 05:26:10.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:26:10.439: INFO: namespace: e2e-tests-pod-network-test-kz8ml, resource: bindings, ignored listing per whitelist
May 29 05:26:10.469: INFO: namespace e2e-tests-pod-network-test-kz8ml deletion completed in 22.090210566s

â€¢ [SLOW TEST:50.606 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:26:10.470: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
May 29 05:26:10.557: INFO: Waiting up to 5m0s for pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-containers-bzw87" to be "success or failure"
May 29 05:26:10.565: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.605302ms
May 29 05:26:12.570: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013429437s
May 29 05:26:14.573: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0161535s
May 29 05:26:16.576: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019476661s
May 29 05:26:18.579: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022535229s
STEP: Saw pod success
May 29 05:26:18.579: INFO: Pod "client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:26:18.581: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:26:18.594: INFO: Waiting for pod client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:26:18.597: INFO: Pod client-containers-449975a5-81d2-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:26:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-bzw87" for this suite.
May 29 05:26:24.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:26:24.671: INFO: namespace: e2e-tests-containers-bzw87, resource: bindings, ignored listing per whitelist
May 29 05:26:24.693: INFO: namespace e2e-tests-containers-bzw87 deletion completed in 6.093392274s

â€¢ [SLOW TEST:14.224 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:26:24.693: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-m4clj
May 29 05:26:30.787: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-m4clj
STEP: checking the pod's current state and verifying that restartCount is present
May 29 05:26:30.789: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:30:31.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-m4clj" for this suite.
May 29 05:30:37.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:30:37.255: INFO: namespace: e2e-tests-container-probe-m4clj, resource: bindings, ignored listing per whitelist
May 29 05:30:37.272: INFO: namespace e2e-tests-container-probe-m4clj deletion completed in 6.081000102s

â€¢ [SLOW TEST:252.578 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:30:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:30:37.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-d2g29" to be "success or failure"
May 29 05:30:37.360: INFO: Pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.809135ms
May 29 05:30:39.364: INFO: Pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009731731s
May 29 05:30:41.368: INFO: Pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013891827s
May 29 05:30:43.371: INFO: Pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017374634s
STEP: Saw pod success
May 29 05:30:43.371: INFO: Pod "downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:30:43.374: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:30:43.396: INFO: Waiting for pod downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:30:43.398: INFO: Pod downwardapi-volume-e39f45f8-81d2-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:30:43.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-d2g29" for this suite.
May 29 05:30:49.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:30:49.476: INFO: namespace: e2e-tests-downward-api-d2g29, resource: bindings, ignored listing per whitelist
May 29 05:30:49.479: INFO: namespace e2e-tests-downward-api-d2g29 deletion completed in 6.077858684s

â€¢ [SLOW TEST:12.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:30:49.479: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 29 05:30:56.577: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:30:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-qlfxt" for this suite.
May 29 05:31:18.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:31:18.643: INFO: namespace: e2e-tests-replicaset-qlfxt, resource: bindings, ignored listing per whitelist
May 29 05:31:18.696: INFO: namespace e2e-tests-replicaset-qlfxt deletion completed in 22.095080784s

â€¢ [SLOW TEST:29.216 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:31:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 29 05:31:25.309: INFO: Successfully updated pod "labelsupdatefc502e1b-81d2-11e9-b4f9-d20c9d8615e3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:31:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s8k96" for this suite.
May 29 05:31:49.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:31:49.414: INFO: namespace: e2e-tests-projected-s8k96, resource: bindings, ignored listing per whitelist
May 29 05:31:49.418: INFO: namespace e2e-tests-projected-s8k96 deletion completed in 22.091919118s

â€¢ [SLOW TEST:30.722 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:31:49.418: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
May 29 05:31:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-fldjw'
May 29 05:31:49.780: INFO: stderr: ""
May 29 05:31:49.780: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
May 29 05:31:50.783: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:50.783: INFO: Found 0 / 1
May 29 05:31:51.783: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:51.783: INFO: Found 0 / 1
May 29 05:31:52.783: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:52.783: INFO: Found 0 / 1
May 29 05:31:53.783: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:53.783: INFO: Found 0 / 1
May 29 05:31:54.783: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:54.783: INFO: Found 1 / 1
May 29 05:31:54.783: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 05:31:54.785: INFO: Selector matched 1 pods for map[app:redis]
May 29 05:31:54.785: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 29 05:31:54.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 logs redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw'
May 29 05:31:54.868: INFO: stderr: ""
May 29 05:31:54.868: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 05:31:53.735 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 05:31:53.736 # Server started, Redis version 3.2.12\n1:M 29 May 05:31:53.736 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 05:31:53.736 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 29 05:31:54.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 log redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw --tail=1'
May 29 05:31:54.945: INFO: stderr: ""
May 29 05:31:54.945: INFO: stdout: "1:M 29 May 05:31:53.736 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 29 05:31:54.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 log redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw --limit-bytes=1'
May 29 05:31:55.042: INFO: stderr: ""
May 29 05:31:55.042: INFO: stdout: " "
STEP: exposing timestamps
May 29 05:31:55.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 log redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw --tail=1 --timestamps'
May 29 05:31:55.133: INFO: stderr: ""
May 29 05:31:55.133: INFO: stdout: "2019-05-29T05:31:53.736205498Z 1:M 29 May 05:31:53.736 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 29 05:31:57.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 log redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw --since=1s'
May 29 05:31:57.720: INFO: stderr: ""
May 29 05:31:57.720: INFO: stdout: ""
May 29 05:31:57.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 log redis-master-5cmrc redis-master --namespace=e2e-tests-kubectl-fldjw --since=24h'
May 29 05:31:57.803: INFO: stderr: ""
May 29 05:31:57.803: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 05:31:53.735 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 05:31:53.736 # Server started, Redis version 3.2.12\n1:M 29 May 05:31:53.736 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 05:31:53.736 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
May 29 05:31:57.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fldjw'
May 29 05:31:57.880: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 05:31:57.880: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 29 05:31:57.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-fldjw'
May 29 05:31:57.958: INFO: stderr: "No resources found.\n"
May 29 05:31:57.958: INFO: stdout: ""
May 29 05:31:57.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -l name=nginx --namespace=e2e-tests-kubectl-fldjw -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 05:31:58.052: INFO: stderr: ""
May 29 05:31:58.052: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:31:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fldjw" for this suite.
May 29 05:32:20.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:32:20.106: INFO: namespace: e2e-tests-kubectl-fldjw, resource: bindings, ignored listing per whitelist
May 29 05:32:20.139: INFO: namespace e2e-tests-kubectl-fldjw deletion completed in 22.083671887s

â€¢ [SLOW TEST:30.721 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:32:20.140: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-20ef87ac-81d3-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-20ef87ac-81d3-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:33:56.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wcw4p" for this suite.
May 29 05:34:18.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:34:18.731: INFO: namespace: e2e-tests-projected-wcw4p, resource: bindings, ignored listing per whitelist
May 29 05:34:18.753: INFO: namespace e2e-tests-projected-wcw4p deletion completed in 22.089384482s

â€¢ [SLOW TEST:118.613 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:34:18.754: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-mx6x6
May 29 05:34:24.841: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-mx6x6
STEP: checking the pod's current state and verifying that restartCount is present
May 29 05:34:24.843: INFO: Initial restart count of pod liveness-http is 0
May 29 05:34:44.873: INFO: Restart count of pod e2e-tests-container-probe-mx6x6/liveness-http is now 1 (20.029745204s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:34:44.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mx6x6" for this suite.
May 29 05:34:50.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:34:50.970: INFO: namespace: e2e-tests-container-probe-mx6x6, resource: bindings, ignored listing per whitelist
May 29 05:34:50.974: INFO: namespace e2e-tests-container-probe-mx6x6 deletion completed in 6.083527362s

â€¢ [SLOW TEST:32.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:34:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 05:34:51.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-8hh9t'
May 29 05:34:52.826: INFO: stderr: ""
May 29 05:34:52.826: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
May 29 05:34:52.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-8hh9t'
May 29 05:34:59.330: INFO: stderr: ""
May 29 05:34:59.330: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:34:59.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8hh9t" for this suite.
May 29 05:35:05.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:35:05.376: INFO: namespace: e2e-tests-kubectl-8hh9t, resource: bindings, ignored listing per whitelist
May 29 05:35:05.418: INFO: namespace e2e-tests-kubectl-8hh9t deletion completed in 6.084134189s

â€¢ [SLOW TEST:14.444 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:35:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:35:05.502: INFO: Creating deployment "nginx-deployment"
May 29 05:35:05.506: INFO: Waiting for observed generation 1
May 29 05:35:07.511: INFO: Waiting for all required pods to come up
May 29 05:35:07.514: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 29 05:35:15.526: INFO: Waiting for deployment "nginx-deployment" to complete
May 29 05:35:15.536: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 29 05:35:15.542: INFO: Updating deployment nginx-deployment
May 29 05:35:15.542: INFO: Waiting for observed generation 2
May 29 05:35:17.548: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 29 05:35:17.550: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 29 05:35:17.553: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 29 05:35:17.559: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 29 05:35:17.559: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 29 05:35:17.561: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 29 05:35:17.564: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 29 05:35:17.564: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 29 05:35:17.571: INFO: Updating deployment nginx-deployment
May 29 05:35:17.571: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 29 05:35:17.581: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 29 05:35:19.603: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 05:35:19.609: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-z827w,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z827w/deployments/nginx-deployment,UID:837363f2-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13543,Generation:3,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-05-29 05:35:17 +0000 UTC 2019-05-29 05:35:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-29 05:35:17 +0000 UTC 2019-05-29 05:35:05 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

May 29 05:35:19.612: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-z827w,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z827w/replicasets/nginx-deployment-65bbdb5f8,UID:896fdfa8-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13534,Generation:3,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 837363f2-81d3-11e9-85ba-000d3a6e4ecc 0xc002014997 0xc002014998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 05:35:19.612: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 29 05:35:19.612: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-z827w,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z827w/replicasets/nginx-deployment-555b55d965,UID:83745ac9-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13540,Generation:3,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 837363f2-81d3-11e9-85ba-000d3a6e4ecc 0xc0020148d7 0xc0020148d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-5lqrq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5lqrq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-5lqrq,UID:8aad7b28-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13520,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001838e57 0xc001838e58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018391e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001839200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-85t9j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-85t9j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-85t9j,UID:8aa93949-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13563,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001839450 0xc001839451}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001839690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018396b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-8dmjg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8dmjg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-8dmjg,UID:8aa8d1ed-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13558,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001839950 0xc001839951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001839b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001839bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-bv668" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bv668,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-bv668,UID:8aa52415-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13569,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001839ee0 0xc001839ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001839fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001839fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-gbwnp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gbwnp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-gbwnp,UID:8aa8b701-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13502,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c303e0 0xc001c303e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c30450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c30470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.620: INFO: Pod "nginx-deployment-555b55d965-gwkdr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gwkdr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-gwkdr,UID:837ca905-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13384,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c304e0 0xc001c304e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c308a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c30980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.26,StartTime:2019-05-29 05:35:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://15ecb757a9bcd8ca490c7722b7a5b1e9d191b2464c14d1b98c70a6c5f0f5dce1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-jsj66" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jsj66,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-jsj66,UID:8aadb3fe-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13584,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c30a40 0xc001c30a41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c30aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c30ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-m6b8q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-m6b8q,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-m6b8q,UID:837a6f78-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13395,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c30d10 0xc001c30d11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c30e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c30e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.49,StartTime:2019-05-29 05:35:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://42f2c589b50cee0adc78c0657aaa43f0d45a9d1bb10f0d3ff3b5dd4ee9d916a9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-mn65s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mn65s,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-mn65s,UID:8aad7ee3-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13581,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c30f40 0xc001c30f41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c30fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c30fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-n7n79" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-n7n79,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-n7n79,UID:8aa8e8bf-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13546,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31160 0xc001c31161}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c311c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c311e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-ngcmp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ngcmp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-ngcmp,UID:837aaea4-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13386,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c312c0 0xc001c312c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c31390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.17,StartTime:2019-05-29 05:35:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7fa2dbefdb57ead90d1ef5486d2aa94b356cf57da9a88f06a17f9a0bc24b75ab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-nqnlb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nqnlb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-nqnlb,UID:8376c239-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13398,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31450 0xc001c31451}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c314b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c314d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.52,StartTime:2019-05-29 05:35:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://36d77b1a3a40c6b67dbc348e8f4cedd34fba8977f82332e6cf629bb0a29e91f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-qjsv2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qjsv2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-qjsv2,UID:837aa103-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13391,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c315c0 0xc001c315c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c316b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.61,StartTime:2019-05-29 05:35:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3cf3385aada36d507b396aedbdb41aa0e503137781d9c1e7813c95d7d3834e2c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.621: INFO: Pod "nginx-deployment-555b55d965-v8rkr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v8rkr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-v8rkr,UID:837a7979-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13413,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31810 0xc001c31811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c31890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.69,StartTime:2019-05-29 05:35:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://aefeb81dd286ef47f154be1d74afdbd0625e361d678aa451b06d89f6adad4548}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-wkb4d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wkb4d,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-wkb4d,UID:8377ba46-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13410,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31950 0xc001c31951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c319b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c319d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.73,StartTime:2019-05-29 05:35:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7acf0638b95afb8a449222b68b2055b28d51956aa91c75e0460dacfe25279784}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-wwj5v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wwj5v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-wwj5v,UID:8aa5ea78-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13580,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31a90 0xc001c31a91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c31b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-05-29 05:35:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-xp4qr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xp4qr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-xp4qr,UID:8377a4ae-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13370,Generation:0,CreationTimestamp:2019-05-29 05:35:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31bd0 0xc001c31bd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c31c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.13,StartTime:2019-05-29 05:35:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 05:35:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f4ca0543007a5c63ca086cf095e96635dd35b6dd169ce906adcdf3f56bddd107}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-z7gwh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z7gwh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-z7gwh,UID:8aa5eb35-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13523,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001c31d70 0xc001c31d71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c31dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c31f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-zcmdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zcmdq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-zcmdq,UID:8aadabcc-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13528,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001712000 0xc001712001}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001712060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001712080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-555b55d965-zjfkn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zjfkn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-555b55d965-zjfkn,UID:8aad9675-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13576,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83745ac9-81d3-11e9-85ba-000d3a6e4ecc 0xc001712240 0xc001712241}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017122b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017122d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-65bbdb5f8-24rh5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-24rh5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-24rh5,UID:8aa67aca-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13539,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc001712470 0xc001712471}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017124e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001712500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.622: INFO: Pod "nginx-deployment-65bbdb5f8-2nbs8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2nbs8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-2nbs8,UID:897c61c3-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13470,Generation:0,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0017128b0 0xc0017128b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001712920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001712940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-05-29 05:35:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-2s49f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2s49f,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-2s49f,UID:897d4c56-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13465,Generation:0,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc001712a00 0xc001712a01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001713240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001713260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-457wr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-457wr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-457wr,UID:897295c5-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13447,Generation:0,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc001713320 0xc001713321}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001713390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001713440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-8dczw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8dczw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-8dczw,UID:8973f810-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13462,Generation:0,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc001713520 0xc001713521}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017135a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017135c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-9pbcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-9pbcq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-9pbcq,UID:8aad58d0-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13566,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0017139a0 0xc0017139a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001713c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001713ca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-nrpgf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nrpgf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-nrpgf,UID:8aaf8b6b-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13532,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4040 0xc0011e4041}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e40b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e40d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-q9hsd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-q9hsd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-q9hsd,UID:897401a7-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13468,Generation:0,CreationTimestamp:2019-05-29 05:35:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4200 0xc0011e4201}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e4270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e4290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:15 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-05-29 05:35:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-rfn6d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rfn6d,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-rfn6d,UID:8aad6e9d-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13551,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e43e0 0xc0011e43e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e4450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e4470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-rtphr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rtphr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-rtphr,UID:8aa8c18d-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13544,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4540 0xc0011e4541}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e4620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e4640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-sq6kt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sq6kt,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-sq6kt,UID:8aad69a6-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13522,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4700 0xc0011e4701}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e47d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e47f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.623: INFO: Pod "nginx-deployment-65bbdb5f8-xkn9c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xkn9c,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-xkn9c,UID:8aa87f8d-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13495,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4860 0xc0011e4861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e48d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e48f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 05:35:19.624: INFO: Pod "nginx-deployment-65bbdb5f8-zz7j9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zz7j9,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-z827w,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z827w/pods/nginx-deployment-65bbdb5f8-zz7j9,UID:8aaca1e4-81d3-11e9-85ba-000d3a6e4ecc,ResourceVersion:13548,Generation:0,CreationTimestamp:2019-05-29 05:35:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 896fdfa8-81d3-11e9-85ba-000d3a6e4ecc 0xc0011e4960 0xc0011e4961}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-72kxl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-72kxl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-72kxl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011e49d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011e49f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 05:35:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-29 05:35:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:35:19.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-z827w" for this suite.
May 29 05:35:27.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:35:27.682: INFO: namespace: e2e-tests-deployment-z827w, resource: bindings, ignored listing per whitelist
May 29 05:35:27.729: INFO: namespace e2e-tests-deployment-z827w deletion completed in 8.100581078s

â€¢ [SLOW TEST:22.311 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:35:27.729: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hn64p
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 05:35:27.805: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 05:36:01.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.26:8080/dial?request=hostName&protocol=http&host=10.240.0.11&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-hn64p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:36:01.896: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:36:02.021: INFO: Waiting for endpoints: map[]
May 29 05:36:02.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.26:8080/dial?request=hostName&protocol=http&host=10.240.0.40&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-hn64p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:36:02.024: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:36:02.151: INFO: Waiting for endpoints: map[]
May 29 05:36:02.153: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.26:8080/dial?request=hostName&protocol=http&host=10.240.0.75&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-hn64p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:36:02.153: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:36:02.286: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:36:02.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hn64p" for this suite.
May 29 05:36:24.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:36:24.339: INFO: namespace: e2e-tests-pod-network-test-hn64p, resource: bindings, ignored listing per whitelist
May 29 05:36:24.374: INFO: namespace e2e-tests-pod-network-test-hn64p deletion completed in 22.081196421s

â€¢ [SLOW TEST:56.645 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:36:24.374: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
May 29 05:36:24.968: INFO: created pod pod-service-account-defaultsa
May 29 05:36:24.968: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 29 05:36:24.972: INFO: created pod pod-service-account-mountsa
May 29 05:36:24.972: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 29 05:36:24.984: INFO: created pod pod-service-account-nomountsa
May 29 05:36:24.984: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 29 05:36:24.996: INFO: created pod pod-service-account-defaultsa-mountspec
May 29 05:36:24.996: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 29 05:36:25.002: INFO: created pod pod-service-account-mountsa-mountspec
May 29 05:36:25.002: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 29 05:36:25.013: INFO: created pod pod-service-account-nomountsa-mountspec
May 29 05:36:25.013: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 29 05:36:25.034: INFO: created pod pod-service-account-defaultsa-nomountspec
May 29 05:36:25.034: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 29 05:36:25.048: INFO: created pod pod-service-account-mountsa-nomountspec
May 29 05:36:25.048: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 29 05:36:25.054: INFO: created pod pod-service-account-nomountsa-nomountspec
May 29 05:36:25.054: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:36:25.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-k64lh" for this suite.
May 29 05:36:47.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:36:47.164: INFO: namespace: e2e-tests-svcaccounts-k64lh, resource: bindings, ignored listing per whitelist
May 29 05:36:47.183: INFO: namespace e2e-tests-svcaccounts-k64lh deletion completed in 22.106011014s

â€¢ [SLOW TEST:22.808 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:36:47.183: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:36:47.256: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:36:53.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-cn2ss" for this suite.
May 29 05:37:33.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:37:33.424: INFO: namespace: e2e-tests-pods-cn2ss, resource: bindings, ignored listing per whitelist
May 29 05:37:33.472: INFO: namespace e2e-tests-pods-cn2ss deletion completed in 40.083284647s

â€¢ [SLOW TEST:46.290 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:37:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-w77rz/secret-test-dbb261c4-81d3-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 05:37:33.557: INFO: Waiting up to 5m0s for pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-w77rz" to be "success or failure"
May 29 05:37:33.567: INFO: Pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048656ms
May 29 05:37:35.570: INFO: Pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012740655s
May 29 05:37:37.573: INFO: Pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015715041s
May 29 05:37:39.576: INFO: Pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01831083s
STEP: Saw pod success
May 29 05:37:39.576: INFO: Pod "pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:37:39.578: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3 container env-test: <nil>
STEP: delete the pod
May 29 05:37:39.599: INFO: Waiting for pod pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:37:39.601: INFO: Pod pod-configmaps-dbb2bef2-81d3-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:37:39.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-w77rz" for this suite.
May 29 05:37:45.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:37:45.660: INFO: namespace: e2e-tests-secrets-w77rz, resource: bindings, ignored listing per whitelist
May 29 05:37:45.688: INFO: namespace e2e-tests-secrets-w77rz deletion completed in 6.084163689s

â€¢ [SLOW TEST:12.216 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:37:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0529 05:38:16.293376      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 05:38:16.293: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:38:16.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zsk84" for this suite.
May 29 05:38:22.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:38:22.365: INFO: namespace: e2e-tests-gc-zsk84, resource: bindings, ignored listing per whitelist
May 29 05:38:22.382: INFO: namespace e2e-tests-gc-zsk84 deletion completed in 6.085978013s

â€¢ [SLOW TEST:36.693 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:38:22.382: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 29 05:38:22.661: INFO: Pod name wrapped-volume-race-f8f5b25e-81d3-11e9-b4f9-d20c9d8615e3: Found 0 pods out of 5
May 29 05:38:27.667: INFO: Pod name wrapped-volume-race-f8f5b25e-81d3-11e9-b4f9-d20c9d8615e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f8f5b25e-81d3-11e9-b4f9-d20c9d8615e3 in namespace e2e-tests-emptydir-wrapper-nmszl, will wait for the garbage collector to delete the pods
May 29 05:40:09.748: INFO: Deleting ReplicationController wrapped-volume-race-f8f5b25e-81d3-11e9-b4f9-d20c9d8615e3 took: 5.366686ms
May 29 05:40:09.848: INFO: Terminating ReplicationController wrapped-volume-race-f8f5b25e-81d3-11e9-b4f9-d20c9d8615e3 pods took: 100.192277ms
STEP: Creating RC which spawns configmap-volume pods
May 29 05:40:51.366: INFO: Pod name wrapped-volume-race-51982737-81d4-11e9-b4f9-d20c9d8615e3: Found 0 pods out of 5
May 29 05:40:56.372: INFO: Pod name wrapped-volume-race-51982737-81d4-11e9-b4f9-d20c9d8615e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-51982737-81d4-11e9-b4f9-d20c9d8615e3 in namespace e2e-tests-emptydir-wrapper-nmszl, will wait for the garbage collector to delete the pods
May 29 05:42:52.448: INFO: Deleting ReplicationController wrapped-volume-race-51982737-81d4-11e9-b4f9-d20c9d8615e3 took: 5.025706ms
May 29 05:42:52.548: INFO: Terminating ReplicationController wrapped-volume-race-51982737-81d4-11e9-b4f9-d20c9d8615e3 pods took: 100.15782ms
STEP: Creating RC which spawns configmap-volume pods
May 29 05:43:32.670: INFO: Pod name wrapped-volume-race-b1bc8b79-81d4-11e9-b4f9-d20c9d8615e3: Found 0 pods out of 5
May 29 05:43:37.675: INFO: Pod name wrapped-volume-race-b1bc8b79-81d4-11e9-b4f9-d20c9d8615e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b1bc8b79-81d4-11e9-b4f9-d20c9d8615e3 in namespace e2e-tests-emptydir-wrapper-nmszl, will wait for the garbage collector to delete the pods
May 29 05:45:33.748: INFO: Deleting ReplicationController wrapped-volume-race-b1bc8b79-81d4-11e9-b4f9-d20c9d8615e3 took: 5.142712ms
May 29 05:45:33.848: INFO: Terminating ReplicationController wrapped-volume-race-b1bc8b79-81d4-11e9-b4f9-d20c9d8615e3 pods took: 100.201793ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:46:13.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-nmszl" for this suite.
May 29 05:46:19.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:46:19.372: INFO: namespace: e2e-tests-emptydir-wrapper-nmszl, resource: bindings, ignored listing per whitelist
May 29 05:46:19.422: INFO: namespace e2e-tests-emptydir-wrapper-nmszl deletion completed in 6.094265477s

â€¢ [SLOW TEST:477.040 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:46:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
May 29 05:46:19.509: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-tx5s9" to be "success or failure"
May 29 05:46:19.519: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.189331ms
May 29 05:46:21.522: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013541727s
May 29 05:46:23.525: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016304867s
May 29 05:46:25.528: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019105839s
May 29 05:46:27.530: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021409854s
STEP: Saw pod success
May 29 05:46:27.530: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 29 05:46:27.532: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 29 05:46:27.554: INFO: Waiting for pod pod-host-path-test to disappear
May 29 05:46:27.559: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:46:27.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-tx5s9" for this suite.
May 29 05:46:33.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:46:33.624: INFO: namespace: e2e-tests-hostpath-tx5s9, resource: bindings, ignored listing per whitelist
May 29 05:46:33.659: INFO: namespace e2e-tests-hostpath-tx5s9 deletion completed in 6.087371514s

â€¢ [SLOW TEST:14.236 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:46:33.659: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 29 05:46:33.742: INFO: Waiting up to 5m0s for pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-7xs64" to be "success or failure"
May 29 05:46:33.745: INFO: Pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.395244ms
May 29 05:46:35.748: INFO: Pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006005188s
May 29 05:46:37.751: INFO: Pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008959259s
May 29 05:46:39.754: INFO: Pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011720366s
STEP: Saw pod success
May 29 05:46:39.754: INFO: Pod "pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:46:39.756: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:46:39.774: INFO: Waiting for pod pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:46:39.777: INFO: Pod pod-1dabe5af-81d5-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:46:39.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7xs64" for this suite.
May 29 05:46:45.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:46:45.839: INFO: namespace: e2e-tests-emptydir-7xs64, resource: bindings, ignored listing per whitelist
May 29 05:46:45.873: INFO: namespace e2e-tests-emptydir-7xs64 deletion completed in 6.091697845s

â€¢ [SLOW TEST:12.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:46:45.873: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-64kzf
May 29 05:46:52.058: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-64kzf
STEP: checking the pod's current state and verifying that restartCount is present
May 29 05:46:52.060: INFO: Initial restart count of pod liveness-http is 0
May 29 05:47:12.092: INFO: Restart count of pod e2e-tests-container-probe-64kzf/liveness-http is now 1 (20.032371475s elapsed)
May 29 05:47:32.136: INFO: Restart count of pod e2e-tests-container-probe-64kzf/liveness-http is now 2 (40.075759911s elapsed)
May 29 05:47:52.165: INFO: Restart count of pod e2e-tests-container-probe-64kzf/liveness-http is now 3 (1m0.104932499s elapsed)
May 29 05:48:12.194: INFO: Restart count of pod e2e-tests-container-probe-64kzf/liveness-http is now 4 (1m20.133833301s elapsed)
May 29 05:49:18.298: INFO: Restart count of pod e2e-tests-container-probe-64kzf/liveness-http is now 5 (2m26.23772643s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:49:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-64kzf" for this suite.
May 29 05:49:24.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:49:24.357: INFO: namespace: e2e-tests-container-probe-64kzf, resource: bindings, ignored listing per whitelist
May 29 05:49:24.391: INFO: namespace e2e-tests-container-probe-64kzf deletion completed in 6.081602558s

â€¢ [SLOW TEST:158.517 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:49:24.391: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-8370be16-81d5-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 05:49:24.480: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-k4zz4" to be "success or failure"
May 29 05:49:24.483: INFO: Pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882255ms
May 29 05:49:26.486: INFO: Pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005884561s
May 29 05:49:28.489: INFO: Pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008987589s
May 29 05:49:30.493: INFO: Pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012310037s
STEP: Saw pod success
May 29 05:49:30.493: INFO: Pod "pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:49:30.495: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 05:49:30.518: INFO: Waiting for pod pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:49:30.520: INFO: Pod pod-projected-configmaps-83712b27-81d5-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:49:30.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k4zz4" for this suite.
May 29 05:49:36.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:49:36.601: INFO: namespace: e2e-tests-projected-k4zz4, resource: bindings, ignored listing per whitelist
May 29 05:49:36.608: INFO: namespace e2e-tests-projected-k4zz4 deletion completed in 6.084225349s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:49:36.608: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-mmbgv
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 05:49:36.682: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 05:50:04.761: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.240.0.75 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mmbgv PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:50:04.761: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:50:05.889: INFO: Found all expected endpoints: [netserver-0]
May 29 05:50:05.892: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.240.0.10 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mmbgv PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:50:05.892: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:50:07.029: INFO: Found all expected endpoints: [netserver-1]
May 29 05:50:07.032: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.240.0.51 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mmbgv PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:50:07.032: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:50:08.165: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:50:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-mmbgv" for this suite.
May 29 05:50:30.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:50:30.196: INFO: namespace: e2e-tests-pod-network-test-mmbgv, resource: bindings, ignored listing per whitelist
May 29 05:50:30.258: INFO: namespace e2e-tests-pod-network-test-mmbgv deletion completed in 22.089172472s

â€¢ [SLOW TEST:53.650 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:50:30.258: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
May 29 05:50:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:32.344: INFO: stderr: ""
May 29 05:50:32.344: INFO: stdout: "pod/pause created\n"
May 29 05:50:32.344: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 29 05:50:32.344: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-6h5rp" to be "running and ready"
May 29 05:50:32.348: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219837ms
May 29 05:50:34.351: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007177786s
May 29 05:50:36.354: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009640763s
May 29 05:50:38.356: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.012051761s
May 29 05:50:38.356: INFO: Pod "pause" satisfied condition "running and ready"
May 29 05:50:38.356: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
May 29 05:50:38.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.432: INFO: stderr: ""
May 29 05:50:38.432: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 29 05:50:38.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pod pause -L testing-label --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.503: INFO: stderr: ""
May 29 05:50:38.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 29 05:50:38.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 label pods pause testing-label- --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.580: INFO: stderr: ""
May 29 05:50:38.580: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 29 05:50:38.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pod pause -L testing-label --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.660: INFO: stderr: ""
May 29 05:50:38.660: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
May 29 05:50:38.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.748: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 05:50:38.749: INFO: stdout: "pod \"pause\" force deleted\n"
May 29 05:50:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-6h5rp'
May 29 05:50:38.846: INFO: stderr: "No resources found.\n"
May 29 05:50:38.846: INFO: stdout: ""
May 29 05:50:38.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -l name=pause --namespace=e2e-tests-kubectl-6h5rp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 05:50:38.941: INFO: stderr: ""
May 29 05:50:38.941: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:50:38.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6h5rp" for this suite.
May 29 05:50:44.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:50:45.024: INFO: namespace: e2e-tests-kubectl-6h5rp, resource: bindings, ignored listing per whitelist
May 29 05:50:45.026: INFO: namespace e2e-tests-kubectl-6h5rp deletion completed in 6.081546026s

â€¢ [SLOW TEST:14.768 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:50:45.026: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 29 05:50:45.116: INFO: Waiting up to 5m0s for pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-npqkd" to be "success or failure"
May 29 05:50:45.120: INFO: Pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08144ms
May 29 05:50:47.124: INFO: Pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008310799s
May 29 05:50:49.128: INFO: Pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01238798s
May 29 05:50:51.132: INFO: Pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016127587s
STEP: Saw pod success
May 29 05:50:51.132: INFO: Pod "pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:50:51.134: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:50:51.158: INFO: Waiting for pod pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:50:51.161: INFO: Pod pod-b3810233-81d5-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:50:51.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-npqkd" for this suite.
May 29 05:50:57.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:50:57.194: INFO: namespace: e2e-tests-emptydir-npqkd, resource: bindings, ignored listing per whitelist
May 29 05:50:57.272: INFO: namespace e2e-tests-emptydir-npqkd deletion completed in 6.107969804s

â€¢ [SLOW TEST:12.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:50:57.272: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 29 05:50:57.370: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16266,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 05:50:57.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16267,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 29 05:50:57.371: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16268,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 29 05:51:07.399: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16287,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 05:51:07.399: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16288,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 29 05:51:07.399: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-lvs2b,SelfLink:/api/v1/namespaces/e2e-tests-watch-lvs2b/configmaps/e2e-watch-test-label-changed,UID:baccc481-81d5-11e9-85ba-000d3a6e4ecc,ResourceVersion:16289,Generation:0,CreationTimestamp:2019-05-29 05:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:51:07.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lvs2b" for this suite.
May 29 05:51:13.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:51:13.485: INFO: namespace: e2e-tests-watch-lvs2b, resource: bindings, ignored listing per whitelist
May 29 05:51:13.488: INFO: namespace e2e-tests-watch-lvs2b deletion completed in 6.084536428s

â€¢ [SLOW TEST:16.216 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:51:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 29 05:51:13.569: INFO: Waiting up to 5m0s for pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-8s89z" to be "success or failure"
May 29 05:51:13.580: INFO: Pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.772641ms
May 29 05:51:15.583: INFO: Pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014210188s
May 29 05:51:17.586: INFO: Pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01729016s
May 29 05:51:19.589: INFO: Pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020142354s
STEP: Saw pod success
May 29 05:51:19.589: INFO: Pod "downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:51:19.591: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 05:51:19.614: INFO: Waiting for pod downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:51:19.622: INFO: Pod downward-api-c4769df8-81d5-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:51:19.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8s89z" for this suite.
May 29 05:51:25.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:51:25.693: INFO: namespace: e2e-tests-downward-api-8s89z, resource: bindings, ignored listing per whitelist
May 29 05:51:25.704: INFO: namespace e2e-tests-downward-api-8s89z deletion completed in 6.07827627s

â€¢ [SLOW TEST:12.216 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:51:25.704: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-5lx9f
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-5lx9f
STEP: Deleting pre-stop pod
May 29 05:51:46.813: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:51:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-5lx9f" for this suite.
May 29 05:52:32.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:52:32.885: INFO: namespace: e2e-tests-prestop-5lx9f, resource: bindings, ignored listing per whitelist
May 29 05:52:32.902: INFO: namespace e2e-tests-prestop-5lx9f deletion completed in 46.07739456s

â€¢ [SLOW TEST:67.197 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:52:32.902: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 29 05:52:45.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 05:52:45.018: INFO: Pod pod-with-prestop-http-hook still exists
May 29 05:52:47.018: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 05:52:47.021: INFO: Pod pod-with-prestop-http-hook still exists
May 29 05:52:49.018: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 05:52:49.020: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:52:49.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-2zgl6" for this suite.
May 29 05:53:11.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:53:11.107: INFO: namespace: e2e-tests-container-lifecycle-hook-2zgl6, resource: bindings, ignored listing per whitelist
May 29 05:53:11.113: INFO: namespace e2e-tests-container-lifecycle-hook-2zgl6 deletion completed in 22.082645877s

â€¢ [SLOW TEST:38.211 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:53:11.113: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 05:53:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:11.277: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 05:53:11.277: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 29 05:53:11.287: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 29 05:53:11.295: INFO: scanned /root for discovery docs: <nil>
May 29 05:53:11.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:28.452: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 29 05:53:28.452: INFO: stdout: "Created e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d\nScaling up e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 29 05:53:28.452: INFO: stdout: "Created e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d\nScaling up e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 29 05:53:28.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:28.536: INFO: stderr: ""
May 29 05:53:28.536: INFO: stdout: "e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d-8sv6g "
May 29 05:53:28.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d-8sv6g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:28.610: INFO: stderr: ""
May 29 05:53:28.610: INFO: stdout: "true"
May 29 05:53:28.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d-8sv6g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:28.687: INFO: stderr: ""
May 29 05:53:28.687: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 29 05:53:28.687: INFO: e2e-test-nginx-rc-75dbb189b903f165617cf40fa1976e5d-8sv6g is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
May 29 05:53:28.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n5hk5'
May 29 05:53:28.773: INFO: stderr: ""
May 29 05:53:28.773: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:53:28.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n5hk5" for this suite.
May 29 05:53:34.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:53:34.801: INFO: namespace: e2e-tests-kubectl-n5hk5, resource: bindings, ignored listing per whitelist
May 29 05:53:34.870: INFO: namespace e2e-tests-kubectl-n5hk5 deletion completed in 6.091085106s

â€¢ [SLOW TEST:23.757 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:53:34.871: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-klp6h
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-klp6h to expose endpoints map[]
May 29 05:53:34.965: INFO: Get endpoints failed (3.355853ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 29 05:53:35.969: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-klp6h exposes endpoints map[] (1.007390025s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-klp6h
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-klp6h to expose endpoints map[pod1:[100]]
May 29 05:53:39.999: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.025321195s elapsed, will retry)
May 29 05:53:41.004: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-klp6h exposes endpoints map[pod1:[100]] (5.030623768s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-klp6h
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-klp6h to expose endpoints map[pod1:[100] pod2:[101]]
May 29 05:53:45.042: INFO: Unexpected endpoints: found map[19583cea-81d6-11e9-85ba-000d3a6e4ecc:[100]], expected map[pod1:[100] pod2:[101]] (4.03498593s elapsed, will retry)
May 29 05:53:46.049: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-klp6h exposes endpoints map[pod1:[100] pod2:[101]] (5.0417253s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-klp6h
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-klp6h to expose endpoints map[pod2:[101]]
May 29 05:53:46.069: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-klp6h exposes endpoints map[pod2:[101]] (14.544494ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-klp6h
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-klp6h to expose endpoints map[]
May 29 05:53:46.079: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-klp6h exposes endpoints map[] (4.866331ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:53:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-klp6h" for this suite.
May 29 05:54:08.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:54:08.135: INFO: namespace: e2e-tests-services-klp6h, resource: bindings, ignored listing per whitelist
May 29 05:54:08.200: INFO: namespace e2e-tests-services-klp6h deletion completed in 22.091654025s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:33.329 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:54:08.200: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4kz77
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 05:54:08.273: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 05:54:36.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.91:8080/dial?request=hostName&protocol=udp&host=10.240.0.19&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-4kz77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:54:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:54:36.483: INFO: Waiting for endpoints: map[]
May 29 05:54:36.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.91:8080/dial?request=hostName&protocol=udp&host=10.240.0.60&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-4kz77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:54:36.485: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:54:36.609: INFO: Waiting for endpoints: map[]
May 29 05:54:36.611: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.91:8080/dial?request=hostName&protocol=udp&host=10.240.0.73&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-4kz77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 05:54:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
May 29 05:54:36.725: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:54:36.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4kz77" for this suite.
May 29 05:54:58.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:54:58.781: INFO: namespace: e2e-tests-pod-network-test-4kz77, resource: bindings, ignored listing per whitelist
May 29 05:54:58.821: INFO: namespace e2e-tests-pod-network-test-4kz77 deletion completed in 22.093385853s

â€¢ [SLOW TEST:50.621 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:54:58.822: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4ac605ce-81d6-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 05:54:58.906: INFO: Waiting up to 5m0s for pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-snzlg" to be "success or failure"
May 29 05:54:58.918: INFO: Pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.955134ms
May 29 05:55:00.921: INFO: Pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015434802s
May 29 05:55:02.925: INFO: Pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018634186s
May 29 05:55:04.928: INFO: Pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021683384s
STEP: Saw pod success
May 29 05:55:04.928: INFO: Pod "pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:55:04.936: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 05:55:04.953: INFO: Waiting for pod pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:55:04.955: INFO: Pod pod-secrets-4ac66d82-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:55:04.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-snzlg" for this suite.
May 29 05:55:10.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:55:11.020: INFO: namespace: e2e-tests-secrets-snzlg, resource: bindings, ignored listing per whitelist
May 29 05:55:11.049: INFO: namespace e2e-tests-secrets-snzlg deletion completed in 6.09041724s

â€¢ [SLOW TEST:12.227 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:55:11.049: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-520fada7-81d6-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 05:55:11.146: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-z4f9h" to be "success or failure"
May 29 05:55:11.149: INFO: Pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280054ms
May 29 05:55:13.152: INFO: Pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194703s
May 29 05:55:15.155: INFO: Pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008880167s
May 29 05:55:17.158: INFO: Pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011950738s
STEP: Saw pod success
May 29 05:55:17.158: INFO: Pod "pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:55:17.160: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 05:55:17.179: INFO: Waiting for pod pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:55:17.184: INFO: Pod pod-projected-secrets-521027d0-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:55:17.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z4f9h" for this suite.
May 29 05:55:23.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:55:23.228: INFO: namespace: e2e-tests-projected-z4f9h, resource: bindings, ignored listing per whitelist
May 29 05:55:23.274: INFO: namespace e2e-tests-projected-z4f9h deletion completed in 6.085800524s

â€¢ [SLOW TEST:12.225 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:55:23.274: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 05:55:23.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-tf6pr'
May 29 05:55:23.437: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 05:55:23.437: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
May 29 05:55:25.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-tf6pr'
May 29 05:55:25.528: INFO: stderr: ""
May 29 05:55:25.528: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:55:25.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tf6pr" for this suite.
May 29 05:55:31.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:55:31.569: INFO: namespace: e2e-tests-kubectl-tf6pr, resource: bindings, ignored listing per whitelist
May 29 05:55:31.611: INFO: namespace e2e-tests-kubectl-tf6pr deletion completed in 6.078238175s

â€¢ [SLOW TEST:8.337 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:55:31.611: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:55:31.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-fwj2z" to be "success or failure"
May 29 05:55:31.687: INFO: Pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434766ms
May 29 05:55:33.690: INFO: Pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00569493s
May 29 05:55:35.693: INFO: Pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008473311s
May 29 05:55:37.696: INFO: Pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0115593s
STEP: Saw pod success
May 29 05:55:37.696: INFO: Pod "downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:55:37.698: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:55:37.713: INFO: Waiting for pod downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:55:37.715: INFO: Pod downwardapi-volume-5e500fec-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:55:37.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fwj2z" for this suite.
May 29 05:55:43.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:55:43.772: INFO: namespace: e2e-tests-downward-api-fwj2z, resource: bindings, ignored listing per whitelist
May 29 05:55:43.815: INFO: namespace e2e-tests-downward-api-fwj2z deletion completed in 6.08300122s

â€¢ [SLOW TEST:12.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:55:43.815: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-4ppr
STEP: Creating a pod to test atomic-volume-subpath
May 29 05:55:43.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4ppr" in namespace "e2e-tests-subpath-lqt27" to be "success or failure"
May 29 05:55:43.906: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.075262ms
May 29 05:55:45.909: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012948999s
May 29 05:55:47.911: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01565665s
May 29 05:55:49.914: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018669208s
May 29 05:55:51.918: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 8.022023273s
May 29 05:55:53.921: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 10.025207751s
May 29 05:55:55.925: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 12.029500824s
May 29 05:55:57.929: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 14.032870021s
May 29 05:55:59.932: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 16.035989132s
May 29 05:56:01.935: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 18.039121554s
May 29 05:56:03.938: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 20.042404585s
May 29 05:56:05.941: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 22.045636227s
May 29 05:56:07.945: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 24.049231175s
May 29 05:56:09.948: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Running", Reason="", readiness=false. Elapsed: 26.052121543s
May 29 05:56:11.951: INFO: Pod "pod-subpath-test-projected-4ppr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.055449516s
STEP: Saw pod success
May 29 05:56:11.951: INFO: Pod "pod-subpath-test-projected-4ppr" satisfied condition "success or failure"
May 29 05:56:11.953: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-subpath-test-projected-4ppr container test-container-subpath-projected-4ppr: <nil>
STEP: delete the pod
May 29 05:56:11.976: INFO: Waiting for pod pod-subpath-test-projected-4ppr to disappear
May 29 05:56:11.979: INFO: Pod pod-subpath-test-projected-4ppr no longer exists
STEP: Deleting pod pod-subpath-test-projected-4ppr
May 29 05:56:11.979: INFO: Deleting pod "pod-subpath-test-projected-4ppr" in namespace "e2e-tests-subpath-lqt27"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:56:11.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-lqt27" for this suite.
May 29 05:56:17.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:56:18.015: INFO: namespace: e2e-tests-subpath-lqt27, resource: bindings, ignored listing per whitelist
May 29 05:56:18.069: INFO: namespace e2e-tests-subpath-lqt27 deletion completed in 6.085864545s

â€¢ [SLOW TEST:34.255 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:56:18.070: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 29 05:56:18.149: INFO: Waiting up to 5m0s for pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-j42nj" to be "success or failure"
May 29 05:56:18.152: INFO: Pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988959ms
May 29 05:56:20.155: INFO: Pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006244476s
May 29 05:56:22.158: INFO: Pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009344005s
May 29 05:56:24.161: INFO: Pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012439645s
STEP: Saw pod success
May 29 05:56:24.161: INFO: Pod "pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:56:24.163: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:56:24.202: INFO: Waiting for pod pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:56:24.204: INFO: Pod pod-7a020ae4-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:56:24.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j42nj" for this suite.
May 29 05:56:30.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:56:30.245: INFO: namespace: e2e-tests-emptydir-j42nj, resource: bindings, ignored listing per whitelist
May 29 05:56:30.290: INFO: namespace e2e-tests-emptydir-j42nj deletion completed in 6.082197487s

â€¢ [SLOW TEST:12.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:56:30.290: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:56:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-smqq5" for this suite.
May 29 05:56:59.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:56:59.468: INFO: namespace: e2e-tests-replication-controller-smqq5, resource: bindings, ignored listing per whitelist
May 29 05:56:59.477: INFO: namespace e2e-tests-replication-controller-smqq5 deletion completed in 22.081167896s

â€¢ [SLOW TEST:29.187 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:56:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:57:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-pvdwk" for this suite.
May 29 05:57:55.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:57:55.632: INFO: namespace: e2e-tests-kubelet-test-pvdwk, resource: bindings, ignored listing per whitelist
May 29 05:57:55.676: INFO: namespace e2e-tests-kubelet-test-pvdwk deletion completed in 50.082945658s

â€¢ [SLOW TEST:56.199 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:57:55.676: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 29 05:57:55.760: INFO: Waiting up to 5m0s for pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-9plgs" to be "success or failure"
May 29 05:57:55.764: INFO: Pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926647ms
May 29 05:57:57.767: INFO: Pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007066228s
May 29 05:57:59.770: INFO: Pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009871922s
May 29 05:58:01.775: INFO: Pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015317289s
STEP: Saw pod success
May 29 05:58:01.776: INFO: Pod "pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:58:01.779: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:58:01.818: INFO: Waiting for pod pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:58:01.820: INFO: Pod pod-b43054b6-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:58:01.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9plgs" for this suite.
May 29 05:58:07.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:58:07.881: INFO: namespace: e2e-tests-emptydir-9plgs, resource: bindings, ignored listing per whitelist
May 29 05:58:07.909: INFO: namespace e2e-tests-emptydir-9plgs deletion completed in 6.085999919s

â€¢ [SLOW TEST:12.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:58:07.910: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:58:07.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-6l9x9" to be "success or failure"
May 29 05:58:07.998: INFO: Pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.725163ms
May 29 05:58:10.001: INFO: Pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005248504s
May 29 05:58:12.004: INFO: Pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008320747s
May 29 05:58:14.008: INFO: Pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011814292s
STEP: Saw pod success
May 29 05:58:14.008: INFO: Pod "downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:58:14.010: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:58:14.036: INFO: Waiting for pod downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:58:14.038: INFO: Pod downwardapi-volume-bb7b4f11-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:58:14.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6l9x9" for this suite.
May 29 05:58:20.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:58:20.091: INFO: namespace: e2e-tests-projected-6l9x9, resource: bindings, ignored listing per whitelist
May 29 05:58:20.130: INFO: namespace e2e-tests-projected-6l9x9 deletion completed in 6.088661138s

â€¢ [SLOW TEST:12.220 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:58:20.130: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:58:20.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 version'
May 29 05:58:20.345: INFO: stderr: ""
May 29 05:58:20.345: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.6\", GitCommit:\"abdda3f9fefa29172298a2e42f5102e777a8ec25\", GitTreeState:\"clean\", BuildDate:\"2019-05-08T13:46:28Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:58:20.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gd5cp" for this suite.
May 29 05:58:26.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:58:26.459: INFO: namespace: e2e-tests-kubectl-gd5cp, resource: bindings, ignored listing per whitelist
May 29 05:58:26.459: INFO: namespace e2e-tests-kubectl-gd5cp deletion completed in 6.110216329s

â€¢ [SLOW TEST:6.329 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:58:26.459: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 05:58:26.565: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c68b2e4e-81d6-11e9-85ba-000d3a6e4ecc", Controller:(*bool)(0xc00083f186), BlockOwnerDeletion:(*bool)(0xc00083f187)}}
May 29 05:58:26.571: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c6892774-81d6-11e9-85ba-000d3a6e4ecc", Controller:(*bool)(0xc002221226), BlockOwnerDeletion:(*bool)(0xc002221227)}}
May 29 05:58:26.588: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c6899ae1-81d6-11e9-85ba-000d3a6e4ecc", Controller:(*bool)(0xc00083f546), BlockOwnerDeletion:(*bool)(0xc00083f547)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:58:31.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-bm4gg" for this suite.
May 29 05:58:37.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:58:37.617: INFO: namespace: e2e-tests-gc-bm4gg, resource: bindings, ignored listing per whitelist
May 29 05:58:37.678: INFO: namespace e2e-tests-gc-bm4gg deletion completed in 6.081106456s

â€¢ [SLOW TEST:11.219 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:58:37.678: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
May 29 05:58:43.791: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-cd39f270-81d6-11e9-b4f9-d20c9d8615e3", GenerateName:"", Namespace:"e2e-tests-pods-wqfpd", SelfLink:"/api/v1/namespaces/e2e-tests-pods-wqfpd/pods/pod-submit-remove-cd39f270-81d6-11e9-b4f9-d20c9d8615e3", UID:"cd3a9339-81d6-11e9-85ba-000d3a6e4ecc", ResourceVersion:"17657", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694706317, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"762327869"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vpzx9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001656280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vpzx9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000f886b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-pool1-29361026-vmss000002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000e28000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000f886f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000f88710)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000f88718), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000f8871c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694706317, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694706322, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694706322, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694706317, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.66", PodIP:"10.240.0.74", StartTime:(*v1.Time)(0xc00148c980), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00148c9a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7", ContainerID:"docker://be67cee6826fbf90e3e15856aa6e6bee0aeeb59acce012896e87fa64a66d1c1a"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:58:52.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wqfpd" for this suite.
May 29 05:58:58.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:58:58.217: INFO: namespace: e2e-tests-pods-wqfpd, resource: bindings, ignored listing per whitelist
May 29 05:58:58.287: INFO: namespace e2e-tests-pods-wqfpd deletion completed in 6.093545235s

â€¢ [SLOW TEST:20.609 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:58:58.287: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 05:58:58.364: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 05:58:58.374: INFO: Waiting for terminating namespaces to be deleted...
May 29 05:58:58.376: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000000 before test
May 29 05:58:58.381: INFO: azure-cni-networkmonitor-nb4kr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:58:58.381: INFO: keyvault-flexvolume-5hhhq from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:58:58.381: INFO: azure-ip-masq-agent-28z5v from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:58:58.381: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-99ztx from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 05:58:58.381: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 05:58:58.381: INFO: blobfuse-flexvol-installer-rr6q5 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:58:58.381: INFO: kube-proxy-hfc5q from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 05:58:58.381: INFO: sonobuoy-e2e-job-89123943a0f146da from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:58:58.381: INFO: 	Container e2e ready: true, restart count 0
May 29 05:58:58.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 05:58:58.381: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000001 before test
May 29 05:58:58.386: INFO: azure-ip-masq-agent-8pcj4 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:58:58.386: INFO: azure-cni-networkmonitor-bg749 from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:58:58.386: INFO: blobfuse-flexvol-installer-stf4c from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:58:58.386: INFO: keyvault-flexvolume-vn2lw from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:58:58.386: INFO: kube-proxy-dtmsl from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 05:58:58.386: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 04:35:39 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 05:58:58.386: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-6w5ft from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 05:58:58.386: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 05:58:58.386: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 05:58:58.386: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000002 before test
May 29 05:58:58.391: INFO: blobfuse-flexvol-installer-lp79k from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 05:58:58.391: INFO: azure-ip-masq-agent-cpsxm from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 05:58:58.391: INFO: kube-proxy-fbq9l from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 05:58:58.391: INFO: keyvault-flexvolume-w2dqp from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 05:58:58.391: INFO: metrics-server-69b44566d5-5fhmr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container metrics-server ready: true, restart count 1
May 29 05:58:58.391: INFO: azure-cni-networkmonitor-qrlgm from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 05:58:58.391: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-7btfr from heptio-sonobuoy started at 2019-05-29 04:35:52 +0000 UTC (2 container statuses recorded)
May 29 05:58:58.391: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 05:58:58.391: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dd1d6eb6-81d6-11e9-b4f9-d20c9d8615e3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-dd1d6eb6-81d6-11e9-b4f9-d20c9d8615e3 off the node k8s-pool1-29361026-vmss000001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dd1d6eb6-81d6-11e9-b4f9-d20c9d8615e3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:59:12.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-xbrs7" for this suite.
May 29 05:59:22.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:59:22.508: INFO: namespace: e2e-tests-sched-pred-xbrs7, resource: bindings, ignored listing per whitelist
May 29 05:59:22.545: INFO: namespace e2e-tests-sched-pred-xbrs7 deletion completed in 10.08505949s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:24.258 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:59:22.546: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 05:59:22.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-spzp7" to be "success or failure"
May 29 05:59:22.634: INFO: Pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.04868ms
May 29 05:59:24.637: INFO: Pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124998s
May 29 05:59:26.641: INFO: Pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016025227s
May 29 05:59:28.644: INFO: Pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019088967s
STEP: Saw pod success
May 29 05:59:28.644: INFO: Pod "downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:59:28.646: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 05:59:28.661: INFO: Waiting for pod downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:59:28.663: INFO: Pod downwardapi-volume-e7f6e25b-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:59:28.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-spzp7" for this suite.
May 29 05:59:34.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:59:34.737: INFO: namespace: e2e-tests-projected-spzp7, resource: bindings, ignored listing per whitelist
May 29 05:59:34.762: INFO: namespace e2e-tests-projected-spzp7 deletion completed in 6.095953114s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:59:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
May 29 05:59:34.853: INFO: Waiting up to 5m0s for pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-b86np" to be "success or failure"
May 29 05:59:34.857: INFO: Pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456455ms
May 29 05:59:36.860: INFO: Pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006906019s
May 29 05:59:38.864: INFO: Pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010719585s
May 29 05:59:40.867: INFO: Pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013641571s
STEP: Saw pod success
May 29 05:59:40.867: INFO: Pod "pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 05:59:40.869: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 05:59:40.888: INFO: Waiting for pod pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3 to disappear
May 29 05:59:40.894: INFO: Pod pod-ef3f035a-81d6-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 05:59:40.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b86np" for this suite.
May 29 05:59:46.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 05:59:46.931: INFO: namespace: e2e-tests-emptydir-b86np, resource: bindings, ignored listing per whitelist
May 29 05:59:47.003: INFO: namespace e2e-tests-emptydir-b86np deletion completed in 6.105349721s

â€¢ [SLOW TEST:12.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 05:59:47.003: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 29 05:59:47.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17869,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 05:59:47.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17869,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 29 05:59:57.090: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17885,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 29 05:59:57.090: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17885,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 29 06:00:07.096: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17903,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 06:00:07.096: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17903,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 29 06:00:17.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17919,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 06:00:17.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-a,UID:f68b44a9-81d6-11e9-85ba-000d3a6e4ecc,ResourceVersion:17919,Generation:0,CreationTimestamp:2019-05-29 05:59:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 29 06:00:27.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-b,UID:0e65a51d-81d7-11e9-85ba-000d3a6e4ecc,ResourceVersion:17935,Generation:0,CreationTimestamp:2019-05-29 06:00:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 06:00:27.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-b,UID:0e65a51d-81d7-11e9-85ba-000d3a6e4ecc,ResourceVersion:17935,Generation:0,CreationTimestamp:2019-05-29 06:00:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 29 06:00:37.108: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-b,UID:0e65a51d-81d7-11e9-85ba-000d3a6e4ecc,ResourceVersion:17951,Generation:0,CreationTimestamp:2019-05-29 06:00:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 06:00:37.108: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d8ltf,SelfLink:/api/v1/namespaces/e2e-tests-watch-d8ltf/configmaps/e2e-watch-test-configmap-b,UID:0e65a51d-81d7-11e9-85ba-000d3a6e4ecc,ResourceVersion:17951,Generation:0,CreationTimestamp:2019-05-29 06:00:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:00:47.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-d8ltf" for this suite.
May 29 06:00:53.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:00:53.181: INFO: namespace: e2e-tests-watch-d8ltf, resource: bindings, ignored listing per whitelist
May 29 06:00:53.194: INFO: namespace e2e-tests-watch-d8ltf deletion completed in 6.082262484s

â€¢ [SLOW TEST:66.192 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:00:53.195: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:01:53.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-xt69c" for this suite.
May 29 06:02:15.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:02:15.356: INFO: namespace: e2e-tests-container-probe-xt69c, resource: bindings, ignored listing per whitelist
May 29 06:02:15.391: INFO: namespace e2e-tests-container-probe-xt69c deletion completed in 22.097232458s

â€¢ [SLOW TEST:82.196 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:02:15.391: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-sqh7
STEP: Creating a pod to test atomic-volume-subpath
May 29 06:02:15.481: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sqh7" in namespace "e2e-tests-subpath-k9v8j" to be "success or failure"
May 29 06:02:15.490: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516489ms
May 29 06:02:17.493: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011637346s
May 29 06:02:19.496: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014942106s
May 29 06:02:21.500: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01829017s
May 29 06:02:23.503: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 8.021348144s
May 29 06:02:25.507: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 10.025158412s
May 29 06:02:27.509: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 12.028023598s
May 29 06:02:29.513: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 14.031350184s
May 29 06:02:31.516: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 16.034545475s
May 29 06:02:33.519: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 18.03791927s
May 29 06:02:35.523: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 20.041182471s
May 29 06:02:37.526: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 22.044328879s
May 29 06:02:39.529: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 24.047422192s
May 29 06:02:41.532: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Running", Reason="", readiness=false. Elapsed: 26.050821006s
May 29 06:02:43.536: INFO: Pod "pod-subpath-test-downwardapi-sqh7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.054497122s
STEP: Saw pod success
May 29 06:02:43.536: INFO: Pod "pod-subpath-test-downwardapi-sqh7" satisfied condition "success or failure"
May 29 06:02:43.538: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-subpath-test-downwardapi-sqh7 container test-container-subpath-downwardapi-sqh7: <nil>
STEP: delete the pod
May 29 06:02:43.570: INFO: Waiting for pod pod-subpath-test-downwardapi-sqh7 to disappear
May 29 06:02:43.576: INFO: Pod pod-subpath-test-downwardapi-sqh7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-sqh7
May 29 06:02:43.576: INFO: Deleting pod "pod-subpath-test-downwardapi-sqh7" in namespace "e2e-tests-subpath-k9v8j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:02:43.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-k9v8j" for this suite.
May 29 06:02:49.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:02:49.639: INFO: namespace: e2e-tests-subpath-k9v8j, resource: bindings, ignored listing per whitelist
May 29 06:02:49.693: INFO: namespace e2e-tests-subpath-k9v8j deletion completed in 6.11178407s

â€¢ [SLOW TEST:34.302 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:02:49.693: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 29 06:02:49.793: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:49.797: INFO: Number of nodes with available pods: 0
May 29 06:02:49.798: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 06:02:50.810: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:50.813: INFO: Number of nodes with available pods: 0
May 29 06:02:50.813: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 06:02:51.801: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:51.804: INFO: Number of nodes with available pods: 0
May 29 06:02:51.804: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 06:02:52.801: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:52.804: INFO: Number of nodes with available pods: 0
May 29 06:02:52.804: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 06:02:53.801: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:53.803: INFO: Number of nodes with available pods: 0
May 29 06:02:53.803: INFO: Node k8s-pool1-29361026-vmss000000 is running more than one daemon pod
May 29 06:02:54.802: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:54.804: INFO: Number of nodes with available pods: 3
May 29 06:02:54.804: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 29 06:02:54.820: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:54.828: INFO: Number of nodes with available pods: 2
May 29 06:02:54.828: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 06:02:55.833: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:55.836: INFO: Number of nodes with available pods: 2
May 29 06:02:55.836: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 06:02:56.832: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:56.834: INFO: Number of nodes with available pods: 2
May 29 06:02:56.835: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 06:02:57.833: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:57.835: INFO: Number of nodes with available pods: 2
May 29 06:02:57.835: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 06:02:58.832: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:58.834: INFO: Number of nodes with available pods: 2
May 29 06:02:58.834: INFO: Node k8s-pool1-29361026-vmss000002 is running more than one daemon pod
May 29 06:02:59.832: INFO: DaemonSet pods can't tolerate node k8s-master-29361026-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 06:02:59.834: INFO: Number of nodes with available pods: 3
May 29 06:02:59.834: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-pfsz9, will wait for the garbage collector to delete the pods
May 29 06:02:59.910: INFO: Deleting DaemonSet.extensions daemon-set took: 20.367837ms
May 29 06:03:00.010: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.152603ms
May 29 06:03:42.213: INFO: Number of nodes with available pods: 0
May 29 06:03:42.213: INFO: Number of running nodes: 0, number of available pods: 0
May 29 06:03:42.215: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-pfsz9/daemonsets","resourceVersion":"18390"},"items":null}

May 29 06:03:42.217: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-pfsz9/pods","resourceVersion":"18390"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:03:42.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-pfsz9" for this suite.
May 29 06:03:48.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:03:48.260: INFO: namespace: e2e-tests-daemonsets-pfsz9, resource: bindings, ignored listing per whitelist
May 29 06:03:48.325: INFO: namespace e2e-tests-daemonsets-pfsz9 deletion completed in 6.095479993s

â€¢ [SLOW TEST:58.632 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:03:48.326: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:03:48.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-2qvvc" to be "success or failure"
May 29 06:03:48.417: INFO: Pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.257129ms
May 29 06:03:50.420: INFO: Pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016633103s
May 29 06:03:52.423: INFO: Pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01926299s
May 29 06:03:54.426: INFO: Pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021969681s
STEP: Saw pod success
May 29 06:03:54.426: INFO: Pod "downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:03:54.428: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:03:54.444: INFO: Waiting for pod downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:03:54.460: INFO: Pod downwardapi-volume-86616b38-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:03:54.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2qvvc" for this suite.
May 29 06:04:00.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:04:00.545: INFO: namespace: e2e-tests-downward-api-2qvvc, resource: bindings, ignored listing per whitelist
May 29 06:04:00.556: INFO: namespace e2e-tests-downward-api-2qvvc deletion completed in 6.093754696s

â€¢ [SLOW TEST:12.231 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:04:00.557: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 29 06:04:00.642: INFO: Waiting up to 5m0s for pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-94sk8" to be "success or failure"
May 29 06:04:00.651: INFO: Pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.837786ms
May 29 06:04:02.654: INFO: Pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011607394s
May 29 06:04:04.657: INFO: Pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015180196s
May 29 06:04:06.660: INFO: Pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018187909s
STEP: Saw pod success
May 29 06:04:06.660: INFO: Pod "downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:04:06.662: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 06:04:06.678: INFO: Waiting for pod downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:04:06.680: INFO: Pod downward-api-8dac999b-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:04:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-94sk8" for this suite.
May 29 06:04:12.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:04:12.756: INFO: namespace: e2e-tests-downward-api-94sk8, resource: bindings, ignored listing per whitelist
May 29 06:04:12.770: INFO: namespace e2e-tests-downward-api-94sk8 deletion completed in 6.087443555s

â€¢ [SLOW TEST:12.214 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:04:12.771: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-94f2f21b-81d7-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:04:12.853: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-j25nq" to be "success or failure"
May 29 06:04:12.857: INFO: Pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332657ms
May 29 06:04:14.860: INFO: Pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006973179s
May 29 06:04:16.863: INFO: Pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010007412s
May 29 06:04:18.867: INFO: Pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013255947s
STEP: Saw pod success
May 29 06:04:18.867: INFO: Pod "pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:04:18.869: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:04:18.889: INFO: Waiting for pod pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:04:18.892: INFO: Pod pod-projected-configmaps-94f359c3-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:04:18.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j25nq" for this suite.
May 29 06:04:24.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:04:24.916: INFO: namespace: e2e-tests-projected-j25nq, resource: bindings, ignored listing per whitelist
May 29 06:04:24.981: INFO: namespace e2e-tests-projected-j25nq deletion completed in 6.086340945s

â€¢ [SLOW TEST:12.211 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:04:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
May 29 06:04:25.068: INFO: Waiting up to 5m0s for pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-containers-jcfgs" to be "success or failure"
May 29 06:04:25.071: INFO: Pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.734365ms
May 29 06:04:27.074: INFO: Pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006208614s
May 29 06:04:29.077: INFO: Pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008737679s
May 29 06:04:31.080: INFO: Pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01188614s
STEP: Saw pod success
May 29 06:04:31.080: INFO: Pod "client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:04:31.082: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 06:04:31.095: INFO: Waiting for pod client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:04:31.097: INFO: Pod client-containers-9c3c0ca3-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:04:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jcfgs" for this suite.
May 29 06:04:37.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:04:37.153: INFO: namespace: e2e-tests-containers-jcfgs, resource: bindings, ignored listing per whitelist
May 29 06:04:37.193: INFO: namespace e2e-tests-containers-jcfgs deletion completed in 6.092224544s

â€¢ [SLOW TEST:12.211 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:04:37.193: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:04:37.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-p45k4" to be "success or failure"
May 29 06:04:37.273: INFO: Pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432669ms
May 29 06:04:39.276: INFO: Pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005879142s
May 29 06:04:41.279: INFO: Pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008620829s
May 29 06:04:43.282: INFO: Pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011811113s
STEP: Saw pod success
May 29 06:04:43.282: INFO: Pod "downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:04:43.284: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:04:43.306: INFO: Waiting for pod downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:04:43.309: INFO: Pod downwardapi-volume-a381e441-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:04:43.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p45k4" for this suite.
May 29 06:04:49.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:04:49.381: INFO: namespace: e2e-tests-projected-p45k4, resource: bindings, ignored listing per whitelist
May 29 06:04:49.399: INFO: namespace e2e-tests-projected-p45k4 deletion completed in 6.087149682s

â€¢ [SLOW TEST:12.206 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:04:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-aac99233-81d7-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:04:49.487: INFO: Waiting up to 5m0s for pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-98wff" to be "success or failure"
May 29 06:04:49.497: INFO: Pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.528078ms
May 29 06:04:51.500: INFO: Pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012303384s
May 29 06:04:53.502: INFO: Pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015090293s
May 29 06:04:55.505: INFO: Pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017882606s
STEP: Saw pod success
May 29 06:04:55.505: INFO: Pod "pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:04:55.507: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 06:04:55.521: INFO: Waiting for pod pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:04:55.523: INFO: Pod pod-secrets-aac9f101-81d7-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:04:55.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-98wff" for this suite.
May 29 06:05:01.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:05:01.584: INFO: namespace: e2e-tests-secrets-98wff, resource: bindings, ignored listing per whitelist
May 29 06:05:01.611: INFO: namespace e2e-tests-secrets-98wff deletion completed in 6.083667797s

â€¢ [SLOW TEST:12.211 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:05:01.611: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-b2107d24-81d7-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:05:09.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8gfjn" for this suite.
May 29 06:05:31.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:05:31.767: INFO: namespace: e2e-tests-configmap-8gfjn, resource: bindings, ignored listing per whitelist
May 29 06:05:31.825: INFO: namespace e2e-tests-configmap-8gfjn deletion completed in 22.097161227s

â€¢ [SLOW TEST:30.214 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:05:31.825: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
May 29 06:05:31.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 cluster-info'
May 29 06:05:33.829: INFO: stderr: ""
May 29 06:05:33.829: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:05:33.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zxkh7" for this suite.
May 29 06:05:39.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:05:39.896: INFO: namespace: e2e-tests-kubectl-zxkh7, resource: bindings, ignored listing per whitelist
May 29 06:05:39.920: INFO: namespace e2e-tests-kubectl-zxkh7 deletion completed in 6.087538059s

â€¢ [SLOW TEST:8.096 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:05:39.921: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-jmlzs
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-jmlzs
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-jmlzs
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-jmlzs
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-jmlzs
May 29 06:05:46.027: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jmlzs, name: ss-0, uid: cbe7c9de-81d7-11e9-85ba-000d3a6e4ecc, status phase: Pending. Waiting for statefulset controller to delete.
May 29 06:05:46.226: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jmlzs, name: ss-0, uid: cbe7c9de-81d7-11e9-85ba-000d3a6e4ecc, status phase: Failed. Waiting for statefulset controller to delete.
May 29 06:05:46.230: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jmlzs, name: ss-0, uid: cbe7c9de-81d7-11e9-85ba-000d3a6e4ecc, status phase: Failed. Waiting for statefulset controller to delete.
May 29 06:05:46.241: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-jmlzs
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-jmlzs
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-jmlzs and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 06:05:52.272: INFO: Deleting all statefulset in ns e2e-tests-statefulset-jmlzs
May 29 06:05:52.274: INFO: Scaling statefulset ss to 0
May 29 06:06:02.285: INFO: Waiting for statefulset status.replicas updated to 0
May 29 06:06:02.287: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:06:02.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-jmlzs" for this suite.
May 29 06:06:08.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:06:08.345: INFO: namespace: e2e-tests-statefulset-jmlzs, resource: bindings, ignored listing per whitelist
May 29 06:06:08.400: INFO: namespace e2e-tests-statefulset-jmlzs deletion completed in 6.101492328s

â€¢ [SLOW TEST:28.479 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:06:08.401: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-wz4r
STEP: Creating a pod to test atomic-volume-subpath
May 29 06:06:08.485: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wz4r" in namespace "e2e-tests-subpath-dvd84" to be "success or failure"
May 29 06:06:08.494: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Pending", Reason="", readiness=false. Elapsed: 9.248582ms
May 29 06:06:10.498: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012702819s
May 29 06:06:12.501: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015956962s
May 29 06:06:14.504: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019003611s
May 29 06:06:16.507: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 8.02227786s
May 29 06:06:18.511: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 10.025431414s
May 29 06:06:20.513: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 12.028371274s
May 29 06:06:22.516: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 14.03110844s
May 29 06:06:24.520: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 16.034481601s
May 29 06:06:26.522: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 18.037295472s
May 29 06:06:28.526: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 20.040581141s
May 29 06:06:30.529: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 22.043475917s
May 29 06:06:32.531: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 24.046307898s
May 29 06:06:34.535: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Running", Reason="", readiness=false. Elapsed: 26.049599176s
May 29 06:06:36.538: INFO: Pod "pod-subpath-test-secret-wz4r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.052506461s
STEP: Saw pod success
May 29 06:06:36.538: INFO: Pod "pod-subpath-test-secret-wz4r" satisfied condition "success or failure"
May 29 06:06:36.540: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-subpath-test-secret-wz4r container test-container-subpath-secret-wz4r: <nil>
STEP: delete the pod
May 29 06:06:36.560: INFO: Waiting for pod pod-subpath-test-secret-wz4r to disappear
May 29 06:06:36.562: INFO: Pod pod-subpath-test-secret-wz4r no longer exists
STEP: Deleting pod pod-subpath-test-secret-wz4r
May 29 06:06:36.562: INFO: Deleting pod "pod-subpath-test-secret-wz4r" in namespace "e2e-tests-subpath-dvd84"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:06:36.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-dvd84" for this suite.
May 29 06:06:42.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:06:42.624: INFO: namespace: e2e-tests-subpath-dvd84, resource: bindings, ignored listing per whitelist
May 29 06:06:42.647: INFO: namespace e2e-tests-subpath-dvd84 deletion completed in 6.080436863s

â€¢ [SLOW TEST:34.247 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:06:42.648: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 06:06:42.720: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:06:48.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-q8wld" for this suite.
May 29 06:07:32.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:07:32.808: INFO: namespace: e2e-tests-pods-q8wld, resource: bindings, ignored listing per whitelist
May 29 06:07:32.839: INFO: namespace e2e-tests-pods-q8wld deletion completed in 44.078066073s

â€¢ [SLOW TEST:50.191 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:07:32.840: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
May 29 06:07:32.912: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-762096317 proxy --unix-socket=/tmp/kubectl-proxy-unix413684984/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:07:32.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nft6h" for this suite.
May 29 06:07:38.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:07:39.035: INFO: namespace: e2e-tests-kubectl-nft6h, resource: bindings, ignored listing per whitelist
May 29 06:07:39.072: INFO: namespace e2e-tests-kubectl-nft6h deletion completed in 6.104182113s

â€¢ [SLOW TEST:6.232 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:07:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:07:39.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-8lkh4" to be "success or failure"
May 29 06:07:39.168: INFO: Pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.665354ms
May 29 06:07:41.171: INFO: Pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006428336s
May 29 06:07:43.174: INFO: Pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009128022s
May 29 06:07:45.176: INFO: Pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011680813s
STEP: Saw pod success
May 29 06:07:45.177: INFO: Pod "downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:07:45.178: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:07:45.192: INFO: Waiting for pod downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:07:45.201: INFO: Pod downwardapi-volume-0fec2f30-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:07:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8lkh4" for this suite.
May 29 06:07:51.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:07:51.252: INFO: namespace: e2e-tests-projected-8lkh4, resource: bindings, ignored listing per whitelist
May 29 06:07:51.288: INFO: namespace e2e-tests-projected-8lkh4 deletion completed in 6.083444127s

â€¢ [SLOW TEST:12.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:07:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 29 06:07:51.363: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:07:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-n7h28" for this suite.
May 29 06:08:04.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:08:04.159: INFO: namespace: e2e-tests-init-container-n7h28, resource: bindings, ignored listing per whitelist
May 29 06:08:04.186: INFO: namespace e2e-tests-init-container-n7h28 deletion completed in 6.087798924s

â€¢ [SLOW TEST:12.898 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:08:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0529 06:08:10.290943      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 06:08:10.290: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:08:10.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-h882s" for this suite.
May 29 06:08:16.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:08:16.343: INFO: namespace: e2e-tests-gc-h882s, resource: bindings, ignored listing per whitelist
May 29 06:08:16.384: INFO: namespace e2e-tests-gc-h882s deletion completed in 6.090499738s

â€¢ [SLOW TEST:12.197 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:08:16.384: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 29 06:08:16.478: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-7jkxk,SelfLink:/api/v1/namespaces/e2e-tests-watch-7jkxk/configmaps/e2e-watch-test-resource-version,UID:26282b90-81d8-11e9-85ba-000d3a6e4ecc,ResourceVersion:19437,Generation:0,CreationTimestamp:2019-05-29 06:08:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 06:08:16.478: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-7jkxk,SelfLink:/api/v1/namespaces/e2e-tests-watch-7jkxk/configmaps/e2e-watch-test-resource-version,UID:26282b90-81d8-11e9-85ba-000d3a6e4ecc,ResourceVersion:19438,Generation:0,CreationTimestamp:2019-05-29 06:08:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:08:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-7jkxk" for this suite.
May 29 06:08:22.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:08:22.515: INFO: namespace: e2e-tests-watch-7jkxk, resource: bindings, ignored listing per whitelist
May 29 06:08:22.564: INFO: namespace e2e-tests-watch-7jkxk deletion completed in 6.082896158s

â€¢ [SLOW TEST:6.180 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:08:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:08:22.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-mjtwz" to be "success or failure"
May 29 06:08:22.657: INFO: Pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.26382ms
May 29 06:08:24.661: INFO: Pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637252s
May 29 06:08:26.664: INFO: Pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012859089s
May 29 06:08:28.667: INFO: Pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016346124s
STEP: Saw pod success
May 29 06:08:28.667: INFO: Pod "downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:08:28.669: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:08:28.690: INFO: Waiting for pod downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:08:28.692: INFO: Pod downwardapi-volume-29d825ee-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:08:28.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mjtwz" for this suite.
May 29 06:08:34.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:08:34.726: INFO: namespace: e2e-tests-projected-mjtwz, resource: bindings, ignored listing per whitelist
May 29 06:08:34.788: INFO: namespace e2e-tests-projected-mjtwz deletion completed in 6.090304311s

â€¢ [SLOW TEST:12.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:08:34.790: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 29 06:08:34.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:35.598: INFO: stderr: ""
May 29 06:08:35.598: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 06:08:35.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:35.686: INFO: stderr: ""
May 29 06:08:35.686: INFO: stdout: "update-demo-nautilus-k7xqd update-demo-nautilus-kcdx7 "
May 29 06:08:35.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-k7xqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:35.765: INFO: stderr: ""
May 29 06:08:35.765: INFO: stdout: ""
May 29 06:08:35.765: INFO: update-demo-nautilus-k7xqd is created but not running
May 29 06:08:40.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:40.841: INFO: stderr: ""
May 29 06:08:40.841: INFO: stdout: "update-demo-nautilus-k7xqd update-demo-nautilus-kcdx7 "
May 29 06:08:40.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-k7xqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:40.919: INFO: stderr: ""
May 29 06:08:40.919: INFO: stdout: "true"
May 29 06:08:40.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-k7xqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:40.995: INFO: stderr: ""
May 29 06:08:40.995: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 06:08:40.995: INFO: validating pod update-demo-nautilus-k7xqd
May 29 06:08:41.001: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 06:08:41.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 06:08:41.001: INFO: update-demo-nautilus-k7xqd is verified up and running
May 29 06:08:41.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-kcdx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:41.087: INFO: stderr: ""
May 29 06:08:41.087: INFO: stdout: "true"
May 29 06:08:41.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods update-demo-nautilus-kcdx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:41.162: INFO: stderr: ""
May 29 06:08:41.162: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 06:08:41.162: INFO: validating pod update-demo-nautilus-kcdx7
May 29 06:08:41.167: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 06:08:41.167: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 06:08:41.167: INFO: update-demo-nautilus-kcdx7 is verified up and running
STEP: using delete to clean up resources
May 29 06:08:41.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:41.251: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:08:41.251: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 29 06:08:41.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-pxg58'
May 29 06:08:41.334: INFO: stderr: "No resources found.\n"
May 29 06:08:41.334: INFO: stdout: ""
May 29 06:08:41.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pods -l name=update-demo --namespace=e2e-tests-kubectl-pxg58 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 06:08:41.405: INFO: stderr: ""
May 29 06:08:41.405: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:08:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pxg58" for this suite.
May 29 06:09:03.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:09:03.480: INFO: namespace: e2e-tests-kubectl-pxg58, resource: bindings, ignored listing per whitelist
May 29 06:09:03.494: INFO: namespace e2e-tests-kubectl-pxg58 deletion completed in 22.084576939s

â€¢ [SLOW TEST:28.705 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:09:03.495: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 06:09:03.588: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
May 29 06:09:03.592: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-6r8ds/daemonsets","resourceVersion":"19599"},"items":null}

May 29 06:09:03.595: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-6r8ds/pods","resourceVersion":"19599"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:09:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-6r8ds" for this suite.
May 29 06:09:09.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:09:09.699: INFO: namespace: e2e-tests-daemonsets-6r8ds, resource: bindings, ignored listing per whitelist
May 29 06:09:09.700: INFO: namespace e2e-tests-daemonsets-6r8ds deletion completed in 6.087171078s

S [SKIPPING] [6.206 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  May 29 06:09:03.588: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:09:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:09:09.779: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-8wmks" to be "success or failure"
May 29 06:09:09.788: INFO: Pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.74809ms
May 29 06:09:11.791: INFO: Pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011852382s
May 29 06:09:13.794: INFO: Pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014772979s
May 29 06:09:15.797: INFO: Pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018487968s
STEP: Saw pod success
May 29 06:09:15.797: INFO: Pod "downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:09:15.800: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:09:15.814: INFO: Waiting for pod downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:09:15.823: INFO: Pod downwardapi-volume-45ef7bd5-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:09:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8wmks" for this suite.
May 29 06:09:21.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:09:21.874: INFO: namespace: e2e-tests-downward-api-8wmks, resource: bindings, ignored listing per whitelist
May 29 06:09:21.925: INFO: namespace e2e-tests-downward-api-8wmks deletion completed in 6.099325767s

â€¢ [SLOW TEST:12.224 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:09:21.925: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
May 29 06:09:22.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 --namespace=e2e-tests-kubectl-fnf4b run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 29 06:09:27.083: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 29 06:09:27.083: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:09:29.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fnf4b" for this suite.
May 29 06:09:35.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:09:35.118: INFO: namespace: e2e-tests-kubectl-fnf4b, resource: bindings, ignored listing per whitelist
May 29 06:09:35.179: INFO: namespace e2e-tests-kubectl-fnf4b deletion completed in 6.088530448s

â€¢ [SLOW TEST:13.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:09:35.180: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-552025eb-81d8-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-552025eb-81d8-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:11:09.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q4qw6" for this suite.
May 29 06:11:31.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:11:31.768: INFO: namespace: e2e-tests-configmap-q4qw6, resource: bindings, ignored listing per whitelist
May 29 06:11:31.795: INFO: namespace e2e-tests-configmap-q4qw6 deletion completed in 22.099402883s

â€¢ [SLOW TEST:116.615 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:11:31.795: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
May 29 06:11:31.875: INFO: Waiting up to 5m0s for pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-var-expansion-khfxk" to be "success or failure"
May 29 06:11:31.878: INFO: Pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866648ms
May 29 06:11:33.881: INFO: Pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005789393s
May 29 06:11:35.883: INFO: Pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00847908s
May 29 06:11:37.887: INFO: Pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011777495s
STEP: Saw pod success
May 29 06:11:37.887: INFO: Pod "var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:11:37.889: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 06:11:37.905: INFO: Waiting for pod var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:11:37.908: INFO: Pod var-expansion-9aa18272-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:11:37.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-khfxk" for this suite.
May 29 06:11:43.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:11:43.951: INFO: namespace: e2e-tests-var-expansion-khfxk, resource: bindings, ignored listing per whitelist
May 29 06:11:43.993: INFO: namespace e2e-tests-var-expansion-khfxk deletion completed in 6.081331284s

â€¢ [SLOW TEST:12.198 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:11:43.993: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
May 29 06:11:44.076: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-762096317 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:11:44.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-db6d5" for this suite.
May 29 06:11:50.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:11:50.217: INFO: namespace: e2e-tests-kubectl-db6d5, resource: bindings, ignored listing per whitelist
May 29 06:11:50.230: INFO: namespace e2e-tests-kubectl-db6d5 deletion completed in 6.083005506s

â€¢ [SLOW TEST:6.237 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:11:50.231: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-a59dc37d-81d8-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:11:50.308: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-qpknf" to be "success or failure"
May 29 06:11:50.311: INFO: Pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.405438ms
May 29 06:11:52.314: INFO: Pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006130431s
May 29 06:11:54.317: INFO: Pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009181354s
May 29 06:11:56.321: INFO: Pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013525991s
STEP: Saw pod success
May 29 06:11:56.321: INFO: Pod "pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:11:56.323: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:11:56.344: INFO: Waiting for pod pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:11:56.349: INFO: Pod pod-projected-configmaps-a59e1ed3-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:11:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qpknf" for this suite.
May 29 06:12:02.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:12:02.418: INFO: namespace: e2e-tests-projected-qpknf, resource: bindings, ignored listing per whitelist
May 29 06:12:02.434: INFO: namespace e2e-tests-projected-qpknf deletion completed in 6.082729786s

â€¢ [SLOW TEST:12.204 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:12:02.435: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:12:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-5kmgp" for this suite.
May 29 06:12:45.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:12:45.812: INFO: namespace: e2e-tests-container-runtime-5kmgp, resource: bindings, ignored listing per whitelist
May 29 06:12:45.846: INFO: namespace e2e-tests-container-runtime-5kmgp deletion completed in 6.093522672s

â€¢ [SLOW TEST:43.411 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:12:45.847: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 29 06:12:45.927: INFO: Waiting up to 5m0s for pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-b6c2l" to be "success or failure"
May 29 06:12:45.938: INFO: Pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.609715ms
May 29 06:12:47.941: INFO: Pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013095976s
May 29 06:12:49.943: INFO: Pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015431472s
May 29 06:12:51.946: INFO: Pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018065595s
STEP: Saw pod success
May 29 06:12:51.946: INFO: Pod "pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:12:51.947: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 06:12:51.967: INFO: Waiting for pod pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:12:51.969: INFO: Pod pod-c6c483d8-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:12:51.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b6c2l" for this suite.
May 29 06:12:57.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:12:58.051: INFO: namespace: e2e-tests-emptydir-b6c2l, resource: bindings, ignored listing per whitelist
May 29 06:12:58.057: INFO: namespace e2e-tests-emptydir-b6c2l deletion completed in 6.084282641s

â€¢ [SLOW TEST:12.210 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:12:58.057: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ce0b9f36-81d8-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:12:58.180: INFO: Waiting up to 5m0s for pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-j7mpb" to be "success or failure"
May 29 06:12:58.190: INFO: Pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.408237ms
May 29 06:13:00.192: INFO: Pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012027194s
May 29 06:13:02.195: INFO: Pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015016676s
May 29 06:13:04.198: INFO: Pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017980991s
STEP: Saw pod success
May 29 06:13:04.198: INFO: Pod "pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:13:04.201: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 06:13:04.221: INFO: Waiting for pod pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:13:04.224: INFO: Pod pod-secrets-ce1252ed-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:13:04.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j7mpb" for this suite.
May 29 06:13:10.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:13:10.282: INFO: namespace: e2e-tests-secrets-j7mpb, resource: bindings, ignored listing per whitelist
May 29 06:13:10.318: INFO: namespace e2e-tests-secrets-j7mpb deletion completed in 6.088702059s
STEP: Destroying namespace "e2e-tests-secret-namespace-vs69l" for this suite.
May 29 06:13:16.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:13:16.381: INFO: namespace: e2e-tests-secret-namespace-vs69l, resource: bindings, ignored listing per whitelist
May 29 06:13:16.399: INFO: namespace e2e-tests-secret-namespace-vs69l deletion completed in 6.081037982s

â€¢ [SLOW TEST:18.343 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:13:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3
May 29 06:13:16.484: INFO: Pod name my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3: Found 0 pods out of 1
May 29 06:13:21.487: INFO: Pod name my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3: Found 1 pods out of 1
May 29 06:13:21.487: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3" are running
May 29 06:13:21.489: INFO: Pod "my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3-r4q9v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 06:13:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 06:13:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 06:13:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 06:13:16 +0000 UTC Reason: Message:}])
May 29 06:13:21.489: INFO: Trying to dial the pod
May 29 06:13:26.500: INFO: Controller my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3: Got expected result from replica 1 [my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3-r4q9v]: "my-hostname-basic-d8fa9c6f-81d8-11e9-b4f9-d20c9d8615e3-r4q9v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:13:26.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-xwdjq" for this suite.
May 29 06:13:32.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:13:32.585: INFO: namespace: e2e-tests-replication-controller-xwdjq, resource: bindings, ignored listing per whitelist
May 29 06:13:32.593: INFO: namespace e2e-tests-replication-controller-xwdjq deletion completed in 6.089914084s

â€¢ [SLOW TEST:16.193 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:13:32.593: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:13:40.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-45mt6" for this suite.
May 29 06:13:46.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:13:46.723: INFO: namespace: e2e-tests-kubelet-test-45mt6, resource: bindings, ignored listing per whitelist
May 29 06:13:46.775: INFO: namespace e2e-tests-kubelet-test-45mt6 deletion completed in 6.091197305s

â€¢ [SLOW TEST:14.182 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:13:46.775: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-eb15c7b5-81d8-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:13:46.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-rxgkh" to be "success or failure"
May 29 06:13:46.866: INFO: Pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213246ms
May 29 06:13:48.869: INFO: Pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006142642s
May 29 06:13:50.872: INFO: Pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009688457s
May 29 06:13:52.875: INFO: Pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012477213s
STEP: Saw pod success
May 29 06:13:52.875: INFO: Pod "pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:13:52.877: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:13:52.896: INFO: Waiting for pod pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:13:52.899: INFO: Pod pod-configmaps-eb162bf0-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:13:52.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rxgkh" for this suite.
May 29 06:13:58.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:13:58.922: INFO: namespace: e2e-tests-configmap-rxgkh, resource: bindings, ignored listing per whitelist
May 29 06:13:58.984: INFO: namespace e2e-tests-configmap-rxgkh deletion completed in 6.082038299s

â€¢ [SLOW TEST:12.209 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:13:58.985: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-f25eed3e-81d8-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:13:59.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-9m8wg" to be "success or failure"
May 29 06:13:59.085: INFO: Pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277727ms
May 29 06:14:01.088: INFO: Pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007625393s
May 29 06:14:03.091: INFO: Pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010443895s
May 29 06:14:05.094: INFO: Pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013422223s
STEP: Saw pod success
May 29 06:14:05.094: INFO: Pod "pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:14:05.096: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:14:05.116: INFO: Waiting for pod pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:14:05.120: INFO: Pod pod-configmaps-f25f4f82-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:14:05.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9m8wg" for this suite.
May 29 06:14:11.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:14:11.185: INFO: namespace: e2e-tests-configmap-9m8wg, resource: bindings, ignored listing per whitelist
May 29 06:14:11.212: INFO: namespace e2e-tests-configmap-9m8wg deletion completed in 6.086287754s

â€¢ [SLOW TEST:12.227 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:14:11.212: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 29 06:14:11.303: INFO: Waiting up to 5m0s for pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-9b4l7" to be "success or failure"
May 29 06:14:11.306: INFO: Pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.860552ms
May 29 06:14:13.309: INFO: Pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005964293s
May 29 06:14:15.312: INFO: Pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008840066s
May 29 06:14:17.316: INFO: Pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012905946s
STEP: Saw pod success
May 29 06:14:17.316: INFO: Pod "pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:14:17.318: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 06:14:17.339: INFO: Waiting for pod pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:14:17.341: INFO: Pod pod-f9a87cbc-81d8-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:14:17.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9b4l7" for this suite.
May 29 06:14:23.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:14:23.371: INFO: namespace: e2e-tests-emptydir-9b4l7, resource: bindings, ignored listing per whitelist
May 29 06:14:23.434: INFO: namespace e2e-tests-emptydir-9b4l7 deletion completed in 6.090110004s

â€¢ [SLOW TEST:12.222 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:14:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-00f05646-81d9-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:14:23.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-57gch" to be "success or failure"
May 29 06:14:23.533: INFO: Pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.944217ms
May 29 06:14:25.536: INFO: Pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007996427s
May 29 06:14:27.539: INFO: Pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010732969s
May 29 06:14:29.542: INFO: Pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013726834s
STEP: Saw pod success
May 29 06:14:29.542: INFO: Pod "pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:14:29.544: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:14:29.564: INFO: Waiting for pod pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:14:29.567: INFO: Pod pod-configmaps-00f195a5-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:14:29.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-57gch" for this suite.
May 29 06:14:35.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:14:35.636: INFO: namespace: e2e-tests-configmap-57gch, resource: bindings, ignored listing per whitelist
May 29 06:14:35.658: INFO: namespace e2e-tests-configmap-57gch deletion completed in 6.08751475s

â€¢ [SLOW TEST:12.224 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:14:35.659: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
May 29 06:14:35.733: INFO: Waiting up to 5m0s for pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-containers-j4slc" to be "success or failure"
May 29 06:14:35.736: INFO: Pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546758ms
May 29 06:14:37.738: INFO: Pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005109139s
May 29 06:14:39.741: INFO: Pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008075541s
May 29 06:14:41.745: INFO: Pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011162066s
STEP: Saw pod success
May 29 06:14:41.745: INFO: Pod "client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:14:41.746: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 06:14:41.767: INFO: Waiting for pod client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:14:41.771: INFO: Pod client-containers-083819fd-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:14:41.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-j4slc" for this suite.
May 29 06:14:47.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:14:47.821: INFO: namespace: e2e-tests-containers-j4slc, resource: bindings, ignored listing per whitelist
May 29 06:14:47.863: INFO: namespace e2e-tests-containers-j4slc deletion completed in 6.086618754s

â€¢ [SLOW TEST:12.205 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:14:47.863: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0529 06:15:27.959705      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 06:15:27.959: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:15:27.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mvn2q" for this suite.
May 29 06:15:33.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:15:34.028: INFO: namespace: e2e-tests-gc-mvn2q, resource: bindings, ignored listing per whitelist
May 29 06:15:34.051: INFO: namespace e2e-tests-gc-mvn2q deletion completed in 6.089166462s

â€¢ [SLOW TEST:46.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:15:34.052: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 29 06:15:34.127: INFO: namespace e2e-tests-kubectl-q78p9
May 29 06:15:34.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-q78p9'
May 29 06:15:39.057: INFO: stderr: ""
May 29 06:15:39.057: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 06:15:40.060: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:40.060: INFO: Found 0 / 1
May 29 06:15:41.061: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:41.061: INFO: Found 0 / 1
May 29 06:15:42.062: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:42.062: INFO: Found 0 / 1
May 29 06:15:43.060: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:43.060: INFO: Found 0 / 1
May 29 06:15:44.060: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:44.060: INFO: Found 1 / 1
May 29 06:15:44.060: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 06:15:44.062: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:15:44.062: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 06:15:44.062: INFO: wait on redis-master startup in e2e-tests-kubectl-q78p9 
May 29 06:15:44.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 logs redis-master-9qmwm redis-master --namespace=e2e-tests-kubectl-q78p9'
May 29 06:15:44.152: INFO: stderr: ""
May 29 06:15:44.152: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 06:15:42.477 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 06:15:42.477 # Server started, Redis version 3.2.12\n1:M 29 May 06:15:42.477 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 06:15:42.477 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 29 06:15:44.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-q78p9'
May 29 06:15:44.250: INFO: stderr: ""
May 29 06:15:44.250: INFO: stdout: "service/rm2 exposed\n"
May 29 06:15:44.254: INFO: Service rm2 in namespace e2e-tests-kubectl-q78p9 found.
STEP: exposing service
May 29 06:15:46.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-q78p9'
May 29 06:15:46.356: INFO: stderr: ""
May 29 06:15:46.357: INFO: stdout: "service/rm3 exposed\n"
May 29 06:15:46.360: INFO: Service rm3 in namespace e2e-tests-kubectl-q78p9 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:15:48.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q78p9" for this suite.
May 29 06:16:10.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:16:10.396: INFO: namespace: e2e-tests-kubectl-q78p9, resource: bindings, ignored listing per whitelist
May 29 06:16:10.509: INFO: namespace e2e-tests-kubectl-q78p9 deletion completed in 22.137721951s

â€¢ [SLOW TEST:36.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:16:10.510: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-40c3a3a5-81d9-11e9-b4f9-d20c9d8615e3
STEP: Creating configMap with name cm-test-opt-upd-40c3a3d9-81d9-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-40c3a3a5-81d9-11e9-b4f9-d20c9d8615e3
STEP: Updating configmap cm-test-opt-upd-40c3a3d9-81d9-11e9-b4f9-d20c9d8615e3
STEP: Creating configMap with name cm-test-opt-create-40c3a3ea-81d9-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:16:20.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vk9dc" for this suite.
May 29 06:16:42.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:16:42.768: INFO: namespace: e2e-tests-projected-vk9dc, resource: bindings, ignored listing per whitelist
May 29 06:16:42.778: INFO: namespace e2e-tests-projected-vk9dc deletion completed in 22.083628912s

â€¢ [SLOW TEST:32.268 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:16:42.778: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 06:16:42.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-mmsml'
May 29 06:16:42.931: INFO: stderr: ""
May 29 06:16:42.931: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 29 06:16:47.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-mmsml -o json'
May 29 06:16:48.051: INFO: stderr: ""
May 29 06:16:48.051: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-29T06:16:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-mmsml\",\n        \"resourceVersion\": \"21059\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-mmsml/pods/e2e-test-nginx-pod\",\n        \"uid\": \"5408175b-81d9-11e9-85ba-000d3a6e4ecc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vdjl9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-pool1-29361026-vmss000001\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vdjl9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vdjl9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T06:16:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T06:16:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T06:16:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T06:16:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://cc97ca25a5047f12f7f2424983ecd26b9031a4dda8015fbc6b0d236bd01e3f71\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-29T06:16:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.35\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.36\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-29T06:16:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 29 06:16:48.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 replace -f - --namespace=e2e-tests-kubectl-mmsml'
May 29 06:16:48.340: INFO: stderr: ""
May 29 06:16:48.340: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
May 29 06:16:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-mmsml'
May 29 06:16:54.996: INFO: stderr: ""
May 29 06:16:54.996: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:16:54.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mmsml" for this suite.
May 29 06:17:01.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:17:01.057: INFO: namespace: e2e-tests-kubectl-mmsml, resource: bindings, ignored listing per whitelist
May 29 06:17:01.089: INFO: namespace e2e-tests-kubectl-mmsml deletion completed in 6.090393342s

â€¢ [SLOW TEST:18.312 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:17:01.090: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 29 06:17:13.198: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 06:17:13.201: INFO: Pod pod-with-poststart-http-hook still exists
May 29 06:17:15.201: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 06:17:15.204: INFO: Pod pod-with-poststart-http-hook still exists
May 29 06:17:17.201: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 06:17:17.205: INFO: Pod pod-with-poststart-http-hook still exists
May 29 06:17:19.201: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 06:17:19.204: INFO: Pod pod-with-poststart-http-hook still exists
May 29 06:17:21.201: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 06:17:21.204: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:17:21.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vnsnx" for this suite.
May 29 06:17:43.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:17:43.281: INFO: namespace: e2e-tests-container-lifecycle-hook-vnsnx, resource: bindings, ignored listing per whitelist
May 29 06:17:43.302: INFO: namespace e2e-tests-container-lifecycle-hook-vnsnx deletion completed in 22.089677077s

â€¢ [SLOW TEST:42.212 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:17:43.302: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 06:17:43.384: INFO: (0) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.603413ms)
May 29 06:17:43.387: INFO: (1) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.895754ms)
May 29 06:17:43.391: INFO: (2) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.356648ms)
May 29 06:17:43.394: INFO: (3) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.508946ms)
May 29 06:17:43.397: INFO: (4) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.000653ms)
May 29 06:17:43.400: INFO: (5) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.311648ms)
May 29 06:17:43.404: INFO: (6) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.040752ms)
May 29 06:17:43.406: INFO: (7) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.807457ms)
May 29 06:17:43.410: INFO: (8) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.113252ms)
May 29 06:17:43.421: INFO: (9) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.244025ms)
May 29 06:17:43.435: INFO: (10) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.058681ms)
May 29 06:17:43.439: INFO: (11) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.572644ms)
May 29 06:17:43.442: INFO: (12) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.704242ms)
May 29 06:17:43.446: INFO: (13) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.492446ms)
May 29 06:17:43.452: INFO: (14) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.439299ms)
May 29 06:17:43.456: INFO: (15) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.886039ms)
May 29 06:17:43.459: INFO: (16) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.195151ms)
May 29 06:17:43.463: INFO: (17) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.476546ms)
May 29 06:17:43.466: INFO: (18) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.463046ms)
May 29 06:17:43.474: INFO: (19) /api/v1/nodes/k8s-pool1-29361026-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.243887ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:17:43.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-fv4jb" for this suite.
May 29 06:17:49.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:17:49.502: INFO: namespace: e2e-tests-proxy-fv4jb, resource: bindings, ignored listing per whitelist
May 29 06:17:49.557: INFO: namespace e2e-tests-proxy-fv4jb deletion completed in 6.080957003s

â€¢ [SLOW TEST:6.255 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:17:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 29 06:17:55.654: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-7bcb65f2-81d9-11e9-b4f9-d20c9d8615e3,GenerateName:,Namespace:e2e-tests-events-8dpmp,SelfLink:/api/v1/namespaces/e2e-tests-events-8dpmp/pods/send-events-7bcb65f2-81d9-11e9-b4f9-d20c9d8615e3,UID:7bcba519-81d9-11e9-85ba-000d3a6e4ecc,ResourceVersion:21247,Generation:0,CreationTimestamp:2019-05-29 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 632463098,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v4fcd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v4fcd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-v4fcd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-29361026-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ced610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ced630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 06:17:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 06:17:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 06:17:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 06:17:49 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.46,StartTime:2019-05-29 06:17:49 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-29 06:17:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6cf84757702834e0d0cbede11d9d1854fe845149ba0a308c2cb69a99d9e6514f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 29 06:17:57.657: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 29 06:17:59.661: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:17:59.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-8dpmp" for this suite.
May 29 06:18:37.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:18:37.695: INFO: namespace: e2e-tests-events-8dpmp, resource: bindings, ignored listing per whitelist
May 29 06:18:37.753: INFO: namespace e2e-tests-events-8dpmp deletion completed in 38.082203331s

â€¢ [SLOW TEST:48.195 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:18:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
May 29 06:18:37.825: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 29 06:18:37.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:38.200: INFO: stderr: ""
May 29 06:18:38.200: INFO: stdout: "service/redis-slave created\n"
May 29 06:18:38.200: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 29 06:18:38.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:38.547: INFO: stderr: ""
May 29 06:18:38.547: INFO: stdout: "service/redis-master created\n"
May 29 06:18:38.548: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 29 06:18:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:38.896: INFO: stderr: ""
May 29 06:18:38.896: INFO: stdout: "service/frontend created\n"
May 29 06:18:38.898: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 29 06:18:38.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:39.239: INFO: stderr: ""
May 29 06:18:39.239: INFO: stdout: "deployment.extensions/frontend created\n"
May 29 06:18:39.240: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 29 06:18:39.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:39.587: INFO: stderr: ""
May 29 06:18:39.587: INFO: stdout: "deployment.extensions/redis-master created\n"
May 29 06:18:39.587: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 29 06:18:39.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:18:40.586: INFO: stderr: ""
May 29 06:18:40.586: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
May 29 06:18:40.586: INFO: Waiting for all frontend pods to be Running.
May 29 06:19:25.638: INFO: Waiting for frontend to serve content.
May 29 06:19:25.653: INFO: Trying to add a new entry to the guestbook.
May 29 06:19:25.664: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 29 06:19:25.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:25.781: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:25.781: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 29 06:19:25.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:25.897: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:25.897: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 29 06:19:25.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:26.008: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:26.008: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 29 06:19:26.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:26.105: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:26.105: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 29 06:19:26.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:26.217: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:26.217: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 29 06:19:26.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ldh2h'
May 29 06:19:26.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 06:19:26.333: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:19:26.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ldh2h" for this suite.
May 29 06:20:06.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:20:06.397: INFO: namespace: e2e-tests-kubectl-ldh2h, resource: bindings, ignored listing per whitelist
May 29 06:20:06.420: INFO: namespace e2e-tests-kubectl-ldh2h deletion completed in 40.081236892s

â€¢ [SLOW TEST:88.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:20:06.420: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 06:20:06.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-6tq5f'
May 29 06:20:06.585: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 06:20:06.585: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
May 29 06:20:10.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-6tq5f'
May 29 06:20:10.699: INFO: stderr: ""
May 29 06:20:10.699: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:20:10.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6tq5f" for this suite.
May 29 06:20:32.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:20:32.774: INFO: namespace: e2e-tests-kubectl-6tq5f, resource: bindings, ignored listing per whitelist
May 29 06:20:32.792: INFO: namespace e2e-tests-kubectl-6tq5f deletion completed in 22.087327968s

â€¢ [SLOW TEST:26.371 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:20:32.792: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 29 06:20:32.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-downward-api-97srf" to be "success or failure"
May 29 06:20:32.882: INFO: Pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114723ms
May 29 06:20:34.885: INFO: Pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008399799s
May 29 06:20:36.888: INFO: Pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011327993s
May 29 06:20:38.891: INFO: Pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014493196s
STEP: Saw pod success
May 29 06:20:38.891: INFO: Pod "downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:20:38.893: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3 container client-container: <nil>
STEP: delete the pod
May 29 06:20:38.937: INFO: Waiting for pod downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:20:38.940: INFO: Pod downwardapi-volume-dd17ec71-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:20:38.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-97srf" for this suite.
May 29 06:20:44.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:20:44.982: INFO: namespace: e2e-tests-downward-api-97srf, resource: bindings, ignored listing per whitelist
May 29 06:20:45.031: INFO: namespace e2e-tests-downward-api-97srf deletion completed in 6.083100093s

â€¢ [SLOW TEST:12.239 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:20:45.032: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
May 29 06:20:45.111: INFO: Waiting up to 5m0s for pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-var-expansion-925nk" to be "success or failure"
May 29 06:20:45.113: INFO: Pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175068ms
May 29 06:20:47.116: INFO: Pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005103228s
May 29 06:20:49.119: INFO: Pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007818105s
May 29 06:20:51.122: INFO: Pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010906988s
STEP: Saw pod success
May 29 06:20:51.122: INFO: Pod "var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:20:51.125: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 06:20:51.149: INFO: Waiting for pod var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:20:51.156: INFO: Pod var-expansion-e462ac8f-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:20:51.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-925nk" for this suite.
May 29 06:20:57.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:20:57.206: INFO: namespace: e2e-tests-var-expansion-925nk, resource: bindings, ignored listing per whitelist
May 29 06:20:57.249: INFO: namespace e2e-tests-var-expansion-925nk deletion completed in 6.089100241s

â€¢ [SLOW TEST:12.217 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:20:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 29 06:20:57.372: INFO: Waiting up to 5m0s for pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-emptydir-p5j7q" to be "success or failure"
May 29 06:20:57.379: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.806784ms
May 29 06:20:59.382: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010475026s
May 29 06:21:01.385: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012867385s
May 29 06:21:03.388: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015883547s
May 29 06:21:05.408: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036249164s
STEP: Saw pod success
May 29 06:21:05.408: INFO: Pod "pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:21:05.410: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3 container test-container: <nil>
STEP: delete the pod
May 29 06:21:05.432: INFO: Waiting for pod pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:21:05.435: INFO: Pod pod-ebb0f368-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:21:05.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-p5j7q" for this suite.
May 29 06:21:11.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:21:11.504: INFO: namespace: e2e-tests-emptydir-p5j7q, resource: bindings, ignored listing per whitelist
May 29 06:21:11.521: INFO: namespace e2e-tests-emptydir-p5j7q deletion completed in 6.083097901s

â€¢ [SLOW TEST:14.272 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:21:11.521: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 06:21:17.630: INFO: Waiting up to 5m0s for pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-pods-gwn5x" to be "success or failure"
May 29 06:21:17.634: INFO: Pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907142ms
May 29 06:21:19.637: INFO: Pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006983303s
May 29 06:21:21.639: INFO: Pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009711181s
May 29 06:21:23.643: INFO: Pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013026963s
STEP: Saw pod success
May 29 06:21:23.643: INFO: Pod "client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:21:23.645: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3 container env3cont: <nil>
STEP: delete the pod
May 29 06:21:23.666: INFO: Waiting for pod client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:21:23.669: INFO: Pod client-envvars-f7c4b9ce-81d9-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:21:23.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gwn5x" for this suite.
May 29 06:22:03.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:22:03.787: INFO: namespace: e2e-tests-pods-gwn5x, resource: bindings, ignored listing per whitelist
May 29 06:22:03.787: INFO: namespace e2e-tests-pods-gwn5x deletion completed in 40.115116371s

â€¢ [SLOW TEST:52.266 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:22:03.788: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-135565e0-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:22:03.885: INFO: Waiting up to 5m0s for pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-gp5xs" to be "success or failure"
May 29 06:22:03.890: INFO: Pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914528ms
May 29 06:22:05.894: INFO: Pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008113154s
May 29 06:22:07.897: INFO: Pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011176193s
May 29 06:22:09.900: INFO: Pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014166045s
STEP: Saw pod success
May 29 06:22:09.900: INFO: Pod "pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:22:09.902: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3 container secret-volume-test: <nil>
STEP: delete the pod
May 29 06:22:09.917: INFO: Waiting for pod pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:22:09.920: INFO: Pod pod-secrets-1355c13d-81da-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:22:09.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-gp5xs" for this suite.
May 29 06:22:15.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:22:15.966: INFO: namespace: e2e-tests-secrets-gp5xs, resource: bindings, ignored listing per whitelist
May 29 06:22:16.020: INFO: namespace e2e-tests-secrets-gp5xs deletion completed in 6.095499853s

â€¢ [SLOW TEST:12.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:22:16.020: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-1a9e4492-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating secret with name secret-projected-all-test-volume-1a9e4481-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test Check all projections for projected volume plugin
May 29 06:22:16.109: INFO: Waiting up to 5m0s for pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-zrhgt" to be "success or failure"
May 29 06:22:16.112: INFO: Pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.138755ms
May 29 06:22:18.115: INFO: Pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006596744s
May 29 06:22:20.118: INFO: Pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009789747s
May 29 06:22:22.122: INFO: Pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013347256s
STEP: Saw pod success
May 29 06:22:22.122: INFO: Pod "projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:22:22.124: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3 container projected-all-volume-test: <nil>
STEP: delete the pod
May 29 06:22:22.144: INFO: Waiting for pod projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:22:22.146: INFO: Pod projected-volume-1a9e4453-81da-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:22:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zrhgt" for this suite.
May 29 06:22:28.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:22:28.213: INFO: namespace: e2e-tests-projected-zrhgt, resource: bindings, ignored listing per whitelist
May 29 06:22:28.234: INFO: namespace e2e-tests-projected-zrhgt deletion completed in 6.08197885s

â€¢ [SLOW TEST:12.214 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:22:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-nb42s
May 29 06:22:34.332: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-nb42s
STEP: checking the pod's current state and verifying that restartCount is present
May 29 06:22:34.334: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:26:34.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nb42s" for this suite.
May 29 06:26:40.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:26:40.813: INFO: namespace: e2e-tests-container-probe-nb42s, resource: bindings, ignored listing per whitelist
May 29 06:26:40.839: INFO: namespace e2e-tests-container-probe-nb42s deletion completed in 6.090200844s

â€¢ [SLOW TEST:252.605 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:26:40.839: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-b87678ea-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating secret with name s-test-opt-upd-b876792f-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b87678ea-81da-11e9-b4f9-d20c9d8615e3
STEP: Updating secret s-test-opt-upd-b876792f-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating secret with name s-test-opt-create-b8767949-81da-11e9-b4f9-d20c9d8615e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:26:53.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wwjxc" for this suite.
May 29 06:27:15.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:27:15.076: INFO: namespace: e2e-tests-projected-wwjxc, resource: bindings, ignored listing per whitelist
May 29 06:27:15.097: INFO: namespace e2e-tests-projected-wwjxc deletion completed in 22.081906176s

â€¢ [SLOW TEST:34.258 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:27:15.097: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-cce1a1da-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:27:15.184: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-tcdgl" to be "success or failure"
May 29 06:27:15.188: INFO: Pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.288954ms
May 29 06:27:17.191: INFO: Pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006608264s
May 29 06:27:19.195: INFO: Pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010560972s
May 29 06:27:21.198: INFO: Pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013713598s
STEP: Saw pod success
May 29 06:27:21.198: INFO: Pod "pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:27:21.200: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 06:27:21.217: INFO: Waiting for pod pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:27:21.225: INFO: Pod pod-projected-secrets-cce261bb-81da-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:27:21.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tcdgl" for this suite.
May 29 06:27:27.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:27:27.254: INFO: namespace: e2e-tests-projected-tcdgl, resource: bindings, ignored listing per whitelist
May 29 06:27:27.312: INFO: namespace e2e-tests-projected-tcdgl deletion completed in 6.083794671s

â€¢ [SLOW TEST:12.215 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:27:27.313: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-d429da3e-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:27:27.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-2d6cw" to be "success or failure"
May 29 06:27:27.409: INFO: Pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.996032ms
May 29 06:27:29.412: INFO: Pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01523508s
May 29 06:27:31.416: INFO: Pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019090625s
May 29 06:27:33.419: INFO: Pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021798893s
STEP: Saw pod success
May 29 06:27:33.419: INFO: Pod "pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:27:33.421: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000001 pod pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 06:27:33.444: INFO: Waiting for pod pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:27:33.446: INFO: Pod pod-projected-secrets-d42a72b1-81da-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:27:33.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2d6cw" for this suite.
May 29 06:27:39.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:27:39.527: INFO: namespace: e2e-tests-projected-2d6cw, resource: bindings, ignored listing per whitelist
May 29 06:27:39.530: INFO: namespace e2e-tests-projected-2d6cw deletion completed in 6.080541124s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:27:39.531: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 29 06:27:39.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 version --client'
May 29 06:27:39.653: INFO: stderr: ""
May 29 06:27:39.653: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 29 06:27:39.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-5xm4w'
May 29 06:27:41.695: INFO: stderr: ""
May 29 06:27:41.695: INFO: stdout: "replicationcontroller/redis-master created\n"
May 29 06:27:41.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 create -f - --namespace=e2e-tests-kubectl-5xm4w'
May 29 06:27:41.991: INFO: stderr: ""
May 29 06:27:41.991: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 06:27:42.995: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:27:42.995: INFO: Found 0 / 1
May 29 06:27:43.994: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:27:43.994: INFO: Found 0 / 1
May 29 06:27:44.994: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:27:44.994: INFO: Found 0 / 1
May 29 06:27:45.995: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:27:45.995: INFO: Found 1 / 1
May 29 06:27:45.995: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 06:27:45.997: INFO: Selector matched 1 pods for map[app:redis]
May 29 06:27:45.997: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 06:27:45.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 describe pod redis-master-h5tjj --namespace=e2e-tests-kubectl-5xm4w'
May 29 06:27:46.091: INFO: stderr: ""
May 29 06:27:46.091: INFO: stdout: "Name:               redis-master-h5tjj\nNamespace:          e2e-tests-kubectl-5xm4w\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-pool1-29361026-vmss000002/10.240.0.66\nStart Time:         Wed, 29 May 2019 06:27:41 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.240.0.68\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3b254b0adfe0ee84b9f74e40b696d630e831f19fa128a73d67ed65e869a88f50\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 29 May 2019 06:27:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9h2hf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9h2hf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9h2hf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  5s    default-scheduler                       Successfully assigned e2e-tests-kubectl-5xm4w/redis-master-h5tjj to k8s-pool1-29361026-vmss000002\n  Normal  Pulled     3s    kubelet, k8s-pool1-29361026-vmss000002  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-pool1-29361026-vmss000002  Created container\n  Normal  Started    1s    kubelet, k8s-pool1-29361026-vmss000002  Started container\n"
May 29 06:27:46.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 describe rc redis-master --namespace=e2e-tests-kubectl-5xm4w'
May 29 06:27:46.192: INFO: stderr: ""
May 29 06:27:46.192: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-5xm4w\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-h5tjj\n"
May 29 06:27:46.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 describe service redis-master --namespace=e2e-tests-kubectl-5xm4w'
May 29 06:27:46.284: INFO: stderr: ""
May 29 06:27:46.284: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-5xm4w\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.121.20\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.240.0.68:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 29 06:27:46.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 describe node k8s-master-29361026-0'
May 29 06:27:46.399: INFO: stderr: ""
May 29 06:27:46.400: INFO: stdout: "Name:               k8s-master-29361026-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westus2\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.azure.com/cluster=levo-1-13-5cee0a5b\n                    kubernetes.io/hostname=k8s-master-29361026-0\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 May 2019 04:31:44 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 29 May 2019 06:27:44 +0000   Wed, 29 May 2019 04:31:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 29 May 2019 06:27:44 +0000   Wed, 29 May 2019 04:31:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 29 May 2019 06:27:44 +0000   Wed, 29 May 2019 04:31:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 29 May 2019 06:27:44 +0000   Wed, 29 May 2019 04:31:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    k8s-master-29361026-0\n  InternalIP:  10.255.255.5\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              30428648Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7113156Ki\n pods:                           30\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              28043041951\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6345156Ki\n pods:                           30\nSystem Info:\n Machine ID:                 38f12c0818cb4175bdc261103fb5c92b\n System UUID:                BD1E67F3-9C22-4546-BD17-E49412478357\n Boot ID:                    fb16db45-7279-4101-95d0-59108ed6aa62\n Kernel Version:             4.15.0-1045-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.5\n Kubelet Version:            v1.13.6\n Kube-Proxy Version:         v1.13.6\nProviderID:                  azure:///subscriptions/01db32b1-e169-43b0-a791-de0e1ca5d8cd/resourceGroups/levo-1-13-5cee0a5b/providers/Microsoft.Compute/virtualMachines/k8s-master-29361026-0\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-47nmb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         111m\n  kube-system                azure-cni-networkmonitor-f8kd8                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         115m\n  kube-system                azure-ip-masq-agent-4wqs6                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     115m\n  kube-system                coredns-59b998c9dd-7k5jj                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     115m\n  kube-system                kube-addon-manager-k8s-master-29361026-0                   5m (0%)       0 (0%)      50Mi (0%)        0 (0%)         114m\n  kube-system                kube-apiserver-k8s-master-29361026-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         115m\n  kube-system                kube-controller-manager-k8s-master-29361026-0              0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                kube-proxy-lcjw7                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         115m\n  kube-system                kube-scheduler-k8s-master-29361026-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            255m (12%)  50m (2%)\n  memory                         170Mi (2%)  420Mi (6%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
May 29 06:27:46.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762096317 describe namespace e2e-tests-kubectl-5xm4w'
May 29 06:27:46.482: INFO: stderr: ""
May 29 06:27:46.482: INFO: stdout: "Name:         e2e-tests-kubectl-5xm4w\nLabels:       e2e-framework=kubectl\n              e2e-run=67496f8e-81cb-11e9-b4f9-d20c9d8615e3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:27:46.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5xm4w" for this suite.
May 29 06:28:08.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:28:08.511: INFO: namespace: e2e-tests-kubectl-5xm4w, resource: bindings, ignored listing per whitelist
May 29 06:28:08.568: INFO: namespace e2e-tests-kubectl-5xm4w deletion completed in 22.082510187s

â€¢ [SLOW TEST:29.037 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:28:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ecc14c0a-81da-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume configMaps
May 29 06:28:08.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-configmap-v7jtz" to be "success or failure"
May 29 06:28:08.664: INFO: Pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.847362ms
May 29 06:28:10.667: INFO: Pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012731632s
May 29 06:28:12.670: INFO: Pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015581507s
May 29 06:28:14.673: INFO: Pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018559286s
STEP: Saw pod success
May 29 06:28:14.673: INFO: Pod "pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:28:14.675: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 06:28:14.690: INFO: Waiting for pod pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:28:14.692: INFO: Pod pod-configmaps-ecc1a584-81da-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:28:14.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-v7jtz" for this suite.
May 29 06:28:20.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:28:20.725: INFO: namespace: e2e-tests-configmap-v7jtz, resource: bindings, ignored listing per whitelist
May 29 06:28:20.782: INFO: namespace e2e-tests-configmap-v7jtz deletion completed in 6.086715685s

â€¢ [SLOW TEST:12.213 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:28:20.782: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 06:28:20.855: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 06:28:20.868: INFO: Waiting for terminating namespaces to be deleted...
May 29 06:28:20.869: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000000 before test
May 29 06:28:20.875: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-99ztx from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 06:28:20.875: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 06:28:20.875: INFO: blobfuse-flexvol-installer-rr6q5 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 06:28:20.875: INFO: kube-proxy-hfc5q from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 06:28:20.875: INFO: sonobuoy-e2e-job-89123943a0f146da from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container e2e ready: true, restart count 0
May 29 06:28:20.875: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 06:28:20.875: INFO: azure-cni-networkmonitor-nb4kr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 06:28:20.875: INFO: keyvault-flexvolume-5hhhq from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 06:28:20.875: INFO: azure-ip-masq-agent-28z5v from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.875: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 06:28:20.875: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000001 before test
May 29 06:28:20.880: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-6w5ft from heptio-sonobuoy started at 2019-05-29 04:35:51 +0000 UTC (2 container statuses recorded)
May 29 06:28:20.880: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 06:28:20.880: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 06:28:20.880: INFO: azure-ip-masq-agent-8pcj4 from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.880: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 06:28:20.880: INFO: azure-cni-networkmonitor-bg749 from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.880: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 06:28:20.880: INFO: blobfuse-flexvol-installer-stf4c from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.880: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 06:28:20.880: INFO: keyvault-flexvolume-vn2lw from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.881: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 06:28:20.881: INFO: kube-proxy-dtmsl from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.881: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 06:28:20.881: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 04:35:39 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.881: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 06:28:20.881: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-29361026-vmss000002 before test
May 29 06:28:20.887: INFO: azure-cni-networkmonitor-qrlgm from kube-system started at 2019-05-29 04:32:00 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 06:28:20.887: INFO: sonobuoy-systemd-logs-daemon-set-3066ca0948e74795-7btfr from heptio-sonobuoy started at 2019-05-29 04:35:52 +0000 UTC (2 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 06:28:20.887: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 06:28:20.887: INFO: keyvault-flexvolume-w2dqp from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 06:28:20.887: INFO: blobfuse-flexvol-installer-lp79k from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 06:28:20.887: INFO: azure-ip-masq-agent-cpsxm from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 06:28:20.887: INFO: kube-proxy-fbq9l from kube-system started at 2019-05-29 04:32:02 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 06:28:20.887: INFO: metrics-server-69b44566d5-5fhmr from kube-system started at 2019-05-29 04:32:01 +0000 UTC (1 container statuses recorded)
May 29 06:28:20.887: INFO: 	Container metrics-server ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a31377b7af6850], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:28:21.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-zbs9v" for this suite.
May 29 06:28:27.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:28:27.985: INFO: namespace: e2e-tests-sched-pred-zbs9v, resource: bindings, ignored listing per whitelist
May 29 06:28:27.996: INFO: namespace e2e-tests-sched-pred-zbs9v deletion completed in 6.081684713s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.214 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:28:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-mjr9b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mjr9b to expose endpoints map[]
May 29 06:28:28.108: INFO: Get endpoints failed (6.875504ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 29 06:28:29.110: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mjr9b exposes endpoints map[] (1.009375749s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-mjr9b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mjr9b to expose endpoints map[pod1:[80]]
May 29 06:28:33.156: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.041017662s elapsed, will retry)
May 29 06:28:34.161: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mjr9b exposes endpoints map[pod1:[80]] (5.046382574s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-mjr9b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mjr9b to expose endpoints map[pod2:[80] pod1:[80]]
May 29 06:28:38.202: INFO: Unexpected endpoints: found map[f8f46177-81da-11e9-85ba-000d3a6e4ecc:[80]], expected map[pod2:[80] pod1:[80]] (4.037930031s elapsed, will retry)
May 29 06:28:39.209: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mjr9b exposes endpoints map[pod1:[80] pod2:[80]] (5.044799729s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-mjr9b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mjr9b to expose endpoints map[pod2:[80]]
May 29 06:28:39.232: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mjr9b exposes endpoints map[pod2:[80]] (20.084221ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-mjr9b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mjr9b to expose endpoints map[]
May 29 06:28:39.240: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mjr9b exposes endpoints map[] (3.280454ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:28:39.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-mjr9b" for this suite.
May 29 06:29:01.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:29:01.339: INFO: namespace: e2e-tests-services-mjr9b, resource: bindings, ignored listing per whitelist
May 29 06:29:01.362: INFO: namespace e2e-tests-services-mjr9b deletion completed in 22.090756704s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:33.367 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:29:01.363: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 29 06:29:13.464: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:13.467: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:15.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:15.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:17.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:17.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:19.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:19.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:21.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:21.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:23.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:23.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:25.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:25.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:27.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:27.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:29.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:29.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:31.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:31.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:33.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:33.470: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:35.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:35.471: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 06:29:37.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 06:29:37.471: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:29:37.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-4qzmb" for this suite.
May 29 06:29:59.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:29:59.504: INFO: namespace: e2e-tests-container-lifecycle-hook-4qzmb, resource: bindings, ignored listing per whitelist
May 29 06:29:59.563: INFO: namespace e2e-tests-container-lifecycle-hook-4qzmb deletion completed in 22.080579459s

â€¢ [SLOW TEST:58.201 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:29:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
May 29 06:29:59.652: INFO: Waiting up to 5m0s for pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-var-expansion-jt47n" to be "success or failure"
May 29 06:29:59.654: INFO: Pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.545265ms
May 29 06:30:01.658: INFO: Pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005967696s
May 29 06:30:03.661: INFO: Pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009301334s
May 29 06:30:05.664: INFO: Pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012510277s
STEP: Saw pod success
May 29 06:30:05.664: INFO: Pod "var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:30:05.666: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3 container dapi-container: <nil>
STEP: delete the pod
May 29 06:30:05.681: INFO: Waiting for pod var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:30:05.684: INFO: Pod var-expansion-2eea35bb-81db-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:30:05.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jt47n" for this suite.
May 29 06:30:11.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:30:11.761: INFO: namespace: e2e-tests-var-expansion-jt47n, resource: bindings, ignored listing per whitelist
May 29 06:30:11.771: INFO: namespace e2e-tests-var-expansion-jt47n deletion completed in 6.083538236s

â€¢ [SLOW TEST:12.207 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:30:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-362fc877-81db-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:30:11.850: INFO: Waiting up to 5m0s for pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-secrets-pgqj5" to be "success or failure"
May 29 06:30:11.853: INFO: Pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.58475ms
May 29 06:30:13.857: INFO: Pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003708s
May 29 06:30:15.859: INFO: Pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009528183s
May 29 06:30:17.862: INFO: Pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01286835s
STEP: Saw pod success
May 29 06:30:17.862: INFO: Pod "pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:30:17.864: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3 container secret-env-test: <nil>
STEP: delete the pod
May 29 06:30:17.881: INFO: Waiting for pod pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:30:17.884: INFO: Pod pod-secrets-36301bda-81db-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:30:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pgqj5" for this suite.
May 29 06:30:23.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:30:23.924: INFO: namespace: e2e-tests-secrets-pgqj5, resource: bindings, ignored listing per whitelist
May 29 06:30:23.969: INFO: namespace e2e-tests-secrets-pgqj5 deletion completed in 6.081539142s

â€¢ [SLOW TEST:12.198 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:30:23.969: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-ztrbb.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-ztrbb.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ztrbb.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-ztrbb.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-ztrbb.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ztrbb.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 29 06:30:52.125: INFO: DNS probes using e2e-tests-dns-ztrbb/dns-test-3d755055-81db-11e9-b4f9-d20c9d8615e3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:30:52.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-ztrbb" for this suite.
May 29 06:30:58.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:30:58.202: INFO: namespace: e2e-tests-dns-ztrbb, resource: bindings, ignored listing per whitelist
May 29 06:30:58.239: INFO: namespace e2e-tests-dns-ztrbb deletion completed in 6.097015639s

â€¢ [SLOW TEST:34.270 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:30:58.239: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-51e2c9e9-81db-11e9-b4f9-d20c9d8615e3
STEP: Creating a pod to test consume secrets
May 29 06:30:58.329: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3" in namespace "e2e-tests-projected-d6dq9" to be "success or failure"
May 29 06:30:58.337: INFO: Pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.720794ms
May 29 06:31:00.339: INFO: Pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596754s
May 29 06:31:02.342: INFO: Pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013540617s
May 29 06:31:04.346: INFO: Pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016838279s
STEP: Saw pod success
May 29 06:31:04.346: INFO: Pod "pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3" satisfied condition "success or failure"
May 29 06:31:04.348: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000002 pod pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 06:31:04.373: INFO: Waiting for pod pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3 to disappear
May 29 06:31:04.376: INFO: Pod pod-projected-secrets-51e33654-81db-11e9-b4f9-d20c9d8615e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:31:04.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d6dq9" for this suite.
May 29 06:31:10.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:31:10.445: INFO: namespace: e2e-tests-projected-d6dq9, resource: bindings, ignored listing per whitelist
May 29 06:31:10.463: INFO: namespace e2e-tests-projected-d6dq9 deletion completed in 6.083443898s

â€¢ [SLOW TEST:12.224 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 29 06:31:10.464: INFO: >>> kubeConfig: /tmp/kubeconfig-762096317
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-ft6v
STEP: Creating a pod to test atomic-volume-subpath
May 29 06:31:10.549: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ft6v" in namespace "e2e-tests-subpath-c8lsr" to be "success or failure"
May 29 06:31:10.552: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953659ms
May 29 06:31:12.555: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006333536s
May 29 06:31:14.558: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009113224s
May 29 06:31:16.561: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012169013s
May 29 06:31:18.563: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 8.014727911s
May 29 06:31:20.567: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 10.018124603s
May 29 06:31:22.570: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 12.021249501s
May 29 06:31:24.574: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 14.024953996s
May 29 06:31:26.577: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 16.028388597s
May 29 06:31:28.580: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 18.03126551s
May 29 06:31:30.583: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 20.034328025s
May 29 06:31:32.586: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 22.037618639s
May 29 06:31:34.590: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 24.040927158s
May 29 06:31:36.593: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Running", Reason="", readiness=false. Elapsed: 26.044061382s
May 29 06:31:38.595: INFO: Pod "pod-subpath-test-configmap-ft6v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.046674217s
STEP: Saw pod success
May 29 06:31:38.595: INFO: Pod "pod-subpath-test-configmap-ft6v" satisfied condition "success or failure"
May 29 06:31:38.598: INFO: Trying to get logs from node k8s-pool1-29361026-vmss000000 pod pod-subpath-test-configmap-ft6v container test-container-subpath-configmap-ft6v: <nil>
STEP: delete the pod
May 29 06:31:38.619: INFO: Waiting for pod pod-subpath-test-configmap-ft6v to disappear
May 29 06:31:38.621: INFO: Pod pod-subpath-test-configmap-ft6v no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ft6v
May 29 06:31:38.621: INFO: Deleting pod "pod-subpath-test-configmap-ft6v" in namespace "e2e-tests-subpath-c8lsr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 29 06:31:38.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-c8lsr" for this suite.
May 29 06:31:44.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 06:31:44.733: INFO: namespace: e2e-tests-subpath-c8lsr, resource: bindings, ignored listing per whitelist
May 29 06:31:44.778: INFO: namespace e2e-tests-subpath-c8lsr deletion completed in 6.150806367s

â€¢ [SLOW TEST:34.315 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
May 29 06:31:44.778: INFO: Running AfterSuite actions on all nodes
May 29 06:31:44.779: INFO: Running AfterSuite actions on node 1
May 29 06:31:44.779: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6881.457 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h54m42.589376733s
Test Suite Passed
