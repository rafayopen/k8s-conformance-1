I0308 13:01:35.628527      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-643275360
I0308 13:01:35.629548      15 e2e.go:224] Starting e2e run "4d7225d4-41a2-11e9-be5e-ae5bc1be57d7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1552050095 - Will randomize all specs
Will run 201 of 1946 specs

Mar  8 13:01:35.769: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:01:35.771: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  8 13:16:38.787: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  8 13:16:38.810: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  8 13:16:38.810: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Mar  8 13:16:38.810: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  8 13:16:38.817: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  8 13:16:38.817: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Mar  8 13:16:38.817: INFO: e2e test version: v1.13.0
Mar  8 13:16:38.817: INFO: kube-apiserver version: v1.13.3
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:16:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:16:38.854877      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
Mar  8 13:16:38.877: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  8 13:16:38.886: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-v5zbj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-6832aab0-41a4-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:16:39.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-v5zbj" to be "success or failure"
Mar  8 13:16:39.019: INFO: Pod "pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030044ms
Mar  8 13:16:41.021: INFO: Pod "pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00421417s
STEP: Saw pod success
Mar  8 13:16:41.021: INFO: Pod "pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:16:41.022: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:16:41.039: INFO: Waiting for pod pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:16:41.042: INFO: Pod pod-configmaps-6832fc2c-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:16:41.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-v5zbj" for this suite.
Mar  8 13:16:47.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:16:47.095: INFO: namespace: e2e-tests-configmap-v5zbj, resource: bindings, ignored listing per whitelist
Mar  8 13:16:47.179: INFO: namespace e2e-tests-configmap-v5zbj deletion completed in 6.134894685s

â€¢ [SLOW TEST:8.362 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:16:47.180: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:16:47.207941      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-4k8sw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5wwxn in namespace e2e-tests-proxy-4k8sw
I0308 13:16:47.344613      15 runners.go:184] Created replication controller with name: proxy-service-5wwxn, namespace: e2e-tests-proxy-4k8sw, replica count: 1
I0308 13:16:48.395084      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0308 13:16:49.395308      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:50.395558      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:51.395704      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:52.395955      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:53.396156      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:54.396401      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:55.396652      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:56.396943      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:57.397097      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:58.397315      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:16:59.397530      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0308 13:17:00.397744      15 runners.go:184] proxy-service-5wwxn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  8 13:17:00.399: INFO: setup took 13.070341775s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  8 13:17:00.417: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 16.809118ms)
Mar  8 13:17:00.423: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 23.656281ms)
Mar  8 13:17:00.423: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 23.488609ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 24.544805ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 23.834587ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 24.457774ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 24.618843ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 23.915786ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 23.979448ms)
Mar  8 13:17:00.424: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 24.325545ms)
Mar  8 13:17:00.425: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 25.242616ms)
Mar  8 13:17:00.426: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 25.810025ms)
Mar  8 13:17:00.426: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 25.070805ms)
Mar  8 13:17:00.426: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 25.404297ms)
Mar  8 13:17:00.426: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 25.776336ms)
Mar  8 13:17:00.426: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 26.032434ms)
Mar  8 13:17:00.432: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.495997ms)
Mar  8 13:17:00.432: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.055247ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 7.087423ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 6.120963ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 6.916318ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 6.447658ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.39349ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.283752ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.600057ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.262511ms)
Mar  8 13:17:00.433: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.744501ms)
Mar  8 13:17:00.434: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.267193ms)
Mar  8 13:17:00.435: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.333474ms)
Mar  8 13:17:00.435: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.364749ms)
Mar  8 13:17:00.435: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 8.015265ms)
Mar  8 13:17:00.435: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.590995ms)
Mar  8 13:17:00.441: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.720678ms)
Mar  8 13:17:00.441: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.079708ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.166369ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.286265ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 6.254704ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.070682ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.06273ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.964648ms)
Mar  8 13:17:00.442: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 6.615749ms)
Mar  8 13:17:00.443: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.300217ms)
Mar  8 13:17:00.443: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.855376ms)
Mar  8 13:17:00.443: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 7.500856ms)
Mar  8 13:17:00.444: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.421278ms)
Mar  8 13:17:00.444: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.764427ms)
Mar  8 13:17:00.444: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.03987ms)
Mar  8 13:17:00.445: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.436043ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 4.59403ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 5.602882ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 5.372325ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 5.113517ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.528135ms)
Mar  8 13:17:00.450: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.29086ms)
Mar  8 13:17:00.451: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 5.146937ms)
Mar  8 13:17:00.451: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.517219ms)
Mar  8 13:17:00.451: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.998486ms)
Mar  8 13:17:00.451: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 5.505156ms)
Mar  8 13:17:00.453: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.203208ms)
Mar  8 13:17:00.454: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.684102ms)
Mar  8 13:17:00.454: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 8.849998ms)
Mar  8 13:17:00.454: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.156155ms)
Mar  8 13:17:00.454: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.453329ms)
Mar  8 13:17:00.454: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.504362ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 7.905642ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.288736ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 8.048567ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 7.978905ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.000007ms)
Mar  8 13:17:00.462: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 8.440357ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.889621ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 10.133425ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 10.407595ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 11.088504ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.321698ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 10.925615ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 10.56668ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 9.8551ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 10.560298ms)
Mar  8 13:17:00.465: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 10.891719ms)
Mar  8 13:17:00.468: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 2.40457ms)
Mar  8 13:17:00.472: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.197885ms)
Mar  8 13:17:00.472: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 6.534283ms)
Mar  8 13:17:00.472: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.87206ms)
Mar  8 13:17:00.473: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 7.669871ms)
Mar  8 13:17:00.474: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.246427ms)
Mar  8 13:17:00.474: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.776489ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.495108ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.704909ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.716551ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.62189ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 9.798527ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.558165ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 9.718105ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.199591ms)
Mar  8 13:17:00.475: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.292826ms)
Mar  8 13:17:00.479: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 4.054639ms)
Mar  8 13:17:00.480: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.150464ms)
Mar  8 13:17:00.480: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 4.682002ms)
Mar  8 13:17:00.481: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 4.771979ms)
Mar  8 13:17:00.481: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.079162ms)
Mar  8 13:17:00.482: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.511837ms)
Mar  8 13:17:00.482: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.149593ms)
Mar  8 13:17:00.482: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.422428ms)
Mar  8 13:17:00.482: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.973957ms)
Mar  8 13:17:00.482: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 6.406832ms)
Mar  8 13:17:00.483: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 7.460458ms)
Mar  8 13:17:00.485: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.106154ms)
Mar  8 13:17:00.485: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.361265ms)
Mar  8 13:17:00.485: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.506838ms)
Mar  8 13:17:00.486: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.584446ms)
Mar  8 13:17:00.486: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.329267ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 8.164556ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 7.869711ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 8.36008ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 7.592334ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 7.95888ms)
Mar  8 13:17:00.494: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 7.480637ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.52152ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 8.383364ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.671813ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 9.561366ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.625799ms)
Mar  8 13:17:00.495: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 9.330954ms)
Mar  8 13:17:00.496: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 9.302809ms)
Mar  8 13:17:00.496: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.118614ms)
Mar  8 13:17:00.496: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.987519ms)
Mar  8 13:17:00.496: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 10.409559ms)
Mar  8 13:17:00.502: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 4.778788ms)
Mar  8 13:17:00.502: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.394873ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.359415ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.594233ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.272677ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 6.689436ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.658551ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.752303ms)
Mar  8 13:17:00.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.983535ms)
Mar  8 13:17:00.504: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 6.77899ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.111705ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.987706ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.921356ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.222212ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.162749ms)
Mar  8 13:17:00.505: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 8.019783ms)
Mar  8 13:17:00.509: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 4.045665ms)
Mar  8 13:17:00.509: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 4.211785ms)
Mar  8 13:17:00.509: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.377423ms)
Mar  8 13:17:00.511: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 4.86084ms)
Mar  8 13:17:00.511: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.292766ms)
Mar  8 13:17:00.511: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.604456ms)
Mar  8 13:17:00.511: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.627091ms)
Mar  8 13:17:00.512: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.948985ms)
Mar  8 13:17:00.512: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.113182ms)
Mar  8 13:17:00.512: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.626822ms)
Mar  8 13:17:00.514: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 7.955893ms)
Mar  8 13:17:00.514: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 8.611099ms)
Mar  8 13:17:00.514: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.205531ms)
Mar  8 13:17:00.514: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.855715ms)
Mar  8 13:17:00.515: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 10.323293ms)
Mar  8 13:17:00.516: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 10.302383ms)
Mar  8 13:17:00.520: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 4.656429ms)
Mar  8 13:17:00.520: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.862555ms)
Mar  8 13:17:00.520: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 4.821966ms)
Mar  8 13:17:00.521: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.341752ms)
Mar  8 13:17:00.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.485739ms)
Mar  8 13:17:00.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.604579ms)
Mar  8 13:17:00.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 5.417153ms)
Mar  8 13:17:00.523: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.848216ms)
Mar  8 13:17:00.523: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.302738ms)
Mar  8 13:17:00.523: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.882297ms)
Mar  8 13:17:00.524: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.052948ms)
Mar  8 13:17:00.524: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.783058ms)
Mar  8 13:17:00.525: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.153507ms)
Mar  8 13:17:00.525: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.833008ms)
Mar  8 13:17:00.525: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.270973ms)
Mar  8 13:17:00.526: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.444383ms)
Mar  8 13:17:00.532: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.218731ms)
Mar  8 13:17:00.532: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 5.829735ms)
Mar  8 13:17:00.532: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.103676ms)
Mar  8 13:17:00.532: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.089432ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 6.433659ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 6.190325ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 7.307425ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 7.47298ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.703934ms)
Mar  8 13:17:00.533: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 7.443122ms)
Mar  8 13:17:00.535: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.209725ms)
Mar  8 13:17:00.535: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.017392ms)
Mar  8 13:17:00.535: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.180999ms)
Mar  8 13:17:00.535: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.451425ms)
Mar  8 13:17:00.535: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.213549ms)
Mar  8 13:17:00.536: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 9.071288ms)
Mar  8 13:17:00.542: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 6.181533ms)
Mar  8 13:17:00.542: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 5.950647ms)
Mar  8 13:17:00.542: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.970443ms)
Mar  8 13:17:00.542: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 6.667689ms)
Mar  8 13:17:00.543: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.283368ms)
Mar  8 13:17:00.543: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 7.549548ms)
Mar  8 13:17:00.545: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.750146ms)
Mar  8 13:17:00.545: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.834787ms)
Mar  8 13:17:00.545: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.22456ms)
Mar  8 13:17:00.545: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.1479ms)
Mar  8 13:17:00.545: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.055535ms)
Mar  8 13:17:00.546: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 9.708785ms)
Mar  8 13:17:00.546: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 10.00505ms)
Mar  8 13:17:00.546: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 9.754709ms)
Mar  8 13:17:00.546: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.85664ms)
Mar  8 13:17:00.546: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 10.008674ms)
Mar  8 13:17:00.551: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.107382ms)
Mar  8 13:17:00.552: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 5.483021ms)
Mar  8 13:17:00.552: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.598535ms)
Mar  8 13:17:00.552: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 5.912611ms)
Mar  8 13:17:00.552: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 5.709863ms)
Mar  8 13:17:00.553: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.058206ms)
Mar  8 13:17:00.553: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.152084ms)
Mar  8 13:17:00.553: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.714141ms)
Mar  8 13:17:00.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 7.531254ms)
Mar  8 13:17:00.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 7.818272ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.967355ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.620333ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.748389ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.746582ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.817909ms)
Mar  8 13:17:00.556: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 9.732186ms)
Mar  8 13:17:00.561: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.445271ms)
Mar  8 13:17:00.561: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 4.845099ms)
Mar  8 13:17:00.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.80798ms)
Mar  8 13:17:00.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 6.743571ms)
Mar  8 13:17:00.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 7.122481ms)
Mar  8 13:17:00.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.023608ms)
Mar  8 13:17:00.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 7.820284ms)
Mar  8 13:17:00.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 8.240197ms)
Mar  8 13:17:00.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.333573ms)
Mar  8 13:17:00.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 8.71014ms)
Mar  8 13:17:00.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 9.749486ms)
Mar  8 13:17:00.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 9.41256ms)
Mar  8 13:17:00.567: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 9.960626ms)
Mar  8 13:17:00.567: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 9.221586ms)
Mar  8 13:17:00.567: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 9.921963ms)
Mar  8 13:17:00.567: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 9.467114ms)
Mar  8 13:17:00.572: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 4.300916ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.671116ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.125448ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 5.576919ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 5.684792ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.376072ms)
Mar  8 13:17:00.573: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 6.249595ms)
Mar  8 13:17:00.574: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.581208ms)
Mar  8 13:17:00.574: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 7.535862ms)
Mar  8 13:17:00.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.954466ms)
Mar  8 13:17:00.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.924932ms)
Mar  8 13:17:00.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.994896ms)
Mar  8 13:17:00.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 7.221553ms)
Mar  8 13:17:00.576: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.163964ms)
Mar  8 13:17:00.576: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 8.729736ms)
Mar  8 13:17:00.576: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.811172ms)
Mar  8 13:17:00.579: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 2.398645ms)
Mar  8 13:17:00.582: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 5.363893ms)
Mar  8 13:17:00.582: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.102092ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.012086ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 5.350397ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 5.859191ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.208989ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 5.679292ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 6.094028ms)
Mar  8 13:17:00.583: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.742595ms)
Mar  8 13:17:00.584: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.653883ms)
Mar  8 13:17:00.585: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.479021ms)
Mar  8 13:17:00.585: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.059144ms)
Mar  8 13:17:00.585: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.037726ms)
Mar  8 13:17:00.585: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.673478ms)
Mar  8 13:17:00.586: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.610087ms)
Mar  8 13:17:00.590: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 4.351763ms)
Mar  8 13:17:00.590: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.602909ms)
Mar  8 13:17:00.590: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 4.589667ms)
Mar  8 13:17:00.590: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 4.564199ms)
Mar  8 13:17:00.593: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 7.221989ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 7.714987ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 7.153719ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 7.765704ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.420824ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 7.721175ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 7.264742ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 7.432249ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.136245ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.811007ms)
Mar  8 13:17:00.594: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 7.695627ms)
Mar  8 13:17:00.595: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.542724ms)
Mar  8 13:17:00.600: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 4.561049ms)
Mar  8 13:17:00.600: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 4.974669ms)
Mar  8 13:17:00.601: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 5.320649ms)
Mar  8 13:17:00.601: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.430109ms)
Mar  8 13:17:00.601: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 5.59336ms)
Mar  8 13:17:00.601: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 5.98846ms)
Mar  8 13:17:00.602: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 6.585597ms)
Mar  8 13:17:00.602: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 7.108167ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 7.91792ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.007628ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.441139ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 8.304673ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 8.348889ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 8.117552ms)
Mar  8 13:17:00.603: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 8.169224ms)
Mar  8 13:17:00.604: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.557698ms)
Mar  8 13:17:00.607: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:460/proxy/: tls baz (200; 3.43718ms)
Mar  8 13:17:00.607: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:1080/proxy/... (200; 3.487096ms)
Mar  8 13:17:00.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:1080/proxy/rewri... (200; 5.193093ms)
Mar  8 13:17:00.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.155088ms)
Mar  8 13:17:00.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.267968ms)
Mar  8 13:17:00.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:162/proxy/: bar (200; 6.204316ms)
Mar  8 13:17:00.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:443/proxy/... (200; 6.642472ms)
Mar  8 13:17:00.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/http:proxy-service-5wwxn-s5d4d:160/proxy/: foo (200; 6.376367ms)
Mar  8 13:17:00.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/https:proxy-service-5wwxn-s5d4d:462/proxy/: tls qux (200; 7.325342ms)
Mar  8 13:17:00.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4k8sw/pods/proxy-service-5wwxn-s5d4d/proxy/rewriteme"... (200; 6.642084ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname2/proxy/: tls qux (200; 7.664223ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname1/proxy/: foo (200; 7.497732ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/https:proxy-service-5wwxn:tlsportname1/proxy/: tls baz (200; 7.82958ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname1/proxy/: foo (200; 8.366307ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/http:proxy-service-5wwxn:portname2/proxy/: bar (200; 7.783559ms)
Mar  8 13:17:00.612: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4k8sw/services/proxy-service-5wwxn:portname2/proxy/: bar (200; 8.346492ms)
STEP: deleting ReplicationController proxy-service-5wwxn in namespace e2e-tests-proxy-4k8sw, will wait for the garbage collector to delete the pods
Mar  8 13:17:00.667: INFO: Deleting ReplicationController proxy-service-5wwxn took: 3.323617ms
Mar  8 13:17:00.767: INFO: Terminating ReplicationController proxy-service-5wwxn pods took: 100.207933ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:17:10.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-4k8sw" for this suite.
Mar  8 13:17:17.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:17:17.295: INFO: namespace: e2e-tests-proxy-4k8sw, resource: bindings, ignored listing per whitelist
Mar  8 13:17:17.303: INFO: namespace e2e-tests-proxy-4k8sw deletion completed in 6.432202123s

â€¢ [SLOW TEST:30.123 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:17:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:17:17.331175      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mpgsg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7f1cb7cf-41a4-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:17:17.461: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-mpgsg" to be "success or failure"
Mar  8 13:17:17.465: INFO: Pod "pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820025ms
Mar  8 13:17:19.468: INFO: Pod "pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006891029s
STEP: Saw pod success
Mar  8 13:17:19.468: INFO: Pod "pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:17:19.469: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:17:19.480: INFO: Waiting for pod pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:17:19.484: INFO: Pod pod-projected-configmaps-7f1d0353-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:17:19.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mpgsg" for this suite.
Mar  8 13:17:25.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:17:25.520: INFO: namespace: e2e-tests-projected-mpgsg, resource: bindings, ignored listing per whitelist
Mar  8 13:17:25.543: INFO: namespace e2e-tests-projected-mpgsg deletion completed in 6.056582937s

â€¢ [SLOW TEST:8.240 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:17:25.543: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:17:25.570144      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-f8dq9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-84058e4b-41a4-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:17:25.699: INFO: Waiting up to 5m0s for pod "pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-f8dq9" to be "success or failure"
Mar  8 13:17:25.702: INFO: Pod "pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.819162ms
Mar  8 13:17:27.705: INFO: Pod "pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004972817s
STEP: Saw pod success
Mar  8 13:17:27.705: INFO: Pod "pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:17:27.706: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:17:27.717: INFO: Waiting for pod pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:17:27.718: INFO: Pod pod-secrets-840633dc-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:17:27.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-f8dq9" for this suite.
Mar  8 13:17:33.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:17:33.752: INFO: namespace: e2e-tests-secrets-f8dq9, resource: bindings, ignored listing per whitelist
Mar  8 13:17:33.784: INFO: namespace e2e-tests-secrets-f8dq9 deletion completed in 6.063227845s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:17:33.784: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:17:33.824147      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-vfl4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vfl4x
Mar  8 13:17:36.017: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vfl4x
STEP: checking the pod's current state and verifying that restartCount is present
Mar  8 13:17:36.018: INFO: Initial restart count of pod liveness-http is 0
Mar  8 13:18:00.215: INFO: Restart count of pod e2e-tests-container-probe-vfl4x/liveness-http is now 1 (24.196802888s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:18:00.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vfl4x" for this suite.
Mar  8 13:18:06.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:18:06.350: INFO: namespace: e2e-tests-container-probe-vfl4x, resource: bindings, ignored listing per whitelist
Mar  8 13:18:06.358: INFO: namespace e2e-tests-container-probe-vfl4x deletion completed in 6.131308684s

â€¢ [SLOW TEST:32.574 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:18:06.358: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:18:06.387386      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cnkqn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:18:06.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-cnkqn" to be "success or failure"
Mar  8 13:18:06.517: INFO: Pod "downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609922ms
Mar  8 13:18:08.519: INFO: Pod "downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006864518s
STEP: Saw pod success
Mar  8 13:18:08.519: INFO: Pod "downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:18:08.521: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:18:08.532: INFO: Waiting for pod downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:18:08.534: INFO: Pod downwardapi-volume-9c5a042b-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:18:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cnkqn" for this suite.
Mar  8 13:18:14.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:18:14.557: INFO: namespace: e2e-tests-downward-api-cnkqn, resource: bindings, ignored listing per whitelist
Mar  8 13:18:14.593: INFO: namespace e2e-tests-downward-api-cnkqn deletion completed in 6.056636249s

â€¢ [SLOW TEST:8.235 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:18:14.593: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:18:14.621322      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-x4rnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:18:14.749: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  8 13:18:19.751: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  8 13:18:19.751: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  8 13:18:22.122: INFO: Creating deployment "test-rollover-deployment"
Mar  8 13:18:22.131: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  8 13:18:24.137: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  8 13:18:24.140: INFO: Ensure that both replica sets have 1 created replica
Mar  8 13:18:24.143: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  8 13:18:24.147: INFO: Updating deployment test-rollover-deployment
Mar  8 13:18:24.147: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  8 13:18:26.151: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  8 13:18:26.154: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  8 13:18:26.157: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:26.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647904, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:28.160: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:28.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647906, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:30.162: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:30.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647906, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:32.160: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:32.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647906, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:34.161: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:34.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647906, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:36.164: INFO: all replica sets need to contain the pod-template-hash label
Mar  8 13:18:36.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647906, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687647902, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 13:18:38.161: INFO: 
Mar  8 13:18:38.161: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  8 13:18:38.168: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-x4rnq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x4rnq/deployments/test-rollover-deployment,UID:a5a8ef0d-41a4-11e9-8665-02fd46f865f2,ResourceVersion:6037,Generation:2,CreationTimestamp:2019-03-08 13:18:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-08 13:18:22 +0000 UTC 2019-03-08 13:18:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-08 13:18:36 +0000 UTC 2019-03-08 13:18:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  8 13:18:38.170: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-x4rnq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x4rnq/replicasets/test-rollover-deployment-6b7f9d6597,UID:a6ddf8bd-41a4-11e9-8665-02fd46f865f2,ResourceVersion:6028,Generation:2,CreationTimestamp:2019-03-08 13:18:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a5a8ef0d-41a4-11e9-8665-02fd46f865f2 0xc000e6db07 0xc000e6db08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  8 13:18:38.170: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  8 13:18:38.170: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-x4rnq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x4rnq/replicasets/test-rollover-controller,UID:a1427cda-41a4-11e9-8665-02fd46f865f2,ResourceVersion:6036,Generation:2,CreationTimestamp:2019-03-08 13:18:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a5a8ef0d-41a4-11e9-8665-02fd46f865f2 0xc000e6d97f 0xc000e6d990}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 13:18:38.170: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-x4rnq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x4rnq/replicasets/test-rollover-deployment-6586df867b,UID:a5ab0950-41a4-11e9-8665-02fd46f865f2,ResourceVersion:5990,Generation:2,CreationTimestamp:2019-03-08 13:18:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a5a8ef0d-41a4-11e9-8665-02fd46f865f2 0xc000e6da47 0xc000e6da48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 13:18:38.172: INFO: Pod "test-rollover-deployment-6b7f9d6597-n6rtt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-n6rtt,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-x4rnq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-x4rnq/pods/test-rollover-deployment-6b7f9d6597-n6rtt,UID:a6e0b884-41a4-11e9-8665-02fd46f865f2,ResourceVersion:6004,Generation:0,CreationTimestamp:2019-03-08 13:18:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 a6ddf8bd-41a4-11e9-8665-02fd46f865f2 0xc0018e4627 0xc0018e4628}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-ht2zh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ht2zh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ht2zh true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018e4690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018e46b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:18:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:18:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:18:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:18:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.3,StartTime:2019-03-08 13:18:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-08 13:18:25 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://949fa211675c2f9afd951cbecc7d85ea3d646498ad0c30e76278bc3ef79ba9b4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:18:38.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-x4rnq" for this suite.
Mar  8 13:18:44.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:18:44.229: INFO: namespace: e2e-tests-deployment-x4rnq, resource: bindings, ignored listing per whitelist
Mar  8 13:18:44.232: INFO: namespace e2e-tests-deployment-x4rnq deletion completed in 6.058103269s

â€¢ [SLOW TEST:29.639 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:18:44.232: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:18:44.259584      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-275s7
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  8 13:18:44.385: INFO: Waiting up to 5m0s for pod "pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-275s7" to be "success or failure"
Mar  8 13:18:44.388: INFO: Pod "pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.150027ms
Mar  8 13:18:46.390: INFO: Pod "pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005225334s
STEP: Saw pod success
Mar  8 13:18:46.390: INFO: Pod "pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:18:46.391: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:18:46.400: INFO: Waiting for pod pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:18:46.403: INFO: Pod pod-b2ecdfad-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:18:46.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-275s7" for this suite.
Mar  8 13:18:52.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:18:52.435: INFO: namespace: e2e-tests-emptydir-275s7, resource: bindings, ignored listing per whitelist
Mar  8 13:18:52.465: INFO: namespace e2e-tests-emptydir-275s7 deletion completed in 6.05940497s

â€¢ [SLOW TEST:8.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:18:52.465: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:18:52.497980      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-6v68h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:18:54.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-6v68h" for this suite.
Mar  8 13:19:00.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:19:00.699: INFO: namespace: e2e-tests-emptydir-wrapper-6v68h, resource: bindings, ignored listing per whitelist
Mar  8 13:19:00.715: INFO: namespace e2e-tests-emptydir-wrapper-6v68h deletion completed in 6.061713787s

â€¢ [SLOW TEST:8.251 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:19:00.716: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:19:00.743055      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-f5v6x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:19:00.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-f5v6x" to be "success or failure"
Mar  8 13:19:00.880: INFO: Pod "downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220381ms
Mar  8 13:19:02.882: INFO: Pod "downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006831365s
STEP: Saw pod success
Mar  8 13:19:02.882: INFO: Pod "downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:19:02.886: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:19:02.905: INFO: Waiting for pod downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:19:02.908: INFO: Pod downwardapi-volume-bcc12a0c-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:19:02.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f5v6x" for this suite.
Mar  8 13:19:08.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:19:08.930: INFO: namespace: e2e-tests-downward-api-f5v6x, resource: bindings, ignored listing per whitelist
Mar  8 13:19:08.974: INFO: namespace e2e-tests-downward-api-f5v6x deletion completed in 6.063466773s

â€¢ [SLOW TEST:8.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:19:08.974: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:19:09.003386      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-zxkwd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-zxkwd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  8 13:19:09.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  8 13:19:37.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.192.7:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-zxkwd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:19:37.200: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:19:37.297: INFO: Found all expected endpoints: [netserver-0]
Mar  8 13:19:37.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.64.14:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-zxkwd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:19:37.299: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:19:37.400: INFO: Found all expected endpoints: [netserver-1]
Mar  8 13:19:37.402: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.160.3:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-zxkwd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:19:37.402: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:19:37.489: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:19:37.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-zxkwd" for this suite.
Mar  8 13:19:59.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:19:59.517: INFO: namespace: e2e-tests-pod-network-test-zxkwd, resource: bindings, ignored listing per whitelist
Mar  8 13:19:59.596: INFO: namespace e2e-tests-pod-network-test-zxkwd deletion completed in 22.104023877s

â€¢ [SLOW TEST:50.621 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:19:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:19:59.629296      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rr9ht
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  8 13:19:59.755: INFO: Waiting up to 5m0s for pod "pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-rr9ht" to be "success or failure"
Mar  8 13:19:59.759: INFO: Pod "pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01917ms
Mar  8 13:20:01.762: INFO: Pod "pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006260422s
STEP: Saw pod success
Mar  8 13:20:01.762: INFO: Pod "pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:20:01.763: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:20:01.773: INFO: Waiting for pod pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:20:01.776: INFO: Pod pod-dfd983d3-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:20:01.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rr9ht" for this suite.
Mar  8 13:20:07.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:20:07.821: INFO: namespace: e2e-tests-emptydir-rr9ht, resource: bindings, ignored listing per whitelist
Mar  8 13:20:07.837: INFO: namespace e2e-tests-emptydir-rr9ht deletion completed in 6.058943078s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:20:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:20:07.867522      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-h9k49
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:20:07.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-h9k49" to be "success or failure"
Mar  8 13:20:07.993: INFO: Pod "downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470153ms
Mar  8 13:20:09.996: INFO: Pod "downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004682696s
STEP: Saw pod success
Mar  8 13:20:09.996: INFO: Pod "downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:20:09.997: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:20:10.010: INFO: Waiting for pod downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:20:10.012: INFO: Pod downwardapi-volume-e4c2219e-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:20:10.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-h9k49" for this suite.
Mar  8 13:20:16.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:20:16.065: INFO: namespace: e2e-tests-downward-api-h9k49, resource: bindings, ignored listing per whitelist
Mar  8 13:20:16.076: INFO: namespace e2e-tests-downward-api-h9k49 deletion completed in 6.061432867s

â€¢ [SLOW TEST:8.239 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:20:16.076: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:20:16.108433      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7k6fk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  8 13:20:16.235: INFO: Waiting up to 5m0s for pod "pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-7k6fk" to be "success or failure"
Mar  8 13:20:16.238: INFO: Pod "pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267456ms
Mar  8 13:20:18.240: INFO: Pod "pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005607499s
Mar  8 13:20:20.242: INFO: Pod "pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007786015s
STEP: Saw pod success
Mar  8 13:20:20.242: INFO: Pod "pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:20:20.244: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:20:20.255: INFO: Waiting for pod pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:20:20.256: INFO: Pod pod-e9ac0be5-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:20:20.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7k6fk" for this suite.
Mar  8 13:20:26.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:20:26.300: INFO: namespace: e2e-tests-emptydir-7k6fk, resource: bindings, ignored listing per whitelist
Mar  8 13:20:26.314: INFO: namespace e2e-tests-emptydir-7k6fk deletion completed in 6.054917356s

â€¢ [SLOW TEST:10.237 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:20:26.314: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:20:26.340133      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-8d6k2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar  8 13:20:26.466: INFO: Waiting up to 5m0s for pod "client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-containers-8d6k2" to be "success or failure"
Mar  8 13:20:26.472: INFO: Pod "client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.409549ms
Mar  8 13:20:28.475: INFO: Pod "client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008517016s
Mar  8 13:20:30.477: INFO: Pod "client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010659395s
STEP: Saw pod success
Mar  8 13:20:30.477: INFO: Pod "client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:20:30.478: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:20:30.487: INFO: Waiting for pod client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:20:30.491: INFO: Pod client-containers-efc5471c-41a4-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:20:30.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-8d6k2" for this suite.
Mar  8 13:20:36.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:20:36.509: INFO: namespace: e2e-tests-containers-8d6k2, resource: bindings, ignored listing per whitelist
Mar  8 13:20:36.550: INFO: namespace e2e-tests-containers-8d6k2 deletion completed in 6.057493425s

â€¢ [SLOW TEST:10.237 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:20:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:20:36.578608      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6r7mt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 13:20:36.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:36.942: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  8 13:20:36.942: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  8 13:20:36.949: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  8 13:20:36.955: INFO: scanned /root for discovery docs: <nil>
Mar  8 13:20:36.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:52.783: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  8 13:20:52.783: INFO: stdout: "Created e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325\nScaling up e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  8 13:20:52.783: INFO: stdout: "Created e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325\nScaling up e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  8 13:20:52.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:52.864: INFO: stderr: ""
Mar  8 13:20:52.864: INFO: stdout: "e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325-89n2s "
Mar  8 13:20:52.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325-89n2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:52.938: INFO: stderr: ""
Mar  8 13:20:52.938: INFO: stdout: "true"
Mar  8 13:20:52.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325-89n2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:53.015: INFO: stderr: ""
Mar  8 13:20:53.015: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  8 13:20:53.015: INFO: e2e-test-nginx-rc-dfb07591caf6186ba37ac37ef0f15325-89n2s is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Mar  8 13:20:53.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-6r7mt'
Mar  8 13:20:53.098: INFO: stderr: ""
Mar  8 13:20:53.098: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:20:53.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6r7mt" for this suite.
Mar  8 13:21:15.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:21:15.141: INFO: namespace: e2e-tests-kubectl-6r7mt, resource: bindings, ignored listing per whitelist
Mar  8 13:21:15.163: INFO: namespace e2e-tests-kubectl-6r7mt deletion completed in 22.058837544s

â€¢ [SLOW TEST:38.612 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:21:15.163: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:21:15.190640      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-jl289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:22:15.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jl289" for this suite.
Mar  8 13:22:37.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:22:37.376: INFO: namespace: e2e-tests-container-probe-jl289, resource: bindings, ignored listing per whitelist
Mar  8 13:22:37.387: INFO: namespace e2e-tests-container-probe-jl289 deletion completed in 22.063875554s

â€¢ [SLOW TEST:82.224 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:22:37.387: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:22:37.414429      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bg5p6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:22:37.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bg5p6" for this suite.
Mar  8 13:22:59.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:22:59.608: INFO: namespace: e2e-tests-pods-bg5p6, resource: bindings, ignored listing per whitelist
Mar  8 13:22:59.650: INFO: namespace e2e-tests-pods-bg5p6 deletion completed in 22.102486224s

â€¢ [SLOW TEST:22.263 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:22:59.651: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:22:59.677314      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-gvnd8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  8 13:22:59.801: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:23:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-gvnd8" for this suite.
Mar  8 13:23:11.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:23:11.569: INFO: namespace: e2e-tests-init-container-gvnd8, resource: bindings, ignored listing per whitelist
Mar  8 13:23:11.579: INFO: namespace e2e-tests-init-container-gvnd8 deletion completed in 6.059323581s

â€¢ [SLOW TEST:11.929 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:23:11.579: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:23:11.608012      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-229rt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  8 13:23:14.254: INFO: Successfully updated pod "labelsupdate5247147b-41a5-11e9-be5e-ae5bc1be57d7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:23:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-229rt" for this suite.
Mar  8 13:23:40.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:23:40.710: INFO: namespace: e2e-tests-projected-229rt, resource: bindings, ignored listing per whitelist
Mar  8 13:23:40.753: INFO: namespace e2e-tests-projected-229rt deletion completed in 22.481140209s

â€¢ [SLOW TEST:29.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:23:40.753: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:23:40.780794      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dhr82
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  8 13:23:41.119: INFO: Waiting up to 5m0s for pod "pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-dhr82" to be "success or failure"
Mar  8 13:23:41.122: INFO: Pod "pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22103ms
Mar  8 13:23:43.124: INFO: Pod "pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00533699s
STEP: Saw pod success
Mar  8 13:23:43.124: INFO: Pod "pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:23:43.126: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:23:43.138: INFO: Waiting for pod pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:23:43.139: INFO: Pod pod-63ca5a98-41a5-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:23:43.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dhr82" for this suite.
Mar  8 13:23:49.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:23:49.186: INFO: namespace: e2e-tests-emptydir-dhr82, resource: bindings, ignored listing per whitelist
Mar  8 13:23:49.204: INFO: namespace e2e-tests-emptydir-dhr82 deletion completed in 6.061766209s

â€¢ [SLOW TEST:8.451 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:23:49.204: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:23:49.231538      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s24bl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:23:49.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-s24bl" to be "success or failure"
Mar  8 13:23:49.364: INFO: Pod "downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442917ms
Mar  8 13:23:51.366: INFO: Pod "downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007648617s
STEP: Saw pod success
Mar  8 13:23:51.366: INFO: Pod "downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:23:51.368: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:23:51.377: INFO: Waiting for pod downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:23:51.380: INFO: Pod downwardapi-volume-68b3c3aa-41a5-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:23:51.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s24bl" for this suite.
Mar  8 13:23:57.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:23:57.416: INFO: namespace: e2e-tests-projected-s24bl, resource: bindings, ignored listing per whitelist
Mar  8 13:23:57.441: INFO: namespace e2e-tests-projected-s24bl deletion completed in 6.058481914s

â€¢ [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:23:57.441: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:23:57.468719      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-tdzkh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:23:58.490: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  8 13:23:58.495: INFO: Number of nodes with available pods: 0
Mar  8 13:23:58.495: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  8 13:23:58.509: INFO: Number of nodes with available pods: 0
Mar  8 13:23:58.509: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:23:59.512: INFO: Number of nodes with available pods: 0
Mar  8 13:23:59.512: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:00.513: INFO: Number of nodes with available pods: 0
Mar  8 13:24:00.513: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:01.511: INFO: Number of nodes with available pods: 1
Mar  8 13:24:01.511: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  8 13:24:01.522: INFO: Number of nodes with available pods: 1
Mar  8 13:24:01.522: INFO: Number of running nodes: 0, number of available pods: 1
Mar  8 13:24:02.524: INFO: Number of nodes with available pods: 0
Mar  8 13:24:02.524: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  8 13:24:02.530: INFO: Number of nodes with available pods: 0
Mar  8 13:24:02.530: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:03.533: INFO: Number of nodes with available pods: 0
Mar  8 13:24:03.533: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:04.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:04.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:05.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:05.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:06.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:06.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:07.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:07.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:08.533: INFO: Number of nodes with available pods: 0
Mar  8 13:24:08.533: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:09.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:09.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:10.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:10.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:11.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:11.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:12.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:12.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:13.533: INFO: Number of nodes with available pods: 0
Mar  8 13:24:13.533: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:14.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:14.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:15.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:15.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:16.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:16.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:17.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:17.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:18.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:18.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:19.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:19.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:20.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:20.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:21.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:21.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:22.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:22.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:23.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:23.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:24.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:24.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:25.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:25.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:26.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:26.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:27.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:27.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:28.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:28.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:29.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:29.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:30.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:30.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:31.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:31.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:32.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:32.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:33.533: INFO: Number of nodes with available pods: 0
Mar  8 13:24:33.533: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:34.533: INFO: Number of nodes with available pods: 0
Mar  8 13:24:34.533: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:35.532: INFO: Number of nodes with available pods: 0
Mar  8 13:24:35.532: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:24:36.532: INFO: Number of nodes with available pods: 1
Mar  8 13:24:36.532: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tdzkh, will wait for the garbage collector to delete the pods
Mar  8 13:24:36.593: INFO: Deleting DaemonSet.extensions daemon-set took: 3.798871ms
Mar  8 13:24:36.693: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.252903ms
Mar  8 13:25:20.495: INFO: Number of nodes with available pods: 0
Mar  8 13:25:20.495: INFO: Number of running nodes: 0, number of available pods: 0
Mar  8 13:25:20.498: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tdzkh/daemonsets","resourceVersion":"7517"},"items":null}

Mar  8 13:25:20.499: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tdzkh/pods","resourceVersion":"7517"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:25:20.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tdzkh" for this suite.
Mar  8 13:25:26.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:25:26.543: INFO: namespace: e2e-tests-daemonsets-tdzkh, resource: bindings, ignored listing per whitelist
Mar  8 13:25:26.585: INFO: namespace e2e-tests-daemonsets-tdzkh deletion completed in 6.069240824s

â€¢ [SLOW TEST:89.144 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:25:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:25:26.628592      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5s2dn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a2c7dab5-41a5-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:25:26.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-5s2dn" to be "success or failure"
Mar  8 13:25:26.804: INFO: Pod "pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025997ms
Mar  8 13:25:28.806: INFO: Pod "pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007211782s
STEP: Saw pod success
Mar  8 13:25:28.806: INFO: Pod "pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:25:28.807: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:25:28.817: INFO: Waiting for pod pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:25:28.820: INFO: Pod pod-configmaps-a2c8322e-41a5-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:25:28.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5s2dn" for this suite.
Mar  8 13:25:34.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:25:34.871: INFO: namespace: e2e-tests-configmap-5s2dn, resource: bindings, ignored listing per whitelist
Mar  8 13:25:34.882: INFO: namespace e2e-tests-configmap-5s2dn deletion completed in 6.06034207s

â€¢ [SLOW TEST:8.297 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:25:34.882: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:25:34.910729      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-ctxwc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:25:39.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-ctxwc" for this suite.
Mar  8 13:26:23.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:26:23.141: INFO: namespace: e2e-tests-kubelet-test-ctxwc, resource: bindings, ignored listing per whitelist
Mar  8 13:26:23.159: INFO: namespace e2e-tests-kubelet-test-ctxwc deletion completed in 44.061406444s

â€¢ [SLOW TEST:48.277 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:26:23.159: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:26:23.186810      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5nmhq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar  8 13:26:23.310: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  8 13:26:23.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:23.484: INFO: stderr: ""
Mar  8 13:26:23.485: INFO: stdout: "service/redis-slave created\n"
Mar  8 13:26:23.485: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  8 13:26:23.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:23.650: INFO: stderr: ""
Mar  8 13:26:23.650: INFO: stdout: "service/redis-master created\n"
Mar  8 13:26:23.650: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  8 13:26:23.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:23.804: INFO: stderr: ""
Mar  8 13:26:23.804: INFO: stdout: "service/frontend created\n"
Mar  8 13:26:23.804: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  8 13:26:23.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:23.963: INFO: stderr: ""
Mar  8 13:26:23.963: INFO: stdout: "deployment.extensions/frontend created\n"
Mar  8 13:26:23.963: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  8 13:26:23.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:24.126: INFO: stderr: ""
Mar  8 13:26:24.126: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar  8 13:26:24.126: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  8 13:26:24.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:24.285: INFO: stderr: ""
Mar  8 13:26:24.285: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar  8 13:26:24.285: INFO: Waiting for all frontend pods to be Running.
Mar  8 13:26:39.336: INFO: Waiting for frontend to serve content.
Mar  8 13:26:46.756: INFO: Trying to add a new entry to the guestbook.
Mar  8 13:26:46.769: INFO: Verifying that added entry can be retrieved.
Mar  8 13:26:46.778: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Mar  8 13:26:51.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:51.882: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:51.882: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  8 13:26:51.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:51.972: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:51.972: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  8 13:26:51.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:52.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:52.065: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  8 13:26:52.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:52.161: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:52.161: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  8 13:26:52.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:52.298: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:52.298: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  8 13:26:52.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5nmhq'
Mar  8 13:26:52.418: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:26:52.418: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:26:52.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5nmhq" for this suite.
Mar  8 13:27:32.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:27:32.469: INFO: namespace: e2e-tests-kubectl-5nmhq, resource: bindings, ignored listing per whitelist
Mar  8 13:27:32.489: INFO: namespace e2e-tests-kubectl-5nmhq deletion completed in 40.067674777s

â€¢ [SLOW TEST:69.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:27:32.489: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:27:32.516894      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-kspzh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-jq2f6
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Mar  8 13:27:35.005: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-k6n7z
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:27:59.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-kspzh" for this suite.
Mar  8 13:28:05.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:28:05.181: INFO: namespace: e2e-tests-namespaces-kspzh, resource: bindings, ignored listing per whitelist
Mar  8 13:28:05.208: INFO: namespace e2e-tests-namespaces-kspzh deletion completed in 6.061777077s
STEP: Destroying namespace "e2e-tests-nsdeletetest-jq2f6" for this suite.
Mar  8 13:28:05.209: INFO: Namespace e2e-tests-nsdeletetest-jq2f6 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-k6n7z" for this suite.
Mar  8 13:28:11.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:28:11.224: INFO: namespace: e2e-tests-nsdeletetest-k6n7z, resource: bindings, ignored listing per whitelist
Mar  8 13:28:11.265: INFO: namespace e2e-tests-nsdeletetest-k6n7z deletion completed in 6.055928584s

â€¢ [SLOW TEST:38.777 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:28:11.266: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:28:11.293052      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4skhz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-cpjp
STEP: Creating a pod to test atomic-volume-subpath
Mar  8 13:28:11.423: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cpjp" in namespace "e2e-tests-subpath-4skhz" to be "success or failure"
Mar  8 13:28:11.426: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066042ms
Mar  8 13:28:13.428: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 2.005414246s
Mar  8 13:28:15.430: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 4.007702388s
Mar  8 13:28:17.432: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 6.009885602s
Mar  8 13:28:19.435: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 8.012358838s
Mar  8 13:28:21.437: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 10.014744366s
Mar  8 13:28:23.440: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 12.01710322s
Mar  8 13:28:25.442: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 14.019456329s
Mar  8 13:28:27.444: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 16.021763164s
Mar  8 13:28:29.778: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 18.355630505s
Mar  8 13:28:31.780: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Running", Reason="", readiness=false. Elapsed: 20.357782806s
Mar  8 13:28:33.783: INFO: Pod "pod-subpath-test-projected-cpjp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.360057837s
STEP: Saw pod success
Mar  8 13:28:33.783: INFO: Pod "pod-subpath-test-projected-cpjp" satisfied condition "success or failure"
Mar  8 13:28:33.784: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-subpath-test-projected-cpjp container test-container-subpath-projected-cpjp: <nil>
STEP: delete the pod
Mar  8 13:28:33.800: INFO: Waiting for pod pod-subpath-test-projected-cpjp to disappear
Mar  8 13:28:33.804: INFO: Pod pod-subpath-test-projected-cpjp no longer exists
STEP: Deleting pod pod-subpath-test-projected-cpjp
Mar  8 13:28:33.804: INFO: Deleting pod "pod-subpath-test-projected-cpjp" in namespace "e2e-tests-subpath-4skhz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:28:33.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4skhz" for this suite.
Mar  8 13:28:39.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:28:39.863: INFO: namespace: e2e-tests-subpath-4skhz, resource: bindings, ignored listing per whitelist
Mar  8 13:28:39.900: INFO: namespace e2e-tests-subpath-4skhz deletion completed in 6.091642706s

â€¢ [SLOW TEST:28.635 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:28:39.901: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:28:39.927960      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-p8brg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  8 13:28:42.572: INFO: Successfully updated pod "pod-update-15f8d3b2-41a6-11e9-be5e-ae5bc1be57d7"
STEP: verifying the updated pod is in kubernetes
Mar  8 13:28:42.576: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:28:42.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-p8brg" for this suite.
Mar  8 13:29:04.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:29:05.071: INFO: namespace: e2e-tests-pods-p8brg, resource: bindings, ignored listing per whitelist
Mar  8 13:29:05.074: INFO: namespace e2e-tests-pods-p8brg deletion completed in 22.49581843s

â€¢ [SLOW TEST:25.174 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:29:05.074: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:29:05.110549      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-29dnr
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-24fba756-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating secret with name s-test-opt-upd-24fba7a4-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-24fba756-41a6-11e9-be5e-ae5bc1be57d7
STEP: Updating secret s-test-opt-upd-24fba7a4-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating secret with name s-test-opt-create-24fba7c3-41a6-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:29:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-29dnr" for this suite.
Mar  8 13:29:33.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:29:33.409: INFO: namespace: e2e-tests-projected-29dnr, resource: bindings, ignored listing per whitelist
Mar  8 13:29:33.438: INFO: namespace e2e-tests-projected-29dnr deletion completed in 22.14650045s

â€¢ [SLOW TEST:28.364 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:29:33.438: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:29:33.470759      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-m4hhp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-35e2c0a0-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:29:33.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-m4hhp" to be "success or failure"
Mar  8 13:29:33.602: INFO: Pod "pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185134ms
Mar  8 13:29:35.604: INFO: Pod "pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005410858s
STEP: Saw pod success
Mar  8 13:29:35.604: INFO: Pod "pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:29:35.605: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:29:35.624: INFO: Waiting for pod pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:29:35.625: INFO: Pod pod-projected-configmaps-35e315ad-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:29:35.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m4hhp" for this suite.
Mar  8 13:29:41.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:29:41.644: INFO: namespace: e2e-tests-projected-m4hhp, resource: bindings, ignored listing per whitelist
Mar  8 13:29:41.683: INFO: namespace e2e-tests-projected-m4hhp deletion completed in 6.055913707s

â€¢ [SLOW TEST:8.245 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:29:41.684: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:29:41.710741      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-sjcr2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  8 13:29:45.857: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  8 13:29:45.861: INFO: Pod pod-with-poststart-http-hook still exists
Mar  8 13:29:47.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  8 13:29:47.863: INFO: Pod pod-with-poststart-http-hook still exists
Mar  8 13:29:49.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  8 13:29:49.863: INFO: Pod pod-with-poststart-http-hook still exists
Mar  8 13:29:51.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  8 13:29:51.863: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:29:51.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-sjcr2" for this suite.
Mar  8 13:30:13.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:30:13.907: INFO: namespace: e2e-tests-container-lifecycle-hook-sjcr2, resource: bindings, ignored listing per whitelist
Mar  8 13:30:13.924: INFO: namespace e2e-tests-container-lifecycle-hook-sjcr2 deletion completed in 22.058158596s

â€¢ [SLOW TEST:32.240 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:30:13.924: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:30:13.951911      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7g8cb
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-4e040085-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating configMap with name cm-test-opt-upd-4e0400c8-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4e040085-41a6-11e9-be5e-ae5bc1be57d7
STEP: Updating configmap cm-test-opt-upd-4e0400c8-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating configMap with name cm-test-opt-create-4e0400e6-41a6-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:30:18.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7g8cb" for this suite.
Mar  8 13:30:40.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:30:40.177: INFO: namespace: e2e-tests-projected-7g8cb, resource: bindings, ignored listing per whitelist
Mar  8 13:30:40.178: INFO: namespace e2e-tests-projected-7g8cb deletion completed in 22.057228177s

â€¢ [SLOW TEST:26.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:30:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:30:40.206290      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-zwmkl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:30:40.684: INFO: Creating deployment "nginx-deployment"
Mar  8 13:30:40.689: INFO: Waiting for observed generation 1
Mar  8 13:30:42.693: INFO: Waiting for all required pods to come up
Mar  8 13:30:42.696: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  8 13:30:44.701: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  8 13:30:44.704: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  8 13:30:44.709: INFO: Updating deployment nginx-deployment
Mar  8 13:30:44.709: INFO: Waiting for observed generation 2
Mar  8 13:30:46.713: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  8 13:30:46.714: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  8 13:30:46.715: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  8 13:30:46.719: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  8 13:30:46.719: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  8 13:30:46.720: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  8 13:30:46.723: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  8 13:30:46.723: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  8 13:30:46.726: INFO: Updating deployment nginx-deployment
Mar  8 13:30:46.726: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  8 13:30:46.740: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  8 13:30:46.756: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  8 13:30:48.764: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwmkl/deployments/nginx-deployment,UID:5de059d9-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9040,Generation:3,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-03-08 13:30:46 +0000 UTC 2019-03-08 13:30:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-08 13:30:46 +0000 UTC 2019-03-08 13:30:40 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  8 13:30:48.765: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwmkl/replicasets/nginx-deployment-65bbdb5f8,UID:60464cc6-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9037,Generation:3,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5de059d9-41a6-11e9-8665-02fd46f865f2 0xc001c90e47 0xc001c90e48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 13:30:48.766: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  8 13:30:48.766: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwmkl/replicasets/nginx-deployment-555b55d965,UID:5de1ad2e-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9026,Generation:3,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5de059d9-41a6-11e9-8665-02fd46f865f2 0xc001c90d87 0xc001c90d88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  8 13:30:48.769: INFO: Pod "nginx-deployment-555b55d965-22tz4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-22tz4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-22tz4,UID:61812682-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9022,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c917b0 0xc001c917b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.769: INFO: Pod "nginx-deployment-555b55d965-4b86z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4b86z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-4b86z,UID:5de5c780-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8868,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c918a0 0xc001c918a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:10.20.192.8,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://b258a212429c3248e229ba67c663a4a2507bb1869f319ab184431217bbf6028c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-4nrg7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4nrg7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-4nrg7,UID:617d9944-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9060,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c919e0 0xc001c919e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-5jr66" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5jr66,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-5jr66,UID:61811a91-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9082,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c91b17 0xc001c91b18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-884zj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-884zj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-884zj,UID:617be9d8-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9014,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c91c77 0xc001c91c78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-bnvxg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bnvxg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-bnvxg,UID:617a7c64-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8993,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c91de7 0xc001c91de8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c91e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-cb65h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cb65h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-cb65h,UID:617d9b31-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9104,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001c91f37 0xc001c91f38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c91fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-jtr2f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jtr2f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-jtr2f,UID:5de34a8b-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8897,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc0c7 0xc001ecc0c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.3,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:41 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://bebda054acd23a76bfb742ad138ff3fb6c010d70f52b249c9058e46b66369c09}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-m6vkf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-m6vkf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-m6vkf,UID:617d91ff-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9064,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc210 0xc001ecc211}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-mg7r2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mg7r2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-mg7r2,UID:6180f238-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9031,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc347 0xc001ecc348}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-mrkxn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mrkxn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-mrkxn,UID:617d695b-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9084,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc487 0xc001ecc488}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.770: INFO: Pod "nginx-deployment-555b55d965-mtn8n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mtn8n,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-mtn8n,UID:5de7e4f6-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8862,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc5c7 0xc001ecc5c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:10.20.192.9,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://63edfae1a0d2a96cc08c6b75332b8de153600c988b1200e173278bc33eb78cea}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-ncxpv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ncxpv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-ncxpv,UID:5de44b7d-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8866,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc710 0xc001ecc711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:10.20.192.7,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://df8712978ef74b8c97bd920188756d8310da131c5849b9b728f16944257a1a61}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-pxjnr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-pxjnr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-pxjnr,UID:618128e5-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9113,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc850 0xc001ecc851}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecc8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-rc4tf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rc4tf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-rc4tf,UID:5de45648-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8878,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecc987 0xc001ecc988}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecc9f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecca20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.6,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://304abccfc6c1631bd3fa5db4403036f03c8435a56d431a41b8a9a40542a8f555}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-rd5ck" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rd5ck,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-rd5ck,UID:5de5d14b-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8885,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001eccae0 0xc001eccae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eccb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eccb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.4,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://3d6c730368df76e58c6735064e8ccf98ffb16eb0de6b51880fca721be85dafe2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-v7n8q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v7n8q,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-v7n8q,UID:61812f81-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9017,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001eccc30 0xc001eccc31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eccc90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecccb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.771: INFO: Pod "nginx-deployment-555b55d965-vgv4s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vgv4s,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-vgv4s,UID:5de5dd8c-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8894,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001eccd40 0xc001eccd41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eccda0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eccdc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.5,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://922b2552fc937cd542bb780ce896382885d5eff01bf5e495be66d5a14aeac9c7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-555b55d965-x8jgb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x8jgb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-x8jgb,UID:617b89eb-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9020,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001ecce90 0xc001ecce91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eccef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eccf10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-555b55d965-xg6h2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xg6h2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-555b55d965-xg6h2,UID:5de7dae2-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8876,Generation:0,CreationTimestamp:2019-03-08 13:30:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5de1ad2e-41a6-11e9-8665-02fd46f865f2 0xc001eccfc7 0xc001eccfc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.7,StartTime:2019-03-08 13:30:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 13:30:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://ccbac49ef2ceaf28837087d370a00181749e69a51bbe7006e1d8510add8714d5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-2d59v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2d59v,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-2d59v,UID:604e8d82-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8946,Generation:0,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd120 0xc001ecd121}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-4n2wm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4n2wm,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-4n2wm,UID:618134f8-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9066,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd280 0xc001ecd281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd2f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-526fr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-526fr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-526fr,UID:61811242-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9021,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd3e0 0xc001ecd3e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-cd46s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cd46s,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-cd46s,UID:60476cd3-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9103,Generation:0,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd4e0 0xc001ecd4e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.9,StartTime:2019-03-08 13:30:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-chrjk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-chrjk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-chrjk,UID:61812d12-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9052,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd660 0xc001ecd661}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.772: INFO: Pod "nginx-deployment-65bbdb5f8-d8f8v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-d8f8v,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-d8f8v,UID:60506b31-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8960,Generation:0,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd7b0 0xc001ecd7b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-gc7ml" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gc7ml,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-gc7ml,UID:604770af-41a6-11e9-8665-02fd46f865f2,ResourceVersion:8925,Generation:0,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecd910 0xc001ecd911}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecd980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecd9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-l5554" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l5554,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-l5554,UID:617d8ee4-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9068,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecda70 0xc001ecda71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecdae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecdb00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-l9w4g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l9w4g,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-l9w4g,UID:617da0c0-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9098,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecdbd0 0xc001ecdbd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecdc40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecdc60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-nf6r7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nf6r7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-nf6r7,UID:617b245f-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9059,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecdd30 0xc001ecdd31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecdda0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecddc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-nxjww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nxjww,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-nxjww,UID:6187332f-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9097,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecde80 0xc001ecde81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ecdf00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ecdf20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:,StartTime:2019-03-08 13:30:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-rbrh4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rbrh4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-rbrh4,UID:6046a081-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9112,Generation:0,CreationTimestamp:2019-03-08 13:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ecdfe0 0xc001ecdfe1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ed4050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ed4070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.8,StartTime:2019-03-08 13:30:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  8 13:30:48.773: INFO: Pod "nginx-deployment-65bbdb5f8-s5kch" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-s5kch,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-zwmkl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwmkl/pods/nginx-deployment-65bbdb5f8-s5kch,UID:6181218c-41a6-11e9-8665-02fd46f865f2,ResourceVersion:9024,Generation:0,CreationTimestamp:2019-03-08 13:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 60464cc6-41a6-11e9-8665-02fd46f865f2 0xc001ed4150 0xc001ed4151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zp25h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zp25h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zp25h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ed41c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ed41e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:30:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:30:48.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-zwmkl" for this suite.
Mar  8 13:30:54.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:30:54.837: INFO: namespace: e2e-tests-deployment-zwmkl, resource: bindings, ignored listing per whitelist
Mar  8 13:30:54.858: INFO: namespace e2e-tests-deployment-zwmkl deletion completed in 6.082496277s

â€¢ [SLOW TEST:14.680 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:30:54.858: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:30:54.933184      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mtghx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-6673eea5-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:30:55.724: INFO: Waiting up to 5m0s for pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-mtghx" to be "success or failure"
Mar  8 13:30:55.734: INFO: Pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.601686ms
Mar  8 13:30:57.736: INFO: Pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011749898s
Mar  8 13:30:59.738: INFO: Pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013960747s
Mar  8 13:31:01.741: INFO: Pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016253688s
STEP: Saw pod success
Mar  8 13:31:01.741: INFO: Pod "pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:31:01.742: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:31:01.752: INFO: Waiting for pod pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:31:01.755: INFO: Pod pod-configmaps-667440a5-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:31:01.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mtghx" for this suite.
Mar  8 13:31:07.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:31:07.790: INFO: namespace: e2e-tests-configmap-mtghx, resource: bindings, ignored listing per whitelist
Mar  8 13:31:07.817: INFO: namespace e2e-tests-configmap-mtghx deletion completed in 6.058881333s

â€¢ [SLOW TEST:12.959 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:31:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:31:07.845337      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-4f7ws
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:31:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:31:09.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-4f7ws" for this suite.
Mar  8 13:31:15.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:31:15.062: INFO: namespace: e2e-tests-custom-resource-definition-4f7ws, resource: bindings, ignored listing per whitelist
Mar  8 13:31:15.063: INFO: namespace e2e-tests-custom-resource-definition-4f7ws deletion completed in 6.060239686s

â€¢ [SLOW TEST:7.245 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:31:15.063: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:31:15.092594      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bz79l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Mar  8 13:31:15.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:15.597: INFO: stderr: ""
Mar  8 13:31:15.597: INFO: stdout: "pod/pause created\n"
Mar  8 13:31:15.597: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  8 13:31:15.597: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-bz79l" to be "running and ready"
Mar  8 13:31:15.601: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078992ms
Mar  8 13:31:17.604: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006418735s
Mar  8 13:31:17.604: INFO: Pod "pause" satisfied condition "running and ready"
Mar  8 13:31:17.604: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  8 13:31:17.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:17.689: INFO: stderr: ""
Mar  8 13:31:17.689: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  8 13:31:17.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pod pause -L testing-label --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:17.767: INFO: stderr: ""
Mar  8 13:31:17.768: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  8 13:31:17.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 label pods pause testing-label- --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:17.849: INFO: stderr: ""
Mar  8 13:31:17.849: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  8 13:31:17.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pod pause -L testing-label --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:17.926: INFO: stderr: ""
Mar  8 13:31:17.926: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Mar  8 13:31:17.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:18.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:31:18.013: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  8 13:31:18.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-bz79l'
Mar  8 13:31:18.145: INFO: stderr: "No resources found.\n"
Mar  8 13:31:18.145: INFO: stdout: ""
Mar  8 13:31:18.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -l name=pause --namespace=e2e-tests-kubectl-bz79l -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  8 13:31:18.241: INFO: stderr: ""
Mar  8 13:31:18.241: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:31:18.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bz79l" for this suite.
Mar  8 13:31:24.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:31:24.318: INFO: namespace: e2e-tests-kubectl-bz79l, resource: bindings, ignored listing per whitelist
Mar  8 13:31:24.353: INFO: namespace e2e-tests-kubectl-bz79l deletion completed in 6.109459709s

â€¢ [SLOW TEST:9.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:31:24.354: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:31:24.382171      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-479kx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-479kx;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-479kx;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-479kx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 10.174.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.174.10_udp@PTR;check="$$(dig +tcp +noall +answer +search 10.174.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.174.10_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-479kx;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-479kx;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-479kx.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-479kx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-479kx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 10.174.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.174.10_udp@PTR;check="$$(dig +tcp +noall +answer +search 10.174.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.174.10_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  8 13:31:36.544: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.546: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.548: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-479kx from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.550: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-479kx from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.551: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.553: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.554: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.556: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.558: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.560: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.561: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.563: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.565: INFO: Unable to read 10.10.174.10_udp@PTR from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.566: INFO: Unable to read 10.10.174.10_tcp@PTR from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.568: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.569: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.573: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-479kx from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.575: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-479kx from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.577: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.579: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.580: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.582: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.584: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.585: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.587: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.589: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.590: INFO: Unable to read 10.10.174.10_udp@PTR from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.592: INFO: Unable to read 10.10.174.10_tcp@PTR from pod e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7)
Mar  8 13:31:36.592: INFO: Lookups using e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-479kx wheezy_tcp@dns-test-service.e2e-tests-dns-479kx wheezy_udp@dns-test-service.e2e-tests-dns-479kx.svc wheezy_tcp@dns-test-service.e2e-tests-dns-479kx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.10.174.10_udp@PTR 10.10.174.10_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-479kx jessie_tcp@dns-test-service.e2e-tests-dns-479kx jessie_udp@dns-test-service.e2e-tests-dns-479kx.svc jessie_tcp@dns-test-service.e2e-tests-dns-479kx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-479kx.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-479kx.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.10.174.10_udp@PTR 10.10.174.10_tcp@PTR]

Mar  8 13:31:41.683: INFO: DNS probes using e2e-tests-dns-479kx/dns-test-7800f98f-41a6-11e9-be5e-ae5bc1be57d7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:31:41.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-479kx" for this suite.
Mar  8 13:31:47.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:31:47.759: INFO: namespace: e2e-tests-dns-479kx, resource: bindings, ignored listing per whitelist
Mar  8 13:31:47.789: INFO: namespace e2e-tests-dns-479kx deletion completed in 6.061328504s

â€¢ [SLOW TEST:23.435 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:31:47.789: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:31:47.828587      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-8qk4c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar  8 13:31:47.955: INFO: Waiting up to 5m0s for pod "var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-var-expansion-8qk4c" to be "success or failure"
Mar  8 13:31:47.965: INFO: Pod "var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.823965ms
Mar  8 13:31:49.967: INFO: Pod "var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011852699s
STEP: Saw pod success
Mar  8 13:31:49.967: INFO: Pod "var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:31:49.968: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 13:31:49.981: INFO: Waiting for pod var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:31:49.983: INFO: Pod var-expansion-85f82cc6-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:31:49.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-8qk4c" for this suite.
Mar  8 13:31:55.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:31:56.041: INFO: namespace: e2e-tests-var-expansion-8qk4c, resource: bindings, ignored listing per whitelist
Mar  8 13:31:56.053: INFO: namespace e2e-tests-var-expansion-8qk4c deletion completed in 6.066570525s

â€¢ [SLOW TEST:8.264 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:31:56.053: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:31:56.080669      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-j8r5x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  8 13:31:56.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:31:56.349: INFO: stderr: ""
Mar  8 13:31:56.349: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 13:31:56.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:31:56.433: INFO: stderr: ""
Mar  8 13:31:56.433: INFO: stdout: "update-demo-nautilus-w6v2c update-demo-nautilus-xtf4c "
Mar  8 13:31:56.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-w6v2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:31:56.512: INFO: stderr: ""
Mar  8 13:31:56.512: INFO: stdout: ""
Mar  8 13:31:56.512: INFO: update-demo-nautilus-w6v2c is created but not running
Mar  8 13:32:01.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:01.596: INFO: stderr: ""
Mar  8 13:32:01.596: INFO: stdout: "update-demo-nautilus-w6v2c update-demo-nautilus-xtf4c "
Mar  8 13:32:01.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-w6v2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:01.676: INFO: stderr: ""
Mar  8 13:32:01.676: INFO: stdout: "true"
Mar  8 13:32:01.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-w6v2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:01.758: INFO: stderr: ""
Mar  8 13:32:01.758: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:32:01.758: INFO: validating pod update-demo-nautilus-w6v2c
Mar  8 13:32:01.762: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:32:01.762: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:32:01.762: INFO: update-demo-nautilus-w6v2c is verified up and running
Mar  8 13:32:01.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:01.841: INFO: stderr: ""
Mar  8 13:32:01.841: INFO: stdout: "true"
Mar  8 13:32:01.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:01.936: INFO: stderr: ""
Mar  8 13:32:01.936: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:32:01.936: INFO: validating pod update-demo-nautilus-xtf4c
Mar  8 13:32:01.940: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:32:01.940: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:32:01.940: INFO: update-demo-nautilus-xtf4c is verified up and running
STEP: scaling down the replication controller
Mar  8 13:32:01.941: INFO: scanned /root for discovery docs: <nil>
Mar  8 13:32:01.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:03.496: INFO: stderr: ""
Mar  8 13:32:03.496: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 13:32:03.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:03.576: INFO: stderr: ""
Mar  8 13:32:03.576: INFO: stdout: "update-demo-nautilus-w6v2c update-demo-nautilus-xtf4c "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  8 13:32:08.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:08.655: INFO: stderr: ""
Mar  8 13:32:08.655: INFO: stdout: "update-demo-nautilus-w6v2c update-demo-nautilus-xtf4c "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  8 13:32:13.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:13.739: INFO: stderr: ""
Mar  8 13:32:13.739: INFO: stdout: "update-demo-nautilus-xtf4c "
Mar  8 13:32:13.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:13.819: INFO: stderr: ""
Mar  8 13:32:13.819: INFO: stdout: "true"
Mar  8 13:32:13.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:13.901: INFO: stderr: ""
Mar  8 13:32:13.901: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:32:13.901: INFO: validating pod update-demo-nautilus-xtf4c
Mar  8 13:32:13.904: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:32:13.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:32:13.904: INFO: update-demo-nautilus-xtf4c is verified up and running
STEP: scaling up the replication controller
Mar  8 13:32:13.905: INFO: scanned /root for discovery docs: <nil>
Mar  8 13:32:13.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.057: INFO: stderr: ""
Mar  8 13:32:15.057: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 13:32:15.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.142: INFO: stderr: ""
Mar  8 13:32:15.142: INFO: stdout: "update-demo-nautilus-kdlwg update-demo-nautilus-xtf4c "
Mar  8 13:32:15.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-kdlwg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.228: INFO: stderr: ""
Mar  8 13:32:15.228: INFO: stdout: "true"
Mar  8 13:32:15.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-kdlwg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.311: INFO: stderr: ""
Mar  8 13:32:15.311: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:32:15.311: INFO: validating pod update-demo-nautilus-kdlwg
Mar  8 13:32:15.316: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:32:15.316: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:32:15.316: INFO: update-demo-nautilus-kdlwg is verified up and running
Mar  8 13:32:15.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.396: INFO: stderr: ""
Mar  8 13:32:15.396: INFO: stdout: "true"
Mar  8 13:32:15.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xtf4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.475: INFO: stderr: ""
Mar  8 13:32:15.475: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:32:15.475: INFO: validating pod update-demo-nautilus-xtf4c
Mar  8 13:32:15.477: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:32:15.477: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:32:15.477: INFO: update-demo-nautilus-xtf4c is verified up and running
STEP: using delete to clean up resources
Mar  8 13:32:15.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.552: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:32:15.552: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  8 13:32:15.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-j8r5x'
Mar  8 13:32:15.648: INFO: stderr: "No resources found.\n"
Mar  8 13:32:15.648: INFO: stdout: ""
Mar  8 13:32:15.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -l name=update-demo --namespace=e2e-tests-kubectl-j8r5x -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  8 13:32:15.728: INFO: stderr: ""
Mar  8 13:32:15.728: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:32:15.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j8r5x" for this suite.
Mar  8 13:32:21.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:32:21.746: INFO: namespace: e2e-tests-kubectl-j8r5x, resource: bindings, ignored listing per whitelist
Mar  8 13:32:21.789: INFO: namespace e2e-tests-kubectl-j8r5x deletion completed in 6.058054989s

â€¢ [SLOW TEST:25.736 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:32:21.789: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:32:21.825227      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lg552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:32:21.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 version'
Mar  8 13:32:22.007: INFO: stderr: ""
Mar  8 13:32:22.007: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:32:22.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lg552" for this suite.
Mar  8 13:32:28.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:32:28.073: INFO: namespace: e2e-tests-kubectl-lg552, resource: bindings, ignored listing per whitelist
Mar  8 13:32:28.080: INFO: namespace e2e-tests-kubectl-lg552 deletion completed in 6.070776659s

â€¢ [SLOW TEST:6.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:32:28.081: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:32:28.113465      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gvlj5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9dfadafd-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:32:28.239: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-gvlj5" to be "success or failure"
Mar  8 13:32:28.243: INFO: Pod "pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.552721ms
Mar  8 13:32:30.245: INFO: Pod "pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005707462s
STEP: Saw pod success
Mar  8 13:32:30.245: INFO: Pod "pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:32:30.247: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:32:30.259: INFO: Waiting for pod pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:32:30.260: INFO: Pod pod-projected-configmaps-9dfb2d4f-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:32:30.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gvlj5" for this suite.
Mar  8 13:32:36.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:32:36.306: INFO: namespace: e2e-tests-projected-gvlj5, resource: bindings, ignored listing per whitelist
Mar  8 13:32:36.324: INFO: namespace e2e-tests-projected-gvlj5 deletion completed in 6.060806102s

â€¢ [SLOW TEST:8.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:32:36.325: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:32:36.355787      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-8kl64
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8kl64
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-8kl64
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-8kl64
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-8kl64
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-8kl64
Mar  8 13:32:40.514: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8kl64, name: ss-0, uid: a54bca44-41a6-11e9-8665-02fd46f865f2, status phase: Pending. Waiting for statefulset controller to delete.
Mar  8 13:32:41.094: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8kl64, name: ss-0, uid: a54bca44-41a6-11e9-8665-02fd46f865f2, status phase: Failed. Waiting for statefulset controller to delete.
Mar  8 13:32:41.099: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8kl64, name: ss-0, uid: a54bca44-41a6-11e9-8665-02fd46f865f2, status phase: Failed. Waiting for statefulset controller to delete.
Mar  8 13:32:41.102: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-8kl64
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-8kl64
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-8kl64 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  8 13:32:43.118: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8kl64
Mar  8 13:32:43.120: INFO: Scaling statefulset ss to 0
Mar  8 13:32:53.132: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:32:53.134: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:32:53.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8kl64" for this suite.
Mar  8 13:32:59.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:32:59.173: INFO: namespace: e2e-tests-statefulset-8kl64, resource: bindings, ignored listing per whitelist
Mar  8 13:32:59.217: INFO: namespace e2e-tests-statefulset-8kl64 deletion completed in 6.067313534s

â€¢ [SLOW TEST:22.893 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:32:59.218: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:32:59.245171      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5v4xk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  8 13:32:59.372: INFO: Waiting up to 5m0s for pod "pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-5v4xk" to be "success or failure"
Mar  8 13:32:59.378: INFO: Pod "pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043922ms
Mar  8 13:33:01.381: INFO: Pod "pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008250896s
STEP: Saw pod success
Mar  8 13:33:01.381: INFO: Pod "pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:33:01.382: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:33:01.393: INFO: Waiting for pod pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:33:01.395: INFO: Pod pod-b0898cde-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:33:01.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5v4xk" for this suite.
Mar  8 13:33:07.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:33:07.423: INFO: namespace: e2e-tests-emptydir-5v4xk, resource: bindings, ignored listing per whitelist
Mar  8 13:33:07.454: INFO: namespace e2e-tests-emptydir-5v4xk deletion completed in 6.056257188s

â€¢ [SLOW TEST:8.236 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:33:07.454: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:33:07.481747      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-jxgnx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0308 13:33:38.187574      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 13:33:38.187: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:33:38.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jxgnx" for this suite.
Mar  8 13:33:44.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:33:44.243: INFO: namespace: e2e-tests-gc-jxgnx, resource: bindings, ignored listing per whitelist
Mar  8 13:33:44.248: INFO: namespace e2e-tests-gc-jxgnx deletion completed in 6.058463545s

â€¢ [SLOW TEST:36.794 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:33:44.248: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:33:44.275661      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-dd4j7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:33:44.430: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cb62c722-41a6-11e9-8665-02fd46f865f2", Controller:(*bool)(0xc001b9086e), BlockOwnerDeletion:(*bool)(0xc001b9086f)}}
Mar  8 13:33:44.437: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cb61f5d8-41a6-11e9-8665-02fd46f865f2", Controller:(*bool)(0xc0016fd94e), BlockOwnerDeletion:(*bool)(0xc0016fd94f)}}
Mar  8 13:33:44.441: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"cb624b7c-41a6-11e9-8665-02fd46f865f2", Controller:(*bool)(0xc0016fdb96), BlockOwnerDeletion:(*bool)(0xc0016fdb97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:33:49.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dd4j7" for this suite.
Mar  8 13:33:55.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:33:55.499: INFO: namespace: e2e-tests-gc-dd4j7, resource: bindings, ignored listing per whitelist
Mar  8 13:33:55.507: INFO: namespace e2e-tests-gc-dd4j7 deletion completed in 6.057892012s

â€¢ [SLOW TEST:11.259 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:33:55.507: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:33:55.534516      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-grw4z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-grw4z
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-grw4z
STEP: Deleting pre-stop pod
Mar  8 13:34:06.691: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:34:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-grw4z" for this suite.
Mar  8 13:34:44.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:34:44.748: INFO: namespace: e2e-tests-prestop-grw4z, resource: bindings, ignored listing per whitelist
Mar  8 13:34:44.763: INFO: namespace e2e-tests-prestop-grw4z deletion completed in 38.063648091s

â€¢ [SLOW TEST:49.255 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:34:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:34:44.791402      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kbbrn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-kbbrn/secret-test-ef7291b4-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:34:44.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-kbbrn" to be "success or failure"
Mar  8 13:34:44.924: INFO: Pod "pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961782ms
Mar  8 13:34:47.081: INFO: Pod "pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.159633429s
STEP: Saw pod success
Mar  8 13:34:47.081: INFO: Pod "pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:34:47.082: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7 container env-test: <nil>
STEP: delete the pod
Mar  8 13:34:47.093: INFO: Waiting for pod pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:34:47.095: INFO: Pod pod-configmaps-ef72dae2-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:34:47.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kbbrn" for this suite.
Mar  8 13:34:53.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:34:53.158: INFO: namespace: e2e-tests-secrets-kbbrn, resource: bindings, ignored listing per whitelist
Mar  8 13:34:53.162: INFO: namespace e2e-tests-secrets-kbbrn deletion completed in 6.064525217s

â€¢ [SLOW TEST:8.399 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:34:53.162: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:34:53.191565      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rffl7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f473f179-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:34:53.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-rffl7" to be "success or failure"
Mar  8 13:34:53.326: INFO: Pod "pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.851683ms
Mar  8 13:34:55.329: INFO: Pod "pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005019888s
STEP: Saw pod success
Mar  8 13:34:55.329: INFO: Pod "pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:34:55.330: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:34:55.342: INFO: Waiting for pod pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:34:55.343: INFO: Pod pod-projected-configmaps-f4746660-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:34:55.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rffl7" for this suite.
Mar  8 13:35:01.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:35:01.367: INFO: namespace: e2e-tests-projected-rffl7, resource: bindings, ignored listing per whitelist
Mar  8 13:35:01.407: INFO: namespace e2e-tests-projected-rffl7 deletion completed in 6.061803869s

â€¢ [SLOW TEST:8.245 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:35:01.408: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:35:01.435602      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-626g4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-f95e44a3-41a6-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:35:01.566: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-626g4" to be "success or failure"
Mar  8 13:35:01.568: INFO: Pod "pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315108ms
Mar  8 13:35:03.571: INFO: Pod "pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004979284s
STEP: Saw pod success
Mar  8 13:35:03.571: INFO: Pod "pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:35:03.572: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:35:03.582: INFO: Waiting for pod pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:35:03.585: INFO: Pod pod-projected-secrets-f95e936e-41a6-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:35:03.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-626g4" for this suite.
Mar  8 13:35:09.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:35:09.611: INFO: namespace: e2e-tests-projected-626g4, resource: bindings, ignored listing per whitelist
Mar  8 13:35:09.648: INFO: namespace e2e-tests-projected-626g4 deletion completed in 6.059757891s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:35:09.648: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:35:09.680946      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-nvhgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0308 13:35:49.827208      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 13:35:49.827: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:35:49.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nvhgt" for this suite.
Mar  8 13:35:55.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:35:55.900: INFO: namespace: e2e-tests-gc-nvhgt, resource: bindings, ignored listing per whitelist
Mar  8 13:35:55.913: INFO: namespace e2e-tests-gc-nvhgt deletion completed in 6.081893007s

â€¢ [SLOW TEST:46.265 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:35:55.913: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:35:55.971755      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bc4gx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-bc4gx
Mar  8 13:35:58.108: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-bc4gx
STEP: checking the pod's current state and verifying that restartCount is present
Mar  8 13:35:58.109: INFO: Initial restart count of pod liveness-exec is 0
Mar  8 13:36:46.574: INFO: Restart count of pod e2e-tests-container-probe-bc4gx/liveness-exec is now 1 (48.46451517s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:36:46.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bc4gx" for this suite.
Mar  8 13:36:52.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:36:52.618: INFO: namespace: e2e-tests-container-probe-bc4gx, resource: bindings, ignored listing per whitelist
Mar  8 13:36:52.644: INFO: namespace e2e-tests-container-probe-bc4gx deletion completed in 6.057659868s

â€¢ [SLOW TEST:56.730 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:36:52.644: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:36:52.671612      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2678r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:36:52.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-2678r" to be "success or failure"
Mar  8 13:36:52.804: INFO: Pod "downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.633069ms
Mar  8 13:36:54.813: INFO: Pod "downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014792348s
STEP: Saw pod success
Mar  8 13:36:54.813: INFO: Pod "downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:36:54.815: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:36:54.825: INFO: Waiting for pod downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:36:54.828: INFO: Pod downwardapi-volume-3bab8cca-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:36:54.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2678r" for this suite.
Mar  8 13:37:00.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:00.870: INFO: namespace: e2e-tests-downward-api-2678r, resource: bindings, ignored listing per whitelist
Mar  8 13:37:00.890: INFO: namespace e2e-tests-downward-api-2678r deletion completed in 6.060057427s

â€¢ [SLOW TEST:8.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:00.891: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:00.919231      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-p2dqn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-smhlt
STEP: Creating secret with name secret-test-4097ea25-41a7-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:37:01.180: INFO: Waiting up to 5m0s for pod "pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-p2dqn" to be "success or failure"
Mar  8 13:37:01.183: INFO: Pod "pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.969258ms
Mar  8 13:37:03.185: INFO: Pod "pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005242191s
STEP: Saw pod success
Mar  8 13:37:03.185: INFO: Pod "pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:37:03.187: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:37:03.199: INFO: Waiting for pod pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:37:03.201: INFO: Pod pod-secrets-40aa996f-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:37:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-p2dqn" for this suite.
Mar  8 13:37:09.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:09.290: INFO: namespace: e2e-tests-secrets-p2dqn, resource: bindings, ignored listing per whitelist
Mar  8 13:37:09.290: INFO: namespace e2e-tests-secrets-p2dqn deletion completed in 6.087211931s
STEP: Destroying namespace "e2e-tests-secret-namespace-smhlt" for this suite.
Mar  8 13:37:15.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:15.352: INFO: namespace: e2e-tests-secret-namespace-smhlt, resource: bindings, ignored listing per whitelist
Mar  8 13:37:15.354: INFO: namespace e2e-tests-secret-namespace-smhlt deletion completed in 6.064320423s

â€¢ [SLOW TEST:14.464 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:15.355: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:15.385846      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dcplh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:37:15.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-dcplh" to be "success or failure"
Mar  8 13:37:15.519: INFO: Pod "downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672985ms
Mar  8 13:37:17.522: INFO: Pod "downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005879033s
STEP: Saw pod success
Mar  8 13:37:17.522: INFO: Pod "downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:37:17.523: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:37:17.537: INFO: Waiting for pod downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:37:17.539: INFO: Pod downwardapi-volume-49356d64-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:37:17.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dcplh" for this suite.
Mar  8 13:37:23.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:23.593: INFO: namespace: e2e-tests-projected-dcplh, resource: bindings, ignored listing per whitelist
Mar  8 13:37:23.599: INFO: namespace e2e-tests-projected-dcplh deletion completed in 6.058047023s

â€¢ [SLOW TEST:8.245 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:23.599: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:23.628045      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rgmkm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4e63bc80-41a7-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:37:24.207: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-rgmkm" to be "success or failure"
Mar  8 13:37:24.209: INFO: Pod "pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108976ms
Mar  8 13:37:26.212: INFO: Pod "pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00437901s
STEP: Saw pod success
Mar  8 13:37:26.212: INFO: Pod "pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:37:26.213: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:37:26.225: INFO: Waiting for pod pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:37:26.228: INFO: Pod pod-projected-configmaps-4e64055f-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:37:26.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rgmkm" for this suite.
Mar  8 13:37:32.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:32.293: INFO: namespace: e2e-tests-projected-rgmkm, resource: bindings, ignored listing per whitelist
Mar  8 13:37:32.294: INFO: namespace e2e-tests-projected-rgmkm deletion completed in 6.064090112s

â€¢ [SLOW TEST:8.695 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:32.295: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:32.323377      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-h2dcl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0308 13:37:42.486147      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 13:37:42.486: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:37:42.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-h2dcl" for this suite.
Mar  8 13:37:48.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:48.548: INFO: namespace: e2e-tests-gc-h2dcl, resource: bindings, ignored listing per whitelist
Mar  8 13:37:48.557: INFO: namespace e2e-tests-gc-h2dcl deletion completed in 6.068643989s

â€¢ [SLOW TEST:16.262 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:48.589828      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-w7wc7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:37:48.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-w7wc7" to be "success or failure"
Mar  8 13:37:48.722: INFO: Pod "downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222573ms
Mar  8 13:37:50.725: INFO: Pod "downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007194422s
STEP: Saw pod success
Mar  8 13:37:50.725: INFO: Pod "downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:37:50.727: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:37:50.744: INFO: Waiting for pod downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:37:50.748: INFO: Pod downwardapi-volume-5cfff908-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:37:50.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w7wc7" for this suite.
Mar  8 13:37:56.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:37:56.778: INFO: namespace: e2e-tests-downward-api-w7wc7, resource: bindings, ignored listing per whitelist
Mar  8 13:37:56.808: INFO: namespace e2e-tests-downward-api-w7wc7 deletion completed in 6.057870038s

â€¢ [SLOW TEST:8.251 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:37:56.809: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:37:56.835894      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-dlp58
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  8 13:37:57.470: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:37:57.472: INFO: Number of nodes with available pods: 0
Mar  8 13:37:57.472: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:37:58.475: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:37:58.477: INFO: Number of nodes with available pods: 0
Mar  8 13:37:58.477: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:37:59.478: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:37:59.481: INFO: Number of nodes with available pods: 1
Mar  8 13:37:59.481: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:00.475: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:00.477: INFO: Number of nodes with available pods: 3
Mar  8 13:38:00.477: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  8 13:38:00.491: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:00.492: INFO: Number of nodes with available pods: 2
Mar  8 13:38:00.492: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:01.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:01.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:01.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:02.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:02.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:02.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:03.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:03.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:03.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:04.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:04.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:04.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:05.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:05.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:05.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:06.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:06.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:06.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:07.497: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:07.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:07.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:08.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:08.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:08.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:09.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:09.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:09.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:10.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:10.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:10.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:11.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:11.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:11.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:12.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:12.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:12.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:13.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:13.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:13.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:14.497: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:14.499: INFO: Number of nodes with available pods: 2
Mar  8 13:38:14.499: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:15.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:15.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:15.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:16.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:16.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:16.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:17.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:17.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:17.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:18.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:18.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:18.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:19.500: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:19.501: INFO: Number of nodes with available pods: 2
Mar  8 13:38:19.502: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:20.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:20.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:20.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:21.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:21.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:21.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:22.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:22.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:22.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:23.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:23.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:23.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:24.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:24.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:24.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:25.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:25.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:25.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:26.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:26.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:26.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:27.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:27.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:27.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:28.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:28.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:28.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:29.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:29.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:29.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:30.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:30.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:30.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:31.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:31.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:31.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:32.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:32.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:32.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:33.762: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:33.767: INFO: Number of nodes with available pods: 2
Mar  8 13:38:33.767: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:34.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:34.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:34.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:35.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:35.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:35.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:36.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:36.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:36.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:37.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:37.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:37.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:38.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:38.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:38.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:39.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:39.498: INFO: Number of nodes with available pods: 2
Mar  8 13:38:39.498: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:40.863: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:40.865: INFO: Number of nodes with available pods: 2
Mar  8 13:38:40.865: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:41.495: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:41.497: INFO: Number of nodes with available pods: 2
Mar  8 13:38:41.497: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 13:38:42.496: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 13:38:42.497: INFO: Number of nodes with available pods: 3
Mar  8 13:38:42.498: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-dlp58, will wait for the garbage collector to delete the pods
Mar  8 13:38:42.553: INFO: Deleting DaemonSet.extensions daemon-set took: 2.936906ms
Mar  8 13:38:42.654: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.237094ms
Mar  8 13:39:20.855: INFO: Number of nodes with available pods: 0
Mar  8 13:39:20.855: INFO: Number of running nodes: 0, number of available pods: 0
Mar  8 13:39:20.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dlp58/daemonsets","resourceVersion":"11831"},"items":null}

Mar  8 13:39:20.858: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dlp58/pods","resourceVersion":"11831"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:39:20.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dlp58" for this suite.
Mar  8 13:39:26.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:39:26.899: INFO: namespace: e2e-tests-daemonsets-dlp58, resource: bindings, ignored listing per whitelist
Mar  8 13:39:26.931: INFO: namespace e2e-tests-daemonsets-dlp58 deletion completed in 6.064048098s

â€¢ [SLOW TEST:90.123 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:39:26.932: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:39:26.961591      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4rxkm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  8 13:39:27.089: INFO: Waiting up to 5m0s for pod "downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-4rxkm" to be "success or failure"
Mar  8 13:39:27.094: INFO: Pod "downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893932ms
Mar  8 13:39:29.097: INFO: Pod "downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007115478s
STEP: Saw pod success
Mar  8 13:39:29.097: INFO: Pod "downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:39:29.098: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 13:39:29.110: INFO: Waiting for pod downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:39:29.113: INFO: Pod downward-api-97a2382e-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:39:29.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4rxkm" for this suite.
Mar  8 13:39:35.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:39:35.152: INFO: namespace: e2e-tests-downward-api-4rxkm, resource: bindings, ignored listing per whitelist
Mar  8 13:39:35.174: INFO: namespace e2e-tests-downward-api-4rxkm deletion completed in 6.059036268s

â€¢ [SLOW TEST:8.242 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:39:35.174: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:39:35.202923      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-98l76
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9c92d299-41a7-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:39:35.377: INFO: Waiting up to 5m0s for pod "pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-98l76" to be "success or failure"
Mar  8 13:39:35.380: INFO: Pod "pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755038ms
Mar  8 13:39:37.382: INFO: Pod "pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004893068s
STEP: Saw pod success
Mar  8 13:39:37.382: INFO: Pod "pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:39:37.384: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:39:37.394: INFO: Waiting for pod pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:39:37.397: INFO: Pod pod-secrets-9c933144-41a7-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:39:37.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-98l76" for this suite.
Mar  8 13:39:43.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:39:43.425: INFO: namespace: e2e-tests-secrets-98l76, resource: bindings, ignored listing per whitelist
Mar  8 13:39:43.458: INFO: namespace e2e-tests-secrets-98l76 deletion completed in 6.059255301s

â€¢ [SLOW TEST:8.284 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:39:43.458: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:39:43.486723      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-w6mdc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-w6mdc
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-w6mdc
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-w6mdc
Mar  8 13:39:43.625: INFO: Found 0 stateful pods, waiting for 1
Mar  8 13:39:53.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  8 13:39:53.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:39:53.805: INFO: stderr: ""
Mar  8 13:39:53.805: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:39:53.805: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:39:53.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  8 13:40:03.809: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:40:03.809: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:40:03.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999005s
Mar  8 13:40:04.820: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996905183s
Mar  8 13:40:05.822: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994781679s
Mar  8 13:40:06.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99227123s
Mar  8 13:40:07.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.990080333s
Mar  8 13:40:08.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987577108s
Mar  8 13:40:09.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985113879s
Mar  8 13:40:10.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.982792237s
Mar  8 13:40:11.836: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.980706181s
Mar  8 13:40:12.841: INFO: Verifying statefulset ss doesn't scale past 1 for another 978.334171ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-w6mdc
Mar  8 13:40:13.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:40:14.008: INFO: stderr: ""
Mar  8 13:40:14.008: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:40:14.008: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:40:14.010: INFO: Found 1 stateful pods, waiting for 3
Mar  8 13:40:24.015: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 13:40:24.015: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 13:40:24.015: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  8 13:40:24.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:40:24.185: INFO: stderr: ""
Mar  8 13:40:24.185: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:40:24.185: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:40:24.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:40:24.352: INFO: stderr: ""
Mar  8 13:40:24.352: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:40:24.352: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:40:24.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:40:24.512: INFO: stderr: ""
Mar  8 13:40:24.512: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:40:24.512: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:40:24.512: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:40:24.514: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  8 13:40:34.518: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:40:34.518: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:40:34.518: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:40:34.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999059s
Mar  8 13:40:35.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997863703s
Mar  8 13:40:36.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.803313389s
Mar  8 13:40:37.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.80072141s
Mar  8 13:40:38.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.798141887s
Mar  8 13:40:39.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.795389893s
Mar  8 13:40:40.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.786276718s
Mar  8 13:40:41.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.783450926s
Mar  8 13:40:42.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.774177711s
Mar  8 13:40:43.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 771.62061ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-w6mdc
Mar  8 13:40:44.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:40:44.916: INFO: stderr: ""
Mar  8 13:40:44.916: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:40:44.916: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:40:44.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:40:45.090: INFO: stderr: ""
Mar  8 13:40:45.090: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:40:45.090: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:40:45.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-w6mdc ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:40:45.263: INFO: stderr: ""
Mar  8 13:40:45.263: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:40:45.263: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:40:45.263: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  8 13:40:55.271: INFO: Deleting all statefulset in ns e2e-tests-statefulset-w6mdc
Mar  8 13:40:55.274: INFO: Scaling statefulset ss to 0
Mar  8 13:40:55.278: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:40:55.280: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:40:55.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-w6mdc" for this suite.
Mar  8 13:41:01.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:41:01.618: INFO: namespace: e2e-tests-statefulset-w6mdc, resource: bindings, ignored listing per whitelist
Mar  8 13:41:01.621: INFO: namespace e2e-tests-statefulset-w6mdc deletion completed in 6.063540183s

â€¢ [SLOW TEST:78.163 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:41:01.622: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:41:01.651756      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-nxxj5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:41:01.844: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar  8 13:41:01.848: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-nxxj5/daemonsets","resourceVersion":"12331"},"items":null}

Mar  8 13:41:01.849: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-nxxj5/pods","resourceVersion":"12331"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:41:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-nxxj5" for this suite.
Mar  8 13:41:07.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:41:07.899: INFO: namespace: e2e-tests-daemonsets-nxxj5, resource: bindings, ignored listing per whitelist
Mar  8 13:41:07.921: INFO: namespace e2e-tests-daemonsets-nxxj5 deletion completed in 6.055830211s

S [SKIPPING] [6.299 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar  8 13:41:01.844: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:41:07.921: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:41:07.950437      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-8m5kk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8m5kk
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-8m5kk
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-8m5kk
Mar  8 13:41:08.085: INFO: Found 0 stateful pods, waiting for 1
Mar  8 13:41:18.488: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  8 13:41:18.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:41:18.637: INFO: stderr: ""
Mar  8 13:41:18.637: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:41:18.637: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:41:18.640: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  8 13:41:28.642: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:41:28.642: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:41:28.650: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:41:28.650: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:41:28.650: INFO: 
Mar  8 13:41:28.650: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  8 13:41:29.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996955285s
Mar  8 13:41:30.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99427075s
Mar  8 13:41:31.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99170377s
Mar  8 13:41:32.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989056316s
Mar  8 13:41:33.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986511294s
Mar  8 13:41:34.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983898505s
Mar  8 13:41:35.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9814731s
Mar  8 13:41:36.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.978872076s
Mar  8 13:41:37.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.459474ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-8m5kk
Mar  8 13:41:38.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:41:38.827: INFO: stderr: ""
Mar  8 13:41:38.827: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:41:38.827: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:41:38.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:41:38.988: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  8 13:41:38.988: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:41:38.988: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:41:38.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:41:39.136: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  8 13:41:39.136: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 13:41:39.136: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 13:41:39.138: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  8 13:41:49.141: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 13:41:49.141: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 13:41:49.141: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  8 13:41:49.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:41:49.303: INFO: stderr: ""
Mar  8 13:41:49.303: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:41:49.303: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:41:49.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:41:49.466: INFO: stderr: ""
Mar  8 13:41:49.466: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:41:49.466: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:41:49.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 13:41:49.629: INFO: stderr: ""
Mar  8 13:41:49.629: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 13:41:49.629: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 13:41:49.629: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:41:49.630: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  8 13:42:00.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:42:00.051: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:42:00.051: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  8 13:42:00.059: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:00.059: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:00.059: INFO: ss-1  ip-192-168-72-251.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  }]
Mar  8 13:42:00.059: INFO: ss-2  ip-192-168-72-251.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  }]
Mar  8 13:42:00.059: INFO: 
Mar  8 13:42:00.059: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  8 13:42:01.062: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:01.062: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:01.062: INFO: ss-1  ip-192-168-72-251.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  }]
Mar  8 13:42:01.062: INFO: ss-2  ip-192-168-72-251.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:28 +0000 UTC  }]
Mar  8 13:42:01.062: INFO: 
Mar  8 13:42:01.062: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  8 13:42:02.065: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:02.065: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:02.065: INFO: 
Mar  8 13:42:02.065: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:03.070: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:03.070: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:03.070: INFO: 
Mar  8 13:42:03.070: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:04.073: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:04.073: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:04.073: INFO: 
Mar  8 13:42:04.073: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:05.076: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:05.076: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:05.076: INFO: 
Mar  8 13:42:05.076: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:06.079: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:06.079: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:06.079: INFO: 
Mar  8 13:42:06.079: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:07.081: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:07.081: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:07.081: INFO: 
Mar  8 13:42:07.081: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:08.084: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:08.084: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:08.084: INFO: 
Mar  8 13:42:08.084: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  8 13:42:09.086: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Mar  8 13:42:09.086: INFO: ss-0  ip-192-168-64-146.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 13:41:08 +0000 UTC  }]
Mar  8 13:42:09.086: INFO: 
Mar  8 13:42:09.086: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-8m5kk
Mar  8 13:42:10.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:42:10.165: INFO: rc: 1
Mar  8 13:42:10.165: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0014a8cf0 exit status 1 <nil> <nil> true [0xc0011d8e28 0xc0011d8e40 0xc0011d8e58] [0xc0011d8e28 0xc0011d8e40 0xc0011d8e58] [0xc0011d8e38 0xc0011d8e50] [0x92f8e0 0x92f8e0] 0xc00196d500 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Mar  8 13:42:20.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:42:20.225: INFO: rc: 1
Mar  8 13:42:20.225: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00183b170 exit status 1 <nil> <nil> true [0xc001e0a818 0xc001e0a830 0xc001e0a848] [0xc001e0a818 0xc001e0a830 0xc001e0a848] [0xc001e0a828 0xc001e0a840] [0x92f8e0 0x92f8e0] 0xc001b24c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:42:30.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:42:30.738: INFO: rc: 1
Mar  8 13:42:30.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00183b590 exit status 1 <nil> <nil> true [0xc001e0a850 0xc001e0a868 0xc001e0a880] [0xc001e0a850 0xc001e0a868 0xc001e0a880] [0xc001e0a860 0xc001e0a878] [0x92f8e0 0x92f8e0] 0xc001b25200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:42:40.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:42:40.797: INFO: rc: 1
Mar  8 13:42:40.797: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00183b980 exit status 1 <nil> <nil> true [0xc001e0a888 0xc001e0a8a0 0xc001e0a8b8] [0xc001e0a888 0xc001e0a8a0 0xc001e0a8b8] [0xc001e0a898 0xc001e0a8b0] [0x92f8e0 0x92f8e0] 0xc001b256e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:42:50.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:42:50.855: INFO: rc: 1
Mar  8 13:42:50.855: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7a3f0 exit status 1 <nil> <nil> true [0xc00152c0c0 0xc00152c130 0xc00152c3a8] [0xc00152c0c0 0xc00152c130 0xc00152c3a8] [0xc00152c0f8 0xc00152c300] [0x92f8e0 0x92f8e0] 0xc000f26240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:00.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:00.915: INFO: rc: 1
Mar  8 13:43:00.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae3c0 exit status 1 <nil> <nil> true [0xc000676060 0xc0006766b0 0xc000676aa0] [0xc000676060 0xc0006766b0 0xc000676aa0] [0xc000676598 0xc000676908] [0x92f8e0 0x92f8e0] 0xc000a4eba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:10.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:10.973: INFO: rc: 1
Mar  8 13:43:10.973: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae810 exit status 1 <nil> <nil> true [0xc000676ad8 0xc000676c98 0xc000676f90] [0xc000676ad8 0xc000676c98 0xc000676f90] [0xc000676c50 0xc000676ea0] [0x92f8e0 0x92f8e0] 0xc000a4fda0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:20.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:21.029: INFO: rc: 1
Mar  8 13:43:21.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7a810 exit status 1 <nil> <nil> true [0xc00152c4d8 0xc00152c758 0xc00152c9a0] [0xc00152c4d8 0xc00152c758 0xc00152c9a0] [0xc00152c648 0xc00152c918] [0x92f8e0 0x92f8e0] 0xc000f26540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:31.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:31.087: INFO: rc: 1
Mar  8 13:43:31.087: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaebd0 exit status 1 <nil> <nil> true [0xc000676fd0 0xc000677350 0xc000677750] [0xc000676fd0 0xc000677350 0xc000677750] [0xc000677110 0xc0006775f0] [0x92f8e0 0x92f8e0] 0xc001723500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:41.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:41.145: INFO: rc: 1
Mar  8 13:43:41.145: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaef60 exit status 1 <nil> <nil> true [0xc000677808 0xc0006779a8 0xc000677b00] [0xc000677808 0xc0006779a8 0xc000677b00] [0xc000677918 0xc000677a88] [0x92f8e0 0x92f8e0] 0xc001894240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:43:51.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:43:51.205: INFO: rc: 1
Mar  8 13:43:51.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7ac00 exit status 1 <nil> <nil> true [0xc00152c9e0 0xc00152ca88 0xc00152cbe0] [0xc00152c9e0 0xc00152ca88 0xc00152cbe0] [0xc00152ca40 0xc00152cbc0] [0x92f8e0 0x92f8e0] 0xc000f26840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:01.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:01.263: INFO: rc: 1
Mar  8 13:44:01.263: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf320 exit status 1 <nil> <nil> true [0xc000677b50 0xc000677d18 0xc000677e80] [0xc000677b50 0xc000677d18 0xc000677e80] [0xc000677bf0 0xc000677e50] [0x92f8e0 0x92f8e0] 0xc001894540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:11.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:11.323: INFO: rc: 1
Mar  8 13:44:11.323: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7af90 exit status 1 <nil> <nil> true [0xc00152cd30 0xc00152cea0 0xc00152d148] [0xc00152cd30 0xc00152cea0 0xc00152d148] [0xc00152ce20 0xc00152cff0] [0x92f8e0 0x92f8e0] 0xc000f26b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:21.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:21.381: INFO: rc: 1
Mar  8 13:44:21.381: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf710 exit status 1 <nil> <nil> true [0xc000677f78 0xc00000fdf8 0xc00000fe58] [0xc000677f78 0xc00000fdf8 0xc00000fe58] [0xc00000fde0 0xc00000fe28] [0x92f8e0 0x92f8e0] 0xc001894840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:31.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:31.438: INFO: rc: 1
Mar  8 13:44:31.438: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7b380 exit status 1 <nil> <nil> true [0xc00152d168 0xc00152d290 0xc00152d370] [0xc00152d168 0xc00152d290 0xc00152d370] [0xc00152d230 0xc00152d2c8] [0x92f8e0 0x92f8e0] 0xc000f26e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:41.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:41.495: INFO: rc: 1
Mar  8 13:44:41.495: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7b710 exit status 1 <nil> <nil> true [0xc00152d400 0xc00152d4c8 0xc00152d508] [0xc00152d400 0xc00152d4c8 0xc00152d508] [0xc00152d490 0xc00152d4f8] [0x92f8e0 0x92f8e0] 0xc000f27140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:44:51.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:44:51.553: INFO: rc: 1
Mar  8 13:44:51.553: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7a420 exit status 1 <nil> <nil> true [0xc000676500 0xc000676780 0xc000676ad8] [0xc000676500 0xc000676780 0xc000676ad8] [0xc0006766b0 0xc000676aa0] [0x92f8e0 0x92f8e0] 0xc0017233e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:01.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:01.614: INFO: rc: 1
Mar  8 13:45:01.614: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae3f0 exit status 1 <nil> <nil> true [0xc00152c010 0xc00152c0f8 0xc00152c300] [0xc00152c010 0xc00152c0f8 0xc00152c300] [0xc00152c0e0 0xc00152c1a0] [0x92f8e0 0x92f8e0] 0xc000a4ef60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:11.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:11.675: INFO: rc: 1
Mar  8 13:45:11.675: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7a7e0 exit status 1 <nil> <nil> true [0xc000676c08 0xc000676e58 0xc000676fd0] [0xc000676c08 0xc000676e58 0xc000676fd0] [0xc000676c98 0xc000676f90] [0x92f8e0 0x92f8e0] 0xc000f260c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:21.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:21.735: INFO: rc: 1
Mar  8 13:45:21.735: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae8a0 exit status 1 <nil> <nil> true [0xc00152c3a8 0xc00152c648 0xc00152c918] [0xc00152c3a8 0xc00152c648 0xc00152c918] [0xc00152c5f8 0xc00152c8a0] [0x92f8e0 0x92f8e0] 0xc001894060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:31.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:31.793: INFO: rc: 1
Mar  8 13:45:31.793: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7abd0 exit status 1 <nil> <nil> true [0xc000677060 0xc000677520 0xc000677808] [0xc000677060 0xc000677520 0xc000677808] [0xc000677350 0xc000677750] [0x92f8e0 0x92f8e0] 0xc000f263c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:41.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:41.851: INFO: rc: 1
Mar  8 13:45:41.851: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7aff0 exit status 1 <nil> <nil> true [0xc0006778e0 0xc0006779e0 0xc000677b50] [0xc0006778e0 0xc0006779e0 0xc000677b50] [0xc0006779a8 0xc000677b00] [0x92f8e0 0x92f8e0] 0xc000f266c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:45:51.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:45:51.907: INFO: rc: 1
Mar  8 13:45:51.908: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaec90 exit status 1 <nil> <nil> true [0xc00152c9a0 0xc00152ca40 0xc00152cbc0] [0xc00152c9a0 0xc00152ca40 0xc00152cbc0] [0xc00152ca20 0xc00152cab8] [0x92f8e0 0x92f8e0] 0xc0018943c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:01.970: INFO: rc: 1
Mar  8 13:46:01.970: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf050 exit status 1 <nil> <nil> true [0xc00152cbe0 0xc00152ce20 0xc00152cff0] [0xc00152cbe0 0xc00152ce20 0xc00152cff0] [0xc00152cdf0 0xc00152ced8] [0x92f8e0 0x92f8e0] 0xc0018946c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:11.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:12.033: INFO: rc: 1
Mar  8 13:46:12.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf410 exit status 1 <nil> <nil> true [0xc00152d148 0xc00152d230 0xc00152d2c8] [0xc00152d148 0xc00152d230 0xc00152d2c8] [0xc00152d1f0 0xc00152d2a0] [0x92f8e0 0x92f8e0] 0xc0018949c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:22.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:22.091: INFO: rc: 1
Mar  8 13:46:22.091: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c7b440 exit status 1 <nil> <nil> true [0xc000677ba8 0xc000677d40 0xc000677f78] [0xc000677ba8 0xc000677d40 0xc000677f78] [0xc000677d18 0xc000677e80] [0x92f8e0 0x92f8e0] 0xc000f269c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:32.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:32.151: INFO: rc: 1
Mar  8 13:46:32.151: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf800 exit status 1 <nil> <nil> true [0xc00152d370 0xc00152d490 0xc00152d4f8] [0xc00152d370 0xc00152d490 0xc00152d4f8] [0xc00152d420 0xc00152d4d0] [0x92f8e0 0x92f8e0] 0xc001894cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:42.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:42.209: INFO: rc: 1
Mar  8 13:46:42.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eafb90 exit status 1 <nil> <nil> true [0xc00152d508 0xc00152d560 0xc00152d5c8] [0xc00152d508 0xc00152d560 0xc00152d5c8] [0xc00152d530 0xc00152d5b0] [0x92f8e0 0x92f8e0] 0xc001894fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:46:52.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:46:52.272: INFO: rc: 1
Mar  8 13:46:52.272: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eafe90 exit status 1 <nil> <nil> true [0xc00152d5e8 0xc00152d630 0xc00152d690] [0xc00152d5e8 0xc00152d630 0xc00152d690] [0xc00152d610 0xc00152d678] [0x92f8e0 0x92f8e0] 0xc001895260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:47:02.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:47:02.334: INFO: rc: 1
Mar  8 13:47:02.334: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae3c0 exit status 1 <nil> <nil> true [0xc000676060 0xc0006766b0 0xc000676aa0] [0xc000676060 0xc0006766b0 0xc000676aa0] [0xc000676598 0xc000676908] [0x92f8e0 0x92f8e0] 0xc000a4eba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  8 13:47:12.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-8m5kk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 13:47:12.393: INFO: rc: 1
Mar  8 13:47:12.393: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Mar  8 13:47:12.393: INFO: Scaling statefulset ss to 0
Mar  8 13:47:12.398: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  8 13:47:12.400: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8m5kk
Mar  8 13:47:12.401: INFO: Scaling statefulset ss to 0
Mar  8 13:47:12.406: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 13:47:12.412: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:47:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8m5kk" for this suite.
Mar  8 13:47:18.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:47:18.458: INFO: namespace: e2e-tests-statefulset-8m5kk, resource: bindings, ignored listing per whitelist
Mar  8 13:47:18.480: INFO: namespace e2e-tests-statefulset-8m5kk deletion completed in 6.057991737s

â€¢ [SLOW TEST:370.559 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:47:18.480: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:47:18.509911      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-27w76
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar  8 13:47:19.143: INFO: Waiting up to 5m0s for pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl" in namespace "e2e-tests-svcaccounts-27w76" to be "success or failure"
Mar  8 13:47:19.145: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319441ms
Mar  8 13:47:21.147: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004454243s
STEP: Saw pod success
Mar  8 13:47:21.147: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl" satisfied condition "success or failure"
Mar  8 13:47:21.149: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl container token-test: <nil>
STEP: delete the pod
Mar  8 13:47:21.160: INFO: Waiting for pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl to disappear
Mar  8 13:47:21.163: INFO: Pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-lsnpl no longer exists
STEP: Creating a pod to test consume service account root CA
Mar  8 13:47:21.167: INFO: Waiting up to 5m0s for pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv" in namespace "e2e-tests-svcaccounts-27w76" to be "success or failure"
Mar  8 13:47:21.168: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599546ms
Mar  8 13:47:23.171: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003755388s
STEP: Saw pod success
Mar  8 13:47:23.171: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv" satisfied condition "success or failure"
Mar  8 13:47:23.172: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv container root-ca-test: <nil>
STEP: delete the pod
Mar  8 13:47:23.185: INFO: Waiting for pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv to disappear
Mar  8 13:47:23.187: INFO: Pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-gtwfv no longer exists
STEP: Creating a pod to test consume service account namespace
Mar  8 13:47:23.190: INFO: Waiting up to 5m0s for pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7" in namespace "e2e-tests-svcaccounts-27w76" to be "success or failure"
Mar  8 13:47:23.192: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577127ms
Mar  8 13:47:25.194: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003686419s
STEP: Saw pod success
Mar  8 13:47:25.194: INFO: Pod "pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7" satisfied condition "success or failure"
Mar  8 13:47:25.195: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7 container namespace-test: <nil>
STEP: delete the pod
Mar  8 13:47:25.206: INFO: Waiting for pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7 to disappear
Mar  8 13:47:25.210: INFO: Pod pod-service-account-b0fffb1a-41a8-11e9-be5e-ae5bc1be57d7-26hp7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:47:25.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-27w76" for this suite.
Mar  8 13:47:31.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:47:31.231: INFO: namespace: e2e-tests-svcaccounts-27w76, resource: bindings, ignored listing per whitelist
Mar  8 13:47:31.270: INFO: namespace e2e-tests-svcaccounts-27w76 deletion completed in 6.057593877s

â€¢ [SLOW TEST:12.789 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:47:31.270: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:47:31.297521      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cd5z5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 13:47:31.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-cd5z5" to be "success or failure"
Mar  8 13:47:31.426: INFO: Pod "downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208853ms
Mar  8 13:47:33.428: INFO: Pod "downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00553635s
STEP: Saw pod success
Mar  8 13:47:33.428: INFO: Pod "downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:47:33.430: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 13:47:33.439: INFO: Waiting for pod downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:47:33.442: INFO: Pod downwardapi-volume-b851f836-41a8-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:47:33.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cd5z5" for this suite.
Mar  8 13:47:39.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:47:39.484: INFO: namespace: e2e-tests-projected-cd5z5, resource: bindings, ignored listing per whitelist
Mar  8 13:47:39.508: INFO: namespace e2e-tests-projected-cd5z5 deletion completed in 6.062993974s

â€¢ [SLOW TEST:8.238 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:47:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:47:39.535242      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-mlw52
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  8 13:47:43.681: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:43.681: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:43.785: INFO: Exec stderr: ""
Mar  8 13:47:43.785: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:43.878: INFO: Exec stderr: ""
Mar  8 13:47:43.878: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:43.973: INFO: Exec stderr: ""
Mar  8 13:47:43.973: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:43.973: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.072: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  8 13:47:44.072: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.167: INFO: Exec stderr: ""
Mar  8 13:47:44.167: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.167: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.265: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  8 13:47:44.265: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.265: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.364: INFO: Exec stderr: ""
Mar  8 13:47:44.364: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.473: INFO: Exec stderr: ""
Mar  8 13:47:44.473: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.473: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.571: INFO: Exec stderr: ""
Mar  8 13:47:44.571: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mlw52 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 13:47:44.571: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 13:47:44.660: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:47:44.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-mlw52" for this suite.
Mar  8 13:48:36.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:48:36.705: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-mlw52, resource: bindings, ignored listing per whitelist
Mar  8 13:48:36.724: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-mlw52 deletion completed in 52.060560496s

â€¢ [SLOW TEST:57.216 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:48:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:48:36.768330      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hqr7s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  8 13:48:36.895: INFO: Waiting up to 5m0s for pod "pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-hqr7s" to be "success or failure"
Mar  8 13:48:36.897: INFO: Pod "pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.650498ms
Mar  8 13:48:38.899: INFO: Pod "pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003728317s
STEP: Saw pod success
Mar  8 13:48:38.899: INFO: Pod "pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:48:38.901: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 13:48:38.910: INFO: Waiting for pod pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:48:38.913: INFO: Pod pod-df5816a0-41a8-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:48:38.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hqr7s" for this suite.
Mar  8 13:48:44.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:48:45.047: INFO: namespace: e2e-tests-emptydir-hqr7s, resource: bindings, ignored listing per whitelist
Mar  8 13:48:45.074: INFO: namespace e2e-tests-emptydir-hqr7s deletion completed in 6.159196889s

â€¢ [SLOW TEST:8.350 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:48:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:48:45.102445      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qrf4z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  8 13:48:45.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:45.519: INFO: stderr: ""
Mar  8 13:48:45.519: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 13:48:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:45.607: INFO: stderr: ""
Mar  8 13:48:45.607: INFO: stdout: "update-demo-nautilus-chlwm update-demo-nautilus-xwx7x "
Mar  8 13:48:45.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-chlwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:45.685: INFO: stderr: ""
Mar  8 13:48:45.685: INFO: stdout: ""
Mar  8 13:48:45.685: INFO: update-demo-nautilus-chlwm is created but not running
Mar  8 13:48:50.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:50.769: INFO: stderr: ""
Mar  8 13:48:50.769: INFO: stdout: "update-demo-nautilus-chlwm update-demo-nautilus-xwx7x "
Mar  8 13:48:50.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-chlwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:50.845: INFO: stderr: ""
Mar  8 13:48:50.846: INFO: stdout: "true"
Mar  8 13:48:50.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-chlwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:50.920: INFO: stderr: ""
Mar  8 13:48:50.920: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:48:50.920: INFO: validating pod update-demo-nautilus-chlwm
Mar  8 13:48:50.927: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:48:50.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:48:50.927: INFO: update-demo-nautilus-chlwm is verified up and running
Mar  8 13:48:50.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xwx7x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:51.007: INFO: stderr: ""
Mar  8 13:48:51.007: INFO: stdout: "true"
Mar  8 13:48:51.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-xwx7x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:51.084: INFO: stderr: ""
Mar  8 13:48:51.084: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 13:48:51.084: INFO: validating pod update-demo-nautilus-xwx7x
Mar  8 13:48:51.090: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 13:48:51.090: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 13:48:51.090: INFO: update-demo-nautilus-xwx7x is verified up and running
STEP: using delete to clean up resources
Mar  8 13:48:51.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:51.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:48:51.163: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  8 13:48:51.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-qrf4z'
Mar  8 13:48:51.277: INFO: stderr: "No resources found.\n"
Mar  8 13:48:51.277: INFO: stdout: ""
Mar  8 13:48:51.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -l name=update-demo --namespace=e2e-tests-kubectl-qrf4z -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  8 13:48:51.384: INFO: stderr: ""
Mar  8 13:48:51.384: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:48:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qrf4z" for this suite.
Mar  8 13:48:57.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:48:57.436: INFO: namespace: e2e-tests-kubectl-qrf4z, resource: bindings, ignored listing per whitelist
Mar  8 13:48:57.448: INFO: namespace e2e-tests-kubectl-qrf4z deletion completed in 6.061004357s

â€¢ [SLOW TEST:12.374 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:48:57.448: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:48:57.479265      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-slwv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-ebb0f4c5-41a8-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:48:59.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-slwv6" for this suite.
Mar  8 13:49:21.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:49:21.783: INFO: namespace: e2e-tests-configmap-slwv6, resource: bindings, ignored listing per whitelist
Mar  8 13:49:21.787: INFO: namespace e2e-tests-configmap-slwv6 deletion completed in 22.06642462s

â€¢ [SLOW TEST:24.339 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:49:21.787: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:49:21.816267      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fxk9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  8 13:49:21.940: INFO: namespace e2e-tests-kubectl-fxk9x
Mar  8 13:49:21.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-fxk9x'
Mar  8 13:49:22.092: INFO: stderr: ""
Mar  8 13:49:22.092: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  8 13:49:23.094: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:49:23.094: INFO: Found 0 / 1
Mar  8 13:49:24.094: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:49:24.094: INFO: Found 1 / 1
Mar  8 13:49:24.094: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  8 13:49:24.096: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:49:24.096: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  8 13:49:24.096: INFO: wait on redis-master startup in e2e-tests-kubectl-fxk9x 
Mar  8 13:49:24.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 logs redis-master-szjgl redis-master --namespace=e2e-tests-kubectl-fxk9x'
Mar  8 13:49:24.177: INFO: stderr: ""
Mar  8 13:49:24.177: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Mar 13:49:22.686 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Mar 13:49:22.686 # Server started, Redis version 3.2.12\n1:M 08 Mar 13:49:22.686 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Mar 13:49:22.686 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  8 13:49:24.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-fxk9x'
Mar  8 13:49:24.281: INFO: stderr: ""
Mar  8 13:49:24.281: INFO: stdout: "service/rm2 exposed\n"
Mar  8 13:49:24.283: INFO: Service rm2 in namespace e2e-tests-kubectl-fxk9x found.
STEP: exposing service
Mar  8 13:49:26.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-fxk9x'
Mar  8 13:49:26.383: INFO: stderr: ""
Mar  8 13:49:26.383: INFO: stdout: "service/rm3 exposed\n"
Mar  8 13:49:26.386: INFO: Service rm3 in namespace e2e-tests-kubectl-fxk9x found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:49:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fxk9x" for this suite.
Mar  8 13:49:50.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:49:50.407: INFO: namespace: e2e-tests-kubectl-fxk9x, resource: bindings, ignored listing per whitelist
Mar  8 13:49:50.451: INFO: namespace e2e-tests-kubectl-fxk9x deletion completed in 22.058980884s

â€¢ [SLOW TEST:28.664 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:49:50.451: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:49:50.478518      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-dnf58
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  8 13:49:50.603: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:49:54.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-dnf58" for this suite.
Mar  8 13:50:16.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:50:16.502: INFO: namespace: e2e-tests-init-container-dnf58, resource: bindings, ignored listing per whitelist
Mar  8 13:50:16.554: INFO: namespace e2e-tests-init-container-dnf58 deletion completed in 22.243569126s

â€¢ [SLOW TEST:26.103 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:50:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:50:16.582424      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hfpnx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1ae6362a-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:50:16.909: INFO: Waiting up to 5m0s for pod "pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-hfpnx" to be "success or failure"
Mar  8 13:50:16.913: INFO: Pod "pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718501ms
Mar  8 13:50:18.916: INFO: Pod "pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00695475s
STEP: Saw pod success
Mar  8 13:50:18.916: INFO: Pod "pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:50:18.917: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:50:18.928: INFO: Waiting for pod pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:50:18.930: INFO: Pod pod-secrets-1af42054-41a9-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:50:18.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hfpnx" for this suite.
Mar  8 13:50:24.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:50:24.962: INFO: namespace: e2e-tests-secrets-hfpnx, resource: bindings, ignored listing per whitelist
Mar  8 13:50:24.989: INFO: namespace e2e-tests-secrets-hfpnx deletion completed in 6.057056607s

â€¢ [SLOW TEST:8.435 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:50:24.989: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:50:25.017814      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-x24hn
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-1fddde78-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating configMap with name cm-test-opt-upd-1fdddeb9-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1fddde78-41a9-11e9-be5e-ae5bc1be57d7
STEP: Updating configmap cm-test-opt-upd-1fdddeb9-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating configMap with name cm-test-opt-create-1fdddeff-41a9-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:51:35.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x24hn" for this suite.
Mar  8 13:51:57.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:51:58.009: INFO: namespace: e2e-tests-configmap-x24hn, resource: bindings, ignored listing per whitelist
Mar  8 13:51:58.013: INFO: namespace e2e-tests-configmap-x24hn deletion completed in 22.057559587s

â€¢ [SLOW TEST:93.024 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:51:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:51:58.042285      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2k8z8
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-57508e4f-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating secret with name s-test-opt-upd-57508e99-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-57508e4f-41a9-11e9-be5e-ae5bc1be57d7
STEP: Updating secret s-test-opt-upd-57508e99-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating secret with name s-test-opt-create-57508ec0-41a9-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:52:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2k8z8" for this suite.
Mar  8 13:52:24.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:52:24.245: INFO: namespace: e2e-tests-secrets-2k8z8, resource: bindings, ignored listing per whitelist
Mar  8 13:52:24.330: INFO: namespace e2e-tests-secrets-2k8z8 deletion completed in 22.107250217s

â€¢ [SLOW TEST:26.316 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:52:24.330: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:52:24.365989      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-h7pmk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:52:26.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-h7pmk" for this suite.
Mar  8 13:53:12.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:53:12.588: INFO: namespace: e2e-tests-kubelet-test-h7pmk, resource: bindings, ignored listing per whitelist
Mar  8 13:53:12.592: INFO: namespace e2e-tests-kubelet-test-h7pmk deletion completed in 46.068418049s

â€¢ [SLOW TEST:48.263 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:53:12.593: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:53:12.620551      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-d8k4j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-hjqz
STEP: Creating a pod to test atomic-volume-subpath
Mar  8 13:53:12.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hjqz" in namespace "e2e-tests-subpath-d8k4j" to be "success or failure"
Mar  8 13:53:12.755: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699392ms
Mar  8 13:53:14.757: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 2.00595961s
Mar  8 13:53:16.760: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 4.008233019s
Mar  8 13:53:18.762: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 6.010502873s
Mar  8 13:53:20.764: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 8.012679868s
Mar  8 13:53:22.766: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 10.01496493s
Mar  8 13:53:24.769: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 12.01740875s
Mar  8 13:53:26.771: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 14.019628031s
Mar  8 13:53:28.773: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 16.021450794s
Mar  8 13:53:30.775: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 18.023341614s
Mar  8 13:53:32.777: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Running", Reason="", readiness=false. Elapsed: 20.025452923s
Mar  8 13:53:34.779: INFO: Pod "pod-subpath-test-secret-hjqz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.027766453s
STEP: Saw pod success
Mar  8 13:53:34.779: INFO: Pod "pod-subpath-test-secret-hjqz" satisfied condition "success or failure"
Mar  8 13:53:34.781: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-subpath-test-secret-hjqz container test-container-subpath-secret-hjqz: <nil>
STEP: delete the pod
Mar  8 13:53:34.792: INFO: Waiting for pod pod-subpath-test-secret-hjqz to disappear
Mar  8 13:53:34.797: INFO: Pod pod-subpath-test-secret-hjqz no longer exists
STEP: Deleting pod pod-subpath-test-secret-hjqz
Mar  8 13:53:34.797: INFO: Deleting pod "pod-subpath-test-secret-hjqz" in namespace "e2e-tests-subpath-d8k4j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:53:34.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-d8k4j" for this suite.
Mar  8 13:53:40.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:53:40.835: INFO: namespace: e2e-tests-subpath-d8k4j, resource: bindings, ignored listing per whitelist
Mar  8 13:53:40.864: INFO: namespace e2e-tests-subpath-d8k4j deletion completed in 6.060786829s

â€¢ [SLOW TEST:28.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:53:40.864: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:53:40.897080      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-ftg58
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-949e7d51-41a9-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 13:53:41.806: INFO: Waiting up to 5m0s for pod "pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-ftg58" to be "success or failure"
Mar  8 13:53:41.815: INFO: Pod "pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.928789ms
Mar  8 13:53:43.818: INFO: Pod "pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012258096s
STEP: Saw pod success
Mar  8 13:53:43.818: INFO: Pod "pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:53:43.820: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 13:53:43.832: INFO: Waiting for pod pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:53:43.833: INFO: Pod pod-secrets-949ecb75-41a9-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:53:43.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ftg58" for this suite.
Mar  8 13:53:49.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:53:49.857: INFO: namespace: e2e-tests-secrets-ftg58, resource: bindings, ignored listing per whitelist
Mar  8 13:53:49.892: INFO: namespace e2e-tests-secrets-ftg58 deletion completed in 6.056495253s

â€¢ [SLOW TEST:9.028 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:53:49.892: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:53:49.931465      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2jlfp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar  8 13:53:50.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 --namespace=e2e-tests-kubectl-2jlfp run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  8 13:53:51.669: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  8 13:53:51.669: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:53:53.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2jlfp" for this suite.
Mar  8 13:53:59.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:53:59.713: INFO: namespace: e2e-tests-kubectl-2jlfp, resource: bindings, ignored listing per whitelist
Mar  8 13:53:59.732: INFO: namespace e2e-tests-kubectl-2jlfp deletion completed in 6.056087754s

â€¢ [SLOW TEST:9.840 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:53:59.732: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:53:59.759129      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4jkvl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:53:59.882: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:54:02.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4jkvl" for this suite.
Mar  8 13:54:52.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:54:52.562: INFO: namespace: e2e-tests-pods-4jkvl, resource: bindings, ignored listing per whitelist
Mar  8 13:54:52.599: INFO: namespace e2e-tests-pods-4jkvl deletion completed in 50.061061746s

â€¢ [SLOW TEST:52.867 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:54:52.599: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:54:52.627464      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-s9hp5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar  8 13:54:54.762: INFO: Pod pod-hostip-bf5f9cac-41a9-11e9-be5e-ae5bc1be57d7 has hostIP: 192.168.72.251
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:54:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-s9hp5" for this suite.
Mar  8 13:55:16.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:55:16.828: INFO: namespace: e2e-tests-pods-s9hp5, resource: bindings, ignored listing per whitelist
Mar  8 13:55:16.839: INFO: namespace e2e-tests-pods-s9hp5 deletion completed in 22.074468172s

â€¢ [SLOW TEST:24.239 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:55:16.839: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:55:16.869698      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-ddqwk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  8 13:55:17.000: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:55:19.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-ddqwk" for this suite.
Mar  8 13:55:25.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:55:25.821: INFO: namespace: e2e-tests-init-container-ddqwk, resource: bindings, ignored listing per whitelist
Mar  8 13:55:25.855: INFO: namespace e2e-tests-init-container-ddqwk deletion completed in 6.06154574s

â€¢ [SLOW TEST:9.016 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:55:25.855: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:55:25.883484      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-pwszz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  8 13:55:26.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-pwszz,SelfLink:/api/v1/namespaces/e2e-tests-watch-pwszz/configmaps/e2e-watch-test-resource-version,UID:d331e6f4-41a9-11e9-8665-02fd46f865f2,ResourceVersion:15043,Generation:0,CreationTimestamp:2019-03-08 13:55:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  8 13:55:26.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-pwszz,SelfLink:/api/v1/namespaces/e2e-tests-watch-pwszz/configmaps/e2e-watch-test-resource-version,UID:d331e6f4-41a9-11e9-8665-02fd46f865f2,ResourceVersion:15044,Generation:0,CreationTimestamp:2019-03-08 13:55:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:55:26.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-pwszz" for this suite.
Mar  8 13:55:32.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:55:32.041: INFO: namespace: e2e-tests-watch-pwszz, resource: bindings, ignored listing per whitelist
Mar  8 13:55:32.077: INFO: namespace e2e-tests-watch-pwszz deletion completed in 6.058302056s

â€¢ [SLOW TEST:6.223 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:55:32.077: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:55:32.105034      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-99mw9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  8 13:55:32.271: INFO: Waiting up to 5m0s for pod "downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-99mw9" to be "success or failure"
Mar  8 13:55:32.275: INFO: Pod "downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998599ms
Mar  8 13:55:34.277: INFO: Pod "downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006145791s
STEP: Saw pod success
Mar  8 13:55:34.277: INFO: Pod "downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:55:34.278: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 13:55:34.290: INFO: Waiting for pod downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:55:34.293: INFO: Pod downward-api-d6ed31af-41a9-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:55:34.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-99mw9" for this suite.
Mar  8 13:55:40.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:55:40.322: INFO: namespace: e2e-tests-downward-api-99mw9, resource: bindings, ignored listing per whitelist
Mar  8 13:55:40.357: INFO: namespace e2e-tests-downward-api-99mw9 deletion completed in 6.059984201s

â€¢ [SLOW TEST:8.279 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:55:40.357: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:55:40.383984      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4n7lj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:55:40.505: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:55:42.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4n7lj" for this suite.
Mar  8 13:56:32.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:56:32.661: INFO: namespace: e2e-tests-pods-4n7lj, resource: bindings, ignored listing per whitelist
Mar  8 13:56:32.661: INFO: namespace e2e-tests-pods-4n7lj deletion completed in 50.057237338s

â€¢ [SLOW TEST:52.304 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:56:32.661: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:56:32.689027      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-6wj4l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-4m2tq
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-778s9
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:56:39.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-6wj4l" for this suite.
Mar  8 13:56:45.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:56:45.143: INFO: namespace: e2e-tests-namespaces-6wj4l, resource: bindings, ignored listing per whitelist
Mar  8 13:56:45.145: INFO: namespace e2e-tests-namespaces-6wj4l deletion completed in 6.059962366s
STEP: Destroying namespace "e2e-tests-nsdeletetest-4m2tq" for this suite.
Mar  8 13:56:45.147: INFO: Namespace e2e-tests-nsdeletetest-4m2tq was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-778s9" for this suite.
Mar  8 13:56:51.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:56:51.187: INFO: namespace: e2e-tests-nsdeletetest-778s9, resource: bindings, ignored listing per whitelist
Mar  8 13:56:51.205: INFO: namespace e2e-tests-nsdeletetest-778s9 deletion completed in 6.058120621s

â€¢ [SLOW TEST:18.544 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:56:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:56:51.232753      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-72q4s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-0611e655-41aa-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 13:56:51.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-72q4s" to be "success or failure"
Mar  8 13:56:51.372: INFO: Pod "pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.27454ms
Mar  8 13:56:53.376: INFO: Pod "pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009903966s
STEP: Saw pod success
Mar  8 13:56:53.376: INFO: Pod "pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 13:56:53.377: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 13:56:53.389: INFO: Waiting for pod pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 13:56:53.391: INFO: Pod pod-configmaps-06123159-41aa-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:56:53.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-72q4s" for this suite.
Mar  8 13:56:59.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:56:59.426: INFO: namespace: e2e-tests-configmap-72q4s, resource: bindings, ignored listing per whitelist
Mar  8 13:56:59.462: INFO: namespace e2e-tests-configmap-72q4s deletion completed in 6.068656222s

â€¢ [SLOW TEST:8.257 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:56:59.462: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:56:59.491768      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-m9r7j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 13:56:59.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 version --client'
Mar  8 13:56:59.669: INFO: stderr: ""
Mar  8 13:56:59.669: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar  8 13:56:59.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-m9r7j'
Mar  8 13:56:59.849: INFO: stderr: ""
Mar  8 13:56:59.849: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  8 13:56:59.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-m9r7j'
Mar  8 13:57:00.001: INFO: stderr: ""
Mar  8 13:57:00.001: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  8 13:57:01.003: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:01.003: INFO: Found 0 / 1
Mar  8 13:57:02.004: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:02.004: INFO: Found 0 / 1
Mar  8 13:57:03.003: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:03.003: INFO: Found 0 / 1
Mar  8 13:57:04.003: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:04.003: INFO: Found 1 / 1
Mar  8 13:57:04.003: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  8 13:57:04.005: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:04.005: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  8 13:57:04.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 describe pod redis-master-2vqd6 --namespace=e2e-tests-kubectl-m9r7j'
Mar  8 13:57:04.102: INFO: stderr: ""
Mar  8 13:57:04.102: INFO: stdout: "Name:               redis-master-2vqd6\nNamespace:          e2e-tests-kubectl-m9r7j\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-192-168-64-146.us-west-2.compute.internal/192.168.64.146\nStart Time:         Fri, 08 Mar 2019 13:56:59 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.20.192.7\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://3d5d9aa8641d1649d64551769a79a2a4a5c62ed3da48fde26a93b8fe1c03f167\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 08 Mar 2019 13:57:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wtbxr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wtbxr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wtbxr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                   Message\n  ----    ------     ----  ----                                                   -------\n  Normal  Scheduled  5s    default-scheduler                                      Successfully assigned e2e-tests-kubectl-m9r7j/redis-master-2vqd6 to ip-192-168-64-146.us-west-2.compute.internal\n  Normal  Pulling    4s    kubelet, ip-192-168-64-146.us-west-2.compute.internal  pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, ip-192-168-64-146.us-west-2.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, ip-192-168-64-146.us-west-2.compute.internal  Created container\n  Normal  Started    2s    kubelet, ip-192-168-64-146.us-west-2.compute.internal  Started container\n"
Mar  8 13:57:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 describe rc redis-master --namespace=e2e-tests-kubectl-m9r7j'
Mar  8 13:57:04.196: INFO: stderr: ""
Mar  8 13:57:04.196: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-m9r7j\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-2vqd6\n"
Mar  8 13:57:04.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 describe service redis-master --namespace=e2e-tests-kubectl-m9r7j'
Mar  8 13:57:04.283: INFO: stderr: ""
Mar  8 13:57:04.283: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-m9r7j\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.11.172\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.20.192.7:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  8 13:57:04.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 describe node ip-192-168-64-146.us-west-2.compute.internal'
Mar  8 13:57:04.393: INFO: stderr: ""
Mar  8 13:57:04.393: INFO: stdout: "Name:               ip-192-168-64-146.us-west-2.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    kubernetes.io/hostname=ip-192-168-64-146.us-west-2.compute.internal\n                    node.banzaicloud.io/ondemand=false\n                    nodepool.banzaicloud.io/name=pool3\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    nodepool.banzaicloud.io/managed-labels: [\"node.banzaicloud.io/ondemand\",\"nodepool.banzaicloud.io/name\"]\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 08 Mar 2019 12:45:50 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 08 Mar 2019 12:46:05 +0000   Fri, 08 Mar 2019 12:46:05 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Fri, 08 Mar 2019 13:56:57 +0000   Fri, 08 Mar 2019 12:45:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 08 Mar 2019 13:56:57 +0000   Fri, 08 Mar 2019 12:45:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 08 Mar 2019 13:56:57 +0000   Fri, 08 Mar 2019 12:45:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 08 Mar 2019 13:56:57 +0000   Fri, 08 Mar 2019 12:46:10 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   192.168.64.146\n  ExternalIP:   52.40.214.74\n  InternalDNS:  ip-192-168-64-146.us-west-2.compute.internal\n  Hostname:     ip-192-168-64-146.us-west-2.compute.internal\n  ExternalDNS:  ec2-52-40-214-74.us-west-2.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           52417516Ki\n hugepages-2Mi:               0\n memory:                      8008336Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           48307982666\n hugepages-2Mi:               0\n memory:                      7905936Ki\n pods:                        110\nSystem Info:\n Machine ID:                 b30d0f2110ac3807b210c19ede3ce88f\n System UUID:                EC20D619-0794-74B5-A18E-3B0465CF2EE1\n Boot ID:                    5901afdb-941e-4b68-beaa-f287c4d03a61\n Kernel Version:             3.10.0-862.3.2.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.4\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     10.20.2.0/24\nProviderID:                  aws:///us-west-2a/i-0a6deeafc14e585d1\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-m9r7j    redis-master-2vqd6                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-pns4f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                kube-proxy-dj5qw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m\n  kube-system                tiller-deploy-5f865894d6-kw9sw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m\n  kube-system                weave-net-ftrz9                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         71m\n  pipeline-system            dns-external-dns-784c569497-x5mzr                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\n  pipeline-system            ingress-traefik-7db7799d65-6h7fs                           100m (5%)     200m (10%)  20Mi (0%)        250Mi (3%)     70m\n  pipeline-system            ith-instance-termination-handler-dtxjt                     120m (6%)     0 (0%)      256Mi (3%)       0 (0%)         69m\n  pipeline-system            monitor-prometheus-node-exporter-9vjmd                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  pipeline-system            npls-nodepool-labels-operator-5f8b4685bf-qgwq6             0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         240m (12%)  200m (10%)\n  memory                      276Mi (3%)  250Mi (3%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Mar  8 13:57:04.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 describe namespace e2e-tests-kubectl-m9r7j'
Mar  8 13:57:04.481: INFO: stderr: ""
Mar  8 13:57:04.481: INFO: stdout: "Name:         e2e-tests-kubectl-m9r7j\nLabels:       e2e-framework=kubectl\n              e2e-run=4d7225d4-41a2-11e9-be5e-ae5bc1be57d7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:57:04.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m9r7j" for this suite.
Mar  8 13:57:26.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:57:26.512: INFO: namespace: e2e-tests-kubectl-m9r7j, resource: bindings, ignored listing per whitelist
Mar  8 13:57:26.541: INFO: namespace e2e-tests-kubectl-m9r7j deletion completed in 22.057396808s

â€¢ [SLOW TEST:27.079 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:57:26.541: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:57:26.567829      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nsjrt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Mar  8 13:57:26.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-nsjrt'
Mar  8 13:57:26.842: INFO: stderr: ""
Mar  8 13:57:26.842: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar  8 13:57:27.844: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:27.844: INFO: Found 0 / 1
Mar  8 13:57:28.844: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:28.844: INFO: Found 1 / 1
Mar  8 13:57:28.844: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  8 13:57:28.846: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 13:57:28.846: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  8 13:57:28.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 logs redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt'
Mar  8 13:57:28.923: INFO: stderr: ""
Mar  8 13:57:28.923: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Mar 13:57:27.436 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Mar 13:57:27.436 # Server started, Redis version 3.2.12\n1:M 08 Mar 13:57:27.436 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Mar 13:57:27.436 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  8 13:57:28.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 log redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt --tail=1'
Mar  8 13:57:29.001: INFO: stderr: ""
Mar  8 13:57:29.001: INFO: stdout: "1:M 08 Mar 13:57:27.436 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  8 13:57:29.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 log redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt --limit-bytes=1'
Mar  8 13:57:29.080: INFO: stderr: ""
Mar  8 13:57:29.080: INFO: stdout: " "
STEP: exposing timestamps
Mar  8 13:57:29.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 log redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt --tail=1 --timestamps'
Mar  8 13:57:29.159: INFO: stderr: ""
Mar  8 13:57:29.159: INFO: stdout: "2019-03-08T13:57:27.437541919Z 1:M 08 Mar 13:57:27.436 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  8 13:57:31.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 log redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt --since=1s'
Mar  8 13:57:31.740: INFO: stderr: ""
Mar  8 13:57:31.740: INFO: stdout: ""
Mar  8 13:57:31.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 log redis-master-p6tf8 redis-master --namespace=e2e-tests-kubectl-nsjrt --since=24h'
Mar  8 13:57:31.828: INFO: stderr: ""
Mar  8 13:57:31.828: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Mar 13:57:27.436 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Mar 13:57:27.436 # Server started, Redis version 3.2.12\n1:M 08 Mar 13:57:27.436 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Mar 13:57:27.436 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Mar  8 13:57:31.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-nsjrt'
Mar  8 13:57:31.913: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  8 13:57:31.913: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  8 13:57:31.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-nsjrt'
Mar  8 13:57:32.025: INFO: stderr: "No resources found.\n"
Mar  8 13:57:32.025: INFO: stdout: ""
Mar  8 13:57:32.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -l name=nginx --namespace=e2e-tests-kubectl-nsjrt -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  8 13:57:32.141: INFO: stderr: ""
Mar  8 13:57:32.141: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:57:32.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nsjrt" for this suite.
Mar  8 13:57:54.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:57:54.380: INFO: namespace: e2e-tests-kubectl-nsjrt, resource: bindings, ignored listing per whitelist
Mar  8 13:57:54.390: INFO: namespace e2e-tests-kubectl-nsjrt deletion completed in 22.24593903s

â€¢ [SLOW TEST:27.848 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:57:54.390: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:57:54.436756      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-njrng
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 13:57:54.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-njrng'
Mar  8 13:57:54.642: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  8 13:57:54.642: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  8 13:57:54.664: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-qrdhs]
Mar  8 13:57:54.664: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-qrdhs" in namespace "e2e-tests-kubectl-njrng" to be "running and ready"
Mar  8 13:57:54.669: INFO: Pod "e2e-test-nginx-rc-qrdhs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555933ms
Mar  8 13:57:56.671: INFO: Pod "e2e-test-nginx-rc-qrdhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00692632s
Mar  8 13:57:56.671: INFO: Pod "e2e-test-nginx-rc-qrdhs" satisfied condition "running and ready"
Mar  8 13:57:56.671: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-qrdhs]
Mar  8 13:57:56.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-njrng'
Mar  8 13:57:56.762: INFO: stderr: ""
Mar  8 13:57:56.762: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Mar  8 13:57:56.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-njrng'
Mar  8 13:57:56.845: INFO: stderr: ""
Mar  8 13:57:56.845: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 13:57:56.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-njrng" for this suite.
Mar  8 13:58:14.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 13:58:14.894: INFO: namespace: e2e-tests-kubectl-njrng, resource: bindings, ignored listing per whitelist
Mar  8 13:58:14.914: INFO: namespace e2e-tests-kubectl-njrng deletion completed in 18.063399052s

â€¢ [SLOW TEST:20.524 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 13:58:14.914: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 13:58:14.942672      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-2q6xd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  8 13:58:15.280: INFO: Pod name wrapped-volume-race-3802a557-41aa-11e9-be5e-ae5bc1be57d7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3802a557-41aa-11e9-be5e-ae5bc1be57d7 in namespace e2e-tests-emptydir-wrapper-2q6xd, will wait for the garbage collector to delete the pods
Mar  8 13:58:29.382: INFO: Deleting ReplicationController wrapped-volume-race-3802a557-41aa-11e9-be5e-ae5bc1be57d7 took: 3.44838ms
Mar  8 13:58:29.483: INFO: Terminating ReplicationController wrapped-volume-race-3802a557-41aa-11e9-be5e-ae5bc1be57d7 pods took: 100.239771ms
STEP: Creating RC which spawns configmap-volume pods
Mar  8 13:59:10.903: INFO: Pod name wrapped-volume-race-593b9310-41aa-11e9-be5e-ae5bc1be57d7: Found 0 pods out of 5
Mar  8 13:59:15.907: INFO: Pod name wrapped-volume-race-593b9310-41aa-11e9-be5e-ae5bc1be57d7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-593b9310-41aa-11e9-be5e-ae5bc1be57d7 in namespace e2e-tests-emptydir-wrapper-2q6xd, will wait for the garbage collector to delete the pods
Mar  8 13:59:26.326: INFO: Deleting ReplicationController wrapped-volume-race-593b9310-41aa-11e9-be5e-ae5bc1be57d7 took: 355.776688ms
Mar  8 13:59:26.426: INFO: Terminating ReplicationController wrapped-volume-race-593b9310-41aa-11e9-be5e-ae5bc1be57d7 pods took: 100.229641ms
STEP: Creating RC which spawns configmap-volume pods
Mar  8 14:00:01.837: INFO: Pod name wrapped-volume-race-7798eb26-41aa-11e9-be5e-ae5bc1be57d7: Found 0 pods out of 5
Mar  8 14:00:06.841: INFO: Pod name wrapped-volume-race-7798eb26-41aa-11e9-be5e-ae5bc1be57d7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7798eb26-41aa-11e9-be5e-ae5bc1be57d7 in namespace e2e-tests-emptydir-wrapper-2q6xd, will wait for the garbage collector to delete the pods
Mar  8 14:00:16.909: INFO: Deleting ReplicationController wrapped-volume-race-7798eb26-41aa-11e9-be5e-ae5bc1be57d7 took: 4.645724ms
Mar  8 14:00:17.009: INFO: Terminating ReplicationController wrapped-volume-race-7798eb26-41aa-11e9-be5e-ae5bc1be57d7 pods took: 100.237701ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:00:52.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-2q6xd" for this suite.
Mar  8 14:00:58.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:00:58.298: INFO: namespace: e2e-tests-emptydir-wrapper-2q6xd, resource: bindings, ignored listing per whitelist
Mar  8 14:00:58.316: INFO: namespace e2e-tests-emptydir-wrapper-2q6xd deletion completed in 6.089441837s

â€¢ [SLOW TEST:163.402 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:00:58.316: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:00:58.345345      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ctfqg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:00:58.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-ctfqg" to be "success or failure"
Mar  8 14:00:58.475: INFO: Pod "downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206318ms
Mar  8 14:01:00.477: INFO: Pod "downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004427944s
STEP: Saw pod success
Mar  8 14:01:00.477: INFO: Pod "downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:01:00.479: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:01:00.809: INFO: Waiting for pod downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:01:00.812: INFO: Pod downwardapi-volume-995bc109-41aa-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:01:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ctfqg" for this suite.
Mar  8 14:01:06.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:01:06.844: INFO: namespace: e2e-tests-projected-ctfqg, resource: bindings, ignored listing per whitelist
Mar  8 14:01:06.876: INFO: namespace e2e-tests-projected-ctfqg deletion completed in 6.061108604s

â€¢ [SLOW TEST:8.560 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:01:06.876: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:01:06.906858      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-pjlvm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0308 14:01:13.255068      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 14:01:13.255: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:01:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pjlvm" for this suite.
Mar  8 14:01:19.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:01:19.293: INFO: namespace: e2e-tests-gc-pjlvm, resource: bindings, ignored listing per whitelist
Mar  8 14:01:19.319: INFO: namespace e2e-tests-gc-pjlvm deletion completed in 6.061555007s

â€¢ [SLOW TEST:12.443 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:01:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:01:19.347587      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-d7w86
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-a5e0c8a0-41aa-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:01:19.479: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-d7w86" to be "success or failure"
Mar  8 14:01:19.481: INFO: Pod "pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.770848ms
Mar  8 14:01:21.483: INFO: Pod "pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004245458s
STEP: Saw pod success
Mar  8 14:01:21.483: INFO: Pod "pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:01:21.485: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 14:01:21.495: INFO: Waiting for pod pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:01:21.497: INFO: Pod pod-projected-configmaps-a5e11e52-41aa-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:01:21.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d7w86" for this suite.
Mar  8 14:01:27.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:01:27.524: INFO: namespace: e2e-tests-projected-d7w86, resource: bindings, ignored listing per whitelist
Mar  8 14:01:27.559: INFO: namespace e2e-tests-projected-d7w86 deletion completed in 6.05948057s

â€¢ [SLOW TEST:8.240 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:01:27.559: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:01:27.587353      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gqw9s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 14:01:27.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-gqw9s'
Mar  8 14:01:28.350: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  8 14:01:28.350: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Mar  8 14:01:30.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-gqw9s'
Mar  8 14:01:30.444: INFO: stderr: ""
Mar  8 14:01:30.445: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:01:30.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gqw9s" for this suite.
Mar  8 14:01:52.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:01:52.486: INFO: namespace: e2e-tests-kubectl-gqw9s, resource: bindings, ignored listing per whitelist
Mar  8 14:01:52.509: INFO: namespace e2e-tests-kubectl-gqw9s deletion completed in 22.061591229s

â€¢ [SLOW TEST:24.950 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:01:52.510: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:01:52.538037      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-pbhbs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:01:52.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-pbhbs" for this suite.
Mar  8 14:01:58.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:01:58.714: INFO: namespace: e2e-tests-services-pbhbs, resource: bindings, ignored listing per whitelist
Mar  8 14:01:58.729: INFO: namespace e2e-tests-services-pbhbs deletion completed in 6.05812386s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.219 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:01:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:01:58.762421      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tf7m2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar  8 14:01:58.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:01:59.228: INFO: stderr: ""
Mar  8 14:01:59.228: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 14:01:59.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:01:59.324: INFO: stderr: ""
Mar  8 14:01:59.324: INFO: stdout: "update-demo-nautilus-b8hzr update-demo-nautilus-llf64 "
Mar  8 14:01:59.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-b8hzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:01:59.407: INFO: stderr: ""
Mar  8 14:01:59.407: INFO: stdout: ""
Mar  8 14:01:59.407: INFO: update-demo-nautilus-b8hzr is created but not running
Mar  8 14:02:04.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:04.502: INFO: stderr: ""
Mar  8 14:02:04.502: INFO: stdout: "update-demo-nautilus-b8hzr update-demo-nautilus-llf64 "
Mar  8 14:02:04.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-b8hzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:04.596: INFO: stderr: ""
Mar  8 14:02:04.596: INFO: stdout: "true"
Mar  8 14:02:04.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-b8hzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:04.683: INFO: stderr: ""
Mar  8 14:02:04.684: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 14:02:04.684: INFO: validating pod update-demo-nautilus-b8hzr
Mar  8 14:02:04.687: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 14:02:04.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 14:02:04.688: INFO: update-demo-nautilus-b8hzr is verified up and running
Mar  8 14:02:04.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-llf64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:04.772: INFO: stderr: ""
Mar  8 14:02:04.772: INFO: stdout: "true"
Mar  8 14:02:04.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-nautilus-llf64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:04.857: INFO: stderr: ""
Mar  8 14:02:04.857: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  8 14:02:04.857: INFO: validating pod update-demo-nautilus-llf64
Mar  8 14:02:04.860: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  8 14:02:04.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  8 14:02:04.860: INFO: update-demo-nautilus-llf64 is verified up and running
STEP: rolling-update to new replication controller
Mar  8 14:02:04.861: INFO: scanned /root for discovery docs: <nil>
Mar  8 14:02:04.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.277: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  8 14:02:27.277: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  8 14:02:27.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.364: INFO: stderr: ""
Mar  8 14:02:27.364: INFO: stdout: "update-demo-kitten-gb7sf update-demo-kitten-h6k7d "
Mar  8 14:02:27.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-kitten-gb7sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.450: INFO: stderr: ""
Mar  8 14:02:27.450: INFO: stdout: "true"
Mar  8 14:02:27.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-kitten-gb7sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.541: INFO: stderr: ""
Mar  8 14:02:27.541: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  8 14:02:27.541: INFO: validating pod update-demo-kitten-gb7sf
Mar  8 14:02:27.546: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  8 14:02:27.546: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  8 14:02:27.546: INFO: update-demo-kitten-gb7sf is verified up and running
Mar  8 14:02:27.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-kitten-h6k7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.631: INFO: stderr: ""
Mar  8 14:02:27.631: INFO: stdout: "true"
Mar  8 14:02:27.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pods update-demo-kitten-h6k7d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tf7m2'
Mar  8 14:02:27.715: INFO: stderr: ""
Mar  8 14:02:27.715: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  8 14:02:27.715: INFO: validating pod update-demo-kitten-h6k7d
Mar  8 14:02:27.719: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  8 14:02:27.719: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  8 14:02:27.719: INFO: update-demo-kitten-h6k7d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:02:27.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tf7m2" for this suite.
Mar  8 14:02:49.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:02:49.776: INFO: namespace: e2e-tests-kubectl-tf7m2, resource: bindings, ignored listing per whitelist
Mar  8 14:02:49.804: INFO: namespace e2e-tests-kubectl-tf7m2 deletion completed in 22.082244475s

â€¢ [SLOW TEST:51.075 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:02:49.804: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:02:49.839292      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9lbvj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-dbd1fa40-41aa-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:02:49.986: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-9lbvj" to be "success or failure"
Mar  8 14:02:49.997: INFO: Pod "pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.819132ms
Mar  8 14:02:52.000: INFO: Pod "pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014146119s
STEP: Saw pod success
Mar  8 14:02:52.000: INFO: Pod "pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:02:52.001: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:02:52.018: INFO: Waiting for pod pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:02:52.020: INFO: Pod pod-projected-secrets-dbd25ea3-41aa-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:02:52.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9lbvj" for this suite.
Mar  8 14:02:58.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:02:58.075: INFO: namespace: e2e-tests-projected-9lbvj, resource: bindings, ignored listing per whitelist
Mar  8 14:02:58.087: INFO: namespace e2e-tests-projected-9lbvj deletion completed in 6.063699562s

â€¢ [SLOW TEST:8.283 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:02:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:02:58.119247      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-zr7cz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-wqwp
STEP: Creating a pod to test atomic-volume-subpath
Mar  8 14:02:58.251: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wqwp" in namespace "e2e-tests-subpath-zr7cz" to be "success or failure"
Mar  8 14:02:58.255: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.461876ms
Mar  8 14:03:00.257: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 2.005738291s
Mar  8 14:03:02.259: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 4.007961379s
Mar  8 14:03:04.262: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 6.01036627s
Mar  8 14:03:06.264: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 8.012646473s
Mar  8 14:03:08.266: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 10.014955023s
Mar  8 14:03:10.269: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 12.017245543s
Mar  8 14:03:12.271: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 14.01946034s
Mar  8 14:03:14.273: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 16.021742452s
Mar  8 14:03:16.275: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 18.024036127s
Mar  8 14:03:18.278: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Running", Reason="", readiness=false. Elapsed: 20.026232327s
Mar  8 14:03:20.280: INFO: Pod "pod-subpath-test-configmap-wqwp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.028450288s
STEP: Saw pod success
Mar  8 14:03:20.280: INFO: Pod "pod-subpath-test-configmap-wqwp" satisfied condition "success or failure"
Mar  8 14:03:20.281: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-subpath-test-configmap-wqwp container test-container-subpath-configmap-wqwp: <nil>
STEP: delete the pod
Mar  8 14:03:20.291: INFO: Waiting for pod pod-subpath-test-configmap-wqwp to disappear
Mar  8 14:03:20.294: INFO: Pod pod-subpath-test-configmap-wqwp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wqwp
Mar  8 14:03:20.294: INFO: Deleting pod "pod-subpath-test-configmap-wqwp" in namespace "e2e-tests-subpath-zr7cz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:03:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-zr7cz" for this suite.
Mar  8 14:03:26.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:03:26.349: INFO: namespace: e2e-tests-subpath-zr7cz, resource: bindings, ignored listing per whitelist
Mar  8 14:03:26.358: INFO: namespace e2e-tests-subpath-zr7cz deletion completed in 6.056806058s

â€¢ [SLOW TEST:28.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:03:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:03:26.388998      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-gdhdw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-gdhdw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-gdhdw to expose endpoints map[]
Mar  8 14:03:26.520: INFO: Get endpoints failed (4.210018ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  8 14:03:27.522: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-gdhdw exposes endpoints map[] (1.006215607s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-gdhdw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-gdhdw to expose endpoints map[pod1:[100]]
Mar  8 14:03:28.536: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-gdhdw exposes endpoints map[pod1:[100]] (1.010476923s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-gdhdw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-gdhdw to expose endpoints map[pod1:[100] pod2:[101]]
Mar  8 14:03:30.561: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-gdhdw exposes endpoints map[pod1:[100] pod2:[101]] (2.021364694s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-gdhdw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-gdhdw to expose endpoints map[pod2:[101]]
Mar  8 14:03:30.572: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-gdhdw exposes endpoints map[pod2:[101]] (6.579449ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-gdhdw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-gdhdw to expose endpoints map[]
Mar  8 14:03:30.582: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-gdhdw exposes endpoints map[] (2.232109ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:03:30.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-gdhdw" for this suite.
Mar  8 14:03:52.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:03:52.648: INFO: namespace: e2e-tests-services-gdhdw, resource: bindings, ignored listing per whitelist
Mar  8 14:03:52.657: INFO: namespace e2e-tests-services-gdhdw deletion completed in 22.058965352s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:26.299 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:03:52.657: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:03:52.684552      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-nh258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar  8 14:03:52.811: INFO: Waiting up to 5m0s for pod "var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-var-expansion-nh258" to be "success or failure"
Mar  8 14:03:52.814: INFO: Pod "var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097235ms
Mar  8 14:03:54.816: INFO: Pod "var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005352419s
STEP: Saw pod success
Mar  8 14:03:54.816: INFO: Pod "var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:03:54.818: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 14:03:54.829: INFO: Waiting for pod var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:03:54.832: INFO: Pod var-expansion-0145f067-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:03:54.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-nh258" for this suite.
Mar  8 14:04:00.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:04:00.889: INFO: namespace: e2e-tests-var-expansion-nh258, resource: bindings, ignored listing per whitelist
Mar  8 14:04:00.895: INFO: namespace e2e-tests-var-expansion-nh258 deletion completed in 6.060967432s

â€¢ [SLOW TEST:8.239 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:04:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:04:00.923528      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-pf59p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:04:01.052: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  8 14:04:06.054: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  8 14:04:06.054: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  8 14:04:06.068: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-pf59p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pf59p/deployments/test-cleanup-deployment,UID:092be143-41ab-11e9-8665-02fd46f865f2,ResourceVersion:17917,Generation:1,CreationTimestamp:2019-03-08 14:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  8 14:04:06.071: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar  8 14:04:06.071: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  8 14:04:06.071: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-pf59p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pf59p/replicasets/test-cleanup-controller,UID:062f175b-41ab-11e9-8665-02fd46f865f2,ResourceVersion:17918,Generation:1,CreationTimestamp:2019-03-08 14:04:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 092be143-41ab-11e9-8665-02fd46f865f2 0xc00236f617 0xc00236f618}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  8 14:04:06.073: INFO: Pod "test-cleanup-controller-hq6w8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-hq6w8,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-pf59p,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pf59p/pods/test-cleanup-controller-hq6w8,UID:062ff850-41ab-11e9-8665-02fd46f865f2,ResourceVersion:17910,Generation:0,CreationTimestamp:2019-03-08 14:04:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 062f175b-41ab-11e9-8665-02fd46f865f2 0xc00236fbc7 0xc00236fbc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-x4sp5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-x4sp5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-x4sp5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236fc30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236fc50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.3,StartTime:2019-03-08 14:04:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-08 14:04:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://783bbd34aad155836d99d75c328fe5f48a7dbc125080da0ed9fbd8f42450464b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:04:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-pf59p" for this suite.
Mar  8 14:04:12.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:04:12.134: INFO: namespace: e2e-tests-deployment-pf59p, resource: bindings, ignored listing per whitelist
Mar  8 14:04:12.168: INFO: namespace e2e-tests-deployment-pf59p deletion completed in 6.090986675s

â€¢ [SLOW TEST:11.272 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:04:12.168: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:04:12.197406      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xwl5n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:04:12.321: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  8 14:04:12.328: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  8 14:04:17.330: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  8 14:04:17.330: INFO: Creating deployment "test-rolling-update-deployment"
Mar  8 14:04:17.335: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  8 14:04:17.341: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  8 14:04:19.346: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  8 14:04:19.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687650657, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687650657, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687650657, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687650657, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  8 14:04:21.350: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  8 14:04:21.355: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-xwl5n,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xwl5n/deployments/test-rolling-update-deployment,UID:0fe42678-41ab-11e9-8665-02fd46f865f2,ResourceVersion:18034,Generation:1,CreationTimestamp:2019-03-08 14:04:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-08 14:04:17 +0000 UTC 2019-03-08 14:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-08 14:04:19 +0000 UTC 2019-03-08 14:04:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  8 14:04:21.356: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-xwl5n,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xwl5n/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:0fe63ee2-41ab-11e9-8665-02fd46f865f2,ResourceVersion:18024,Generation:1,CreationTimestamp:2019-03-08 14:04:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0fe42678-41ab-11e9-8665-02fd46f865f2 0xc001f4b417 0xc001f4b418}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  8 14:04:21.356: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  8 14:04:21.356: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-xwl5n,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xwl5n/replicasets/test-rolling-update-controller,UID:0ce7cb5f-41ab-11e9-8665-02fd46f865f2,ResourceVersion:18033,Generation:2,CreationTimestamp:2019-03-08 14:04:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0fe42678-41ab-11e9-8665-02fd46f865f2 0xc001f4b357 0xc001f4b358}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 14:04:21.358: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-m8hmv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-m8hmv,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-xwl5n,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xwl5n/pods/test-rolling-update-deployment-68b55d7bc6-m8hmv,UID:0fe699cb-41ab-11e9-8665-02fd46f865f2,ResourceVersion:18023,Generation:0,CreationTimestamp:2019-03-08 14:04:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 0fe63ee2-41ab-11e9-8665-02fd46f865f2 0xc001f4bf27 0xc001f4bf28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mjh7z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mjh7z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mjh7z true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008c6010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008c6030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:04:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:10.20.192.7,StartTime:2019-03-08 14:04:17 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-08 14:04:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://aea74c930760147a51630d330f3bec54437b0f26ec3e894fdf75c40208d2a5dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:04:21.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xwl5n" for this suite.
Mar  8 14:04:27.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:04:27.381: INFO: namespace: e2e-tests-deployment-xwl5n, resource: bindings, ignored listing per whitelist
Mar  8 14:04:27.418: INFO: namespace e2e-tests-deployment-xwl5n deletion completed in 6.057460415s

â€¢ [SLOW TEST:15.250 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:04:27.418: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:04:27.446293      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7vcwn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  8 14:04:27.573: INFO: Waiting up to 5m0s for pod "pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-7vcwn" to be "success or failure"
Mar  8 14:04:27.575: INFO: Pod "pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274642ms
Mar  8 14:04:29.578: INFO: Pod "pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004474308s
Mar  8 14:04:31.580: INFO: Pod "pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006589606s
STEP: Saw pod success
Mar  8 14:04:31.580: INFO: Pod "pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:04:31.581: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:04:31.592: INFO: Waiting for pod pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:04:31.594: INFO: Pod pod-15fdf876-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:04:31.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7vcwn" for this suite.
Mar  8 14:04:37.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:04:37.634: INFO: namespace: e2e-tests-emptydir-7vcwn, resource: bindings, ignored listing per whitelist
Mar  8 14:04:37.654: INFO: namespace e2e-tests-emptydir-7vcwn deletion completed in 6.058292537s

â€¢ [SLOW TEST:10.236 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:04:37.655: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:04:37.683079      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-km4sg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  8 14:04:37.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 create -f - --namespace=e2e-tests-kubectl-km4sg'
Mar  8 14:04:37.990: INFO: stderr: ""
Mar  8 14:04:37.990: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  8 14:04:38.993: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 14:04:38.993: INFO: Found 0 / 1
Mar  8 14:04:39.993: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 14:04:39.993: INFO: Found 1 / 1
Mar  8 14:04:39.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  8 14:04:39.995: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 14:04:39.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  8 14:04:39.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 patch pod redis-master-5qxnv --namespace=e2e-tests-kubectl-km4sg -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  8 14:04:40.085: INFO: stderr: ""
Mar  8 14:04:40.085: INFO: stdout: "pod/redis-master-5qxnv patched\n"
STEP: checking annotations
Mar  8 14:04:40.087: INFO: Selector matched 1 pods for map[app:redis]
Mar  8 14:04:40.087: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:04:40.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-km4sg" for this suite.
Mar  8 14:05:02.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:05:02.159: INFO: namespace: e2e-tests-kubectl-km4sg, resource: bindings, ignored listing per whitelist
Mar  8 14:05:02.177: INFO: namespace e2e-tests-kubectl-km4sg deletion completed in 22.087698558s

â€¢ [SLOW TEST:24.523 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:05:02.177: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:05:02.207417      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4hf8d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 14:05:02.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-4hf8d'
Mar  8 14:05:02.420: INFO: stderr: ""
Mar  8 14:05:02.420: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  8 14:05:07.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-4hf8d -o json'
Mar  8 14:05:07.555: INFO: stderr: ""
Mar  8 14:05:07.555: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-03-08T14:05:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-4hf8d\",\n        \"resourceVersion\": \"18228\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-4hf8d/pods/e2e-test-nginx-pod\",\n        \"uid\": \"2ac20b04-41ab-11e9-8665-02fd46f865f2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rcg2d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-192-168-72-251.us-west-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rcg2d\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rcg2d\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-08T14:05:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-08T14:05:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-08T14:05:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-08T14:05:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://5523da0a3abe45e344aa85681dd9cf3662d540bc65d9df737f553c99c2fb0a80\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-08T14:05:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.72.251\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.160.3\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-08T14:05:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  8 14:05:07.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 replace -f - --namespace=e2e-tests-kubectl-4hf8d'
Mar  8 14:05:07.714: INFO: stderr: ""
Mar  8 14:05:07.714: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Mar  8 14:05:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-4hf8d'
Mar  8 14:05:09.329: INFO: stderr: ""
Mar  8 14:05:09.330: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:05:09.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4hf8d" for this suite.
Mar  8 14:05:15.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:05:15.350: INFO: namespace: e2e-tests-kubectl-4hf8d, resource: bindings, ignored listing per whitelist
Mar  8 14:05:15.399: INFO: namespace e2e-tests-kubectl-4hf8d deletion completed in 6.065077457s

â€¢ [SLOW TEST:13.221 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:05:15.399: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:05:15.427415      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-j6nxt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  8 14:05:17.568: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-32975b40-41ab-11e9-be5e-ae5bc1be57d7,GenerateName:,Namespace:e2e-tests-events-j6nxt,SelfLink:/api/v1/namespaces/e2e-tests-events-j6nxt/pods/send-events-32975b40-41ab-11e9-be5e-ae5bc1be57d7,UID:32978f3f-41ab-11e9-8665-02fd46f865f2,ResourceVersion:18287,Generation:0,CreationTimestamp:2019-03-08 14:05:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 550400730,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-pxgrn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pxgrn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-pxgrn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-251.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023ca5d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023ca5f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:05:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:05:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:05:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:05:15 +0000 UTC  }],Message:,Reason:,HostIP:192.168.72.251,PodIP:10.20.160.3,StartTime:2019-03-08 14:05:15 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-08 14:05:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://5ffdd4655831f8f581cb6fd16c7826680dea69967bb339f1e9807c6cf6ecea0b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  8 14:05:19.572: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  8 14:05:21.575: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:05:21.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-j6nxt" for this suite.
Mar  8 14:05:59.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:05:59.629: INFO: namespace: e2e-tests-events-j6nxt, resource: bindings, ignored listing per whitelist
Mar  8 14:05:59.644: INFO: namespace e2e-tests-events-j6nxt deletion completed in 38.062635715s

â€¢ [SLOW TEST:44.245 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:05:59.644: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:05:59.682747      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-swqg2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-swqg2
I0308 14:05:59.825982      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-swqg2, replica count: 1
I0308 14:06:00.876453      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0308 14:06:01.876673      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  8 14:06:01.982: INFO: Created: latency-svc-tdlh9
Mar  8 14:06:01.988: INFO: Got endpoints: latency-svc-tdlh9 [11.124353ms]
Mar  8 14:06:01.996: INFO: Created: latency-svc-n92xt
Mar  8 14:06:02.000: INFO: Got endpoints: latency-svc-n92xt [12.572189ms]
Mar  8 14:06:02.002: INFO: Created: latency-svc-hzmdj
Mar  8 14:06:02.008: INFO: Created: latency-svc-dwz7k
Mar  8 14:06:02.008: INFO: Got endpoints: latency-svc-hzmdj [20.486211ms]
Mar  8 14:06:02.015: INFO: Got endpoints: latency-svc-dwz7k [26.912162ms]
Mar  8 14:06:02.018: INFO: Created: latency-svc-wgsgx
Mar  8 14:06:02.021: INFO: Got endpoints: latency-svc-wgsgx [32.992591ms]
Mar  8 14:06:02.026: INFO: Created: latency-svc-xmpt4
Mar  8 14:06:02.029: INFO: Got endpoints: latency-svc-xmpt4 [40.819462ms]
Mar  8 14:06:02.035: INFO: Created: latency-svc-zmhct
Mar  8 14:06:02.037: INFO: Got endpoints: latency-svc-zmhct [49.020915ms]
Mar  8 14:06:02.041: INFO: Created: latency-svc-hrkfh
Mar  8 14:06:02.046: INFO: Created: latency-svc-ggd8n
Mar  8 14:06:02.047: INFO: Got endpoints: latency-svc-hrkfh [59.001755ms]
Mar  8 14:06:02.050: INFO: Got endpoints: latency-svc-ggd8n [61.800156ms]
Mar  8 14:06:02.054: INFO: Created: latency-svc-kkj7p
Mar  8 14:06:02.062: INFO: Created: latency-svc-zpw8g
Mar  8 14:06:02.062: INFO: Got endpoints: latency-svc-kkj7p [74.148112ms]
Mar  8 14:06:02.066: INFO: Got endpoints: latency-svc-zpw8g [77.372294ms]
Mar  8 14:06:02.068: INFO: Created: latency-svc-4v7mv
Mar  8 14:06:02.076: INFO: Got endpoints: latency-svc-4v7mv [87.565305ms]
Mar  8 14:06:02.080: INFO: Created: latency-svc-mlvch
Mar  8 14:06:02.082: INFO: Got endpoints: latency-svc-mlvch [93.606842ms]
Mar  8 14:06:02.087: INFO: Created: latency-svc-tbw5s
Mar  8 14:06:02.090: INFO: Got endpoints: latency-svc-tbw5s [101.927418ms]
Mar  8 14:06:02.096: INFO: Created: latency-svc-s9k2k
Mar  8 14:06:02.097: INFO: Got endpoints: latency-svc-s9k2k [108.861216ms]
Mar  8 14:06:02.103: INFO: Created: latency-svc-qhjv9
Mar  8 14:06:02.119: INFO: Got endpoints: latency-svc-qhjv9 [130.298624ms]
Mar  8 14:06:02.131: INFO: Created: latency-svc-n4wpw
Mar  8 14:06:02.134: INFO: Got endpoints: latency-svc-n4wpw [133.874372ms]
Mar  8 14:06:02.140: INFO: Created: latency-svc-r8tzw
Mar  8 14:06:02.151: INFO: Created: latency-svc-fpznz
Mar  8 14:06:02.152: INFO: Got endpoints: latency-svc-r8tzw [143.642126ms]
Mar  8 14:06:02.160: INFO: Created: latency-svc-9rdrz
Mar  8 14:06:02.167: INFO: Got endpoints: latency-svc-fpznz [151.548878ms]
Mar  8 14:06:02.171: INFO: Got endpoints: latency-svc-9rdrz [149.997615ms]
Mar  8 14:06:02.175: INFO: Created: latency-svc-st5t7
Mar  8 14:06:02.182: INFO: Got endpoints: latency-svc-st5t7 [152.955972ms]
Mar  8 14:06:02.185: INFO: Created: latency-svc-v8qlw
Mar  8 14:06:02.188: INFO: Got endpoints: latency-svc-v8qlw [150.959081ms]
Mar  8 14:06:02.190: INFO: Created: latency-svc-2vblp
Mar  8 14:06:02.193: INFO: Got endpoints: latency-svc-2vblp [145.736974ms]
Mar  8 14:06:02.197: INFO: Created: latency-svc-hggc7
Mar  8 14:06:02.202: INFO: Got endpoints: latency-svc-hggc7 [19.950883ms]
Mar  8 14:06:02.207: INFO: Created: latency-svc-49cx6
Mar  8 14:06:02.210: INFO: Got endpoints: latency-svc-49cx6 [160.219507ms]
Mar  8 14:06:02.215: INFO: Created: latency-svc-tl9m2
Mar  8 14:06:02.219: INFO: Created: latency-svc-jl7ds
Mar  8 14:06:02.223: INFO: Created: latency-svc-lh5xj
Mar  8 14:06:02.226: INFO: Got endpoints: latency-svc-jl7ds [160.584451ms]
Mar  8 14:06:02.227: INFO: Got endpoints: latency-svc-tl9m2 [164.063448ms]
Mar  8 14:06:02.230: INFO: Got endpoints: latency-svc-lh5xj [154.284452ms]
Mar  8 14:06:02.235: INFO: Created: latency-svc-nptjd
Mar  8 14:06:02.239: INFO: Got endpoints: latency-svc-nptjd [156.746579ms]
Mar  8 14:06:02.252: INFO: Created: latency-svc-6jn7k
Mar  8 14:06:02.252: INFO: Got endpoints: latency-svc-6jn7k [161.981198ms]
Mar  8 14:06:02.253: INFO: Created: latency-svc-jjl8l
Mar  8 14:06:02.255: INFO: Got endpoints: latency-svc-jjl8l [158.140952ms]
Mar  8 14:06:02.261: INFO: Created: latency-svc-nd5zf
Mar  8 14:06:02.265: INFO: Got endpoints: latency-svc-nd5zf [146.429341ms]
Mar  8 14:06:02.271: INFO: Created: latency-svc-vxcj6
Mar  8 14:06:02.275: INFO: Got endpoints: latency-svc-vxcj6 [140.36501ms]
Mar  8 14:06:02.275: INFO: Created: latency-svc-cmv2x
Mar  8 14:06:02.287: INFO: Created: latency-svc-mzk8f
Mar  8 14:06:02.287: INFO: Got endpoints: latency-svc-cmv2x [135.298001ms]
Mar  8 14:06:02.293: INFO: Got endpoints: latency-svc-mzk8f [126.347214ms]
Mar  8 14:06:02.296: INFO: Created: latency-svc-6cdq6
Mar  8 14:06:02.303: INFO: Got endpoints: latency-svc-6cdq6 [131.840673ms]
Mar  8 14:06:02.306: INFO: Created: latency-svc-v5tgp
Mar  8 14:06:02.311: INFO: Created: latency-svc-8wd9w
Mar  8 14:06:02.314: INFO: Created: latency-svc-b6c4m
Mar  8 14:06:02.318: INFO: Created: latency-svc-h5gbt
Mar  8 14:06:02.327: INFO: Created: latency-svc-s8jsw
Mar  8 14:06:02.327: INFO: Created: latency-svc-4292g
Mar  8 14:06:02.335: INFO: Created: latency-svc-s5vfl
Mar  8 14:06:02.337: INFO: Got endpoints: latency-svc-v5tgp [148.688107ms]
Mar  8 14:06:02.354: INFO: Created: latency-svc-tfhvv
Mar  8 14:06:02.440: INFO: Got endpoints: latency-svc-8wd9w [247.157831ms]
Mar  8 14:06:02.443: INFO: Created: latency-svc-mkdh8
Mar  8 14:06:02.461: INFO: Got endpoints: latency-svc-b6c4m [258.341522ms]
Mar  8 14:06:02.482: INFO: Created: latency-svc-fnwsb
Mar  8 14:06:02.484: INFO: Created: latency-svc-phpd4
Mar  8 14:06:02.485: INFO: Got endpoints: latency-svc-h5gbt [274.847675ms]
Mar  8 14:06:02.488: INFO: Created: latency-svc-ffpx5
Mar  8 14:06:02.491: INFO: Created: latency-svc-gzpgk
Mar  8 14:06:02.497: INFO: Created: latency-svc-jknlw
Mar  8 14:06:02.503: INFO: Created: latency-svc-blrnt
Mar  8 14:06:02.504: INFO: Created: latency-svc-fxgtt
Mar  8 14:06:02.509: INFO: Created: latency-svc-trbmm
Mar  8 14:06:02.517: INFO: Created: latency-svc-mkzzk
Mar  8 14:06:02.521: INFO: Created: latency-svc-m6tdg
Mar  8 14:06:02.534: INFO: Got endpoints: latency-svc-4292g [307.622816ms]
Mar  8 14:06:02.540: INFO: Created: latency-svc-k2hz2
Mar  8 14:06:02.584: INFO: Got endpoints: latency-svc-s8jsw [357.291727ms]
Mar  8 14:06:02.590: INFO: Created: latency-svc-qnds5
Mar  8 14:06:02.637: INFO: Got endpoints: latency-svc-s5vfl [406.71369ms]
Mar  8 14:06:02.645: INFO: Created: latency-svc-7stf2
Mar  8 14:06:02.684: INFO: Got endpoints: latency-svc-tfhvv [445.115958ms]
Mar  8 14:06:02.691: INFO: Created: latency-svc-jdpl2
Mar  8 14:06:02.735: INFO: Got endpoints: latency-svc-mkdh8 [482.480697ms]
Mar  8 14:06:02.741: INFO: Created: latency-svc-6wcdk
Mar  8 14:06:02.784: INFO: Got endpoints: latency-svc-fnwsb [528.974117ms]
Mar  8 14:06:02.791: INFO: Created: latency-svc-pvx46
Mar  8 14:06:02.837: INFO: Got endpoints: latency-svc-phpd4 [572.025968ms]
Mar  8 14:06:02.846: INFO: Created: latency-svc-jhqf5
Mar  8 14:06:02.889: INFO: Got endpoints: latency-svc-ffpx5 [614.712221ms]
Mar  8 14:06:02.896: INFO: Created: latency-svc-kncwk
Mar  8 14:06:02.934: INFO: Got endpoints: latency-svc-gzpgk [646.274852ms]
Mar  8 14:06:02.941: INFO: Created: latency-svc-5bksc
Mar  8 14:06:02.986: INFO: Got endpoints: latency-svc-jknlw [693.129423ms]
Mar  8 14:06:02.994: INFO: Created: latency-svc-wj84r
Mar  8 14:06:03.036: INFO: Got endpoints: latency-svc-blrnt [733.217124ms]
Mar  8 14:06:03.042: INFO: Created: latency-svc-rlqbz
Mar  8 14:06:03.084: INFO: Got endpoints: latency-svc-fxgtt [747.087661ms]
Mar  8 14:06:03.089: INFO: Created: latency-svc-sngcl
Mar  8 14:06:03.136: INFO: Got endpoints: latency-svc-trbmm [695.424535ms]
Mar  8 14:06:03.143: INFO: Created: latency-svc-4qknb
Mar  8 14:06:03.184: INFO: Got endpoints: latency-svc-mkzzk [723.357649ms]
Mar  8 14:06:03.193: INFO: Created: latency-svc-rtllt
Mar  8 14:06:03.371: INFO: Got endpoints: latency-svc-qnds5 [787.541524ms]
Mar  8 14:06:03.372: INFO: Got endpoints: latency-svc-m6tdg [886.501708ms]
Mar  8 14:06:03.372: INFO: Got endpoints: latency-svc-k2hz2 [837.877791ms]
Mar  8 14:06:03.394: INFO: Got endpoints: latency-svc-7stf2 [757.152654ms]
Mar  8 14:06:03.394: INFO: Created: latency-svc-s6f2s
Mar  8 14:06:03.406: INFO: Created: latency-svc-lk4q5
Mar  8 14:06:03.430: INFO: Created: latency-svc-zw8rq
Mar  8 14:06:03.435: INFO: Got endpoints: latency-svc-jdpl2 [751.366087ms]
Mar  8 14:06:03.439: INFO: Created: latency-svc-lk6vw
Mar  8 14:06:03.586: INFO: Created: latency-svc-2c2g4
Mar  8 14:06:03.587: INFO: Got endpoints: latency-svc-pvx46 [802.962436ms]
Mar  8 14:06:03.590: INFO: Got endpoints: latency-svc-6wcdk [855.659837ms]
Mar  8 14:06:03.592: INFO: Got endpoints: latency-svc-jhqf5 [754.725154ms]
Mar  8 14:06:03.598: INFO: Created: latency-svc-grv72
Mar  8 14:06:03.601: INFO: Created: latency-svc-6vxg9
Mar  8 14:06:03.607: INFO: Created: latency-svc-zpm9n
Mar  8 14:06:03.635: INFO: Got endpoints: latency-svc-kncwk [745.62084ms]
Mar  8 14:06:03.642: INFO: Created: latency-svc-2mmjv
Mar  8 14:06:03.685: INFO: Got endpoints: latency-svc-5bksc [750.870254ms]
Mar  8 14:06:03.691: INFO: Created: latency-svc-zr5rc
Mar  8 14:06:03.734: INFO: Got endpoints: latency-svc-wj84r [748.235982ms]
Mar  8 14:06:03.743: INFO: Created: latency-svc-cmvwr
Mar  8 14:06:03.784: INFO: Got endpoints: latency-svc-rlqbz [747.551236ms]
Mar  8 14:06:03.791: INFO: Created: latency-svc-vbdbp
Mar  8 14:06:03.834: INFO: Got endpoints: latency-svc-sngcl [750.405206ms]
Mar  8 14:06:03.842: INFO: Created: latency-svc-b7wp4
Mar  8 14:06:03.884: INFO: Got endpoints: latency-svc-4qknb [748.34903ms]
Mar  8 14:06:03.890: INFO: Created: latency-svc-lz4f6
Mar  8 14:06:03.936: INFO: Got endpoints: latency-svc-rtllt [751.632251ms]
Mar  8 14:06:03.946: INFO: Created: latency-svc-zjsgz
Mar  8 14:06:03.984: INFO: Got endpoints: latency-svc-s6f2s [612.851191ms]
Mar  8 14:06:03.991: INFO: Created: latency-svc-pxxh5
Mar  8 14:06:04.034: INFO: Got endpoints: latency-svc-lk4q5 [662.104385ms]
Mar  8 14:06:04.041: INFO: Created: latency-svc-8rgfq
Mar  8 14:06:04.085: INFO: Got endpoints: latency-svc-zw8rq [713.463156ms]
Mar  8 14:06:04.091: INFO: Created: latency-svc-x7vqg
Mar  8 14:06:04.134: INFO: Got endpoints: latency-svc-lk6vw [739.545537ms]
Mar  8 14:06:04.140: INFO: Created: latency-svc-fnn2j
Mar  8 14:06:04.184: INFO: Got endpoints: latency-svc-2c2g4 [748.988729ms]
Mar  8 14:06:04.190: INFO: Created: latency-svc-hzncn
Mar  8 14:06:04.234: INFO: Got endpoints: latency-svc-grv72 [646.427479ms]
Mar  8 14:06:04.240: INFO: Created: latency-svc-ndn7x
Mar  8 14:06:04.285: INFO: Got endpoints: latency-svc-6vxg9 [694.959221ms]
Mar  8 14:06:04.292: INFO: Created: latency-svc-4kw2v
Mar  8 14:06:04.335: INFO: Got endpoints: latency-svc-zpm9n [742.651085ms]
Mar  8 14:06:04.340: INFO: Created: latency-svc-s6ml7
Mar  8 14:06:04.385: INFO: Got endpoints: latency-svc-2mmjv [749.835817ms]
Mar  8 14:06:04.393: INFO: Created: latency-svc-jljbp
Mar  8 14:06:04.434: INFO: Got endpoints: latency-svc-zr5rc [749.384392ms]
Mar  8 14:06:04.440: INFO: Created: latency-svc-lvmr9
Mar  8 14:06:04.484: INFO: Got endpoints: latency-svc-cmvwr [750.138218ms]
Mar  8 14:06:04.494: INFO: Created: latency-svc-pk664
Mar  8 14:06:04.535: INFO: Got endpoints: latency-svc-vbdbp [751.031548ms]
Mar  8 14:06:04.551: INFO: Created: latency-svc-gfsgc
Mar  8 14:06:04.585: INFO: Got endpoints: latency-svc-b7wp4 [750.16331ms]
Mar  8 14:06:04.593: INFO: Created: latency-svc-l67b2
Mar  8 14:06:04.637: INFO: Got endpoints: latency-svc-lz4f6 [752.47499ms]
Mar  8 14:06:04.644: INFO: Created: latency-svc-mftfj
Mar  8 14:06:04.686: INFO: Got endpoints: latency-svc-zjsgz [750.64658ms]
Mar  8 14:06:04.693: INFO: Created: latency-svc-kdfb7
Mar  8 14:06:04.801: INFO: Got endpoints: latency-svc-pxxh5 [816.948859ms]
Mar  8 14:06:04.803: INFO: Got endpoints: latency-svc-8rgfq [768.775142ms]
Mar  8 14:06:04.811: INFO: Created: latency-svc-mbzmk
Mar  8 14:06:04.816: INFO: Created: latency-svc-5fp5p
Mar  8 14:06:04.835: INFO: Got endpoints: latency-svc-x7vqg [749.693207ms]
Mar  8 14:06:05.249: INFO: Got endpoints: latency-svc-fnn2j [1.115203978s]
Mar  8 14:06:05.250: INFO: Created: latency-svc-wfdl2
Mar  8 14:06:05.254: INFO: Got endpoints: latency-svc-4kw2v [968.788988ms]
Mar  8 14:06:05.254: INFO: Got endpoints: latency-svc-hzncn [1.070164638s]
Mar  8 14:06:05.255: INFO: Got endpoints: latency-svc-ndn7x [1.020719341s]
Mar  8 14:06:05.257: INFO: Got endpoints: latency-svc-s6ml7 [922.194591ms]
Mar  8 14:06:05.264: INFO: Got endpoints: latency-svc-jljbp [878.898299ms]
Mar  8 14:06:05.268: INFO: Got endpoints: latency-svc-lvmr9 [833.529086ms]
Mar  8 14:06:05.268: INFO: Got endpoints: latency-svc-pk664 [783.274326ms]
Mar  8 14:06:05.274: INFO: Created: latency-svc-rz7md
Mar  8 14:06:05.278: INFO: Created: latency-svc-kwvn9
Mar  8 14:06:05.282: INFO: Created: latency-svc-nr9nk
Mar  8 14:06:05.288: INFO: Got endpoints: latency-svc-gfsgc [753.302506ms]
Mar  8 14:06:05.291: INFO: Created: latency-svc-sxdph
Mar  8 14:06:05.295: INFO: Created: latency-svc-kf2ws
Mar  8 14:06:05.300: INFO: Created: latency-svc-mr769
Mar  8 14:06:05.309: INFO: Created: latency-svc-vl95q
Mar  8 14:06:05.314: INFO: Created: latency-svc-tzzjb
Mar  8 14:06:05.322: INFO: Created: latency-svc-fdz82
Mar  8 14:06:05.335: INFO: Got endpoints: latency-svc-l67b2 [750.112934ms]
Mar  8 14:06:05.344: INFO: Created: latency-svc-fj9xn
Mar  8 14:06:05.387: INFO: Got endpoints: latency-svc-mftfj [750.715679ms]
Mar  8 14:06:05.393: INFO: Created: latency-svc-nlj2n
Mar  8 14:06:05.436: INFO: Got endpoints: latency-svc-kdfb7 [749.584807ms]
Mar  8 14:06:05.448: INFO: Created: latency-svc-crxvw
Mar  8 14:06:05.484: INFO: Got endpoints: latency-svc-mbzmk [683.099982ms]
Mar  8 14:06:05.492: INFO: Created: latency-svc-6ctbq
Mar  8 14:06:05.534: INFO: Got endpoints: latency-svc-5fp5p [731.544394ms]
Mar  8 14:06:05.545: INFO: Created: latency-svc-sp5j8
Mar  8 14:06:05.585: INFO: Got endpoints: latency-svc-wfdl2 [749.613513ms]
Mar  8 14:06:05.592: INFO: Created: latency-svc-fddfc
Mar  8 14:06:05.634: INFO: Got endpoints: latency-svc-rz7md [385.195885ms]
Mar  8 14:06:05.642: INFO: Created: latency-svc-hrj5n
Mar  8 14:06:05.684: INFO: Got endpoints: latency-svc-kwvn9 [429.425659ms]
Mar  8 14:06:05.691: INFO: Created: latency-svc-8hd6t
Mar  8 14:06:05.737: INFO: Got endpoints: latency-svc-nr9nk [482.847985ms]
Mar  8 14:06:06.311: INFO: Got endpoints: latency-svc-mr769 [1.046763191s]
Mar  8 14:06:06.311: INFO: Got endpoints: latency-svc-sxdph [1.056504858s]
Mar  8 14:06:06.311: INFO: Got endpoints: latency-svc-kf2ws [1.054102385s]
Mar  8 14:06:06.311: INFO: Created: latency-svc-qfvjd
Mar  8 14:06:06.311: INFO: Got endpoints: latency-svc-tzzjb [1.043506268s]
Mar  8 14:06:06.311: INFO: Got endpoints: latency-svc-vl95q [1.043855383s]
Mar  8 14:06:06.323: INFO: Got endpoints: latency-svc-fdz82 [1.034477122s]
Mar  8 14:06:06.325: INFO: Got endpoints: latency-svc-fj9xn [989.822168ms]
Mar  8 14:06:06.328: INFO: Got endpoints: latency-svc-6ctbq [843.273399ms]
Mar  8 14:06:06.328: INFO: Got endpoints: latency-svc-nlj2n [940.597742ms]
Mar  8 14:06:06.328: INFO: Got endpoints: latency-svc-crxvw [892.151713ms]
Mar  8 14:06:06.330: INFO: Got endpoints: latency-svc-sp5j8 [795.880786ms]
Mar  8 14:06:06.336: INFO: Created: latency-svc-zhtv9
Mar  8 14:06:06.338: INFO: Got endpoints: latency-svc-fddfc [753.395606ms]
Mar  8 14:06:06.345: INFO: Created: latency-svc-mnpzd
Mar  8 14:06:06.347: INFO: Created: latency-svc-z9tq8
Mar  8 14:06:06.351: INFO: Created: latency-svc-f2srp
Mar  8 14:06:06.358: INFO: Created: latency-svc-dsqqp
Mar  8 14:06:06.362: INFO: Created: latency-svc-p8ncw
Mar  8 14:06:06.366: INFO: Created: latency-svc-rpcn2
Mar  8 14:06:06.371: INFO: Created: latency-svc-7985z
Mar  8 14:06:06.373: INFO: Created: latency-svc-p8tmm
Mar  8 14:06:06.379: INFO: Created: latency-svc-844jr
Mar  8 14:06:06.387: INFO: Created: latency-svc-pj7wt
Mar  8 14:06:06.390: INFO: Got endpoints: latency-svc-hrj5n [755.455528ms]
Mar  8 14:06:06.393: INFO: Created: latency-svc-q6tzw
Mar  8 14:06:06.397: INFO: Created: latency-svc-6b9h7
Mar  8 14:06:06.434: INFO: Got endpoints: latency-svc-8hd6t [750.094986ms]
Mar  8 14:06:06.441: INFO: Created: latency-svc-2tgfr
Mar  8 14:06:06.484: INFO: Got endpoints: latency-svc-qfvjd [747.115546ms]
Mar  8 14:06:06.497: INFO: Created: latency-svc-lwqkl
Mar  8 14:06:06.535: INFO: Got endpoints: latency-svc-zhtv9 [224.484406ms]
Mar  8 14:06:06.543: INFO: Created: latency-svc-6p5zg
Mar  8 14:06:06.585: INFO: Got endpoints: latency-svc-mnpzd [274.010736ms]
Mar  8 14:06:06.592: INFO: Created: latency-svc-5kgp2
Mar  8 14:06:06.636: INFO: Got endpoints: latency-svc-z9tq8 [325.313128ms]
Mar  8 14:06:06.644: INFO: Created: latency-svc-6bzbg
Mar  8 14:06:06.684: INFO: Got endpoints: latency-svc-f2srp [372.828565ms]
Mar  8 14:06:06.690: INFO: Created: latency-svc-kxxnk
Mar  8 14:06:06.735: INFO: Got endpoints: latency-svc-dsqqp [423.704158ms]
Mar  8 14:06:06.742: INFO: Created: latency-svc-bfk87
Mar  8 14:06:06.786: INFO: Got endpoints: latency-svc-p8ncw [463.490863ms]
Mar  8 14:06:06.864: INFO: Created: latency-svc-xwfz7
Mar  8 14:06:06.864: INFO: Got endpoints: latency-svc-rpcn2 [539.807527ms]
Mar  8 14:06:06.873: INFO: Created: latency-svc-5l8xp
Mar  8 14:06:06.887: INFO: Got endpoints: latency-svc-7985z [559.563024ms]
Mar  8 14:06:06.896: INFO: Created: latency-svc-tqqdx
Mar  8 14:06:06.935: INFO: Got endpoints: latency-svc-p8tmm [606.969192ms]
Mar  8 14:06:07.101: INFO: Got endpoints: latency-svc-q6tzw [763.020425ms]
Mar  8 14:06:07.101: INFO: Got endpoints: latency-svc-844jr [773.131928ms]
Mar  8 14:06:07.101: INFO: Got endpoints: latency-svc-pj7wt [771.194751ms]
Mar  8 14:06:07.108: INFO: Created: latency-svc-tkv54
Mar  8 14:06:07.112: INFO: Created: latency-svc-n5xxr
Mar  8 14:06:07.119: INFO: Created: latency-svc-mgpz7
Mar  8 14:06:07.126: INFO: Created: latency-svc-52nlf
Mar  8 14:06:07.135: INFO: Got endpoints: latency-svc-6b9h7 [745.512596ms]
Mar  8 14:06:07.141: INFO: Created: latency-svc-77gsc
Mar  8 14:06:07.185: INFO: Got endpoints: latency-svc-2tgfr [750.572869ms]
Mar  8 14:06:07.191: INFO: Created: latency-svc-tsdtf
Mar  8 14:06:07.234: INFO: Got endpoints: latency-svc-lwqkl [749.509455ms]
Mar  8 14:06:07.242: INFO: Created: latency-svc-hdg5r
Mar  8 14:06:07.285: INFO: Got endpoints: latency-svc-6p5zg [749.472729ms]
Mar  8 14:06:07.291: INFO: Created: latency-svc-sj7zx
Mar  8 14:06:07.335: INFO: Got endpoints: latency-svc-5kgp2 [749.688054ms]
Mar  8 14:06:07.341: INFO: Created: latency-svc-p5v55
Mar  8 14:06:07.385: INFO: Got endpoints: latency-svc-6bzbg [748.724323ms]
Mar  8 14:06:07.393: INFO: Created: latency-svc-hkmhl
Mar  8 14:06:07.434: INFO: Got endpoints: latency-svc-kxxnk [749.735168ms]
Mar  8 14:06:07.440: INFO: Created: latency-svc-mbhk7
Mar  8 14:06:07.484: INFO: Got endpoints: latency-svc-bfk87 [748.724111ms]
Mar  8 14:06:07.490: INFO: Created: latency-svc-n4xk5
Mar  8 14:06:07.535: INFO: Got endpoints: latency-svc-xwfz7 [749.077145ms]
Mar  8 14:06:07.543: INFO: Created: latency-svc-vw4r4
Mar  8 14:06:07.584: INFO: Got endpoints: latency-svc-5l8xp [719.337117ms]
Mar  8 14:06:07.591: INFO: Created: latency-svc-mm4lr
Mar  8 14:06:07.634: INFO: Got endpoints: latency-svc-tqqdx [747.032121ms]
Mar  8 14:06:07.642: INFO: Created: latency-svc-d8xhf
Mar  8 14:06:07.684: INFO: Got endpoints: latency-svc-tkv54 [749.31192ms]
Mar  8 14:06:07.692: INFO: Created: latency-svc-4stbt
Mar  8 14:06:07.736: INFO: Got endpoints: latency-svc-n5xxr [634.534946ms]
Mar  8 14:06:07.743: INFO: Created: latency-svc-7n4p7
Mar  8 14:06:07.786: INFO: Got endpoints: latency-svc-mgpz7 [684.487563ms]
Mar  8 14:06:07.792: INFO: Created: latency-svc-kw777
Mar  8 14:06:07.835: INFO: Got endpoints: latency-svc-52nlf [733.909245ms]
Mar  8 14:06:07.841: INFO: Created: latency-svc-sshg8
Mar  8 14:06:07.885: INFO: Got endpoints: latency-svc-77gsc [750.114663ms]
Mar  8 14:06:07.891: INFO: Created: latency-svc-b855s
Mar  8 14:06:07.934: INFO: Got endpoints: latency-svc-tsdtf [749.572877ms]
Mar  8 14:06:07.941: INFO: Created: latency-svc-9b7gr
Mar  8 14:06:07.985: INFO: Got endpoints: latency-svc-hdg5r [751.057056ms]
Mar  8 14:06:07.994: INFO: Created: latency-svc-2526z
Mar  8 14:06:08.036: INFO: Got endpoints: latency-svc-sj7zx [751.35069ms]
Mar  8 14:06:08.044: INFO: Created: latency-svc-5j9mb
Mar  8 14:06:08.084: INFO: Got endpoints: latency-svc-p5v55 [749.511737ms]
Mar  8 14:06:08.090: INFO: Created: latency-svc-wj8wb
Mar  8 14:06:08.135: INFO: Got endpoints: latency-svc-hkmhl [750.185525ms]
Mar  8 14:06:08.140: INFO: Created: latency-svc-fddrf
Mar  8 14:06:08.184: INFO: Got endpoints: latency-svc-mbhk7 [750.058345ms]
Mar  8 14:06:08.191: INFO: Created: latency-svc-qf7fk
Mar  8 14:06:08.234: INFO: Got endpoints: latency-svc-n4xk5 [750.372273ms]
Mar  8 14:06:08.248: INFO: Created: latency-svc-m74jl
Mar  8 14:06:08.285: INFO: Got endpoints: latency-svc-vw4r4 [749.878884ms]
Mar  8 14:06:08.292: INFO: Created: latency-svc-n8d6v
Mar  8 14:06:08.335: INFO: Got endpoints: latency-svc-mm4lr [751.617702ms]
Mar  8 14:06:08.345: INFO: Created: latency-svc-czkjk
Mar  8 14:06:08.384: INFO: Got endpoints: latency-svc-d8xhf [749.593677ms]
Mar  8 14:06:08.391: INFO: Created: latency-svc-c5ncg
Mar  8 14:06:08.435: INFO: Got endpoints: latency-svc-4stbt [751.168076ms]
Mar  8 14:06:08.443: INFO: Created: latency-svc-rglzk
Mar  8 14:06:08.488: INFO: Got endpoints: latency-svc-7n4p7 [752.183923ms]
Mar  8 14:06:08.494: INFO: Created: latency-svc-6vjqp
Mar  8 14:06:08.534: INFO: Got endpoints: latency-svc-kw777 [747.704469ms]
Mar  8 14:06:08.542: INFO: Created: latency-svc-nbdmx
Mar  8 14:06:08.584: INFO: Got endpoints: latency-svc-sshg8 [748.632652ms]
Mar  8 14:06:08.591: INFO: Created: latency-svc-vh66q
Mar  8 14:06:08.636: INFO: Got endpoints: latency-svc-b855s [750.591815ms]
Mar  8 14:06:08.644: INFO: Created: latency-svc-x8qvv
Mar  8 14:06:08.685: INFO: Got endpoints: latency-svc-9b7gr [750.136833ms]
Mar  8 14:06:08.690: INFO: Created: latency-svc-bft2j
Mar  8 14:06:08.736: INFO: Got endpoints: latency-svc-2526z [750.386944ms]
Mar  8 14:06:08.745: INFO: Created: latency-svc-f2dd6
Mar  8 14:06:08.785: INFO: Got endpoints: latency-svc-5j9mb [749.32582ms]
Mar  8 14:06:08.792: INFO: Created: latency-svc-s6wlk
Mar  8 14:06:08.835: INFO: Got endpoints: latency-svc-wj8wb [750.894726ms]
Mar  8 14:06:08.841: INFO: Created: latency-svc-hc59f
Mar  8 14:06:08.890: INFO: Got endpoints: latency-svc-fddrf [754.980213ms]
Mar  8 14:06:08.895: INFO: Created: latency-svc-x2mv6
Mar  8 14:06:08.935: INFO: Got endpoints: latency-svc-qf7fk [751.213325ms]
Mar  8 14:06:08.942: INFO: Created: latency-svc-hj7bv
Mar  8 14:06:08.986: INFO: Got endpoints: latency-svc-m74jl [751.421695ms]
Mar  8 14:06:08.992: INFO: Created: latency-svc-hhcwv
Mar  8 14:06:09.034: INFO: Got endpoints: latency-svc-n8d6v [749.023696ms]
Mar  8 14:06:09.041: INFO: Created: latency-svc-mczkn
Mar  8 14:06:09.086: INFO: Got endpoints: latency-svc-czkjk [750.237633ms]
Mar  8 14:06:09.092: INFO: Created: latency-svc-9h974
Mar  8 14:06:09.134: INFO: Got endpoints: latency-svc-c5ncg [750.115571ms]
Mar  8 14:06:09.143: INFO: Created: latency-svc-4dq5x
Mar  8 14:06:09.185: INFO: Got endpoints: latency-svc-rglzk [749.249021ms]
Mar  8 14:06:09.190: INFO: Created: latency-svc-mhxvv
Mar  8 14:06:09.444: INFO: Got endpoints: latency-svc-bft2j [759.128541ms]
Mar  8 14:06:09.444: INFO: Got endpoints: latency-svc-6vjqp [956.14899ms]
Mar  8 14:06:09.444: INFO: Got endpoints: latency-svc-nbdmx [910.605081ms]
Mar  8 14:06:09.444: INFO: Got endpoints: latency-svc-vh66q [860.262289ms]
Mar  8 14:06:09.444: INFO: Got endpoints: latency-svc-x8qvv [808.434799ms]
Mar  8 14:06:09.465: INFO: Created: latency-svc-fv842
Mar  8 14:06:09.478: INFO: Created: latency-svc-7kd75
Mar  8 14:06:09.489: INFO: Got endpoints: latency-svc-f2dd6 [753.685548ms]
Mar  8 14:06:09.499: INFO: Created: latency-svc-v95ph
Mar  8 14:06:09.502: INFO: Created: latency-svc-rpwf6
Mar  8 14:06:09.514: INFO: Created: latency-svc-l575q
Mar  8 14:06:09.530: INFO: Created: latency-svc-k5ntn
Mar  8 14:06:09.539: INFO: Got endpoints: latency-svc-s6wlk [753.910317ms]
Mar  8 14:06:09.556: INFO: Created: latency-svc-qzwqz
Mar  8 14:06:09.584: INFO: Got endpoints: latency-svc-hc59f [749.038441ms]
Mar  8 14:06:09.591: INFO: Created: latency-svc-lvvrb
Mar  8 14:06:09.635: INFO: Got endpoints: latency-svc-x2mv6 [744.59401ms]
Mar  8 14:06:09.642: INFO: Created: latency-svc-fq9d8
Mar  8 14:06:09.685: INFO: Got endpoints: latency-svc-hj7bv [750.007611ms]
Mar  8 14:06:09.692: INFO: Created: latency-svc-5n8cm
Mar  8 14:06:09.774: INFO: Got endpoints: latency-svc-hhcwv [787.774276ms]
Mar  8 14:06:09.782: INFO: Created: latency-svc-89r4s
Mar  8 14:06:09.785: INFO: Got endpoints: latency-svc-mczkn [750.625954ms]
Mar  8 14:06:09.799: INFO: Created: latency-svc-4dcrj
Mar  8 14:06:09.836: INFO: Got endpoints: latency-svc-9h974 [750.072067ms]
Mar  8 14:06:09.885: INFO: Got endpoints: latency-svc-4dq5x [750.810186ms]
Mar  8 14:06:09.934: INFO: Got endpoints: latency-svc-mhxvv [749.560198ms]
Mar  8 14:06:09.986: INFO: Got endpoints: latency-svc-fv842 [541.877462ms]
Mar  8 14:06:10.034: INFO: Got endpoints: latency-svc-7kd75 [590.275926ms]
Mar  8 14:06:10.399: INFO: Got endpoints: latency-svc-v95ph [954.962243ms]
Mar  8 14:06:10.401: INFO: Got endpoints: latency-svc-qzwqz [861.773545ms]
Mar  8 14:06:10.401: INFO: Got endpoints: latency-svc-rpwf6 [956.763671ms]
Mar  8 14:06:10.401: INFO: Got endpoints: latency-svc-l575q [956.644024ms]
Mar  8 14:06:10.401: INFO: Got endpoints: latency-svc-k5ntn [911.809378ms]
Mar  8 14:06:10.408: INFO: Got endpoints: latency-svc-fq9d8 [773.134978ms]
Mar  8 14:06:10.411: INFO: Got endpoints: latency-svc-lvvrb [826.340749ms]
Mar  8 14:06:10.436: INFO: Got endpoints: latency-svc-5n8cm [750.150673ms]
Mar  8 14:06:10.485: INFO: Got endpoints: latency-svc-89r4s [711.799218ms]
Mar  8 14:06:10.538: INFO: Got endpoints: latency-svc-4dcrj [752.605445ms]
Mar  8 14:06:10.538: INFO: Latencies: [12.572189ms 19.950883ms 20.486211ms 26.912162ms 32.992591ms 40.819462ms 49.020915ms 59.001755ms 61.800156ms 74.148112ms 77.372294ms 87.565305ms 93.606842ms 101.927418ms 108.861216ms 126.347214ms 130.298624ms 131.840673ms 133.874372ms 135.298001ms 140.36501ms 143.642126ms 145.736974ms 146.429341ms 148.688107ms 149.997615ms 150.959081ms 151.548878ms 152.955972ms 154.284452ms 156.746579ms 158.140952ms 160.219507ms 160.584451ms 161.981198ms 164.063448ms 224.484406ms 247.157831ms 258.341522ms 274.010736ms 274.847675ms 307.622816ms 325.313128ms 357.291727ms 372.828565ms 385.195885ms 406.71369ms 423.704158ms 429.425659ms 445.115958ms 463.490863ms 482.480697ms 482.847985ms 528.974117ms 539.807527ms 541.877462ms 559.563024ms 572.025968ms 590.275926ms 606.969192ms 612.851191ms 614.712221ms 634.534946ms 646.274852ms 646.427479ms 662.104385ms 683.099982ms 684.487563ms 693.129423ms 694.959221ms 695.424535ms 711.799218ms 713.463156ms 719.337117ms 723.357649ms 731.544394ms 733.217124ms 733.909245ms 739.545537ms 742.651085ms 744.59401ms 745.512596ms 745.62084ms 747.032121ms 747.087661ms 747.115546ms 747.551236ms 747.704469ms 748.235982ms 748.34903ms 748.632652ms 748.724111ms 748.724323ms 748.988729ms 749.023696ms 749.038441ms 749.077145ms 749.249021ms 749.31192ms 749.32582ms 749.384392ms 749.472729ms 749.509455ms 749.511737ms 749.560198ms 749.572877ms 749.584807ms 749.593677ms 749.613513ms 749.688054ms 749.693207ms 749.735168ms 749.835817ms 749.878884ms 750.007611ms 750.058345ms 750.072067ms 750.094986ms 750.112934ms 750.114663ms 750.115571ms 750.136833ms 750.138218ms 750.150673ms 750.16331ms 750.185525ms 750.237633ms 750.372273ms 750.386944ms 750.405206ms 750.572869ms 750.591815ms 750.625954ms 750.64658ms 750.715679ms 750.810186ms 750.870254ms 750.894726ms 751.031548ms 751.057056ms 751.168076ms 751.213325ms 751.35069ms 751.366087ms 751.421695ms 751.617702ms 751.632251ms 752.183923ms 752.47499ms 752.605445ms 753.302506ms 753.395606ms 753.685548ms 753.910317ms 754.725154ms 754.980213ms 755.455528ms 757.152654ms 759.128541ms 763.020425ms 768.775142ms 771.194751ms 773.131928ms 773.134978ms 783.274326ms 787.541524ms 787.774276ms 795.880786ms 802.962436ms 808.434799ms 816.948859ms 826.340749ms 833.529086ms 837.877791ms 843.273399ms 855.659837ms 860.262289ms 861.773545ms 878.898299ms 886.501708ms 892.151713ms 910.605081ms 911.809378ms 922.194591ms 940.597742ms 954.962243ms 956.14899ms 956.644024ms 956.763671ms 968.788988ms 989.822168ms 1.020719341s 1.034477122s 1.043506268s 1.043855383s 1.046763191s 1.054102385s 1.056504858s 1.070164638s 1.115203978s]
Mar  8 14:06:10.538: INFO: 50 %ile: 749.384392ms
Mar  8 14:06:10.538: INFO: 90 %ile: 892.151713ms
Mar  8 14:06:10.538: INFO: 99 %ile: 1.070164638s
Mar  8 14:06:10.538: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:06:10.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-swqg2" for this suite.
Mar  8 14:06:22.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:06:22.588: INFO: namespace: e2e-tests-svc-latency-swqg2, resource: bindings, ignored listing per whitelist
Mar  8 14:06:22.602: INFO: namespace e2e-tests-svc-latency-swqg2 deletion completed in 12.059754161s

â€¢ [SLOW TEST:22.958 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:06:22.602: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:06:22.629877      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xld4z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-5aa59605-41ab-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:06:22.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-xld4z" to be "success or failure"
Mar  8 14:06:22.764: INFO: Pod "pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659679ms
Mar  8 14:06:24.766: INFO: Pod "pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00574819s
STEP: Saw pod success
Mar  8 14:06:24.766: INFO: Pod "pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:06:24.768: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:06:24.787: INFO: Waiting for pod pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:06:24.790: INFO: Pod pod-projected-secrets-5aa630fc-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:06:24.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xld4z" for this suite.
Mar  8 14:06:30.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:06:30.839: INFO: namespace: e2e-tests-projected-xld4z, resource: bindings, ignored listing per whitelist
Mar  8 14:06:30.852: INFO: namespace e2e-tests-projected-xld4z deletion completed in 6.059616191s

â€¢ [SLOW TEST:8.250 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:06:30.852: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:06:30.879943      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bmswg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:06:31.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-bmswg" to be "success or failure"
Mar  8 14:06:31.011: INFO: Pod "downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107009ms
Mar  8 14:06:33.014: INFO: Pod "downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005246062s
STEP: Saw pod success
Mar  8 14:06:33.014: INFO: Pod "downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:06:33.015: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:06:33.027: INFO: Waiting for pod downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:06:33.029: INFO: Pod downwardapi-volume-5f90d4ea-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:06:33.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bmswg" for this suite.
Mar  8 14:06:39.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:06:39.085: INFO: namespace: e2e-tests-downward-api-bmswg, resource: bindings, ignored listing per whitelist
Mar  8 14:06:39.093: INFO: namespace e2e-tests-downward-api-bmswg deletion completed in 6.061255829s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:06:39.093: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:06:39.123300      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-44xz6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  8 14:06:39.257: INFO: Waiting up to 5m0s for pod "pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-44xz6" to be "success or failure"
Mar  8 14:06:39.267: INFO: Pod "pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324048ms
Mar  8 14:06:41.270: INFO: Pod "pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012504486s
STEP: Saw pod success
Mar  8 14:06:41.270: INFO: Pod "pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:06:41.271: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:06:41.281: INFO: Waiting for pod pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:06:41.283: INFO: Pod pod-647a83be-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:06:41.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-44xz6" for this suite.
Mar  8 14:06:47.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:06:47.305: INFO: namespace: e2e-tests-emptydir-44xz6, resource: bindings, ignored listing per whitelist
Mar  8 14:06:47.386: INFO: namespace e2e-tests-emptydir-44xz6 deletion completed in 6.100222296s

â€¢ [SLOW TEST:8.292 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:06:47.386: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:06:47.412790      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mhh78
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  8 14:06:50.055: INFO: Successfully updated pod "annotationupdate696aeee3-41ab-11e9-be5e-ae5bc1be57d7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:06:54.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mhh78" for this suite.
Mar  8 14:07:16.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:07:16.125: INFO: namespace: e2e-tests-projected-mhh78, resource: bindings, ignored listing per whitelist
Mar  8 14:07:16.131: INFO: namespace e2e-tests-projected-mhh78 deletion completed in 22.058733315s

â€¢ [SLOW TEST:28.745 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:07:16.132: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:07:16.167583      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jgpgv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar  8 14:07:16.291: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-643275360 proxy --unix-socket=/tmp/kubectl-proxy-unix074747455/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:07:16.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jgpgv" for this suite.
Mar  8 14:07:22.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:07:22.489: INFO: namespace: e2e-tests-kubectl-jgpgv, resource: bindings, ignored listing per whitelist
Mar  8 14:07:22.510: INFO: namespace e2e-tests-kubectl-jgpgv deletion completed in 6.161512868s

â€¢ [SLOW TEST:6.378 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:07:22.510: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:07:22.538991      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-tlnsx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:07:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-tlnsx" for this suite.
Mar  8 14:07:49.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:07:49.883: INFO: namespace: e2e-tests-container-runtime-tlnsx, resource: bindings, ignored listing per whitelist
Mar  8 14:07:49.885: INFO: namespace e2e-tests-container-runtime-tlnsx deletion completed in 6.069127417s

â€¢ [SLOW TEST:27.376 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:07:49.886: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:07:49.924937      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-zxd5v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zxd5v.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zxd5v.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  8 14:08:02.090: INFO: Unable to read wheezy_udp@kubernetes.default from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.092: INFO: Unable to read wheezy_tcp@kubernetes.default from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.094: INFO: Unable to read wheezy_udp@kubernetes.default.svc from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.096: INFO: Unable to read wheezy_tcp@kubernetes.default.svc from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.098: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.100: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.101: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.103: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.110: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.112: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.115: INFO: Unable to read jessie_udp@kubernetes.default from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.117: INFO: Unable to read jessie_tcp@kubernetes.default from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.119: INFO: Unable to read jessie_udp@kubernetes.default.svc from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.121: INFO: Unable to read jessie_tcp@kubernetes.default.svc from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.124: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.136: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.139: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.142: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.145: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.148: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7: the server could not find the requested resource (get pods dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7)
Mar  8 14:08:02.148: INFO: Lookups using e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7 failed for: [wheezy_udp@kubernetes.default wheezy_tcp@kubernetes.default wheezy_udp@kubernetes.default.svc wheezy_tcp@kubernetes.default.svc wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default jessie_tcp@kubernetes.default jessie_udp@kubernetes.default.svc jessie_tcp@kubernetes.default.svc jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zxd5v.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  8 14:08:07.189: INFO: DNS probes using e2e-tests-dns-zxd5v/dns-test-8eafa30f-41ab-11e9-be5e-ae5bc1be57d7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:08:07.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-zxd5v" for this suite.
Mar  8 14:08:13.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:08:13.245: INFO: namespace: e2e-tests-dns-zxd5v, resource: bindings, ignored listing per whitelist
Mar  8 14:08:13.268: INFO: namespace e2e-tests-dns-zxd5v deletion completed in 6.060059647s

â€¢ [SLOW TEST:23.383 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:08:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:08:13.297152      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-58ws8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-58ws8
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-58ws8 to expose endpoints map[]
Mar  8 14:08:13.861: INFO: Get endpoints failed (5.732951ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  8 14:08:14.863: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-58ws8 exposes endpoints map[] (1.007755751s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-58ws8
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-58ws8 to expose endpoints map[pod1:[80]]
Mar  8 14:08:16.883: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-58ws8 exposes endpoints map[pod1:[80]] (2.016905443s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-58ws8
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-58ws8 to expose endpoints map[pod2:[80] pod1:[80]]
Mar  8 14:08:17.901: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-58ws8 exposes endpoints map[pod1:[80] pod2:[80]] (1.012294906s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-58ws8
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-58ws8 to expose endpoints map[pod2:[80]]
Mar  8 14:08:17.911: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-58ws8 exposes endpoints map[pod2:[80]] (6.344032ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-58ws8
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-58ws8 to expose endpoints map[]
Mar  8 14:08:17.919: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-58ws8 exposes endpoints map[] (3.49232ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:08:17.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-58ws8" for this suite.
Mar  8 14:08:39.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:08:39.968: INFO: namespace: e2e-tests-services-58ws8, resource: bindings, ignored listing per whitelist
Mar  8 14:08:39.992: INFO: namespace e2e-tests-services-58ws8 deletion completed in 22.058864449s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:26.724 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:08:39.992: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:08:40.043099      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wlq8q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  8 14:08:40.616: INFO: Waiting up to 5m0s for pod "pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-wlq8q" to be "success or failure"
Mar  8 14:08:40.618: INFO: Pod "pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.901594ms
Mar  8 14:08:42.620: INFO: Pod "pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004045522s
STEP: Saw pod success
Mar  8 14:08:42.621: INFO: Pod "pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:08:42.622: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:08:42.632: INFO: Waiting for pod pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:08:43.047: INFO: Pod pod-acd14dfa-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:08:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wlq8q" for this suite.
Mar  8 14:08:49.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:08:49.082: INFO: namespace: e2e-tests-emptydir-wlq8q, resource: bindings, ignored listing per whitelist
Mar  8 14:08:49.160: INFO: namespace e2e-tests-emptydir-wlq8q deletion completed in 6.1093338s

â€¢ [SLOW TEST:9.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:08:49.160: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:08:49.186972      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-q6sm4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  8 14:08:49.316: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-q6sm4,SelfLink:/api/v1/namespaces/e2e-tests-watch-q6sm4/configmaps/e2e-watch-test-watch-closed,UID:b200bd20-41ab-11e9-8665-02fd46f865f2,ResourceVersion:20307,Generation:0,CreationTimestamp:2019-03-08 14:08:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  8 14:08:49.316: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-q6sm4,SelfLink:/api/v1/namespaces/e2e-tests-watch-q6sm4/configmaps/e2e-watch-test-watch-closed,UID:b200bd20-41ab-11e9-8665-02fd46f865f2,ResourceVersion:20308,Generation:0,CreationTimestamp:2019-03-08 14:08:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  8 14:08:49.325: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-q6sm4,SelfLink:/api/v1/namespaces/e2e-tests-watch-q6sm4/configmaps/e2e-watch-test-watch-closed,UID:b200bd20-41ab-11e9-8665-02fd46f865f2,ResourceVersion:20309,Generation:0,CreationTimestamp:2019-03-08 14:08:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  8 14:08:49.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-q6sm4,SelfLink:/api/v1/namespaces/e2e-tests-watch-q6sm4/configmaps/e2e-watch-test-watch-closed,UID:b200bd20-41ab-11e9-8665-02fd46f865f2,ResourceVersion:20310,Generation:0,CreationTimestamp:2019-03-08 14:08:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:08:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-q6sm4" for this suite.
Mar  8 14:08:55.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:08:55.352: INFO: namespace: e2e-tests-watch-q6sm4, resource: bindings, ignored listing per whitelist
Mar  8 14:08:55.386: INFO: namespace e2e-tests-watch-q6sm4 deletion completed in 6.058206823s

â€¢ [SLOW TEST:6.226 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:08:55.386: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:08:55.414192      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-d6gq2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0308 14:09:05.576676      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 14:09:05.576: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:09:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d6gq2" for this suite.
Mar  8 14:09:11.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:09:11.642: INFO: namespace: e2e-tests-gc-d6gq2, resource: bindings, ignored listing per whitelist
Mar  8 14:09:11.643: INFO: namespace e2e-tests-gc-d6gq2 deletion completed in 6.064041446s

â€¢ [SLOW TEST:16.257 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:09:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:09:11.671461      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dklnh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-bf676392-41ab-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:09:11.800: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-dklnh" to be "success or failure"
Mar  8 14:09:11.803: INFO: Pod "pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588368ms
Mar  8 14:09:13.805: INFO: Pod "pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00579198s
STEP: Saw pod success
Mar  8 14:09:13.806: INFO: Pod "pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:09:13.807: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:09:13.816: INFO: Waiting for pod pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:09:13.821: INFO: Pod pod-projected-secrets-bf67bc49-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:09:13.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dklnh" for this suite.
Mar  8 14:09:19.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:09:19.872: INFO: namespace: e2e-tests-projected-dklnh, resource: bindings, ignored listing per whitelist
Mar  8 14:09:19.882: INFO: namespace e2e-tests-projected-dklnh deletion completed in 6.058257334s

â€¢ [SLOW TEST:8.239 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:09:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:09:19.910006      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-vrs9v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  8 14:09:20.032: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  8 14:09:20.037: INFO: Waiting for terminating namespaces to be deleted...
Mar  8 14:09:20.038: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-64-146.us-west-2.compute.internal before test
Mar  8 14:09:20.044: INFO: dns-external-dns-784c569497-x5mzr from pipeline-system started at 2019-03-08 12:46:36 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container external-dns ready: true, restart count 0
Mar  8 14:09:20.044: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-pns4f from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:09:20.044: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:09:20.044: INFO: npls-nodepool-labels-operator-5f8b4685bf-qgwq6 from pipeline-system started at 2019-03-08 12:46:26 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container nodepool-labels-operator ready: true, restart count 0
Mar  8 14:09:20.044: INFO: ingress-traefik-7db7799d65-6h7fs from pipeline-system started at 2019-03-08 12:46:38 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container ingress-traefik ready: true, restart count 0
Mar  8 14:09:20.044: INFO: ith-instance-termination-handler-dtxjt from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:09:20.044: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-08 13:00:57 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  8 14:09:20.044: INFO: monitor-prometheus-node-exporter-9vjmd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:09:20.044: INFO: kube-proxy-dj5qw from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:09:20.044: INFO: weave-net-ftrz9 from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container weave ready: true, restart count 0
Mar  8 14:09:20.044: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:09:20.044: INFO: tiller-deploy-5f865894d6-kw9sw from kube-system started at 2019-03-08 12:46:10 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.044: INFO: 	Container tiller ready: true, restart count 0
Mar  8 14:09:20.044: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-251.us-west-2.compute.internal before test
Mar  8 14:09:20.048: INFO: weave-net-h4xft from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:09:20.048: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:09:20.048: INFO: sonobuoy-e2e-job-bca5431098ec41f9 from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container e2e ready: true, restart count 0
Mar  8 14:09:20.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  8 14:09:20.048: INFO: ith-instance-termination-handler-ssg67 from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:09:20.048: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2mb7g from heptio-sonobuoy started at 2019-03-08 13:01:03 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:09:20.048: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:09:20.048: INFO: kube-proxy-j5n9b from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:09:20.048: INFO: monitor-prometheus-node-exporter-t72fs from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.048: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:09:20.048: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-74-56.us-west-2.compute.internal before test
Mar  8 14:09:20.078: INFO: hpa-operator-kube-metrics-adapter-75794ffd9f-gm2xz from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container kube-metrics-adapter ready: true, restart count 0
Mar  8 14:09:20.078: INFO: hpa-operator-hpa-operator-795b5fc8c9-2d9t2 from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container hpa-operator ready: true, restart count 0
Mar  8 14:09:20.078: INFO: spot-webhook-spot-config-webhook-696c8bcd-wmkvj from pipeline-system started at 2019-03-08 12:46:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container spot-config-webhook ready: true, restart count 0
Mar  8 14:09:20.078: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2nhct from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:09:20.078: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:09:20.078: INFO: dashboard-kubernetes-dashboard-6b84cccd4-c6dpm from pipeline-system started at 2019-03-08 12:46:40 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  8 14:09:20.078: INFO: hpa-operator-metrics-server-7f9dfb4cd4-5976v from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container metrics-server ready: true, restart count 0
Mar  8 14:09:20.078: INFO: monitor-grafana-698887f8cd-85frd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container grafana ready: true, restart count 0
Mar  8 14:09:20.078: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  8 14:09:20.078: INFO: monitor-prometheus-server-56994dbb47-rf8f4 from pipeline-system started at 2019-03-08 12:47:22 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  8 14:09:20.078: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  8 14:09:20.078: INFO: autoscaler-aws-cluster-autoscaler-7ccb98ddc6-88xp5 from kube-system started at 2019-03-08 12:46:42 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 0
Mar  8 14:09:20.078: INFO: pvc-operator-6575cc9546-fjntk from pipeline-system started at 2019-03-08 12:46:45 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container pvc-operator ready: true, restart count 0
Mar  8 14:09:20.078: INFO: spot-scheduler-6dbbcc8c4b-pp8hj from pipeline-system started at 2019-03-08 12:46:48 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container spot-scheduler ready: true, restart count 0
Mar  8 14:09:20.078: INFO: ith-instance-termination-handler-jrj9d from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:09:20.078: INFO: monitor-prometheus-pushgateway-78c49d6b7f-k228k from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  8 14:09:20.078: INFO: kube-proxy-hplw9 from kube-system started at 2019-03-08 12:45:49 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:09:20.078: INFO: monitor-prometheus-node-exporter-kpgm8 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:09:20.078: INFO: monitor-prometheus-kube-state-metrics-76bf759fcb-c8zr9 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Mar  8 14:09:20.078: INFO: weave-net-xvlch from kube-system started at 2019-03-08 12:45:49 +0000 UTC (2 container statuses recorded)
Mar  8 14:09:20.078: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:09:20.078: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c58b890c-41ab-11e9-be5e-ae5bc1be57d7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c58b890c-41ab-11e9-be5e-ae5bc1be57d7 off the node ip-192-168-64-146.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c58b890c-41ab-11e9-be5e-ae5bc1be57d7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:09:24.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vrs9v" for this suite.
Mar  8 14:09:32.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:09:32.216: INFO: namespace: e2e-tests-sched-pred-vrs9v, resource: bindings, ignored listing per whitelist
Mar  8 14:09:32.217: INFO: namespace e2e-tests-sched-pred-vrs9v deletion completed in 8.091840086s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:12.335 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:09:32.217: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:09:32.248543      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zw94k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:09:32.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-zw94k" to be "success or failure"
Mar  8 14:09:32.381: INFO: Pod "downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.561296ms
Mar  8 14:09:34.383: INFO: Pod "downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006764983s
STEP: Saw pod success
Mar  8 14:09:34.384: INFO: Pod "downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:09:34.385: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:09:34.397: INFO: Waiting for pod downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:09:34.399: INFO: Pod downwardapi-volume-cbab3713-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:09:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zw94k" for this suite.
Mar  8 14:09:40.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:09:40.548: INFO: namespace: e2e-tests-downward-api-zw94k, resource: bindings, ignored listing per whitelist
Mar  8 14:09:40.577: INFO: namespace e2e-tests-downward-api-zw94k deletion completed in 6.17591519s

â€¢ [SLOW TEST:8.360 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:09:40.577: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:09:40.603836      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-lv8gd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-7tcc
STEP: Creating a pod to test atomic-volume-subpath
Mar  8 14:09:40.731: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7tcc" in namespace "e2e-tests-subpath-lv8gd" to be "success or failure"
Mar  8 14:09:40.733: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29604ms
Mar  8 14:09:42.735: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004496943s
Mar  8 14:09:44.737: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 4.006800794s
Mar  8 14:09:46.740: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 6.008919736s
Mar  8 14:09:48.742: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 8.010983557s
Mar  8 14:09:50.744: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 10.01286366s
Mar  8 14:09:52.746: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 12.015047826s
Mar  8 14:09:54.748: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 14.017704467s
Mar  8 14:09:56.751: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 16.019840193s
Mar  8 14:09:58.753: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 18.022098443s
Mar  8 14:10:00.755: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 20.024507316s
Mar  8 14:10:02.898: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Running", Reason="", readiness=false. Elapsed: 22.167691993s
Mar  8 14:10:04.900: INFO: Pod "pod-subpath-test-downwardapi-7tcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.169668499s
STEP: Saw pod success
Mar  8 14:10:04.900: INFO: Pod "pod-subpath-test-downwardapi-7tcc" satisfied condition "success or failure"
Mar  8 14:10:04.902: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-subpath-test-downwardapi-7tcc container test-container-subpath-downwardapi-7tcc: <nil>
STEP: delete the pod
Mar  8 14:10:04.963: INFO: Waiting for pod pod-subpath-test-downwardapi-7tcc to disappear
Mar  8 14:10:04.966: INFO: Pod pod-subpath-test-downwardapi-7tcc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7tcc
Mar  8 14:10:04.966: INFO: Deleting pod "pod-subpath-test-downwardapi-7tcc" in namespace "e2e-tests-subpath-lv8gd"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:10:04.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-lv8gd" for this suite.
Mar  8 14:10:10.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:10:11.002: INFO: namespace: e2e-tests-subpath-lv8gd, resource: bindings, ignored listing per whitelist
Mar  8 14:10:11.035: INFO: namespace e2e-tests-subpath-lv8gd deletion completed in 6.064407914s

â€¢ [SLOW TEST:30.457 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:10:11.035: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:10:11.062654      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-mtmvc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar  8 14:10:11.189: INFO: Waiting up to 5m0s for pod "client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-containers-mtmvc" to be "success or failure"
Mar  8 14:10:11.192: INFO: Pod "client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728678ms
Mar  8 14:10:13.194: INFO: Pod "client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004977496s
STEP: Saw pod success
Mar  8 14:10:13.194: INFO: Pod "client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:10:13.196: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:10:13.207: INFO: Waiting for pod client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:10:13.209: INFO: Pod client-containers-e2cde1bd-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:10:13.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-mtmvc" for this suite.
Mar  8 14:10:19.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:10:19.254: INFO: namespace: e2e-tests-containers-mtmvc, resource: bindings, ignored listing per whitelist
Mar  8 14:10:19.269: INFO: namespace e2e-tests-containers-mtmvc deletion completed in 6.057150303s

â€¢ [SLOW TEST:8.234 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:10:19.269: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:10:19.299617      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hjxqh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  8 14:10:21.943: INFO: Successfully updated pod "annotationupdatee7b68030-41ab-11e9-be5e-ae5bc1be57d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:10:23.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hjxqh" for this suite.
Mar  8 14:10:45.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:10:45.986: INFO: namespace: e2e-tests-downward-api-hjxqh, resource: bindings, ignored listing per whitelist
Mar  8 14:10:46.015: INFO: namespace e2e-tests-downward-api-hjxqh deletion completed in 22.060524143s

â€¢ [SLOW TEST:26.746 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:10:46.015: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:10:46.044685      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xpbnr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  8 14:10:46.170: INFO: Waiting up to 5m0s for pod "downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-xpbnr" to be "success or failure"
Mar  8 14:10:46.174: INFO: Pod "downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614213ms
Mar  8 14:10:48.176: INFO: Pod "downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005915089s
STEP: Saw pod success
Mar  8 14:10:48.176: INFO: Pod "downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:10:48.178: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 14:10:48.189: INFO: Waiting for pod downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:10:48.191: INFO: Pod downward-api-f7a788ff-41ab-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:10:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xpbnr" for this suite.
Mar  8 14:10:54.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:10:54.228: INFO: namespace: e2e-tests-downward-api-xpbnr, resource: bindings, ignored listing per whitelist
Mar  8 14:10:54.250: INFO: namespace e2e-tests-downward-api-xpbnr deletion completed in 6.056249304s

â€¢ [SLOW TEST:8.235 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:10:54.250: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:10:54.326495      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-g8rcf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  8 14:10:54.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  8 14:10:54.546: INFO: Waiting for terminating namespaces to be deleted...
Mar  8 14:10:54.547: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-64-146.us-west-2.compute.internal before test
Mar  8 14:10:54.552: INFO: kube-proxy-dj5qw from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:10:54.552: INFO: weave-net-ftrz9 from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container weave ready: true, restart count 0
Mar  8 14:10:54.552: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:10:54.552: INFO: tiller-deploy-5f865894d6-kw9sw from kube-system started at 2019-03-08 12:46:10 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container tiller ready: true, restart count 0
Mar  8 14:10:54.552: INFO: dns-external-dns-784c569497-x5mzr from pipeline-system started at 2019-03-08 12:46:36 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container external-dns ready: true, restart count 0
Mar  8 14:10:54.552: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-pns4f from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:10:54.552: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:10:54.552: INFO: npls-nodepool-labels-operator-5f8b4685bf-qgwq6 from pipeline-system started at 2019-03-08 12:46:26 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container nodepool-labels-operator ready: true, restart count 0
Mar  8 14:10:54.552: INFO: ingress-traefik-7db7799d65-6h7fs from pipeline-system started at 2019-03-08 12:46:38 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container ingress-traefik ready: true, restart count 0
Mar  8 14:10:54.552: INFO: ith-instance-termination-handler-dtxjt from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.552: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:10:54.553: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-08 13:00:57 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.553: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  8 14:10:54.553: INFO: monitor-prometheus-node-exporter-9vjmd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.553: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:10:54.553: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-251.us-west-2.compute.internal before test
Mar  8 14:10:54.557: INFO: weave-net-h4xft from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:10:54.557: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:10:54.557: INFO: sonobuoy-e2e-job-bca5431098ec41f9 from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container e2e ready: true, restart count 0
Mar  8 14:10:54.557: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  8 14:10:54.557: INFO: ith-instance-termination-handler-ssg67 from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:10:54.557: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2mb7g from heptio-sonobuoy started at 2019-03-08 13:01:03 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:10:54.557: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:10:54.557: INFO: kube-proxy-j5n9b from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:10:54.557: INFO: monitor-prometheus-node-exporter-t72fs from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.557: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:10:54.557: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-74-56.us-west-2.compute.internal before test
Mar  8 14:10:54.567: INFO: autoscaler-aws-cluster-autoscaler-7ccb98ddc6-88xp5 from kube-system started at 2019-03-08 12:46:42 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 0
Mar  8 14:10:54.567: INFO: hpa-operator-metrics-server-7f9dfb4cd4-5976v from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container metrics-server ready: true, restart count 0
Mar  8 14:10:54.567: INFO: monitor-grafana-698887f8cd-85frd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container grafana ready: true, restart count 0
Mar  8 14:10:54.567: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  8 14:10:54.567: INFO: monitor-prometheus-server-56994dbb47-rf8f4 from pipeline-system started at 2019-03-08 12:47:22 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  8 14:10:54.567: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  8 14:10:54.567: INFO: monitor-prometheus-pushgateway-78c49d6b7f-k228k from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  8 14:10:54.567: INFO: kube-proxy-hplw9 from kube-system started at 2019-03-08 12:45:49 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:10:54.567: INFO: pvc-operator-6575cc9546-fjntk from pipeline-system started at 2019-03-08 12:46:45 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container pvc-operator ready: true, restart count 0
Mar  8 14:10:54.567: INFO: spot-scheduler-6dbbcc8c4b-pp8hj from pipeline-system started at 2019-03-08 12:46:48 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container spot-scheduler ready: true, restart count 0
Mar  8 14:10:54.567: INFO: ith-instance-termination-handler-jrj9d from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:10:54.567: INFO: weave-net-xvlch from kube-system started at 2019-03-08 12:45:49 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:10:54.567: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:10:54.567: INFO: monitor-prometheus-node-exporter-kpgm8 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:10:54.567: INFO: monitor-prometheus-kube-state-metrics-76bf759fcb-c8zr9 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Mar  8 14:10:54.567: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2nhct from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:10:54.567: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:10:54.567: INFO: dashboard-kubernetes-dashboard-6b84cccd4-c6dpm from pipeline-system started at 2019-03-08 12:46:40 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  8 14:10:54.567: INFO: hpa-operator-kube-metrics-adapter-75794ffd9f-gm2xz from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container kube-metrics-adapter ready: true, restart count 0
Mar  8 14:10:54.567: INFO: hpa-operator-hpa-operator-795b5fc8c9-2d9t2 from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container hpa-operator ready: true, restart count 0
Mar  8 14:10:54.567: INFO: spot-webhook-spot-config-webhook-696c8bcd-wmkvj from pipeline-system started at 2019-03-08 12:46:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:10:54.567: INFO: 	Container spot-config-webhook ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-192-168-64-146.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-72-251.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod sonobuoy-e2e-job-bca5431098ec41f9 requesting resource cpu=0m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2mb7g requesting resource cpu=0m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2nhct requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-pns4f requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod autoscaler-aws-cluster-autoscaler-7ccb98ddc6-88xp5 requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod kube-proxy-dj5qw requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod kube-proxy-hplw9 requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod kube-proxy-j5n9b requesting resource cpu=0m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod tiller-deploy-5f865894d6-kw9sw requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod weave-net-ftrz9 requesting resource cpu=20m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod weave-net-h4xft requesting resource cpu=20m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod weave-net-xvlch requesting resource cpu=20m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod dashboard-kubernetes-dashboard-6b84cccd4-c6dpm requesting resource cpu=100m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod dns-external-dns-784c569497-x5mzr requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod hpa-operator-hpa-operator-795b5fc8c9-2d9t2 requesting resource cpu=100m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod hpa-operator-kube-metrics-adapter-75794ffd9f-gm2xz requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod hpa-operator-metrics-server-7f9dfb4cd4-5976v requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod ingress-traefik-7db7799d65-6h7fs requesting resource cpu=100m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod ith-instance-termination-handler-dtxjt requesting resource cpu=120m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod ith-instance-termination-handler-jrj9d requesting resource cpu=120m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod ith-instance-termination-handler-ssg67 requesting resource cpu=120m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-grafana-698887f8cd-85frd requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-kube-state-metrics-76bf759fcb-c8zr9 requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-node-exporter-9vjmd requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-node-exporter-kpgm8 requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-node-exporter-t72fs requesting resource cpu=0m on Node ip-192-168-72-251.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-pushgateway-78c49d6b7f-k228k requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod monitor-prometheus-server-56994dbb47-rf8f4 requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod npls-nodepool-labels-operator-5f8b4685bf-qgwq6 requesting resource cpu=0m on Node ip-192-168-64-146.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod pvc-operator-6575cc9546-fjntk requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod spot-scheduler-6dbbcc8c4b-pp8hj requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
Mar  8 14:10:54.609: INFO: Pod spot-webhook-spot-config-webhook-696c8bcd-wmkvj requesting resource cpu=0m on Node ip-192-168-74-56.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcafb38c-41ab-11e9-be5e-ae5bc1be57d7.158a011f1691985d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8rcf/filler-pod-fcafb38c-41ab-11e9-be5e-ae5bc1be57d7 to ip-192-168-72-251.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcafb38c-41ab-11e9-be5e-ae5bc1be57d7.158a011f352ee2dd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcafb38c-41ab-11e9-be5e-ae5bc1be57d7.158a011f37189893], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcafb38c-41ab-11e9-be5e-ae5bc1be57d7.158a011f3ce4580a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb08de3-41ab-11e9-be5e-ae5bc1be57d7.158a011f16f549e4], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8rcf/filler-pod-fcb08de3-41ab-11e9-be5e-ae5bc1be57d7 to ip-192-168-74-56.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb08de3-41ab-11e9-be5e-ae5bc1be57d7.158a011f3b03b83e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb08de3-41ab-11e9-be5e-ae5bc1be57d7.158a011f3d45093d], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb08de3-41ab-11e9-be5e-ae5bc1be57d7.158a011f42bd4976], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb169e1-41ab-11e9-be5e-ae5bc1be57d7.158a011f177519a8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8rcf/filler-pod-fcb169e1-41ab-11e9-be5e-ae5bc1be57d7 to ip-192-168-64-146.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb169e1-41ab-11e9-be5e-ae5bc1be57d7.158a011f33345a5a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb169e1-41ab-11e9-be5e-ae5bc1be57d7.158a011f34e87357], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcb169e1-41ab-11e9-be5e-ae5bc1be57d7.158a011f3958a1f8], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158a011f8f8eb549], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node ip-192-168-64-146.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-72-251.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-74-56.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:10:57.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-g8rcf" for this suite.
Mar  8 14:11:03.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:11:03.712: INFO: namespace: e2e-tests-sched-pred-g8rcf, resource: bindings, ignored listing per whitelist
Mar  8 14:11:03.747: INFO: namespace e2e-tests-sched-pred-g8rcf deletion completed in 6.05844806s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.497 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:11:03.748: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:11:03.776812      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-9m9mr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-9m9mr
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar  8 14:11:03.920: INFO: Found 0 stateful pods, waiting for 3
Mar  8 14:11:13.922: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:11:13.922: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:11:13.922: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:11:13.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-9m9mr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 14:11:14.097: INFO: stderr: ""
Mar  8 14:11:14.097: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 14:11:14.097: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  8 14:11:24.119: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  8 14:11:34.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-9m9mr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 14:11:34.294: INFO: stderr: ""
Mar  8 14:11:34.294: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 14:11:34.294: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 14:11:54.307: INFO: Waiting for StatefulSet e2e-tests-statefulset-9m9mr/ss2 to complete update
Mar  8 14:11:54.307: INFO: Waiting for Pod e2e-tests-statefulset-9m9mr/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  8 14:12:04.312: INFO: Waiting for StatefulSet e2e-tests-statefulset-9m9mr/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  8 14:12:14.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-9m9mr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  8 14:12:14.479: INFO: stderr: ""
Mar  8 14:12:14.479: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  8 14:12:14.479: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  8 14:12:24.508: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  8 14:12:34.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 exec --namespace=e2e-tests-statefulset-9m9mr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  8 14:12:34.688: INFO: stderr: ""
Mar  8 14:12:34.688: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  8 14:12:34.688: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  8 14:12:54.859: INFO: Waiting for StatefulSet e2e-tests-statefulset-9m9mr/ss2 to complete update
Mar  8 14:12:54.859: INFO: Waiting for Pod e2e-tests-statefulset-9m9mr/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  8 14:13:04.863: INFO: Deleting all statefulset in ns e2e-tests-statefulset-9m9mr
Mar  8 14:13:04.865: INFO: Scaling statefulset ss2 to 0
Mar  8 14:13:24.873: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 14:13:24.875: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:13:24.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-9m9mr" for this suite.
Mar  8 14:13:30.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:13:30.934: INFO: namespace: e2e-tests-statefulset-9m9mr, resource: bindings, ignored listing per whitelist
Mar  8 14:13:30.960: INFO: namespace e2e-tests-statefulset-9m9mr deletion completed in 6.063433287s

â€¢ [SLOW TEST:147.212 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:13:30.960: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:13:30.991458      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xkq6x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  8 14:13:31.139: INFO: Waiting up to 5m0s for pod "pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-xkq6x" to be "success or failure"
Mar  8 14:13:31.142: INFO: Pod "pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.936601ms
Mar  8 14:13:33.145: INFO: Pod "pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006116541s
STEP: Saw pod success
Mar  8 14:13:33.145: INFO: Pod "pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:13:33.146: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:13:33.158: INFO: Waiting for pod pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:13:33.160: INFO: Pod pod-59f88a9e-41ac-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:13:33.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xkq6x" for this suite.
Mar  8 14:13:39.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:13:39.205: INFO: namespace: e2e-tests-emptydir-xkq6x, resource: bindings, ignored listing per whitelist
Mar  8 14:13:39.226: INFO: namespace e2e-tests-emptydir-xkq6x deletion completed in 6.064047843s

â€¢ [SLOW TEST:8.266 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:13:39.226: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:13:39.276408      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qjl4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:13:39.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-qjl4x" to be "success or failure"
Mar  8 14:13:39.439: INFO: Pod "downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343091ms
Mar  8 14:13:41.442: INFO: Pod "downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00444675s
STEP: Saw pod success
Mar  8 14:13:41.442: INFO: Pod "downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:13:41.443: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:13:41.453: INFO: Waiting for pod downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:13:41.456: INFO: Pod downwardapi-volume-5eed93f2-41ac-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:13:41.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qjl4x" for this suite.
Mar  8 14:13:47.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:13:47.510: INFO: namespace: e2e-tests-downward-api-qjl4x, resource: bindings, ignored listing per whitelist
Mar  8 14:13:47.516: INFO: namespace e2e-tests-downward-api-qjl4x deletion completed in 6.05704704s

â€¢ [SLOW TEST:8.289 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:13:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:13:47.543375      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-g4dlr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar  8 14:13:47.680: INFO: Waiting up to 5m0s for pod "var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-var-expansion-g4dlr" to be "success or failure"
Mar  8 14:13:47.682: INFO: Pod "var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.724843ms
Mar  8 14:13:49.684: INFO: Pod "var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003877319s
STEP: Saw pod success
Mar  8 14:13:49.684: INFO: Pod "var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:13:49.686: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 14:13:49.697: INFO: Waiting for pod var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:13:49.699: INFO: Pod var-expansion-63d78b13-41ac-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:13:49.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-g4dlr" for this suite.
Mar  8 14:13:55.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:13:55.717: INFO: namespace: e2e-tests-var-expansion-g4dlr, resource: bindings, ignored listing per whitelist
Mar  8 14:13:55.760: INFO: namespace e2e-tests-var-expansion-g4dlr deletion completed in 6.058840218s

â€¢ [SLOW TEST:8.244 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:13:55.760: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:13:55.787023      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-2l29j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  8 14:13:55.920: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21725,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  8 14:13:55.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21726,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  8 14:13:55.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21727,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  8 14:14:05.933: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21750,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  8 14:14:05.933: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21751,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  8 14:14:05.933: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-2l29j,SelfLink:/api/v1/namespaces/e2e-tests-watch-2l29j/configmaps/e2e-watch-test-label-changed,UID:68bfe981-41ac-11e9-8665-02fd46f865f2,ResourceVersion:21752,Generation:0,CreationTimestamp:2019-03-08 14:13:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:14:05.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-2l29j" for this suite.
Mar  8 14:14:11.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:14:11.968: INFO: namespace: e2e-tests-watch-2l29j, resource: bindings, ignored listing per whitelist
Mar  8 14:14:12.003: INFO: namespace e2e-tests-watch-2l29j deletion completed in 6.067509174s

â€¢ [SLOW TEST:16.243 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:14:12.003: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:14:12.032636      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rwsnn
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  8 14:14:12.159: INFO: Waiting up to 5m0s for pod "pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-rwsnn" to be "success or failure"
Mar  8 14:14:12.162: INFO: Pod "pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608927ms
Mar  8 14:14:14.164: INFO: Pod "pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004784095s
STEP: Saw pod success
Mar  8 14:14:14.164: INFO: Pod "pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:14:14.165: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:14:14.176: INFO: Waiting for pod pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:14:14.177: INFO: Pod pod-726ec2e2-41ac-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:14:14.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rwsnn" for this suite.
Mar  8 14:14:20.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:14:20.230: INFO: namespace: e2e-tests-emptydir-rwsnn, resource: bindings, ignored listing per whitelist
Mar  8 14:14:20.244: INFO: namespace e2e-tests-emptydir-rwsnn deletion completed in 6.064588185s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:14:20.244: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:14:20.287082      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-xqkhb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:14:20.419: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  8 14:14:20.425: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:20.427: INFO: Number of nodes with available pods: 0
Mar  8 14:14:20.427: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:14:21.431: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:21.433: INFO: Number of nodes with available pods: 1
Mar  8 14:14:21.433: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:14:22.430: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:22.432: INFO: Number of nodes with available pods: 3
Mar  8 14:14:22.432: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  8 14:14:22.443: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:22.443: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:22.443: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:22.446: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:23.450: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:23.450: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:23.450: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:23.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:24.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:24.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:24.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:24.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:25.571: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:25.572: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:25.572: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:25.575: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:26.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:26.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:26.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:26.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:27.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:27.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:27.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:27.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:28.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:28.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:28.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:28.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:29.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:29.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:29.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:29.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:30.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:30.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:30.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:30.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:31.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:31.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:31.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:31.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:32.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:32.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:32.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:32.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:33.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:33.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:33.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:33.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:34.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:34.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:34.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:34.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:35.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:35.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:35.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:35.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:36.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:36.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:36.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:36.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:37.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:37.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:37.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:37.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:38.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:38.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:38.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:38.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:39.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:39.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:39.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:39.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:40.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:40.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:40.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:40.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:41.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:41.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:41.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:41.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:42.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:42.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:42.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:42.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:43.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:43.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:43.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:43.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:44.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:44.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:44.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:44.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:45.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:45.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:45.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:45.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:46.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:46.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:46.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:46.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:47.452: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:47.452: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:47.452: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:47.456: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:48.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:48.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:48.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:48.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:49.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:49.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:49.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:49.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:50.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:50.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:50.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:50.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:51.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:51.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:51.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:51.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:52.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:52.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:52.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:52.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:53.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:53.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:53.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:53.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:54.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:54.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:54.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:54.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:55.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:55.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:55.449: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:14:55.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:55.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:56.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:56.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:56.449: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:14:56.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:56.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:57.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:57.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:57.449: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:14:57.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:57.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:58.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:58.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:58.449: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:14:58.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:58.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:14:59.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:59.449: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:59.449: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:14:59.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:14:59.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:00.816: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:00.816: INFO: Wrong image for pod: daemon-set-bsm9v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:00.816: INFO: Pod daemon-set-bsm9v is not available
Mar  8 14:15:00.816: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:00.820: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:01.877: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:01.877: INFO: Pod daemon-set-pcxr6 is not available
Mar  8 14:15:01.877: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:01.881: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:02.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:02.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:02.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:03.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:03.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:03.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:04.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:04.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:04.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:05.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:05.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:05.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:06.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:06.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:06.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:07.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:07.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:07.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:08.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:08.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:08.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:10.024: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:10.024: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:10.033: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:10.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:10.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:10.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:11.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:11.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:11.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:12.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:12.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:12.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:13.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:13.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:13.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:14.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:14.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:14.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:15.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:15.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:15.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:16.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:16.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:16.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:17.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:17.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:17.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:18.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:18.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:18.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:19.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:19.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:19.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:20.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:20.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:20.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:21.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:21.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:21.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:22.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:22.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:22.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:23.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:23.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:23.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:24.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:24.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:24.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:25.450: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:25.450: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:25.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:26.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:26.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:26.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:27.451: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:27.451: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:27.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:28.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:28.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:28.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:29.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:29.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:29.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:30.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:30.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:30.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:31.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:31.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:31.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:32.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:32.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:32.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:33.768: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:33.768: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:33.768: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:33.771: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:34.574: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:34.574: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:34.574: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:34.577: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:35.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:35.449: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:35.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:35.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:36.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:36.449: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:36.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:36.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:37.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:37.449: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:37.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:37.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:38.449: INFO: Wrong image for pod: daemon-set-8xfp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:38.449: INFO: Pod daemon-set-8xfp7 is not available
Mar  8 14:15:38.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:38.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:39.449: INFO: Pod daemon-set-8s8lh is not available
Mar  8 14:15:39.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:39.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:40.449: INFO: Pod daemon-set-8s8lh is not available
Mar  8 14:15:40.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:40.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:41.449: INFO: Pod daemon-set-8s8lh is not available
Mar  8 14:15:41.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:41.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:42.449: INFO: Pod daemon-set-8s8lh is not available
Mar  8 14:15:42.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:42.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:43.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:43.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:44.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:44.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:45.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:45.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:46.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:46.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:47.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:47.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:48.687: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:48.690: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:49.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:49.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:50.460: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:50.462: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:51.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:51.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:52.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:52.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:53.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:53.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:54.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:54.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:55.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:55.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:56.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:56.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:57.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:57.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:58.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:58.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:15:59.450: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:15:59.454: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:00.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:00.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:01.451: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:01.455: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:02.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:02.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:03.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:03.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:04.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:04.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:05.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:05.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:06.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:06.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:07.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:07.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:08.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:08.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:09.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:09.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:10.450: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:10.453: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:11.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:11.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:12.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:12.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:13.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:13.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:13.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:14.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:14.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:14.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:15.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:15.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:15.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:16.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:16.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:16.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:17.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:17.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:17.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:18.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:18.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:18.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:19.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:19.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:19.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:20.449: INFO: Wrong image for pod: daemon-set-v6656. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  8 14:16:20.449: INFO: Pod daemon-set-v6656 is not available
Mar  8 14:16:20.452: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:21.449: INFO: Pod daemon-set-q8cr9 is not available
Mar  8 14:16:21.451: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  8 14:16:21.454: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:21.456: INFO: Number of nodes with available pods: 2
Mar  8 14:16:21.456: INFO: Node ip-192-168-72-251.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:16:22.459: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:16:22.461: INFO: Number of nodes with available pods: 3
Mar  8 14:16:22.461: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-xqkhb, will wait for the garbage collector to delete the pods
Mar  8 14:16:22.523: INFO: Deleting DaemonSet.extensions daemon-set took: 3.543646ms
Mar  8 14:16:22.624: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.236039ms
Mar  8 14:16:30.926: INFO: Number of nodes with available pods: 0
Mar  8 14:16:30.926: INFO: Number of running nodes: 0, number of available pods: 0
Mar  8 14:16:30.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-xqkhb/daemonsets","resourceVersion":"22202"},"items":null}

Mar  8 14:16:30.928: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-xqkhb/pods","resourceVersion":"22202"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:16:30.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-xqkhb" for this suite.
Mar  8 14:16:36.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:16:37.006: INFO: namespace: e2e-tests-daemonsets-xqkhb, resource: bindings, ignored listing per whitelist
Mar  8 14:16:37.042: INFO: namespace e2e-tests-daemonsets-xqkhb deletion completed in 6.103667513s

â€¢ [SLOW TEST:136.798 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:16:37.042: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:16:37.071567      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-tbvt4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  8 14:16:37.194: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  8 14:16:37.199: INFO: Waiting for terminating namespaces to be deleted...
Mar  8 14:16:37.201: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-64-146.us-west-2.compute.internal before test
Mar  8 14:16:37.207: INFO: dns-external-dns-784c569497-x5mzr from pipeline-system started at 2019-03-08 12:46:36 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container external-dns ready: true, restart count 0
Mar  8 14:16:37.207: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-pns4f from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:16:37.207: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:16:37.207: INFO: npls-nodepool-labels-operator-5f8b4685bf-qgwq6 from pipeline-system started at 2019-03-08 12:46:26 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container nodepool-labels-operator ready: true, restart count 0
Mar  8 14:16:37.207: INFO: ingress-traefik-7db7799d65-6h7fs from pipeline-system started at 2019-03-08 12:46:38 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container ingress-traefik ready: true, restart count 0
Mar  8 14:16:37.207: INFO: ith-instance-termination-handler-dtxjt from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:16:37.207: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-08 13:00:57 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  8 14:16:37.207: INFO: monitor-prometheus-node-exporter-9vjmd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:16:37.207: INFO: kube-proxy-dj5qw from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:16:37.207: INFO: weave-net-ftrz9 from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container weave ready: true, restart count 0
Mar  8 14:16:37.207: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:16:37.207: INFO: tiller-deploy-5f865894d6-kw9sw from kube-system started at 2019-03-08 12:46:10 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.207: INFO: 	Container tiller ready: true, restart count 0
Mar  8 14:16:37.207: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-251.us-west-2.compute.internal before test
Mar  8 14:16:37.218: INFO: weave-net-h4xft from kube-system started at 2019-03-08 12:45:50 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.218: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:16:37.218: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:16:37.218: INFO: sonobuoy-e2e-job-bca5431098ec41f9 from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.218: INFO: 	Container e2e ready: true, restart count 0
Mar  8 14:16:37.219: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  8 14:16:37.219: INFO: ith-instance-termination-handler-ssg67 from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.219: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:16:37.219: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2mb7g from heptio-sonobuoy started at 2019-03-08 13:01:03 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.219: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:16:37.219: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:16:37.219: INFO: kube-proxy-j5n9b from kube-system started at 2019-03-08 12:45:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.219: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:16:37.219: INFO: monitor-prometheus-node-exporter-t72fs from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.219: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:16:37.219: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-74-56.us-west-2.compute.internal before test
Mar  8 14:16:37.227: INFO: dashboard-kubernetes-dashboard-6b84cccd4-c6dpm from pipeline-system started at 2019-03-08 12:46:40 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  8 14:16:37.227: INFO: hpa-operator-kube-metrics-adapter-75794ffd9f-gm2xz from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container kube-metrics-adapter ready: true, restart count 0
Mar  8 14:16:37.227: INFO: hpa-operator-hpa-operator-795b5fc8c9-2d9t2 from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container hpa-operator ready: true, restart count 0
Mar  8 14:16:37.227: INFO: spot-webhook-spot-config-webhook-696c8bcd-wmkvj from pipeline-system started at 2019-03-08 12:46:50 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container spot-config-webhook ready: true, restart count 0
Mar  8 14:16:37.227: INFO: sonobuoy-systemd-logs-daemon-set-0039880062ac4be9-2nhct from heptio-sonobuoy started at 2019-03-08 13:01:02 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  8 14:16:37.227: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  8 14:16:37.227: INFO: autoscaler-aws-cluster-autoscaler-7ccb98ddc6-88xp5 from kube-system started at 2019-03-08 12:46:42 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 0
Mar  8 14:16:37.227: INFO: hpa-operator-metrics-server-7f9dfb4cd4-5976v from pipeline-system started at 2019-03-08 12:46:44 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container metrics-server ready: true, restart count 0
Mar  8 14:16:37.227: INFO: monitor-grafana-698887f8cd-85frd from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container grafana ready: true, restart count 0
Mar  8 14:16:37.227: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  8 14:16:37.227: INFO: monitor-prometheus-server-56994dbb47-rf8f4 from pipeline-system started at 2019-03-08 12:47:22 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  8 14:16:37.227: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  8 14:16:37.227: INFO: kube-proxy-hplw9 from kube-system started at 2019-03-08 12:45:49 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  8 14:16:37.227: INFO: pvc-operator-6575cc9546-fjntk from pipeline-system started at 2019-03-08 12:46:45 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container pvc-operator ready: true, restart count 0
Mar  8 14:16:37.227: INFO: spot-scheduler-6dbbcc8c4b-pp8hj from pipeline-system started at 2019-03-08 12:46:48 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container spot-scheduler ready: true, restart count 0
Mar  8 14:16:37.227: INFO: ith-instance-termination-handler-jrj9d from pipeline-system started at 2019-03-08 12:47:16 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container instance-termination-handler ready: true, restart count 0
Mar  8 14:16:37.227: INFO: monitor-prometheus-pushgateway-78c49d6b7f-k228k from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  8 14:16:37.227: INFO: weave-net-xvlch from kube-system started at 2019-03-08 12:45:49 +0000 UTC (2 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container weave ready: true, restart count 1
Mar  8 14:16:37.227: INFO: 	Container weave-npc ready: true, restart count 0
Mar  8 14:16:37.227: INFO: monitor-prometheus-node-exporter-kpgm8 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  8 14:16:37.227: INFO: monitor-prometheus-kube-state-metrics-76bf759fcb-c8zr9 from pipeline-system started at 2019-03-08 12:47:21 +0000 UTC (1 container statuses recorded)
Mar  8 14:16:37.227: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158a016edc90559b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:16:38.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-tbvt4" for this suite.
Mar  8 14:16:44.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:16:44.265: INFO: namespace: e2e-tests-sched-pred-tbvt4, resource: bindings, ignored listing per whitelist
Mar  8 14:16:44.308: INFO: namespace e2e-tests-sched-pred-tbvt4 deletion completed in 6.062305291s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.266 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:16:44.308: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:16:44.336145      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-76x6z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:16:44.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-76x6z" for this suite.
Mar  8 14:16:50.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:16:50.535: INFO: namespace: e2e-tests-kubelet-test-76x6z, resource: bindings, ignored listing per whitelist
Mar  8 14:16:50.537: INFO: namespace e2e-tests-kubelet-test-76x6z deletion completed in 6.059587317s

â€¢ [SLOW TEST:6.229 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:16:50.537: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:16:50.567032      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-6tn2h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:16:52.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-6tn2h" for this suite.
Mar  8 14:17:32.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:17:32.727: INFO: namespace: e2e-tests-kubelet-test-6tn2h, resource: bindings, ignored listing per whitelist
Mar  8 14:17:32.766: INFO: namespace e2e-tests-kubelet-test-6tn2h deletion completed in 40.058346864s

â€¢ [SLOW TEST:42.229 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:17:32.767: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:17:32.794322      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lwchw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  8 14:17:32.922: INFO: Waiting up to 5m0s for pod "pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-lwchw" to be "success or failure"
Mar  8 14:17:32.925: INFO: Pod "pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.53239ms
Mar  8 14:17:34.927: INFO: Pod "pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004552294s
STEP: Saw pod success
Mar  8 14:17:34.927: INFO: Pod "pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:17:34.928: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:17:34.938: INFO: Waiting for pod pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:17:34.941: INFO: Pod pod-ea18fb75-41ac-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:17:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lwchw" for this suite.
Mar  8 14:17:40.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:17:40.959: INFO: namespace: e2e-tests-emptydir-lwchw, resource: bindings, ignored listing per whitelist
Mar  8 14:17:41.004: INFO: namespace e2e-tests-emptydir-lwchw deletion completed in 6.061171818s

â€¢ [SLOW TEST:8.238 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:17:41.004: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:17:41.033299      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-24kxx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-24kxx
Mar  8 14:17:43.166: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-24kxx
STEP: checking the pod's current state and verifying that restartCount is present
Mar  8 14:17:43.167: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:21:44.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-24kxx" for this suite.
Mar  8 14:21:50.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:21:50.688: INFO: namespace: e2e-tests-container-probe-24kxx, resource: bindings, ignored listing per whitelist
Mar  8 14:21:50.726: INFO: namespace e2e-tests-container-probe-24kxx deletion completed in 6.059331813s

â€¢ [SLOW TEST:249.722 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:21:50.726: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:21:50.753845      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8v5md
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar  8 14:21:50.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 api-versions'
Mar  8 14:21:50.947: INFO: stderr: ""
Mar  8 14:21:50.947: INFO: stdout: "admission.banzaicloud.com/v1beta1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbanzaicloud.com/v1alpha1\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncustom.metrics.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nlabels.banzaicloud.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:21:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8v5md" for this suite.
Mar  8 14:21:56.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:21:57.001: INFO: namespace: e2e-tests-kubectl-8v5md, resource: bindings, ignored listing per whitelist
Mar  8 14:21:57.011: INFO: namespace e2e-tests-kubectl-8v5md deletion completed in 6.060318807s

â€¢ [SLOW TEST:6.285 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:21:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:21:57.038135      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-p8dr6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar  8 14:21:57.673: INFO: created pod pod-service-account-defaultsa
Mar  8 14:21:57.673: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  8 14:21:57.676: INFO: created pod pod-service-account-mountsa
Mar  8 14:21:57.676: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  8 14:21:57.681: INFO: created pod pod-service-account-nomountsa
Mar  8 14:21:57.681: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  8 14:21:57.686: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  8 14:21:57.686: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  8 14:21:57.693: INFO: created pod pod-service-account-mountsa-mountspec
Mar  8 14:21:57.693: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  8 14:21:57.707: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  8 14:21:57.707: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  8 14:21:57.715: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  8 14:21:57.715: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  8 14:21:57.723: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  8 14:21:57.723: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  8 14:21:57.740: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  8 14:21:57.740: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:21:57.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-p8dr6" for this suite.
Mar  8 14:22:03.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:03.782: INFO: namespace: e2e-tests-svcaccounts-p8dr6, resource: bindings, ignored listing per whitelist
Mar  8 14:22:03.810: INFO: namespace e2e-tests-svcaccounts-p8dr6 deletion completed in 6.060714402s

â€¢ [SLOW TEST:6.799 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:03.841754      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-8ctrm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:22:03.972: INFO: (0) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.044401ms)
Mar  8 14:22:03.974: INFO: (1) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.943105ms)
Mar  8 14:22:03.976: INFO: (2) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.839153ms)
Mar  8 14:22:03.978: INFO: (3) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.943521ms)
Mar  8 14:22:03.980: INFO: (4) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.114454ms)
Mar  8 14:22:03.982: INFO: (5) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.839341ms)
Mar  8 14:22:03.984: INFO: (6) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.86318ms)
Mar  8 14:22:03.986: INFO: (7) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.948426ms)
Mar  8 14:22:03.988: INFO: (8) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.986971ms)
Mar  8 14:22:03.990: INFO: (9) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.897142ms)
Mar  8 14:22:03.992: INFO: (10) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.812869ms)
Mar  8 14:22:03.994: INFO: (11) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.911685ms)
Mar  8 14:22:03.996: INFO: (12) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.163721ms)
Mar  8 14:22:03.998: INFO: (13) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.865652ms)
Mar  8 14:22:04.000: INFO: (14) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.965001ms)
Mar  8 14:22:04.002: INFO: (15) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.029096ms)
Mar  8 14:22:04.023: INFO: (16) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 21.237448ms)
Mar  8 14:22:04.025: INFO: (17) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.225302ms)
Mar  8 14:22:04.027: INFO: (18) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.946171ms)
Mar  8 14:22:04.029: INFO: (19) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.898725ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:04.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-8ctrm" for this suite.
Mar  8 14:22:10.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:10.044: INFO: namespace: e2e-tests-proxy-8ctrm, resource: bindings, ignored listing per whitelist
Mar  8 14:22:10.090: INFO: namespace e2e-tests-proxy-8ctrm deletion completed in 6.058300322s

â€¢ [SLOW TEST:6.280 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:10.090: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:10.119226      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rp2jq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  8 14:22:10.517: INFO: Waiting up to 5m0s for pod "pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-rp2jq" to be "success or failure"
Mar  8 14:22:10.521: INFO: Pod "pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305659ms
Mar  8 14:22:12.523: INFO: Pod "pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005545316s
STEP: Saw pod success
Mar  8 14:22:12.523: INFO: Pod "pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:22:12.524: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:22:12.535: INFO: Waiting for pod pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:22:12.538: INFO: Pod pod-8f8e8821-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rp2jq" for this suite.
Mar  8 14:22:18.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:18.580: INFO: namespace: e2e-tests-emptydir-rp2jq, resource: bindings, ignored listing per whitelist
Mar  8 14:22:18.597: INFO: namespace e2e-tests-emptydir-rp2jq deletion completed in 6.05722182s

â€¢ [SLOW TEST:8.507 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:18.598: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:18.627532      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-6xhf8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  8 14:22:18.758: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  8 14:22:23.761: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-6xhf8" for this suite.
Mar  8 14:22:30.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:30.839: INFO: namespace: e2e-tests-replication-controller-6xhf8, resource: bindings, ignored listing per whitelist
Mar  8 14:22:30.842: INFO: namespace e2e-tests-replication-controller-6xhf8 deletion completed in 6.069742681s

â€¢ [SLOW TEST:12.244 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:30.842: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:30.871225      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-twvjs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-twvjs/configmap-test-9bc43eea-41ad-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:22:31.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-twvjs" to be "success or failure"
Mar  8 14:22:31.007: INFO: Pod "pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.712327ms
Mar  8 14:22:33.009: INFO: Pod "pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003758325s
STEP: Saw pod success
Mar  8 14:22:33.009: INFO: Pod "pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:22:33.011: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7 container env-test: <nil>
STEP: delete the pod
Mar  8 14:22:33.024: INFO: Waiting for pod pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:22:33.026: INFO: Pod pod-configmaps-9bc49846-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:33.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-twvjs" for this suite.
Mar  8 14:22:39.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:39.053: INFO: namespace: e2e-tests-configmap-twvjs, resource: bindings, ignored listing per whitelist
Mar  8 14:22:39.084: INFO: namespace e2e-tests-configmap-twvjs deletion completed in 6.055654227s

â€¢ [SLOW TEST:8.242 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:39.084: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:39.112756      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-g6kxd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:22:39.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-g6kxd" to be "success or failure"
Mar  8 14:22:39.240: INFO: Pod "downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707991ms
Mar  8 14:22:41.242: INFO: Pod "downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004936519s
STEP: Saw pod success
Mar  8 14:22:41.242: INFO: Pod "downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:22:41.244: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:22:41.253: INFO: Waiting for pod downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:22:41.256: INFO: Pod downwardapi-volume-a0acbcf5-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:41.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g6kxd" for this suite.
Mar  8 14:22:47.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:47.272: INFO: namespace: e2e-tests-projected-g6kxd, resource: bindings, ignored listing per whitelist
Mar  8 14:22:47.318: INFO: namespace e2e-tests-projected-g6kxd deletion completed in 6.05878391s

â€¢ [SLOW TEST:8.233 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:47.318: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:47.345411      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-j4cp9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a5957f53-41ad-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:22:47.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-j4cp9" to be "success or failure"
Mar  8 14:22:47.478: INFO: Pod "pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208556ms
Mar  8 14:22:49.480: INFO: Pod "pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004746036s
STEP: Saw pod success
Mar  8 14:22:49.480: INFO: Pod "pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:22:49.482: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 14:22:49.496: INFO: Waiting for pod pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:22:49.501: INFO: Pod pod-configmaps-a595d5db-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:49.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-j4cp9" for this suite.
Mar  8 14:22:55.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:22:55.552: INFO: namespace: e2e-tests-configmap-j4cp9, resource: bindings, ignored listing per whitelist
Mar  8 14:22:55.590: INFO: namespace e2e-tests-configmap-j4cp9 deletion completed in 6.084481496s

â€¢ [SLOW TEST:8.272 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:22:55.590: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:22:55.619444      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lfnr6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-aa854028-41ad-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:22:55.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-lfnr6" to be "success or failure"
Mar  8 14:22:55.912: INFO: Pod "pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 33.580361ms
Mar  8 14:22:57.914: INFO: Pod "pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035882324s
Mar  8 14:22:59.917: INFO: Pod "pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038037782s
STEP: Saw pod success
Mar  8 14:22:59.917: INFO: Pod "pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:22:59.918: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:22:59.944: INFO: Waiting for pod pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:22:59.947: INFO: Pod pod-projected-secrets-aa911a0a-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:22:59.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lfnr6" for this suite.
Mar  8 14:23:05.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:05.980: INFO: namespace: e2e-tests-projected-lfnr6, resource: bindings, ignored listing per whitelist
Mar  8 14:23:06.012: INFO: namespace e2e-tests-projected-lfnr6 deletion completed in 6.06084879s

â€¢ [SLOW TEST:10.422 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:06.012: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:06.041187      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-9z22q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:23:06.165: INFO: Creating deployment "test-recreate-deployment"
Mar  8 14:23:06.170: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  8 14:23:06.174: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  8 14:23:08.178: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  8 14:23:08.179: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  8 14:23:08.183: INFO: Updating deployment test-recreate-deployment
Mar  8 14:23:08.183: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  8 14:23:08.241: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-9z22q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9z22q/deployments/test-recreate-deployment,UID:b0bab259-41ad-11e9-8665-02fd46f865f2,ResourceVersion:23536,Generation:2,CreationTimestamp:2019-03-08 14:23:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-08 14:23:08 +0000 UTC 2019-03-08 14:23:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-08 14:23:08 +0000 UTC 2019-03-08 14:23:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  8 14:23:08.243: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-9z22q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9z22q/replicasets/test-recreate-deployment-697fbf54bf,UID:b1f2189a-41ad-11e9-8665-02fd46f865f2,ResourceVersion:23533,Generation:1,CreationTimestamp:2019-03-08 14:23:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b0bab259-41ad-11e9-8665-02fd46f865f2 0xc00215bbd7 0xc00215bbd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 14:23:08.243: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  8 14:23:08.243: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-9z22q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9z22q/replicasets/test-recreate-deployment-5dfdcc846d,UID:b0bbeb48-41ad-11e9-8665-02fd46f865f2,ResourceVersion:23524,Generation:2,CreationTimestamp:2019-03-08 14:23:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b0bab259-41ad-11e9-8665-02fd46f865f2 0xc00215bb17 0xc00215bb18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  8 14:23:08.245: INFO: Pod "test-recreate-deployment-697fbf54bf-jl24k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-jl24k,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-9z22q,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9z22q/pods/test-recreate-deployment-697fbf54bf-jl24k,UID:b1f27751-41ad-11e9-8665-02fd46f865f2,ResourceVersion:23534,Generation:0,CreationTimestamp:2019-03-08 14:23:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf b1f2189a-41ad-11e9-8665-02fd46f865f2 0xc002456787 0xc002456788}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t9v74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t9v74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t9v74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-64-146.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024567f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002456810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:23:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-08 14:23:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.64.146,PodIP:,StartTime:2019-03-08 14:23:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:08.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9z22q" for this suite.
Mar  8 14:23:14.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:14.281: INFO: namespace: e2e-tests-deployment-9z22q, resource: bindings, ignored listing per whitelist
Mar  8 14:23:14.305: INFO: namespace e2e-tests-deployment-9z22q deletion completed in 6.057362483s

â€¢ [SLOW TEST:8.293 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:14.333665      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2bmtj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  8 14:23:14.461: INFO: Waiting up to 5m0s for pod "downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-2bmtj" to be "success or failure"
Mar  8 14:23:14.466: INFO: Pod "downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.20444ms
Mar  8 14:23:16.469: INFO: Pod "downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007370713s
STEP: Saw pod success
Mar  8 14:23:16.469: INFO: Pod "downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:23:16.470: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 14:23:16.483: INFO: Waiting for pod downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:23:16.484: INFO: Pod downward-api-b5ab613d-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:16.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2bmtj" for this suite.
Mar  8 14:23:22.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:22.512: INFO: namespace: e2e-tests-downward-api-2bmtj, resource: bindings, ignored listing per whitelist
Mar  8 14:23:22.549: INFO: namespace e2e-tests-downward-api-2bmtj deletion completed in 6.05955112s

â€¢ [SLOW TEST:8.244 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:22.549: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:22.577814      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7f8lk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:23:22.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-7f8lk" to be "success or failure"
Mar  8 14:23:22.709: INFO: Pod "downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4589ms
Mar  8 14:23:24.711: INFO: Pod "downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004887517s
STEP: Saw pod success
Mar  8 14:23:24.711: INFO: Pod "downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:23:24.713: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:23:24.733: INFO: Waiting for pod downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:23:24.735: INFO: Pod downwardapi-volume-ba957565-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7f8lk" for this suite.
Mar  8 14:23:30.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:30.788: INFO: namespace: e2e-tests-projected-7f8lk, resource: bindings, ignored listing per whitelist
Mar  8 14:23:30.798: INFO: namespace e2e-tests-projected-7f8lk deletion completed in 6.058695405s

â€¢ [SLOW TEST:8.249 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:30.798: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:30.825248      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-bgkdl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0308 14:23:32.384717      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  8 14:23:32.384: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:32.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-bgkdl" for this suite.
Mar  8 14:23:38.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:38.414: INFO: namespace: e2e-tests-gc-bgkdl, resource: bindings, ignored listing per whitelist
Mar  8 14:23:38.444: INFO: namespace e2e-tests-gc-bgkdl deletion completed in 6.05697835s

â€¢ [SLOW TEST:7.646 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:38.444: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:38.471134      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-v8jxs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  8 14:23:38.603: INFO: Waiting up to 5m0s for pod "downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-v8jxs" to be "success or failure"
Mar  8 14:23:38.607: INFO: Pod "downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.228463ms
Mar  8 14:23:40.609: INFO: Pod "downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005519757s
STEP: Saw pod success
Mar  8 14:23:40.609: INFO: Pod "downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:23:40.611: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7 container dapi-container: <nil>
STEP: delete the pod
Mar  8 14:23:40.623: INFO: Waiting for pod downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:23:40.624: INFO: Pod downward-api-c40f6eac-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:40.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v8jxs" for this suite.
Mar  8 14:23:46.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:46.656: INFO: namespace: e2e-tests-downward-api-v8jxs, resource: bindings, ignored listing per whitelist
Mar  8 14:23:46.682: INFO: namespace e2e-tests-downward-api-v8jxs deletion completed in 6.055627845s

â€¢ [SLOW TEST:8.238 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:46.682: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:46.710466      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bvnfb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c8f74456-41ad-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:23:46.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-bvnfb" to be "success or failure"
Mar  8 14:23:46.838: INFO: Pod "pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389973ms
Mar  8 14:23:48.841: INFO: Pod "pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005373271s
STEP: Saw pod success
Mar  8 14:23:48.841: INFO: Pod "pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:23:48.843: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 14:23:48.854: INFO: Waiting for pod pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:23:48.856: INFO: Pod pod-configmaps-c8f792d7-41ad-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:48.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bvnfb" for this suite.
Mar  8 14:23:54.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:23:54.898: INFO: namespace: e2e-tests-configmap-bvnfb, resource: bindings, ignored listing per whitelist
Mar  8 14:23:54.928: INFO: namespace e2e-tests-configmap-bvnfb deletion completed in 6.070078387s

â€¢ [SLOW TEST:8.246 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:23:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:23:54.956356      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-28d2f
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cde28a59-41ad-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-cde28a59-41ad-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:23:59.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-28d2f" for this suite.
Mar  8 14:24:21.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:24:21.149: INFO: namespace: e2e-tests-projected-28d2f, resource: bindings, ignored listing per whitelist
Mar  8 14:24:21.166: INFO: namespace e2e-tests-projected-28d2f deletion completed in 22.057263059s

â€¢ [SLOW TEST:26.238 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:24:21.166: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:24:21.198587      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-8bjs7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:24:43.339: INFO: Container started at 2019-03-08 14:24:22 +0000 UTC, pod became ready at 2019-03-08 14:24:43 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:24:43.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8bjs7" for this suite.
Mar  8 14:25:05.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:25:05.395: INFO: namespace: e2e-tests-container-probe-8bjs7, resource: bindings, ignored listing per whitelist
Mar  8 14:25:05.404: INFO: namespace e2e-tests-container-probe-8bjs7 deletion completed in 22.061816418s

â€¢ [SLOW TEST:44.238 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:25:05.404: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:25:05.431188      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-8ncj7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  8 14:25:05.571: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:05.576: INFO: Number of nodes with available pods: 0
Mar  8 14:25:05.576: INFO: Node ip-192-168-64-146.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:25:06.933: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:06.935: INFO: Number of nodes with available pods: 2
Mar  8 14:25:06.935: INFO: Node ip-192-168-74-56.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:25:07.579: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:07.581: INFO: Number of nodes with available pods: 3
Mar  8 14:25:07.581: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  8 14:25:07.593: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:07.597: INFO: Number of nodes with available pods: 2
Mar  8 14:25:07.597: INFO: Node ip-192-168-74-56.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:25:08.601: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:08.603: INFO: Number of nodes with available pods: 2
Mar  8 14:25:08.603: INFO: Node ip-192-168-74-56.us-west-2.compute.internal is running more than one daemon pod
Mar  8 14:25:09.601: INFO: DaemonSet pods can't tolerate node ip-192-168-75-219.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  8 14:25:09.603: INFO: Number of nodes with available pods: 3
Mar  8 14:25:09.603: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-8ncj7, will wait for the garbage collector to delete the pods
Mar  8 14:25:09.665: INFO: Deleting DaemonSet.extensions daemon-set took: 7.520998ms
Mar  8 14:25:09.765: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.2612ms
Mar  8 14:25:50.467: INFO: Number of nodes with available pods: 0
Mar  8 14:25:50.467: INFO: Number of running nodes: 0, number of available pods: 0
Mar  8 14:25:50.468: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8ncj7/daemonsets","resourceVersion":"24191"},"items":null}

Mar  8 14:25:50.470: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8ncj7/pods","resourceVersion":"24191"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:25:50.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8ncj7" for this suite.
Mar  8 14:25:56.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:25:56.536: INFO: namespace: e2e-tests-daemonsets-8ncj7, resource: bindings, ignored listing per whitelist
Mar  8 14:25:56.538: INFO: namespace e2e-tests-daemonsets-8ncj7 deletion completed in 6.058816822s

â€¢ [SLOW TEST:51.134 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:25:56.538: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:25:56.565855      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cg5gn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 14:25:56.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cg5gn'
Mar  8 14:25:56.923: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  8 14:25:56.923: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar  8 14:25:56.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-cg5gn'
Mar  8 14:25:57.026: INFO: stderr: ""
Mar  8 14:25:57.026: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:25:57.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cg5gn" for this suite.
Mar  8 14:26:03.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:26:03.535: INFO: namespace: e2e-tests-kubectl-cg5gn, resource: bindings, ignored listing per whitelist
Mar  8 14:26:03.553: INFO: namespace e2e-tests-kubectl-cg5gn deletion completed in 6.523858985s

â€¢ [SLOW TEST:7.015 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:26:03.553: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:26:03.581122      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x48cs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-1a8c5190-41ae-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:26:03.708: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-x48cs" to be "success or failure"
Mar  8 14:26:03.711: INFO: Pod "pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180903ms
Mar  8 14:26:05.714: INFO: Pod "pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005703403s
STEP: Saw pod success
Mar  8 14:26:05.714: INFO: Pod "pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:26:05.715: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:26:05.728: INFO: Waiting for pod pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:26:05.729: INFO: Pod pod-projected-secrets-1a8cae66-41ae-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:26:05.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x48cs" for this suite.
Mar  8 14:26:11.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:26:11.811: INFO: namespace: e2e-tests-projected-x48cs, resource: bindings, ignored listing per whitelist
Mar  8 14:26:11.835: INFO: namespace e2e-tests-projected-x48cs deletion completed in 6.103481673s

â€¢ [SLOW TEST:8.282 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:26:11.835: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:26:11.871060      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ddk25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:26:12.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-ddk25" to be "success or failure"
Mar  8 14:26:12.004: INFO: Pod "downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683758ms
Mar  8 14:26:14.007: INFO: Pod "downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00617693s
STEP: Saw pod success
Mar  8 14:26:14.007: INFO: Pod "downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:26:14.009: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:26:14.021: INFO: Waiting for pod downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:26:14.022: INFO: Pod downwardapi-volume-1f7dceaa-41ae-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:26:14.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ddk25" for this suite.
Mar  8 14:26:20.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:26:20.037: INFO: namespace: e2e-tests-projected-ddk25, resource: bindings, ignored listing per whitelist
Mar  8 14:26:20.080: INFO: namespace e2e-tests-projected-ddk25 deletion completed in 6.055707628s

â€¢ [SLOW TEST:8.245 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:26:20.080: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:26:20.113624      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-q7fh8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-2466d7b8-41ae-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:26:20.241: INFO: Waiting up to 5m0s for pod "pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-q7fh8" to be "success or failure"
Mar  8 14:26:20.245: INFO: Pod "pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976301ms
Mar  8 14:26:22.247: INFO: Pod "pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00617933s
STEP: Saw pod success
Mar  8 14:26:22.247: INFO: Pod "pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:26:22.249: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 14:26:22.258: INFO: Waiting for pod pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:26:22.261: INFO: Pod pod-configmaps-24672e25-41ae-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:26:22.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q7fh8" for this suite.
Mar  8 14:26:28.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:26:28.286: INFO: namespace: e2e-tests-configmap-q7fh8, resource: bindings, ignored listing per whitelist
Mar  8 14:26:28.320: INFO: namespace e2e-tests-configmap-q7fh8 deletion completed in 6.056998059s

â€¢ [SLOW TEST:8.240 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:26:28.320: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:26:28.347103      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-xbhbh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  8 14:26:28.469: INFO: PodSpec: initContainers in spec.initContainers
Mar  8 14:27:09.480: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-294f55fa-41ae-11e9-be5e-ae5bc1be57d7", GenerateName:"", Namespace:"e2e-tests-init-container-xbhbh", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-xbhbh/pods/pod-init-294f55fa-41ae-11e9-be5e-ae5bc1be57d7", UID:"294f8d23-41ae-11e9-8665-02fd46f865f2", ResourceVersion:"24525", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687651988, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"469099243"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ckmh8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00239ab40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckmh8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckmh8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckmh8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0020bd108), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-192-168-72-251.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001d17e60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0020bd2b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0020bd2d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0020bd2d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0020bd2dc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687651988, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687651988, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687651988, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687651988, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.72.251", PodIP:"10.20.160.3", StartTime:(*v1.Time)(0xc00216e8a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0015ee850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0015ee930)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://5196578753ad0ee93c79aef7231595bddee2e63accbabcae89ad76a9d25e6551"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00216eb40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00216e960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:27:09.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xbhbh" for this suite.
Mar  8 14:27:31.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:27:31.521: INFO: namespace: e2e-tests-init-container-xbhbh, resource: bindings, ignored listing per whitelist
Mar  8 14:27:31.542: INFO: namespace e2e-tests-init-container-xbhbh deletion completed in 22.057809013s

â€¢ [SLOW TEST:63.222 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:27:31.543: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:27:31.571835      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z878x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  8 14:27:32.193: INFO: Waiting up to 5m0s for pod "pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-emptydir-z878x" to be "success or failure"
Mar  8 14:27:32.197: INFO: Pod "pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404767ms
Mar  8 14:27:34.525: INFO: Pod "pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.331916298s
STEP: Saw pod success
Mar  8 14:27:34.525: INFO: Pod "pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:27:34.526: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:27:34.538: INFO: Waiting for pod pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:27:34.540: INFO: Pod pod-4f4a7245-41ae-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:27:34.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z878x" for this suite.
Mar  8 14:27:40.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:27:40.593: INFO: namespace: e2e-tests-emptydir-z878x, resource: bindings, ignored listing per whitelist
Mar  8 14:27:40.600: INFO: namespace e2e-tests-emptydir-z878x deletion completed in 6.057531208s

â€¢ [SLOW TEST:9.057 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:27:40.600: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:27:40.630902      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-7qs2w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar  8 14:27:40.755: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-7qs2w" to be "success or failure"
Mar  8 14:27:40.758: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469758ms
Mar  8 14:27:42.761: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005761091s
STEP: Saw pod success
Mar  8 14:27:42.761: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  8 14:27:42.762: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  8 14:27:42.773: INFO: Waiting for pod pod-host-path-test to disappear
Mar  8 14:27:42.775: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:27:42.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-7qs2w" for this suite.
Mar  8 14:27:48.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:27:48.797: INFO: namespace: e2e-tests-hostpath-7qs2w, resource: bindings, ignored listing per whitelist
Mar  8 14:27:48.836: INFO: namespace e2e-tests-hostpath-7qs2w deletion completed in 6.057991304s

â€¢ [SLOW TEST:8.236 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:27:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:27:48.864372      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gxcwm
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-594f420c-41ae-11e9-be5e-ae5bc1be57d7
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-594f420c-41ae-11e9-be5e-ae5bc1be57d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:29:01.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gxcwm" for this suite.
Mar  8 14:29:23.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:29:23.752: INFO: namespace: e2e-tests-configmap-gxcwm, resource: bindings, ignored listing per whitelist
Mar  8 14:29:23.760: INFO: namespace e2e-tests-configmap-gxcwm deletion completed in 22.059067841s

â€¢ [SLOW TEST:94.924 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:29:23.760: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:29:23.787781      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-llzr6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:29:26.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-llzr6" for this suite.
Mar  8 14:29:48.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:29:49.003: INFO: namespace: e2e-tests-replication-controller-llzr6, resource: bindings, ignored listing per whitelist
Mar  8 14:29:49.006: INFO: namespace e2e-tests-replication-controller-llzr6 deletion completed in 22.075917493s

â€¢ [SLOW TEST:25.246 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:29:49.006: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:29:49.034843      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-dpght
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  8 14:29:53.184: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  8 14:29:53.188: INFO: Pod pod-with-prestop-http-hook still exists
Mar  8 14:29:55.188: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  8 14:29:55.323: INFO: Pod pod-with-prestop-http-hook still exists
Mar  8 14:29:57.188: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  8 14:29:57.190: INFO: Pod pod-with-prestop-http-hook still exists
Mar  8 14:29:59.188: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  8 14:29:59.432: INFO: Pod pod-with-prestop-http-hook still exists
Mar  8 14:30:01.188: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  8 14:30:01.190: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:30:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dpght" for this suite.
Mar  8 14:30:23.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:30:23.225: INFO: namespace: e2e-tests-container-lifecycle-hook-dpght, resource: bindings, ignored listing per whitelist
Mar  8 14:30:23.253: INFO: namespace e2e-tests-container-lifecycle-hook-dpght deletion completed in 22.05737289s

â€¢ [SLOW TEST:34.247 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:30:23.253: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:30:23.280698      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fgfjq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-fgfjq
Mar  8 14:30:25.416: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-fgfjq
STEP: checking the pod's current state and verifying that restartCount is present
Mar  8 14:30:25.418: INFO: Initial restart count of pod liveness-http is 0
Mar  8 14:30:35.430: INFO: Restart count of pod e2e-tests-container-probe-fgfjq/liveness-http is now 1 (10.012224664s elapsed)
Mar  8 14:30:55.848: INFO: Restart count of pod e2e-tests-container-probe-fgfjq/liveness-http is now 2 (30.429793282s elapsed)
Mar  8 14:31:16.500: INFO: Restart count of pod e2e-tests-container-probe-fgfjq/liveness-http is now 3 (51.081967369s elapsed)
Mar  8 14:31:36.523: INFO: Restart count of pod e2e-tests-container-probe-fgfjq/liveness-http is now 4 (1m11.105692916s elapsed)
Mar  8 14:32:48.755: INFO: Restart count of pod e2e-tests-container-probe-fgfjq/liveness-http is now 5 (2m23.337332064s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:32:48.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fgfjq" for this suite.
Mar  8 14:32:54.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:32:55.125: INFO: namespace: e2e-tests-container-probe-fgfjq, resource: bindings, ignored listing per whitelist
Mar  8 14:32:55.139: INFO: namespace e2e-tests-container-probe-fgfjq deletion completed in 6.371885763s

â€¢ [SLOW TEST:151.886 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:32:55.139: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:32:55.167787      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-85rlj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  8 14:33:01.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:01.904: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:03.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:03.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:05.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:05.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:07.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:07.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:09.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:09.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:11.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:11.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:13.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:13.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:15.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:15.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:17.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:17.907: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:19.906: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:19.909: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  8 14:33:21.905: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  8 14:33:21.907: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:33:21.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-85rlj" for this suite.
Mar  8 14:33:43.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:33:44.092: INFO: namespace: e2e-tests-container-lifecycle-hook-85rlj, resource: bindings, ignored listing per whitelist
Mar  8 14:33:44.102: INFO: namespace e2e-tests-container-lifecycle-hook-85rlj deletion completed in 22.188006774s

â€¢ [SLOW TEST:48.962 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:33:44.102: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:33:44.128828      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8q9wt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8q9wt/configmap-test-2d6e2441-41af-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:33:44.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-configmap-8q9wt" to be "success or failure"
Mar  8 14:33:44.890: INFO: Pod "pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014589ms
Mar  8 14:33:46.892: INFO: Pod "pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008073636s
STEP: Saw pod success
Mar  8 14:33:46.892: INFO: Pod "pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:33:46.893: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7 container env-test: <nil>
STEP: delete the pod
Mar  8 14:33:46.903: INFO: Waiting for pod pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:33:46.906: INFO: Pod pod-configmaps-2d6e7141-41af-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:33:46.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8q9wt" for this suite.
Mar  8 14:33:52.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:33:52.953: INFO: namespace: e2e-tests-configmap-8q9wt, resource: bindings, ignored listing per whitelist
Mar  8 14:33:52.963: INFO: namespace e2e-tests-configmap-8q9wt deletion completed in 6.054363613s

â€¢ [SLOW TEST:8.862 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:33:52.963: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:33:52.990086      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-dsh7j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-32566e53-41af-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:33:53.116: INFO: Waiting up to 5m0s for pod "pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-dsh7j" to be "success or failure"
Mar  8 14:33:53.120: INFO: Pod "pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664455ms
Mar  8 14:33:55.122: INFO: Pod "pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005903704s
STEP: Saw pod success
Mar  8 14:33:55.122: INFO: Pod "pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:33:55.123: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7 container secret-env-test: <nil>
STEP: delete the pod
Mar  8 14:33:55.135: INFO: Waiting for pod pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:33:55.137: INFO: Pod pod-secrets-3256b32c-41af-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:33:55.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dsh7j" for this suite.
Mar  8 14:34:01.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:34:01.161: INFO: namespace: e2e-tests-secrets-dsh7j, resource: bindings, ignored listing per whitelist
Mar  8 14:34:01.201: INFO: namespace e2e-tests-secrets-dsh7j deletion completed in 6.060974105s

â€¢ [SLOW TEST:8.237 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:34:01.201: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:34:01.229128      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-n9j4c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:34:01.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-n9j4c" to be "success or failure"
Mar  8 14:34:01.364: INFO: Pod "downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985985ms
Mar  8 14:34:03.369: INFO: Pod "downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008806004s
STEP: Saw pod success
Mar  8 14:34:03.369: INFO: Pod "downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:34:03.374: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:34:03.398: INFO: Waiting for pod downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:34:03.402: INFO: Pod downwardapi-volume-37402f8d-41af-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:34:03.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n9j4c" for this suite.
Mar  8 14:34:09.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:34:09.422: INFO: namespace: e2e-tests-projected-n9j4c, resource: bindings, ignored listing per whitelist
Mar  8 14:34:09.465: INFO: namespace e2e-tests-projected-n9j4c deletion completed in 6.05735903s

â€¢ [SLOW TEST:8.264 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:34:09.465: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:34:09.492342      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-mgtlv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar  8 14:34:09.619: INFO: Waiting up to 5m0s for pod "client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-containers-mgtlv" to be "success or failure"
Mar  8 14:34:09.623: INFO: Pod "client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.774934ms
Mar  8 14:34:11.626: INFO: Pod "client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006967601s
Mar  8 14:34:13.628: INFO: Pod "client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009160584s
STEP: Saw pod success
Mar  8 14:34:13.628: INFO: Pod "client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:34:13.629: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:34:13.643: INFO: Waiting for pod client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:34:13.646: INFO: Pod client-containers-3c2c6fe4-41af-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:34:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-mgtlv" for this suite.
Mar  8 14:34:19.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:34:19.699: INFO: namespace: e2e-tests-containers-mgtlv, resource: bindings, ignored listing per whitelist
Mar  8 14:34:19.707: INFO: namespace e2e-tests-containers-mgtlv deletion completed in 6.057952572s

â€¢ [SLOW TEST:10.242 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:34:19.707: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:34:19.743689      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4d6tf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar  8 14:34:19.867: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-643275360 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:34:19.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4d6tf" for this suite.
Mar  8 14:34:25.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:34:25.965: INFO: namespace: e2e-tests-kubectl-4d6tf, resource: bindings, ignored listing per whitelist
Mar  8 14:34:25.990: INFO: namespace e2e-tests-kubectl-4d6tf deletion completed in 6.055339833s

â€¢ [SLOW TEST:6.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:34:25.990: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:34:26.017707      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-hhhhq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hhhhq
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar  8 14:34:26.150: INFO: Found 0 stateful pods, waiting for 3
Mar  8 14:34:36.152: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:34:36.152: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:34:36.152: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  8 14:34:36.173: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  8 14:34:46.197: INFO: Updating stateful set ss2
Mar  8 14:34:46.202: INFO: Waiting for Pod e2e-tests-statefulset-hhhhq/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar  8 14:34:56.269: INFO: Found 2 stateful pods, waiting for 3
Mar  8 14:35:06.272: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:35:06.272: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  8 14:35:06.272: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  8 14:35:06.289: INFO: Updating stateful set ss2
Mar  8 14:35:06.296: INFO: Waiting for Pod e2e-tests-statefulset-hhhhq/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  8 14:35:16.315: INFO: Updating stateful set ss2
Mar  8 14:35:16.321: INFO: Waiting for StatefulSet e2e-tests-statefulset-hhhhq/ss2 to complete update
Mar  8 14:35:16.321: INFO: Waiting for Pod e2e-tests-statefulset-hhhhq/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  8 14:35:26.325: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hhhhq
Mar  8 14:35:26.327: INFO: Scaling statefulset ss2 to 0
Mar  8 14:35:46.337: INFO: Waiting for statefulset status.replicas updated to 0
Mar  8 14:35:46.339: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:35:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hhhhq" for this suite.
Mar  8 14:35:52.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:35:52.364: INFO: namespace: e2e-tests-statefulset-hhhhq, resource: bindings, ignored listing per whitelist
Mar  8 14:35:52.406: INFO: namespace e2e-tests-statefulset-hhhhq deletion completed in 6.05588397s

â€¢ [SLOW TEST:86.416 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:35:52.406: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:35:52.433848      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-rqnq9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  8 14:38:34.586: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:34.588: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:36.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:36.591: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:38.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:38.593: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:40.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:40.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:42.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:42.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:44.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:44.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:46.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:46.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:48.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:48.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:50.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:50.590: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:52.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:52.592: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  8 14:38:54.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  8 14:38:54.590: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:38:54.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-rqnq9" for this suite.
Mar  8 14:39:16.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:39:16.644: INFO: namespace: e2e-tests-container-lifecycle-hook-rqnq9, resource: bindings, ignored listing per whitelist
Mar  8 14:39:16.650: INFO: namespace e2e-tests-container-lifecycle-hook-rqnq9 deletion completed in 22.05708922s

â€¢ [SLOW TEST:204.244 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:39:16.650: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:39:16.677733      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-4mqdk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4mqdk
Mar  8 14:39:18.812: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4mqdk
STEP: checking the pod's current state and verifying that restartCount is present
Mar  8 14:39:18.814: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:43:19.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4mqdk" for this suite.
Mar  8 14:43:25.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:43:25.460: INFO: namespace: e2e-tests-container-probe-4mqdk, resource: bindings, ignored listing per whitelist
Mar  8 14:43:25.502: INFO: namespace e2e-tests-container-probe-4mqdk deletion completed in 6.311629176s

â€¢ [SLOW TEST:248.852 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:43:25.502: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:43:25.528957      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-dkn6v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-87993776-41b0-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:43:25.657: INFO: Waiting up to 5m0s for pod "pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-dkn6v" to be "success or failure"
Mar  8 14:43:25.660: INFO: Pod "pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412234ms
Mar  8 14:43:27.662: INFO: Pod "pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004507973s
STEP: Saw pod success
Mar  8 14:43:27.662: INFO: Pod "pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:43:27.663: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:43:27.674: INFO: Waiting for pod pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:43:27.677: INFO: Pod pod-secrets-8799866c-41b0-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:43:27.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dkn6v" for this suite.
Mar  8 14:43:33.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:43:33.736: INFO: namespace: e2e-tests-secrets-dkn6v, resource: bindings, ignored listing per whitelist
Mar  8 14:43:33.739: INFO: namespace e2e-tests-secrets-dkn6v deletion completed in 6.059798764s

â€¢ [SLOW TEST:8.237 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:43:33.739: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:43:33.766505      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-ld8vm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-gcz4
STEP: Creating a pod to test atomic-volume-subpath
Mar  8 14:43:33.893: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gcz4" in namespace "e2e-tests-subpath-ld8vm" to be "success or failure"
Mar  8 14:43:33.897: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928568ms
Mar  8 14:43:35.899: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 2.005979296s
Mar  8 14:43:37.902: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 4.008220405s
Mar  8 14:43:39.904: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 6.010445523s
Mar  8 14:43:41.906: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 8.012638794s
Mar  8 14:43:43.908: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 10.014962929s
Mar  8 14:43:45.910: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 12.017095719s
Mar  8 14:43:47.999: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 14.105462178s
Mar  8 14:43:50.001: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 16.1076888s
Mar  8 14:43:52.003: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 18.109955954s
Mar  8 14:43:54.005: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Running", Reason="", readiness=false. Elapsed: 20.112123756s
Mar  8 14:43:56.008: INFO: Pod "pod-subpath-test-configmap-gcz4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.114394272s
STEP: Saw pod success
Mar  8 14:43:56.008: INFO: Pod "pod-subpath-test-configmap-gcz4" satisfied condition "success or failure"
Mar  8 14:43:56.010: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-subpath-test-configmap-gcz4 container test-container-subpath-configmap-gcz4: <nil>
STEP: delete the pod
Mar  8 14:43:56.022: INFO: Waiting for pod pod-subpath-test-configmap-gcz4 to disappear
Mar  8 14:43:56.027: INFO: Pod pod-subpath-test-configmap-gcz4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gcz4
Mar  8 14:43:56.027: INFO: Deleting pod "pod-subpath-test-configmap-gcz4" in namespace "e2e-tests-subpath-ld8vm"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:43:56.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-ld8vm" for this suite.
Mar  8 14:44:02.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:44:02.091: INFO: namespace: e2e-tests-subpath-ld8vm, resource: bindings, ignored listing per whitelist
Mar  8 14:44:02.095: INFO: namespace e2e-tests-subpath-ld8vm deletion completed in 6.062362684s

â€¢ [SLOW TEST:28.356 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:44:02.095: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:44:02.134512      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-2qz6p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:44:04.300: INFO: Waiting up to 5m0s for pod "client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-pods-2qz6p" to be "success or failure"
Mar  8 14:44:04.308: INFO: Pod "client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.362799ms
Mar  8 14:44:06.310: INFO: Pod "client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009674613s
STEP: Saw pod success
Mar  8 14:44:06.310: INFO: Pod "client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:44:06.312: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7 container env3cont: <nil>
STEP: delete the pod
Mar  8 14:44:06.323: INFO: Waiting for pod client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:44:06.325: INFO: Pod client-envvars-9ea0c963-41b0-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:44:06.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2qz6p" for this suite.
Mar  8 14:44:44.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:44:44.356: INFO: namespace: e2e-tests-pods-2qz6p, resource: bindings, ignored listing per whitelist
Mar  8 14:44:44.389: INFO: namespace e2e-tests-pods-2qz6p deletion completed in 38.062182959s

â€¢ [SLOW TEST:42.294 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:44:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:44:44.417289      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qtgr6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 14:44:44.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-qtgr6'
Mar  8 14:44:44.739: INFO: stderr: ""
Mar  8 14:44:44.739: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Mar  8 14:44:44.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-qtgr6'
Mar  8 14:44:50.446: INFO: stderr: ""
Mar  8 14:44:50.446: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:44:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qtgr6" for this suite.
Mar  8 14:44:56.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:44:56.465: INFO: namespace: e2e-tests-kubectl-qtgr6, resource: bindings, ignored listing per whitelist
Mar  8 14:44:56.506: INFO: namespace e2e-tests-kubectl-qtgr6 deletion completed in 6.057413281s

â€¢ [SLOW TEST:12.117 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:44:56.507: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:44:56.535356      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-z8wv8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar  8 14:44:56.663: INFO: Waiting up to 5m0s for pod "client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-containers-z8wv8" to be "success or failure"
Mar  8 14:44:56.666: INFO: Pod "client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.776681ms
Mar  8 14:44:58.668: INFO: Pod "client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005021522s
STEP: Saw pod success
Mar  8 14:44:58.668: INFO: Pod "client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:44:58.669: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7 container test-container: <nil>
STEP: delete the pod
Mar  8 14:44:58.678: INFO: Waiting for pod client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:44:58.681: INFO: Pod client-containers-bdd7a6dd-41b0-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:44:58.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-z8wv8" for this suite.
Mar  8 14:45:04.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:45:04.700: INFO: namespace: e2e-tests-containers-z8wv8, resource: bindings, ignored listing per whitelist
Mar  8 14:45:04.742: INFO: namespace e2e-tests-containers-z8wv8 deletion completed in 6.058085983s

â€¢ [SLOW TEST:8.236 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:45:04.742: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:45:04.771169      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-rj85n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:45:05.276: INFO: (0) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.708644ms)
Mar  8 14:45:05.278: INFO: (1) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.124127ms)
Mar  8 14:45:05.280: INFO: (2) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.953594ms)
Mar  8 14:45:05.283: INFO: (3) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.129538ms)
Mar  8 14:45:05.285: INFO: (4) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.970327ms)
Mar  8 14:45:05.286: INFO: (5) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.908746ms)
Mar  8 14:45:05.289: INFO: (6) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.64204ms)
Mar  8 14:45:05.292: INFO: (7) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.558797ms)
Mar  8 14:45:05.295: INFO: (8) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.336426ms)
Mar  8 14:45:05.297: INFO: (9) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.145501ms)
Mar  8 14:45:05.299: INFO: (10) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.035886ms)
Mar  8 14:45:05.301: INFO: (11) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.906045ms)
Mar  8 14:45:05.303: INFO: (12) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.920259ms)
Mar  8 14:45:05.305: INFO: (13) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.98062ms)
Mar  8 14:45:05.307: INFO: (14) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.888666ms)
Mar  8 14:45:05.309: INFO: (15) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.838423ms)
Mar  8 14:45:05.311: INFO: (16) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.005029ms)
Mar  8 14:45:05.313: INFO: (17) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.917993ms)
Mar  8 14:45:05.315: INFO: (18) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.894068ms)
Mar  8 14:45:05.317: INFO: (19) /api/v1/nodes/ip-192-168-64-146.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.911964ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:45:05.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-rj85n" for this suite.
Mar  8 14:45:11.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:45:12.023: INFO: namespace: e2e-tests-proxy-rj85n, resource: bindings, ignored listing per whitelist
Mar  8 14:45:12.032: INFO: namespace e2e-tests-proxy-rj85n deletion completed in 6.71304067s

â€¢ [SLOW TEST:7.290 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:45:12.033: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:45:12.061113      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-w5kwm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  8 14:45:14.707: INFO: Successfully updated pod "labelsupdatec718adce-41b0-11e9-be5e-ae5bc1be57d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:45:18.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w5kwm" for this suite.
Mar  8 14:45:40.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:45:40.735: INFO: namespace: e2e-tests-downward-api-w5kwm, resource: bindings, ignored listing per whitelist
Mar  8 14:45:40.779: INFO: namespace e2e-tests-downward-api-w5kwm deletion completed in 22.057551006s

â€¢ [SLOW TEST:28.747 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:45:40.779: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:45:40.807836      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-985zd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  8 14:45:40.930: INFO: Creating ReplicaSet my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7
Mar  8 14:45:40.937: INFO: Pod name my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7: Found 0 pods out of 1
Mar  8 14:45:45.940: INFO: Pod name my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7: Found 1 pods out of 1
Mar  8 14:45:45.940: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7" is running
Mar  8 14:45:45.941: INFO: Pod "my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7-h4g5v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:45:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:45:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:45:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:45:40 +0000 UTC Reason: Message:}])
Mar  8 14:45:45.941: INFO: Trying to dial the pod
Mar  8 14:45:50.949: INFO: Controller my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7: Got expected result from replica 1 [my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7-h4g5v]: "my-hostname-basic-d83b0b7e-41b0-11e9-be5e-ae5bc1be57d7-h4g5v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:45:50.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-985zd" for this suite.
Mar  8 14:45:56.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:45:57.011: INFO: namespace: e2e-tests-replicaset-985zd, resource: bindings, ignored listing per whitelist
Mar  8 14:45:57.014: INFO: namespace e2e-tests-replicaset-985zd deletion completed in 6.062049015s

â€¢ [SLOW TEST:16.234 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:45:57.014: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:45:57.050853      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ccnpb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e1e997da-41b0-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume configMaps
Mar  8 14:45:57.183: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-ccnpb" to be "success or failure"
Mar  8 14:45:57.187: INFO: Pod "pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.809309ms
Mar  8 14:45:59.189: INFO: Pod "pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006001447s
STEP: Saw pod success
Mar  8 14:45:59.189: INFO: Pod "pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:45:59.191: INFO: Trying to get logs from node ip-192-168-64-146.us-west-2.compute.internal pod pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  8 14:45:59.201: INFO: Waiting for pod pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:45:59.495: INFO: Pod pod-projected-configmaps-e1e9edbe-41b0-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:45:59.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ccnpb" for this suite.
Mar  8 14:46:05.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:46:05.524: INFO: namespace: e2e-tests-projected-ccnpb, resource: bindings, ignored listing per whitelist
Mar  8 14:46:05.560: INFO: namespace e2e-tests-projected-ccnpb deletion completed in 6.061636119s

â€¢ [SLOW TEST:8.546 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:46:05.560: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:46:05.588527      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-46f2d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-46f2d
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  8 14:46:05.710: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  8 14:46:27.778: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.20.64.14 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-46f2d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:46:27.778: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:46:28.856: INFO: Found all expected endpoints: [netserver-0]
Mar  8 14:46:29.203: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.20.192.7 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-46f2d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:46:29.203: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:46:30.290: INFO: Found all expected endpoints: [netserver-1]
Mar  8 14:46:30.291: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.20.160.3 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-46f2d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:46:30.292: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:46:31.365: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:46:31.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-46f2d" for this suite.
Mar  8 14:46:53.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:46:53.416: INFO: namespace: e2e-tests-pod-network-test-46f2d, resource: bindings, ignored listing per whitelist
Mar  8 14:46:53.435: INFO: namespace e2e-tests-pod-network-test-46f2d deletion completed in 22.067491214s

â€¢ [SLOW TEST:47.875 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:46:53.436: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:46:53.462780      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5lmqt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar  8 14:46:53.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 cluster-info'
Mar  8 14:46:54.079: INFO: stderr: ""
Mar  8 14:46:54.079: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:46:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5lmqt" for this suite.
Mar  8 14:47:00.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:47:00.121: INFO: namespace: e2e-tests-kubectl-5lmqt, resource: bindings, ignored listing per whitelist
Mar  8 14:47:00.140: INFO: namespace e2e-tests-kubectl-5lmqt deletion completed in 6.057786336s

â€¢ [SLOW TEST:6.704 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:47:00.140: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:47:00.168642      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-29nxq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-0789a469-41b1-11e9-be5e-ae5bc1be57d7
STEP: Creating secret with name secret-projected-all-test-volume-0789a44d-41b1-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  8 14:47:00.384: INFO: Waiting up to 5m0s for pod "projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-29nxq" to be "success or failure"
Mar  8 14:47:00.396: INFO: Pod "projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.924968ms
Mar  8 14:47:02.399: INFO: Pod "projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014874341s
STEP: Saw pod success
Mar  8 14:47:02.399: INFO: Pod "projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:47:02.401: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  8 14:47:02.411: INFO: Waiting for pod projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:47:02.413: INFO: Pod projected-volume-0789a411-41b1-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:47:02.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-29nxq" for this suite.
Mar  8 14:47:08.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:47:08.440: INFO: namespace: e2e-tests-projected-29nxq, resource: bindings, ignored listing per whitelist
Mar  8 14:47:08.478: INFO: namespace e2e-tests-projected-29nxq deletion completed in 6.061503454s

â€¢ [SLOW TEST:8.338 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:47:08.478: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:47:08.506636      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v95b9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  8 14:47:08.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-v95b9'
Mar  8 14:47:08.711: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  8 14:47:08.711: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Mar  8 14:47:12.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643275360 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-v95b9'
Mar  8 14:47:12.808: INFO: stderr: ""
Mar  8 14:47:12.808: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:47:12.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v95b9" for this suite.
Mar  8 14:47:18.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:47:18.866: INFO: namespace: e2e-tests-kubectl-v95b9, resource: bindings, ignored listing per whitelist
Mar  8 14:47:18.889: INFO: namespace e2e-tests-kubectl-v95b9 deletion completed in 6.076943224s

â€¢ [SLOW TEST:10.411 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:47:18.889: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:47:18.931440      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-qq9v4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  8 14:47:24.074: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:47:25.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-qq9v4" for this suite.
Mar  8 14:47:47.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:47:47.157: INFO: namespace: e2e-tests-replicaset-qq9v4, resource: bindings, ignored listing per whitelist
Mar  8 14:47:47.180: INFO: namespace e2e-tests-replicaset-qq9v4 deletion completed in 22.093797247s

â€¢ [SLOW TEST:28.291 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:47:47.180: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:47:47.207217      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-s5fbr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:47:47.471: INFO: Waiting up to 5m0s for pod "downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-downward-api-s5fbr" to be "success or failure"
Mar  8 14:47:47.473: INFO: Pod "downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016266ms
Mar  8 14:47:49.477: INFO: Pod "downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005247832s
STEP: Saw pod success
Mar  8 14:47:49.477: INFO: Pod "downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:47:49.482: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:47:49.494: INFO: Waiting for pod downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:47:49.499: INFO: Pod downwardapi-volume-239205db-41b1-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:47:49.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-s5fbr" for this suite.
Mar  8 14:47:55.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:47:55.613: INFO: namespace: e2e-tests-downward-api-s5fbr, resource: bindings, ignored listing per whitelist
Mar  8 14:47:55.656: INFO: namespace e2e-tests-downward-api-s5fbr deletion completed in 6.154283412s

â€¢ [SLOW TEST:8.476 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:47:55.656: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:47:55.687950      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-tpxtr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-tpxtr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  8 14:47:55.818: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  8 14:48:09.913: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=http&host=10.20.160.3&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-tpxtr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:48:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:48:10.008: INFO: Waiting for endpoints: map[]
Mar  8 14:48:10.010: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=http&host=10.20.64.14&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-tpxtr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:48:10.010: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:48:10.121: INFO: Waiting for endpoints: map[]
Mar  8 14:48:10.163: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=http&host=10.20.192.7&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-tpxtr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:48:10.163: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:48:10.266: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:48:10.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-tpxtr" for this suite.
Mar  8 14:48:32.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:48:32.316: INFO: namespace: e2e-tests-pod-network-test-tpxtr, resource: bindings, ignored listing per whitelist
Mar  8 14:48:32.328: INFO: namespace e2e-tests-pod-network-test-tpxtr deletion completed in 22.057166337s

â€¢ [SLOW TEST:36.672 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:48:32.328: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:48:32.356267      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-8hkmq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:48:36.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-8hkmq" for this suite.
Mar  8 14:48:42.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:48:42.543: INFO: namespace: e2e-tests-kubelet-test-8hkmq, resource: bindings, ignored listing per whitelist
Mar  8 14:48:42.549: INFO: namespace e2e-tests-kubelet-test-8hkmq deletion completed in 6.05757893s

â€¢ [SLOW TEST:10.220 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:48:42.549: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:48:42.578893      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-46mbt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-449342bc-41b1-11e9-be5e-ae5bc1be57d7
STEP: Creating a pod to test consume secrets
Mar  8 14:48:42.709: INFO: Waiting up to 5m0s for pod "pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-secrets-46mbt" to be "success or failure"
Mar  8 14:48:42.713: INFO: Pod "pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466659ms
Mar  8 14:48:44.715: INFO: Pod "pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005591388s
STEP: Saw pod success
Mar  8 14:48:44.715: INFO: Pod "pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:48:44.717: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  8 14:48:44.730: INFO: Waiting for pod pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:48:44.731: INFO: Pod pod-secrets-44939f16-41b1-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:48:44.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-46mbt" for this suite.
Mar  8 14:48:50.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:48:50.774: INFO: namespace: e2e-tests-secrets-46mbt, resource: bindings, ignored listing per whitelist
Mar  8 14:48:50.796: INFO: namespace e2e-tests-secrets-46mbt deletion completed in 6.061481864s

â€¢ [SLOW TEST:8.247 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:48:50.796: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:48:50.824186      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-ppkrm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  8 14:48:50.949: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28639,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  8 14:48:50.949: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28639,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  8 14:49:00.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28661,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  8 14:49:00.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28661,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  8 14:49:10.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28683,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  8 14:49:10.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28683,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  8 14:49:20.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28705,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  8 14:49:20.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-a,UID:497d48b1-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28705,Generation:0,CreationTimestamp:2019-03-08 14:48:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  8 14:49:30.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-b,UID:61574190-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28727,Generation:0,CreationTimestamp:2019-03-08 14:49:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  8 14:49:30.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-b,UID:61574190-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28727,Generation:0,CreationTimestamp:2019-03-08 14:49:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  8 14:49:40.970: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-b,UID:61574190-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28749,Generation:0,CreationTimestamp:2019-03-08 14:49:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  8 14:49:40.970: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ppkrm,SelfLink:/api/v1/namespaces/e2e-tests-watch-ppkrm/configmaps/e2e-watch-test-configmap-b,UID:61574190-41b1-11e9-8665-02fd46f865f2,ResourceVersion:28749,Generation:0,CreationTimestamp:2019-03-08 14:49:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:49:50.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-ppkrm" for this suite.
Mar  8 14:49:56.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:49:56.991: INFO: namespace: e2e-tests-watch-ppkrm, resource: bindings, ignored listing per whitelist
Mar  8 14:49:57.038: INFO: namespace e2e-tests-watch-ppkrm deletion completed in 6.06441143s

â€¢ [SLOW TEST:66.242 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:49:57.038: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:49:57.065286      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-pxlx2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7
Mar  8 14:49:57.193: INFO: Pod name my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7: Found 0 pods out of 1
Mar  8 14:50:02.196: INFO: Pod name my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7: Found 1 pods out of 1
Mar  8 14:50:02.196: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7" are running
Mar  8 14:50:02.197: INFO: Pod "my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7-m8rsf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:49:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:49:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:49:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-08 14:49:57 +0000 UTC Reason: Message:}])
Mar  8 14:50:02.197: INFO: Trying to dial the pod
Mar  8 14:50:07.204: INFO: Controller my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7: Got expected result from replica 1 [my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7-m8rsf]: "my-hostname-basic-70f8d2f6-41b1-11e9-be5e-ae5bc1be57d7-m8rsf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:50:07.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-pxlx2" for this suite.
Mar  8 14:50:13.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:50:13.258: INFO: namespace: e2e-tests-replication-controller-pxlx2, resource: bindings, ignored listing per whitelist
Mar  8 14:50:13.272: INFO: namespace e2e-tests-replication-controller-pxlx2 deletion completed in 6.064694565s

â€¢ [SLOW TEST:16.234 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:50:13.272: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:50:13.302309      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-f5lmv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  8 14:50:15.956: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7"
Mar  8 14:50:15.956: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-pods-f5lmv" to be "terminated due to deadline exceeded"
Mar  8 14:50:15.958: INFO: Pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7": Phase="Running", Reason="", readiness=true. Elapsed: 1.324887ms
Mar  8 14:50:17.962: INFO: Pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005685285s
Mar  8 14:50:19.964: INFO: Pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00769099s
Mar  8 14:50:19.964: INFO: Pod "pod-update-activedeadlineseconds-7aa8646b-41b1-11e9-be5e-ae5bc1be57d7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:50:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-f5lmv" for this suite.
Mar  8 14:50:25.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:50:25.996: INFO: namespace: e2e-tests-pods-f5lmv, resource: bindings, ignored listing per whitelist
Mar  8 14:50:26.024: INFO: namespace e2e-tests-pods-f5lmv deletion completed in 6.057707606s

â€¢ [SLOW TEST:12.753 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:50:26.024: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:50:26.052816      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-5kq6m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  8 14:50:28.193: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-8240240b-41b1-11e9-be5e-ae5bc1be57d7", GenerateName:"", Namespace:"e2e-tests-pods-5kq6m", SelfLink:"/api/v1/namespaces/e2e-tests-pods-5kq6m/pods/pod-submit-remove-8240240b-41b1-11e9-be5e-ae5bc1be57d7", UID:"8240a800-41b1-11e9-8665-02fd46f865f2", ResourceVersion:"28934", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687653426, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"176928621"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sz4rf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0008ff400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sz4rf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001fe93c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-192-168-64-146.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00209b7a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fe9400)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fe9420)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001fe9428), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001fe942c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687653426, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687653427, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687653427, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687653426, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.64.146", PodIP:"10.20.192.7", StartTime:(*v1.Time)(0xc000f7bfe0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001454000), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d", ContainerID:"containerd://16e573e96ddce0a548d75b6c786e9c641149bc5512e7e98bfb5fb5e0a44434d4"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:50:40.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5kq6m" for this suite.
Mar  8 14:50:46.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:50:46.531: INFO: namespace: e2e-tests-pods-5kq6m, resource: bindings, ignored listing per whitelist
Mar  8 14:50:46.535: INFO: namespace e2e-tests-pods-5kq6m deletion completed in 6.086820687s

â€¢ [SLOW TEST:20.510 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:50:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:50:46.562132      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-rzghj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rzghj
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  8 14:50:46.683: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  8 14:51:06.745: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=udp&host=10.20.160.3&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rzghj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:51:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:51:06.841: INFO: Waiting for endpoints: map[]
Mar  8 14:51:06.842: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=udp&host=10.20.64.14&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rzghj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:51:06.842: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:51:06.923: INFO: Waiting for endpoints: map[]
Mar  8 14:51:06.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.160.4:8080/dial?request=hostName&protocol=udp&host=10.20.192.7&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-rzghj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  8 14:51:06.925: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
Mar  8 14:51:07.027: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:51:07.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rzghj" for this suite.
Mar  8 14:51:29.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:51:29.621: INFO: namespace: e2e-tests-pod-network-test-rzghj, resource: bindings, ignored listing per whitelist
Mar  8 14:51:29.630: INFO: namespace e2e-tests-pod-network-test-rzghj deletion completed in 22.599488762s

â€¢ [SLOW TEST:43.095 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  8 14:51:29.630: INFO: >>> kubeConfig: /tmp/kubeconfig-643275360
E0308 14:51:29.657798      15 memcache.go:135] couldn't get resource list for custom.metrics.k8s.io/v1beta1: <nil>
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s6h79
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  8 14:51:29.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7" in namespace "e2e-tests-projected-s6h79" to be "success or failure"
Mar  8 14:51:29.788: INFO: Pod "downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879036ms
Mar  8 14:51:31.791: INFO: Pod "downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004070725s
STEP: Saw pod success
Mar  8 14:51:31.791: INFO: Pod "downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7" satisfied condition "success or failure"
Mar  8 14:51:31.792: INFO: Trying to get logs from node ip-192-168-72-251.us-west-2.compute.internal pod downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7 container client-container: <nil>
STEP: delete the pod
Mar  8 14:51:31.803: INFO: Waiting for pod downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7 to disappear
Mar  8 14:51:31.806: INFO: Pod downwardapi-volume-a8297db0-41b1-11e9-be5e-ae5bc1be57d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  8 14:51:31.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s6h79" for this suite.
Mar  8 14:51:37.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  8 14:51:37.826: INFO: namespace: e2e-tests-projected-s6h79, resource: bindings, ignored listing per whitelist
Mar  8 14:51:37.870: INFO: namespace e2e-tests-projected-s6h79 deletion completed in 6.059949384s

â€¢ [SLOW TEST:8.239 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSMar  8 14:51:37.870: INFO: Running AfterSuite actions on all nodes
Mar  8 14:51:37.870: INFO: Running AfterSuite actions on node 1
Mar  8 14:51:37.870: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6602.101 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h50m2.799360533s
Test Suite Passed
