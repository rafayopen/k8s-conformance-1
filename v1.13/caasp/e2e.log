I0306 15:12:01.773647      18 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-889989848
I0306 15:12:01.773752      18 e2e.go:224] Starting e2e run "315bbb91-4022-11e9-9071-0a58ac100007" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551885121 - Will randomize all specs
Will run 201 of 1946 specs

Mar  6 15:12:01.874: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:12:01.876: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  6 15:12:01.904: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  6 15:12:01.929: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  6 15:12:01.929: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Mar  6 15:12:01.929: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  6 15:12:01.935: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
Mar  6 15:12:01.935: INFO: e2e test version: v1.13.0
Mar  6 15:12:01.936: INFO: kube-apiserver version: v1.13.2
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:12:01.936: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
Mar  6 15:12:02.005: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  6 15:12:02.012: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-jpwkp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:12:02.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-jpwkp" to be "success or failure"
Mar  6 15:12:02.129: INFO: Pod "downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065657ms
Mar  6 15:12:04.132: INFO: Pod "downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004799086s
Mar  6 15:12:06.135: INFO: Pod "downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007590428s
STEP: Saw pod success
Mar  6 15:12:06.135: INFO: Pod "downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:12:06.137: INFO: Trying to get logs from node themisto pod downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:12:06.194: INFO: Waiting for pod downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007 to disappear
Mar  6 15:12:06.195: INFO: Pod downwardapi-volume-31dd9559-4022-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:12:06.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jpwkp" for this suite.
Mar  6 15:12:12.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:12:12.219: INFO: namespace: e2e-tests-downward-api-jpwkp, resource: bindings, ignored listing per whitelist
Mar  6 15:12:12.278: INFO: namespace e2e-tests-downward-api-jpwkp deletion completed in 6.079185329s

• [SLOW TEST:10.342 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:12:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2nmzm
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  6 15:12:12.460: INFO: Waiting up to 5m0s for pod "pod-38069bf1-4022-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-2nmzm" to be "success or failure"
Mar  6 15:12:12.462: INFO: Pod "pod-38069bf1-4022-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526169ms
Mar  6 15:12:14.464: INFO: Pod "pod-38069bf1-4022-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00433202s
STEP: Saw pod success
Mar  6 15:12:14.465: INFO: Pod "pod-38069bf1-4022-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:12:14.467: INFO: Trying to get logs from node kronos pod pod-38069bf1-4022-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:12:14.487: INFO: Waiting for pod pod-38069bf1-4022-11e9-9071-0a58ac100007 to disappear
Mar  6 15:12:14.491: INFO: Pod pod-38069bf1-4022-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:12:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2nmzm" for this suite.
Mar  6 15:12:20.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:12:20.538: INFO: namespace: e2e-tests-emptydir-2nmzm, resource: bindings, ignored listing per whitelist
Mar  6 15:12:20.570: INFO: namespace e2e-tests-emptydir-2nmzm deletion completed in 6.075972319s

• [SLOW TEST:8.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:12:20.571: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-gxt8g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007
Mar  6 15:12:20.752: INFO: Pod name my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007: Found 0 pods out of 1
Mar  6 15:12:25.755: INFO: Pod name my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007: Found 1 pods out of 1
Mar  6 15:12:25.755: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007" are running
Mar  6 15:12:25.757: INFO: Pod "my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007-lbd44" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:12:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:12:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:12:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:12:15 +0000 UTC Reason: Message:}])
Mar  6 15:12:25.757: INFO: Trying to dial the pod
Mar  6 15:12:30.766: INFO: Controller my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007: Got expected result from replica 1 [my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007-lbd44]: "my-hostname-basic-3cf7c88f-4022-11e9-9071-0a58ac100007-lbd44", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:12:30.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-gxt8g" for this suite.
Mar  6 15:12:36.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:12:36.811: INFO: namespace: e2e-tests-replication-controller-gxt8g, resource: bindings, ignored listing per whitelist
Mar  6 15:12:36.847: INFO: namespace e2e-tests-replication-controller-gxt8g deletion completed in 6.077178204s

• [SLOW TEST:16.277 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:12:36.847: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jpskk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 15:12:39.557: INFO: Successfully updated pod "annotationupdate46abb085-4022-11e9-9071-0a58ac100007"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:12:43.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jpskk" for this suite.
Mar  6 15:13:05.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:13:05.622: INFO: namespace: e2e-tests-projected-jpskk, resource: bindings, ignored listing per whitelist
Mar  6 15:13:05.658: INFO: namespace e2e-tests-projected-jpskk deletion completed in 22.074088662s

• [SLOW TEST:28.811 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:13:05.658: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-25t9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-t7qw
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 15:13:05.850: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t7qw" in namespace "e2e-tests-subpath-25t9t" to be "success or failure"
Mar  6 15:13:05.852: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486277ms
Mar  6 15:13:07.855: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004351208s
Mar  6 15:13:09.858: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 4.00708595s
Mar  6 15:13:11.861: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 6.010583831s
Mar  6 15:13:13.864: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 8.013296053s
Mar  6 15:13:15.869: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 10.018141969s
Mar  6 15:13:17.872: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 12.021205945s
Mar  6 15:13:19.875: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 14.024006016s
Mar  6 15:13:21.878: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 16.027034044s
Mar  6 15:13:23.880: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 18.029931814s
Mar  6 15:13:25.883: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Running", Reason="", readiness=false. Elapsed: 20.032798477s
Mar  6 15:13:27.886: INFO: Pod "pod-subpath-test-projected-t7qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.035601743s
STEP: Saw pod success
Mar  6 15:13:27.886: INFO: Pod "pod-subpath-test-projected-t7qw" satisfied condition "success or failure"
Mar  6 15:13:27.889: INFO: Trying to get logs from node sponde pod pod-subpath-test-projected-t7qw container test-container-subpath-projected-t7qw: <nil>
STEP: delete the pod
Mar  6 15:13:27.908: INFO: Waiting for pod pod-subpath-test-projected-t7qw to disappear
Mar  6 15:13:27.912: INFO: Pod pod-subpath-test-projected-t7qw no longer exists
STEP: Deleting pod pod-subpath-test-projected-t7qw
Mar  6 15:13:27.912: INFO: Deleting pod "pod-subpath-test-projected-t7qw" in namespace "e2e-tests-subpath-25t9t"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:13:27.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-25t9t" for this suite.
Mar  6 15:13:33.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:13:33.961: INFO: namespace: e2e-tests-subpath-25t9t, resource: bindings, ignored listing per whitelist
Mar  6 15:13:33.985: INFO: namespace e2e-tests-subpath-25t9t deletion completed in 6.068923547s

• [SLOW TEST:28.328 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:13:33.986: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-jtlv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-jtlv6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-jtlv6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 15:13:54.198: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.201: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.204: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.206: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.209: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.226: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.229: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.231: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.234: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.237: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:54.237: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  6 15:13:59.254: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.256: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.259: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.262: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.265: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.280: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.282: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.285: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.287: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.290: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:13:59.290: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  6 15:14:04.256: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.259: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.262: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.264: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.267: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.294: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:04.294: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-jtlv6.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar  6 15:14:09.255: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:09.267: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:09.293: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:09.293: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar  6 15:14:14.255: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:14.266: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:14.292: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:19.256: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:19.266: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:19.292: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:24.255: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:24.266: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:24.293: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:29.256: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:29.268: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:29.297: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:34.256: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:34.267: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:34.296: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:39.255: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:39.265: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-68bad224-4022-11e9-9071-0a58ac100007)
Mar  6 15:14:39.292: INFO: Lookups using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_tcp@PodARecord]

Mar  6 15:14:44.292: INFO: DNS probes using e2e-tests-dns-jtlv6/dns-test-68bad224-4022-11e9-9071-0a58ac100007 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:14:44.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-jtlv6" for this suite.
Mar  6 15:14:50.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:14:50.354: INFO: namespace: e2e-tests-dns-jtlv6, resource: bindings, ignored listing per whitelist
Mar  6 15:14:50.392: INFO: namespace e2e-tests-dns-jtlv6 deletion completed in 6.08285315s

• [SLOW TEST:76.406 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:14:50.392: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-l9qd6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:14:50.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-l9qd6" for this suite.
Mar  6 15:14:56.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:14:56.629: INFO: namespace: e2e-tests-kubelet-test-l9qd6, resource: bindings, ignored listing per whitelist
Mar  6 15:14:56.664: INFO: namespace e2e-tests-kubelet-test-l9qd6 deletion completed in 6.071548387s

• [SLOW TEST:6.272 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:14:56.664: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-chfhk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 15:15:00.876: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:00.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:02.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:02.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:04.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:04.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:06.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:06.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:08.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:08.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:10.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:10.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:12.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:12.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:14.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:14.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:16.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:16.881: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:18.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:18.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:20.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:20.882: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:22.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:22.881: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 15:15:24.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 15:15:24.882: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:15:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-chfhk" for this suite.
Mar  6 15:15:46.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:15:46.956: INFO: namespace: e2e-tests-container-lifecycle-hook-chfhk, resource: bindings, ignored listing per whitelist
Mar  6 15:15:46.966: INFO: namespace e2e-tests-container-lifecycle-hook-chfhk deletion completed in 22.072871998s

• [SLOW TEST:50.302 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:15:46.967: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-lr5dn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 15:15:49.667: INFO: Successfully updated pod "pod-update-b7fdbf34-4022-11e9-9071-0a58ac100007"
STEP: verifying the updated pod is in kubernetes
Mar  6 15:15:49.671: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:15:49.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lr5dn" for this suite.
Mar  6 15:16:11.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:16:11.738: INFO: namespace: e2e-tests-pods-lr5dn, resource: bindings, ignored listing per whitelist
Mar  6 15:16:11.746: INFO: namespace e2e-tests-pods-lr5dn deletion completed in 22.071728117s

• [SLOW TEST:24.779 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:16:11.746: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-zvb4p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-zvb4p
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar  6 15:16:11.933: INFO: Found 0 stateful pods, waiting for 3
Mar  6 15:16:21.937: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:16:21.937: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:16:21.937: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:16:21.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-zvb4p ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:16:22.231: INFO: stderr: ""
Mar  6 15:16:22.231: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:16:22.231: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  6 15:16:32.259: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  6 15:16:42.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-zvb4p ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:16:42.481: INFO: stderr: ""
Mar  6 15:16:42.481: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:16:42.481: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Mar  6 15:17:02.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-zvb4p ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:17:02.706: INFO: stderr: ""
Mar  6 15:17:02.706: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:17:02.706: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 15:17:12.735: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  6 15:17:22.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-zvb4p ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:17:22.977: INFO: stderr: ""
Mar  6 15:17:22.977: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:17:22.977: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 15:17:42.993: INFO: Waiting for StatefulSet e2e-tests-statefulset-zvb4p/ss2 to complete update
Mar  6 15:17:42.993: INFO: Waiting for Pod e2e-tests-statefulset-zvb4p/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 15:17:52.998: INFO: Deleting all statefulset in ns e2e-tests-statefulset-zvb4p
Mar  6 15:17:53.000: INFO: Scaling statefulset ss2 to 0
Mar  6 15:18:23.012: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:18:23.014: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:18:23.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-zvb4p" for this suite.
Mar  6 15:18:29.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:18:29.061: INFO: namespace: e2e-tests-statefulset-zvb4p, resource: bindings, ignored listing per whitelist
Mar  6 15:18:29.104: INFO: namespace e2e-tests-statefulset-zvb4p deletion completed in 6.07440277s

• [SLOW TEST:137.358 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:18:29.104: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wrgcf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 15:18:31.807: INFO: Successfully updated pod "annotationupdate18a15423-4023-11e9-9071-0a58ac100007"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:18:33.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wrgcf" for this suite.
Mar  6 15:18:55.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:18:55.904: INFO: namespace: e2e-tests-downward-api-wrgcf, resource: bindings, ignored listing per whitelist
Mar  6 15:18:55.913: INFO: namespace e2e-tests-downward-api-wrgcf deletion completed in 22.087751852s

• [SLOW TEST:26.809 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:18:55.913: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-f4qw6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-f4qw6;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-f4qw6;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-f4qw6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 3.227.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.227.3_udp@PTR;check="$$(dig +tcp +noall +answer +search 3.227.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.227.3_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-f4qw6;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-f4qw6.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-f4qw6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 3.227.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.227.3_udp@PTR;check="$$(dig +tcp +noall +answer +search 3.227.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.227.3_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 15:19:20.131: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.135: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.164: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.167: INFO: Unable to read 172.24.227.3_udp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.170: INFO: Unable to read 172.24.227.3_tcp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.173: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.175: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.178: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-f4qw6 from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.181: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6 from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.184: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.187: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.192: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.194: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.197: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.200: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.202: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.204: INFO: Unable to read 172.24.227.3_udp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.207: INFO: Unable to read 172.24.227.3_tcp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:20.207: INFO: Lookups using e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@PodARecord 172.24.227.3_udp@PTR 172.24.227.3_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-f4qw6 jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6 jessie_udp@dns-test-service.e2e-tests-dns-f4qw6.svc jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc jessie_udp@PodARecord jessie_tcp@PodARecord 172.24.227.3_udp@PTR 172.24.227.3_tcp@PTR]

Mar  6 15:19:25.211: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.214: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.244: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.246: INFO: Unable to read 172.24.227.3_udp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.249: INFO: Unable to read 172.24.227.3_tcp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.252: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.254: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.257: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-f4qw6 from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.260: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6 from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.263: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.266: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.268: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.271: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.274: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.277: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.280: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.283: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.286: INFO: Unable to read 172.24.227.3_udp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.288: INFO: Unable to read 172.24.227.3_tcp@PTR from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:25.288: INFO: Lookups using e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@PodARecord 172.24.227.3_udp@PTR 172.24.227.3_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-f4qw6 jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6 jessie_udp@dns-test-service.e2e-tests-dns-f4qw6.svc jessie_tcp@dns-test-service.e2e-tests-dns-f4qw6.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-f4qw6.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-f4qw6.svc jessie_udp@PodARecord jessie_tcp@PodARecord 172.24.227.3_udp@PTR 172.24.227.3_tcp@PTR]

Mar  6 15:19:30.246: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:30.249: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:30.284: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:30.289: INFO: Lookups using e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_tcp@PodARecord]

Mar  6 15:19:35.253: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:35.256: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:35.286: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007: the server could not find the requested resource (get pods dns-test-289f8d3b-4023-11e9-9071-0a58ac100007)
Mar  6 15:19:35.291: INFO: Lookups using e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_tcp@PodARecord]

Mar  6 15:19:40.285: INFO: DNS probes using e2e-tests-dns-f4qw6/dns-test-289f8d3b-4023-11e9-9071-0a58ac100007 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:19:40.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-f4qw6" for this suite.
Mar  6 15:19:46.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:19:46.397: INFO: namespace: e2e-tests-dns-f4qw6, resource: bindings, ignored listing per whitelist
Mar  6 15:19:46.402: INFO: namespace e2e-tests-dns-f4qw6 deletion completed in 6.073161483s

• [SLOW TEST:50.489 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:19:46.402: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-25szl
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-46b5191e-4023-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:19:52.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-25szl" for this suite.
Mar  6 15:20:14.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:20:14.672: INFO: namespace: e2e-tests-configmap-25szl, resource: bindings, ignored listing per whitelist
Mar  6 15:20:14.692: INFO: namespace e2e-tests-configmap-25szl deletion completed in 22.076032679s

• [SLOW TEST:28.290 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:20:14.692: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bsxg6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-xrvt
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 15:20:14.885: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xrvt" in namespace "e2e-tests-subpath-bsxg6" to be "success or failure"
Mar  6 15:20:14.887: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495183ms
Mar  6 15:20:16.889: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003951781s
Mar  6 15:20:18.892: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 4.006604415s
Mar  6 15:20:20.895: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 6.009291553s
Mar  6 15:20:22.898: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 8.012350649s
Mar  6 15:20:24.901: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 10.015380973s
Mar  6 15:20:26.905: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 12.019042478s
Mar  6 15:20:28.908: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 14.022353013s
Mar  6 15:20:30.911: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 16.025511884s
Mar  6 15:20:32.914: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 18.028354575s
Mar  6 15:20:34.917: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Running", Reason="", readiness=false. Elapsed: 20.031382277s
Mar  6 15:20:36.920: INFO: Pod "pod-subpath-test-configmap-xrvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034280864s
STEP: Saw pod success
Mar  6 15:20:36.920: INFO: Pod "pod-subpath-test-configmap-xrvt" satisfied condition "success or failure"
Mar  6 15:20:36.922: INFO: Trying to get logs from node kronos pod pod-subpath-test-configmap-xrvt container test-container-subpath-configmap-xrvt: <nil>
STEP: delete the pod
Mar  6 15:20:36.942: INFO: Waiting for pod pod-subpath-test-configmap-xrvt to disappear
Mar  6 15:20:36.943: INFO: Pod pod-subpath-test-configmap-xrvt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xrvt
Mar  6 15:20:36.943: INFO: Deleting pod "pod-subpath-test-configmap-xrvt" in namespace "e2e-tests-subpath-bsxg6"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:20:36.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bsxg6" for this suite.
Mar  6 15:20:42.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:20:43.018: INFO: namespace: e2e-tests-subpath-bsxg6, resource: bindings, ignored listing per whitelist
Mar  6 15:20:43.029: INFO: namespace e2e-tests-subpath-bsxg6 deletion completed in 6.080459455s

• [SLOW TEST:28.337 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:20:43.029: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2226r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 15:20:45.737: INFO: Successfully updated pod "labelsupdate68756b98-4023-11e9-9071-0a58ac100007"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:20:49.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2226r" for this suite.
Mar  6 15:21:11.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:21:11.814: INFO: namespace: e2e-tests-projected-2226r, resource: bindings, ignored listing per whitelist
Mar  6 15:21:11.829: INFO: namespace e2e-tests-projected-2226r deletion completed in 22.068579008s

• [SLOW TEST:28.800 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:21:11.829: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-8grb9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar  6 15:21:12.549: INFO: created pod pod-service-account-defaultsa
Mar  6 15:21:12.549: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  6 15:21:12.557: INFO: created pod pod-service-account-mountsa
Mar  6 15:21:12.557: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  6 15:21:12.562: INFO: created pod pod-service-account-nomountsa
Mar  6 15:21:12.562: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  6 15:21:12.568: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  6 15:21:12.568: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  6 15:21:12.575: INFO: created pod pod-service-account-mountsa-mountspec
Mar  6 15:21:12.575: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  6 15:21:12.580: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  6 15:21:12.580: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  6 15:21:12.585: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  6 15:21:12.585: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  6 15:21:12.590: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  6 15:21:12.590: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  6 15:21:12.598: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  6 15:21:12.598: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:21:12.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-8grb9" for this suite.
Mar  6 15:21:34.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:21:34.664: INFO: namespace: e2e-tests-svcaccounts-8grb9, resource: bindings, ignored listing per whitelist
Mar  6 15:21:34.679: INFO: namespace e2e-tests-svcaccounts-8grb9 deletion completed in 22.076244659s

• [SLOW TEST:22.850 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:21:34.679: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lkv8l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:21:34.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 version'
Mar  6 15:21:34.931: INFO: stderr: ""
Mar  6 15:21:34.931: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-29T12:00:00Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:21:34.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lkv8l" for this suite.
Mar  6 15:21:40.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:21:40.952: INFO: namespace: e2e-tests-kubectl-lkv8l, resource: bindings, ignored listing per whitelist
Mar  6 15:21:41.006: INFO: namespace e2e-tests-kubectl-lkv8l deletion completed in 6.07173866s

• [SLOW TEST:6.327 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:21:41.006: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x4d85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Mar  6 15:21:41.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:41.447: INFO: stderr: ""
Mar  6 15:21:41.447: INFO: stdout: "pod/pause created\n"
Mar  6 15:21:41.447: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  6 15:21:41.447: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-x4d85" to be "running and ready"
Mar  6 15:21:41.449: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.944226ms
Mar  6 15:21:43.452: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004741165s
Mar  6 15:21:43.452: INFO: Pod "pause" satisfied condition "running and ready"
Mar  6 15:21:43.452: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  6 15:21:43.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.549: INFO: stderr: ""
Mar  6 15:21:43.549: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  6 15:21:43.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pod pause -L testing-label --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.624: INFO: stderr: ""
Mar  6 15:21:43.624: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  6 15:21:43.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 label pods pause testing-label- --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.701: INFO: stderr: ""
Mar  6 15:21:43.701: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  6 15:21:43.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pod pause -L testing-label --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.770: INFO: stderr: ""
Mar  6 15:21:43.770: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Mar  6 15:21:43.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.847: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:21:43.847: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  6 15:21:43.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-x4d85'
Mar  6 15:21:43.917: INFO: stderr: "No resources found.\n"
Mar  6 15:21:43.917: INFO: stdout: ""
Mar  6 15:21:43.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -l name=pause --namespace=e2e-tests-kubectl-x4d85 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 15:21:43.993: INFO: stderr: ""
Mar  6 15:21:43.993: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:21:43.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x4d85" for this suite.
Mar  6 15:21:50.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:21:50.048: INFO: namespace: e2e-tests-kubectl-x4d85, resource: bindings, ignored listing per whitelist
Mar  6 15:21:50.066: INFO: namespace e2e-tests-kubectl-x4d85 deletion completed in 6.069857721s

• [SLOW TEST:9.060 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:21:50.066: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2jghx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 15:21:50.263: INFO: Waiting up to 5m0s for pod "pod-906c4811-4023-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-2jghx" to be "success or failure"
Mar  6 15:21:50.265: INFO: Pod "pod-906c4811-4023-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.817356ms
Mar  6 15:21:52.267: INFO: Pod "pod-906c4811-4023-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004324277s
STEP: Saw pod success
Mar  6 15:21:52.267: INFO: Pod "pod-906c4811-4023-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:21:52.269: INFO: Trying to get logs from node sponde pod pod-906c4811-4023-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:21:52.286: INFO: Waiting for pod pod-906c4811-4023-11e9-9071-0a58ac100007 to disappear
Mar  6 15:21:52.287: INFO: Pod pod-906c4811-4023-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:21:52.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2jghx" for this suite.
Mar  6 15:21:58.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:21:58.330: INFO: namespace: e2e-tests-emptydir-2jghx, resource: bindings, ignored listing per whitelist
Mar  6 15:21:58.367: INFO: namespace e2e-tests-emptydir-2jghx deletion completed in 6.076573744s

• [SLOW TEST:8.301 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:21:58.367: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-kt2js
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-kt2js
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-kt2js to expose endpoints map[]
Mar  6 15:21:58.558: INFO: Get endpoints failed (1.322291ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  6 15:21:59.561: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-kt2js exposes endpoints map[] (1.004030738s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-kt2js
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-kt2js to expose endpoints map[pod1:[80]]
Mar  6 15:22:01.586: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-kt2js exposes endpoints map[pod1:[80]] (2.016437998s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-kt2js
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-kt2js to expose endpoints map[pod1:[80] pod2:[80]]
Mar  6 15:22:04.622: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-kt2js exposes endpoints map[pod1:[80] pod2:[80]] (3.029056327s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-kt2js
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-kt2js to expose endpoints map[pod2:[80]]
Mar  6 15:22:05.644: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-kt2js exposes endpoints map[pod2:[80]] (1.010936738s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-kt2js
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-kt2js to expose endpoints map[]
Mar  6 15:22:06.655: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-kt2js exposes endpoints map[] (1.004075378s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:22:06.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-kt2js" for this suite.
Mar  6 15:22:12.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:22:12.741: INFO: namespace: e2e-tests-services-kt2js, resource: bindings, ignored listing per whitelist
Mar  6 15:22:12.754: INFO: namespace e2e-tests-services-kt2js deletion completed in 6.072379784s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:14.387 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:22:12.754: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kxvqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-9def8ab5-4023-11e9-9071-0a58ac100007
STEP: Creating secret with name secret-projected-all-test-volume-9def8a95-4023-11e9-9071-0a58ac100007
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  6 15:22:12.943: INFO: Waiting up to 5m0s for pod "projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-kxvqg" to be "success or failure"
Mar  6 15:22:12.944: INFO: Pod "projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505355ms
Mar  6 15:22:14.947: INFO: Pod "projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004348604s
STEP: Saw pod success
Mar  6 15:22:14.947: INFO: Pod "projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:22:14.949: INFO: Trying to get logs from node kronos pod projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  6 15:22:14.969: INFO: Waiting for pod projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007 to disappear
Mar  6 15:22:14.971: INFO: Pod projected-volume-9def8a4f-4023-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:22:14.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kxvqg" for this suite.
Mar  6 15:22:20.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:22:20.990: INFO: namespace: e2e-tests-projected-kxvqg, resource: bindings, ignored listing per whitelist
Mar  6 15:22:21.043: INFO: namespace e2e-tests-projected-kxvqg deletion completed in 6.068824219s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:22:21.043: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-jzr6b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:23:21.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jzr6b" for this suite.
Mar  6 15:23:43.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:23:43.287: INFO: namespace: e2e-tests-container-probe-jzr6b, resource: bindings, ignored listing per whitelist
Mar  6 15:23:43.309: INFO: namespace e2e-tests-container-probe-jzr6b deletion completed in 22.0831847s

• [SLOW TEST:82.266 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:23:43.309: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-sr76r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0306 15:23:44.517581      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 15:23:44.517: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:23:44.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-sr76r" for this suite.
Mar  6 15:23:50.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:23:50.564: INFO: namespace: e2e-tests-gc-sr76r, resource: bindings, ignored listing per whitelist
Mar  6 15:23:50.597: INFO: namespace e2e-tests-gc-sr76r deletion completed in 6.07661816s

• [SLOW TEST:7.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:23:50.597: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-s98tw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d843ca95-4023-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 15:23:50.800: INFO: Waiting up to 5m0s for pod "pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-s98tw" to be "success or failure"
Mar  6 15:23:50.801: INFO: Pod "pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.432928ms
Mar  6 15:23:52.804: INFO: Pod "pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00409382s
STEP: Saw pod success
Mar  6 15:23:52.804: INFO: Pod "pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:23:52.806: INFO: Trying to get logs from node sponde pod pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007 container secret-env-test: <nil>
STEP: delete the pod
Mar  6 15:23:52.824: INFO: Waiting for pod pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007 to disappear
Mar  6 15:23:52.826: INFO: Pod pod-secrets-d844d4a4-4023-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:23:52.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-s98tw" for this suite.
Mar  6 15:23:58.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:23:58.858: INFO: namespace: e2e-tests-secrets-s98tw, resource: bindings, ignored listing per whitelist
Mar  6 15:23:58.905: INFO: namespace e2e-tests-secrets-s98tw deletion completed in 6.0754022s

• [SLOW TEST:8.308 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:23:58.905: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-f6p2l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:24:03.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-f6p2l" for this suite.
Mar  6 15:24:09.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:24:09.144: INFO: namespace: e2e-tests-kubelet-test-f6p2l, resource: bindings, ignored listing per whitelist
Mar  6 15:24:09.171: INFO: namespace e2e-tests-kubelet-test-f6p2l deletion completed in 6.073528599s

• [SLOW TEST:10.266 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:24:09.171: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-9v9xk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:24:09.358: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  6 15:24:14.361: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 15:24:14.361: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 15:24:14.377: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-9v9xk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9v9xk/deployments/test-cleanup-deployment,UID:e651b544-4023-11e9-b3f2-0cc47aaad1b4,ResourceVersion:49712,Generation:1,CreationTimestamp:2019-03-06 15:24:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 15:24:14.379: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:24:14.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9v9xk" for this suite.
Mar  6 15:24:20.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:24:20.428: INFO: namespace: e2e-tests-deployment-9v9xk, resource: bindings, ignored listing per whitelist
Mar  6 15:24:20.459: INFO: namespace e2e-tests-deployment-9v9xk deletion completed in 6.074221394s

• [SLOW TEST:11.288 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:24:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-d27x8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:24:22.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-d27x8" for this suite.
Mar  6 15:25:02.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:02.733: INFO: namespace: e2e-tests-kubelet-test-d27x8, resource: bindings, ignored listing per whitelist
Mar  6 15:25:02.739: INFO: namespace e2e-tests-kubelet-test-d27x8 deletion completed in 40.082062301s

• [SLOW TEST:42.280 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:02.739: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5qdc7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 15:25:02.925: INFO: Waiting up to 5m0s for pod "pod-03421ea8-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-5qdc7" to be "success or failure"
Mar  6 15:25:02.926: INFO: Pod "pod-03421ea8-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631334ms
Mar  6 15:25:04.931: INFO: Pod "pod-03421ea8-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005844894s
Mar  6 15:25:06.933: INFO: Pod "pod-03421ea8-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008575649s
STEP: Saw pod success
Mar  6 15:25:06.933: INFO: Pod "pod-03421ea8-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:25:06.936: INFO: Trying to get logs from node themisto pod pod-03421ea8-4024-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:25:06.961: INFO: Waiting for pod pod-03421ea8-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:25:06.963: INFO: Pod pod-03421ea8-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:06.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5qdc7" for this suite.
Mar  6 15:25:12.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:13.015: INFO: namespace: e2e-tests-emptydir-5qdc7, resource: bindings, ignored listing per whitelist
Mar  6 15:25:13.052: INFO: namespace e2e-tests-emptydir-5qdc7 deletion completed in 6.082035767s

• [SLOW TEST:10.313 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:13.052: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-f7lrw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-09679cd1-4024-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:25:13.242: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-f7lrw" to be "success or failure"
Mar  6 15:25:13.243: INFO: Pod "pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580894ms
Mar  6 15:25:15.246: INFO: Pod "pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004265386s
STEP: Saw pod success
Mar  6 15:25:15.246: INFO: Pod "pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:25:15.248: INFO: Trying to get logs from node sponde pod pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:25:15.268: INFO: Waiting for pod pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:25:15.270: INFO: Pod pod-projected-configmaps-09686a4e-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:15.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f7lrw" for this suite.
Mar  6 15:25:21.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:21.290: INFO: namespace: e2e-tests-projected-f7lrw, resource: bindings, ignored listing per whitelist
Mar  6 15:25:21.342: INFO: namespace e2e-tests-projected-f7lrw deletion completed in 6.068560641s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-br7tl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar  6 15:25:21.514: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-889989848 proxy --unix-socket=/tmp/kubectl-proxy-unix670345303/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:21.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-br7tl" for this suite.
Mar  6 15:25:27.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:27.622: INFO: namespace: e2e-tests-kubectl-br7tl, resource: bindings, ignored listing per whitelist
Mar  6 15:25:27.642: INFO: namespace e2e-tests-kubectl-br7tl deletion completed in 6.080377874s

• [SLOW TEST:6.300 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:27.642: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-gqlgm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar  6 15:25:27.826: INFO: Waiting up to 5m0s for pod "client-containers-1219a9a6-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-containers-gqlgm" to be "success or failure"
Mar  6 15:25:27.827: INFO: Pod "client-containers-1219a9a6-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.697455ms
Mar  6 15:25:29.830: INFO: Pod "client-containers-1219a9a6-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004293892s
STEP: Saw pod success
Mar  6 15:25:29.830: INFO: Pod "client-containers-1219a9a6-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:25:29.832: INFO: Trying to get logs from node kronos pod client-containers-1219a9a6-4024-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:25:29.850: INFO: Waiting for pod client-containers-1219a9a6-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:25:29.852: INFO: Pod client-containers-1219a9a6-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-gqlgm" for this suite.
Mar  6 15:25:35.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:35.870: INFO: namespace: e2e-tests-containers-gqlgm, resource: bindings, ignored listing per whitelist
Mar  6 15:25:35.933: INFO: namespace e2e-tests-containers-gqlgm deletion completed in 6.077876157s

• [SLOW TEST:8.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:35.933: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6plfz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar  6 15:25:36.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 cluster-info'
Mar  6 15:25:36.189: INFO: stderr: ""
Mar  6 15:25:36.189: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443\x1b[0m\n\x1b[0;32mDex\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443/api/v1/namespaces/kube-system/services/dex:dex/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mTiller\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443/api/v1/namespaces/kube-system/services/tiller:tiller/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:36.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6plfz" for this suite.
Mar  6 15:25:42.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:25:42.217: INFO: namespace: e2e-tests-kubectl-6plfz, resource: bindings, ignored listing per whitelist
Mar  6 15:25:42.271: INFO: namespace e2e-tests-kubectl-6plfz deletion completed in 6.078010899s

• [SLOW TEST:6.338 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:25:42.271: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-8lvmc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  6 15:25:48.474: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:48.474: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:48.603: INFO: Exec stderr: ""
Mar  6 15:25:48.603: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:48.603: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:48.712: INFO: Exec stderr: ""
Mar  6 15:25:48.712: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:48.712: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:48.833: INFO: Exec stderr: ""
Mar  6 15:25:48.833: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:48.833: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:48.956: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  6 15:25:48.956: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:48.956: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.077: INFO: Exec stderr: ""
Mar  6 15:25:49.077: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:49.077: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.191: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  6 15:25:49.191: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:49.191: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.325: INFO: Exec stderr: ""
Mar  6 15:25:49.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.443: INFO: Exec stderr: ""
Mar  6 15:25:49.443: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:49.443: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.561: INFO: Exec stderr: ""
Mar  6 15:25:49.561: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8lvmc PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:25:49.561: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:25:49.674: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:25:49.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-8lvmc" for this suite.
Mar  6 15:26:37.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:26:37.751: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-8lvmc, resource: bindings, ignored listing per whitelist
Mar  6 15:26:37.757: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-8lvmc deletion completed in 48.078869883s

• [SLOW TEST:55.487 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:26:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-th9sw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-th9sw
Mar  6 15:26:41.972: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-th9sw
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 15:26:41.973: INFO: Initial restart count of pod liveness-http is 0
Mar  6 15:26:51.987: INFO: Restart count of pod e2e-tests-container-probe-th9sw/liveness-http is now 1 (10.013675834s elapsed)
Mar  6 15:27:12.012: INFO: Restart count of pod e2e-tests-container-probe-th9sw/liveness-http is now 2 (30.038012923s elapsed)
Mar  6 15:27:32.043: INFO: Restart count of pod e2e-tests-container-probe-th9sw/liveness-http is now 3 (50.069746827s elapsed)
Mar  6 15:27:52.073: INFO: Restart count of pod e2e-tests-container-probe-th9sw/liveness-http is now 4 (1m10.099001039s elapsed)
Mar  6 15:28:58.166: INFO: Restart count of pod e2e-tests-container-probe-th9sw/liveness-http is now 5 (2m16.192041672s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:28:58.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-th9sw" for this suite.
Mar  6 15:29:04.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:29:04.252: INFO: namespace: e2e-tests-container-probe-th9sw, resource: bindings, ignored listing per whitelist
Mar  6 15:29:04.265: INFO: namespace e2e-tests-container-probe-th9sw deletion completed in 6.082107567s

• [SLOW TEST:146.507 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:29:04.265: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-xkkhk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-2vqk
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 15:29:04.462: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2vqk" in namespace "e2e-tests-subpath-xkkhk" to be "success or failure"
Mar  6 15:29:04.463: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.549337ms
Mar  6 15:29:06.466: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 2.003889885s
Mar  6 15:29:08.468: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 4.006520252s
Mar  6 15:29:10.473: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 6.011264139s
Mar  6 15:29:12.476: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 8.014484454s
Mar  6 15:29:14.479: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 10.017356799s
Mar  6 15:29:16.482: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 12.020021093s
Mar  6 15:29:18.485: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 14.022890269s
Mar  6 15:29:20.487: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 16.025547749s
Mar  6 15:29:22.491: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 18.0296634s
Mar  6 15:29:24.494: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Running", Reason="", readiness=false. Elapsed: 20.032511782s
Mar  6 15:29:26.497: INFO: Pod "pod-subpath-test-configmap-2vqk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034847089s
STEP: Saw pod success
Mar  6 15:29:26.497: INFO: Pod "pod-subpath-test-configmap-2vqk" satisfied condition "success or failure"
Mar  6 15:29:26.499: INFO: Trying to get logs from node themisto pod pod-subpath-test-configmap-2vqk container test-container-subpath-configmap-2vqk: <nil>
STEP: delete the pod
Mar  6 15:29:26.520: INFO: Waiting for pod pod-subpath-test-configmap-2vqk to disappear
Mar  6 15:29:26.524: INFO: Pod pod-subpath-test-configmap-2vqk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2vqk
Mar  6 15:29:26.524: INFO: Deleting pod "pod-subpath-test-configmap-2vqk" in namespace "e2e-tests-subpath-xkkhk"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:29:26.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-xkkhk" for this suite.
Mar  6 15:29:32.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:29:32.570: INFO: namespace: e2e-tests-subpath-xkkhk, resource: bindings, ignored listing per whitelist
Mar  6 15:29:32.603: INFO: namespace e2e-tests-subpath-xkkhk deletion completed in 6.072085357s

• [SLOW TEST:28.338 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:29:32.603: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-pmtwd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 15:29:36.812: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 15:29:36.815: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 15:29:38.815: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 15:29:38.818: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 15:29:40.815: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 15:29:40.817: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:29:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-pmtwd" for this suite.
Mar  6 15:30:02.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:02.892: INFO: namespace: e2e-tests-container-lifecycle-hook-pmtwd, resource: bindings, ignored listing per whitelist
Mar  6 15:30:02.894: INFO: namespace e2e-tests-container-lifecycle-hook-pmtwd deletion completed in 22.072922311s

• [SLOW TEST:30.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cnxwr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:30:03.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cnxwr'
Mar  6 15:30:03.154: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 15:30:03.154: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar  6 15:30:03.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-cnxwr'
Mar  6 15:30:03.238: INFO: stderr: ""
Mar  6 15:30:03.238: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:30:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cnxwr" for this suite.
Mar  6 15:30:25.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:25.287: INFO: namespace: e2e-tests-kubectl-cnxwr, resource: bindings, ignored listing per whitelist
Mar  6 15:30:25.307: INFO: namespace e2e-tests-kubectl-cnxwr deletion completed in 22.066453858s

• [SLOW TEST:22.413 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:25.307: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-kcldj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar  6 15:30:25.486: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-kcldj" to be "success or failure"
Mar  6 15:30:25.488: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.351919ms
Mar  6 15:30:27.491: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004243363s
STEP: Saw pod success
Mar  6 15:30:27.491: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  6 15:30:27.493: INFO: Trying to get logs from node themisto pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  6 15:30:27.512: INFO: Waiting for pod pod-host-path-test to disappear
Mar  6 15:30:27.514: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:30:27.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-kcldj" for this suite.
Mar  6 15:30:33.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:33.574: INFO: namespace: e2e-tests-hostpath-kcldj, resource: bindings, ignored listing per whitelist
Mar  6 15:30:33.590: INFO: namespace e2e-tests-hostpath-kcldj deletion completed in 6.072218998s

• [SLOW TEST:8.283 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hw7d5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 15:30:33.767: INFO: Waiting up to 5m0s for pod "pod-c874ae7c-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-hw7d5" to be "success or failure"
Mar  6 15:30:33.768: INFO: Pod "pod-c874ae7c-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.43053ms
Mar  6 15:30:35.771: INFO: Pod "pod-c874ae7c-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003751807s
STEP: Saw pod success
Mar  6 15:30:35.771: INFO: Pod "pod-c874ae7c-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:30:35.773: INFO: Trying to get logs from node sponde pod pod-c874ae7c-4024-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:30:35.791: INFO: Waiting for pod pod-c874ae7c-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:30:35.793: INFO: Pod pod-c874ae7c-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:30:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hw7d5" for this suite.
Mar  6 15:30:41.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:41.866: INFO: namespace: e2e-tests-emptydir-hw7d5, resource: bindings, ignored listing per whitelist
Mar  6 15:30:41.867: INFO: namespace e2e-tests-emptydir-hw7d5 deletion completed in 6.071118793s

• [SLOW TEST:8.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:41.867: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-8ljqt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:30:44.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-8ljqt" for this suite.
Mar  6 15:30:50.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:50.140: INFO: namespace: e2e-tests-emptydir-wrapper-8ljqt, resource: bindings, ignored listing per whitelist
Mar  6 15:30:50.175: INFO: namespace e2e-tests-emptydir-wrapper-8ljqt deletion completed in 6.081428811s

• [SLOW TEST:8.308 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:50.175: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nvsnw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:30:50.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-nvsnw" to be "success or failure"
Mar  6 15:30:50.359: INFO: Pod "downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923914ms
Mar  6 15:30:52.362: INFO: Pod "downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004616156s
STEP: Saw pod success
Mar  6 15:30:52.362: INFO: Pod "downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:30:52.364: INFO: Trying to get logs from node sponde pod downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:30:52.382: INFO: Waiting for pod downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:30:52.384: INFO: Pod downwardapi-volume-d25830e9-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:30:52.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nvsnw" for this suite.
Mar  6 15:30:58.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:30:58.424: INFO: namespace: e2e-tests-downward-api-nvsnw, resource: bindings, ignored listing per whitelist
Mar  6 15:30:58.460: INFO: namespace e2e-tests-downward-api-nvsnw deletion completed in 6.072776421s

• [SLOW TEST:8.285 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:30:58.460: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-nlh6n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-nlh6n
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-nlh6n to expose endpoints map[]
Mar  6 15:30:58.643: INFO: Get endpoints failed (1.372125ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  6 15:30:59.645: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-nlh6n exposes endpoints map[] (1.004042788s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-nlh6n
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-nlh6n to expose endpoints map[pod1:[100]]
Mar  6 15:31:01.668: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-nlh6n exposes endpoints map[pod1:[100]] (2.013748179s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-nlh6n
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-nlh6n to expose endpoints map[pod2:[101] pod1:[100]]
Mar  6 15:31:03.695: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-nlh6n exposes endpoints map[pod2:[101] pod1:[100]] (2.02157356s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-nlh6n
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-nlh6n to expose endpoints map[pod2:[101]]
Mar  6 15:31:04.714: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-nlh6n exposes endpoints map[pod2:[101]] (1.011946267s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-nlh6n
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-nlh6n to expose endpoints map[]
Mar  6 15:31:05.726: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-nlh6n exposes endpoints map[] (1.004174066s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:31:05.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-nlh6n" for this suite.
Mar  6 15:31:27.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:31:27.801: INFO: namespace: e2e-tests-services-nlh6n, resource: bindings, ignored listing per whitelist
Mar  6 15:31:27.814: INFO: namespace e2e-tests-services-nlh6n deletion completed in 22.06683386s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:29.354 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:31:27.814: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-szf7w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-e8c6bb9d-4024-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:31:27.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-szf7w" to be "success or failure"
Mar  6 15:31:27.998: INFO: Pod "pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.592144ms
Mar  6 15:31:30.001: INFO: Pod "pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004287602s
STEP: Saw pod success
Mar  6 15:31:30.001: INFO: Pod "pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:31:30.003: INFO: Trying to get logs from node sponde pod pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:31:30.020: INFO: Waiting for pod pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:31:30.022: INFO: Pod pod-configmaps-e8c7894d-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:31:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-szf7w" for this suite.
Mar  6 15:31:36.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:31:36.052: INFO: namespace: e2e-tests-configmap-szf7w, resource: bindings, ignored listing per whitelist
Mar  6 15:31:36.092: INFO: namespace e2e-tests-configmap-szf7w deletion completed in 6.067066756s

• [SLOW TEST:8.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:31:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rlchq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-edb6a409-4024-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:31:36.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-rlchq" to be "success or failure"
Mar  6 15:31:36.281: INFO: Pod "pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525573ms
Mar  6 15:31:38.283: INFO: Pod "pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004075348s
STEP: Saw pod success
Mar  6 15:31:38.284: INFO: Pod "pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:31:38.286: INFO: Trying to get logs from node kronos pod pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:31:38.304: INFO: Waiting for pod pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:31:38.306: INFO: Pod pod-configmaps-edb77043-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:31:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rlchq" for this suite.
Mar  6 15:31:44.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:31:44.332: INFO: namespace: e2e-tests-configmap-rlchq, resource: bindings, ignored listing per whitelist
Mar  6 15:31:44.386: INFO: namespace e2e-tests-configmap-rlchq deletion completed in 6.076490846s

• [SLOW TEST:8.294 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:31:44.386: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6xw69
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-f2a9012f-4024-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 15:31:44.578: INFO: Waiting up to 5m0s for pod "pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-6xw69" to be "success or failure"
Mar  6 15:31:44.580: INFO: Pod "pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.667552ms
Mar  6 15:31:46.582: INFO: Pod "pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00394247s
STEP: Saw pod success
Mar  6 15:31:46.583: INFO: Pod "pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:31:46.584: INFO: Trying to get logs from node themisto pod pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 15:31:46.603: INFO: Waiting for pod pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007 to disappear
Mar  6 15:31:46.605: INFO: Pod pod-secrets-f2a9c3cf-4024-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:31:46.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6xw69" for this suite.
Mar  6 15:31:52.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:31:52.662: INFO: namespace: e2e-tests-secrets-6xw69, resource: bindings, ignored listing per whitelist
Mar  6 15:31:52.681: INFO: namespace e2e-tests-secrets-6xw69 deletion completed in 6.072289103s

• [SLOW TEST:8.295 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:31:52.681: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-vh667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vh667
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 15:31:52.855: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 15:32:10.920: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.16.2.206 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vh667 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:32:10.920: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:32:12.072: INFO: Found all expected endpoints: [netserver-0]
Mar  6 15:32:12.074: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.16.4.169 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vh667 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:32:12.074: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:32:13.205: INFO: Found all expected endpoints: [netserver-1]
Mar  6 15:32:13.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.16.10.22 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vh667 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:32:13.207: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:32:14.352: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:32:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vh667" for this suite.
Mar  6 15:32:36.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:32:36.374: INFO: namespace: e2e-tests-pod-network-test-vh667, resource: bindings, ignored listing per whitelist
Mar  6 15:32:36.432: INFO: namespace e2e-tests-pod-network-test-vh667 deletion completed in 22.075518413s

• [SLOW TEST:43.751 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:32:36.432: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-cvwfz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:32:36.615: INFO: (0) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.664009ms)
Mar  6 15:32:36.618: INFO: (1) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.5837ms)
Mar  6 15:32:36.621: INFO: (2) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.18539ms)
Mar  6 15:32:36.624: INFO: (3) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.916231ms)
Mar  6 15:32:36.627: INFO: (4) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.650784ms)
Mar  6 15:32:36.630: INFO: (5) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.93674ms)
Mar  6 15:32:36.633: INFO: (6) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.942469ms)
Mar  6 15:32:36.636: INFO: (7) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.013038ms)
Mar  6 15:32:36.638: INFO: (8) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.72397ms)
Mar  6 15:32:36.641: INFO: (9) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.733045ms)
Mar  6 15:32:36.644: INFO: (10) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.040254ms)
Mar  6 15:32:36.647: INFO: (11) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.852251ms)
Mar  6 15:32:36.650: INFO: (12) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.87497ms)
Mar  6 15:32:36.653: INFO: (13) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.944819ms)
Mar  6 15:32:36.656: INFO: (14) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.02644ms)
Mar  6 15:32:36.659: INFO: (15) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.805295ms)
Mar  6 15:32:36.662: INFO: (16) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.768373ms)
Mar  6 15:32:36.664: INFO: (17) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.811709ms)
Mar  6 15:32:36.667: INFO: (18) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.688287ms)
Mar  6 15:32:36.670: INFO: (19) /api/v1/nodes/kronos/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.995972ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:32:36.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-cvwfz" for this suite.
Mar  6 15:32:42.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:32:42.706: INFO: namespace: e2e-tests-proxy-cvwfz, resource: bindings, ignored listing per whitelist
Mar  6 15:32:42.741: INFO: namespace e2e-tests-proxy-cvwfz deletion completed in 6.068073176s

• [SLOW TEST:6.309 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:32:42.741: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-v6wkg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar  6 15:32:42.926: INFO: Waiting up to 5m0s for pod "var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-var-expansion-v6wkg" to be "success or failure"
Mar  6 15:32:42.928: INFO: Pod "var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759485ms
Mar  6 15:32:44.931: INFO: Pod "var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004768214s
STEP: Saw pod success
Mar  6 15:32:44.931: INFO: Pod "var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:32:44.933: INFO: Trying to get logs from node sponde pod var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 15:32:44.952: INFO: Waiting for pod var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:32:44.953: INFO: Pod var-expansion-1570c7e2-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:32:44.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-v6wkg" for this suite.
Mar  6 15:32:50.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:32:51.009: INFO: namespace: e2e-tests-var-expansion-v6wkg, resource: bindings, ignored listing per whitelist
Mar  6 15:32:51.034: INFO: namespace e2e-tests-var-expansion-v6wkg deletion completed in 6.077045237s

• [SLOW TEST:8.292 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:32:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-twmfb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-twmfb
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar  6 15:32:51.219: INFO: Found 0 stateful pods, waiting for 3
Mar  6 15:33:01.223: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:33:01.223: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:33:01.223: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  6 15:33:01.250: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  6 15:33:11.277: INFO: Updating stateful set ss2
Mar  6 15:33:11.281: INFO: Waiting for Pod e2e-tests-statefulset-twmfb/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar  6 15:33:21.323: INFO: Found 1 stateful pods, waiting for 3
Mar  6 15:33:31.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:33:31.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:33:31.326: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  6 15:33:31.351: INFO: Updating stateful set ss2
Mar  6 15:33:31.355: INFO: Waiting for Pod e2e-tests-statefulset-twmfb/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  6 15:33:41.381: INFO: Updating stateful set ss2
Mar  6 15:33:41.387: INFO: Waiting for StatefulSet e2e-tests-statefulset-twmfb/ss2 to complete update
Mar  6 15:33:41.387: INFO: Waiting for Pod e2e-tests-statefulset-twmfb/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  6 15:33:51.395: INFO: Waiting for StatefulSet e2e-tests-statefulset-twmfb/ss2 to complete update
Mar  6 15:33:51.395: INFO: Waiting for Pod e2e-tests-statefulset-twmfb/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 15:34:01.398: INFO: Deleting all statefulset in ns e2e-tests-statefulset-twmfb
Mar  6 15:34:01.400: INFO: Scaling statefulset ss2 to 0
Mar  6 15:34:21.415: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:34:21.417: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:34:21.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-twmfb" for this suite.
Mar  6 15:34:27.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:34:27.466: INFO: namespace: e2e-tests-statefulset-twmfb, resource: bindings, ignored listing per whitelist
Mar  6 15:34:27.498: INFO: namespace e2e-tests-statefulset-twmfb deletion completed in 6.066672607s

• [SLOW TEST:96.464 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:34:27.498: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wsc4n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:34:27.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-wsc4n'
Mar  6 15:34:27.839: INFO: stderr: ""
Mar  6 15:34:27.839: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  6 15:34:32.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-wsc4n -o json'
Mar  6 15:34:32.982: INFO: stderr: ""
Mar  6 15:34:32.982: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-03-06T15:34:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-wsc4n\",\n        \"resourceVersion\": \"53212\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-wsc4n/pods/e2e-test-nginx-pod\",\n        \"uid\": \"4ae25c3e-4025-11e9-aa7e-0cc47a6c40e2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-mnnhq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"themisto\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-mnnhq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-mnnhq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T15:35:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T15:35:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T15:35:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T15:34:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://b3ef63257bf605915ca8887f482fa06bd2df65ab49c5ee2bc1dd8e77d48b932b\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-06T15:35:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.100.96.46\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.10.26\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-06T15:35:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  6 15:34:32.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 replace -f - --namespace=e2e-tests-kubectl-wsc4n'
Mar  6 15:34:33.119: INFO: stderr: ""
Mar  6 15:34:33.119: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Mar  6 15:34:33.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-wsc4n'
Mar  6 15:34:39.184: INFO: stderr: ""
Mar  6 15:34:39.184: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:34:39.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wsc4n" for this suite.
Mar  6 15:34:45.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:34:45.238: INFO: namespace: e2e-tests-kubectl-wsc4n, resource: bindings, ignored listing per whitelist
Mar  6 15:34:45.258: INFO: namespace e2e-tests-kubectl-wsc4n deletion completed in 6.069678502s

• [SLOW TEST:17.760 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:34:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dx24l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:34:45.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-dx24l'
Mar  6 15:34:45.515: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 15:34:45.515: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Mar  6 15:34:47.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-dx24l'
Mar  6 15:34:47.604: INFO: stderr: ""
Mar  6 15:34:47.604: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:34:47.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dx24l" for this suite.
Mar  6 15:35:09.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:35:09.621: INFO: namespace: e2e-tests-kubectl-dx24l, resource: bindings, ignored listing per whitelist
Mar  6 15:35:09.676: INFO: namespace e2e-tests-kubectl-dx24l deletion completed in 22.069653716s

• [SLOW TEST:24.418 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:35:09.676: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-576vt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-576vt
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-576vt
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-576vt
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-576vt
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-576vt
Mar  6 15:35:13.878: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-576vt, name: ss-0, uid: 661b27d7-4025-11e9-aa7e-0cc47a6c40e2, status phase: Pending. Waiting for statefulset controller to delete.
Mar  6 15:35:14.278: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-576vt, name: ss-0, uid: 661b27d7-4025-11e9-aa7e-0cc47a6c40e2, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 15:35:14.287: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-576vt, name: ss-0, uid: 661b27d7-4025-11e9-aa7e-0cc47a6c40e2, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 15:35:14.292: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-576vt
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-576vt
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-576vt and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 15:35:18.313: INFO: Deleting all statefulset in ns e2e-tests-statefulset-576vt
Mar  6 15:35:18.316: INFO: Scaling statefulset ss to 0
Mar  6 15:35:28.330: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:35:28.332: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:35:28.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-576vt" for this suite.
Mar  6 15:35:34.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:35:34.394: INFO: namespace: e2e-tests-statefulset-576vt, resource: bindings, ignored listing per whitelist
Mar  6 15:35:34.433: INFO: namespace e2e-tests-statefulset-576vt deletion completed in 6.085406646s

• [SLOW TEST:24.756 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:35:34.433: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-b9fth
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-b9fth
Mar  6 15:35:36.622: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-b9fth
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 15:35:36.624: INFO: Initial restart count of pod liveness-http is 0
Mar  6 15:35:54.653: INFO: Restart count of pod e2e-tests-container-probe-b9fth/liveness-http is now 1 (18.028123643s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:35:54.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b9fth" for this suite.
Mar  6 15:36:00.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:36:00.702: INFO: namespace: e2e-tests-container-probe-b9fth, resource: bindings, ignored listing per whitelist
Mar  6 15:36:00.748: INFO: namespace e2e-tests-container-probe-b9fth deletion completed in 6.078582679s

• [SLOW TEST:26.315 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:36:00.748: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wshxm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-8b76a070-4025-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:36:00.940: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-wshxm" to be "success or failure"
Mar  6 15:36:00.942: INFO: Pod "pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397843ms
Mar  6 15:36:02.945: INFO: Pod "pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004277852s
STEP: Saw pod success
Mar  6 15:36:02.945: INFO: Pod "pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:36:02.946: INFO: Trying to get logs from node themisto pod pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:36:02.964: INFO: Waiting for pod pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:36:02.966: INFO: Pod pod-projected-configmaps-8b7760df-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:36:02.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wshxm" for this suite.
Mar  6 15:36:08.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:36:09.000: INFO: namespace: e2e-tests-projected-wshxm, resource: bindings, ignored listing per whitelist
Mar  6 15:36:09.041: INFO: namespace e2e-tests-projected-wshxm deletion completed in 6.071910044s

• [SLOW TEST:8.293 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:36:09.041: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-f9nzg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 15:36:09.215: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 15:36:09.222: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 15:36:09.223: INFO: 
Logging pods the kubelet thinks is on node kronos before test
Mar  6 15:36:09.228: INFO: haproxy-kronos from kube-system started at <nil> (0 container statuses recorded)
Mar  6 15:36:09.228: INFO: kube-flannel-7krsb from kube-system started at 2019-03-06 12:08:05 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:09.228: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 15:36:09.228: INFO: kube-dns-5575866df4-dkl5f from kube-system started at 2019-03-06 12:08:35 +0000 UTC (3 container statuses recorded)
Mar  6 15:36:09.228: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 15:36:09.228: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 15:36:09.228: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 15:36:09.228: INFO: 
Logging pods the kubelet thinks is on node sponde before test
Mar  6 15:36:09.232: INFO: haproxy-sponde from kube-system started at <nil> (0 container statuses recorded)
Mar  6 15:36:09.232: INFO: kube-dns-5575866df4-q92k8 from kube-system started at 2019-03-06 12:08:33 +0000 UTC (3 container statuses recorded)
Mar  6 15:36:09.232: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 15:36:09.232: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 15:36:09.232: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 15:36:09.232: INFO: kube-flannel-hrd9n from kube-system started at 2019-03-06 12:08:03 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:09.232: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 15:36:09.232: INFO: tiller-deploy-df475665f-sxnwr from kube-system started at 2019-03-06 12:08:35 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:09.232: INFO: 	Container tiller ready: true, restart count 0
Mar  6 15:36:09.232: INFO: 
Logging pods the kubelet thinks is on node themisto before test
Mar  6 15:36:09.236: INFO: haproxy-themisto from kube-system started at <nil> (0 container statuses recorded)
Mar  6 15:36:09.236: INFO: kube-flannel-cc7kg from kube-system started at 2019-03-06 15:05:24 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:09.236: INFO: 	Container kube-flannel ready: true, restart count 2
Mar  6 15:36:09.236: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 15:12:49 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:09.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-919f4be0-4025-11e9-9071-0a58ac100007 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-919f4be0-4025-11e9-9071-0a58ac100007 off the node sponde
STEP: verifying the node doesn't have the label kubernetes.io/e2e-919f4be0-4025-11e9-9071-0a58ac100007
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:36:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-f9nzg" for this suite.
Mar  6 15:36:21.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:36:21.363: INFO: namespace: e2e-tests-sched-pred-f9nzg, resource: bindings, ignored listing per whitelist
Mar  6 15:36:21.375: INFO: namespace e2e-tests-sched-pred-f9nzg deletion completed in 8.073464509s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.334 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:36:21.375: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-vmjdv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 15:36:21.553: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 15:36:21.558: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 15:36:21.560: INFO: 
Logging pods the kubelet thinks is on node kronos before test
Mar  6 15:36:21.564: INFO: haproxy-kronos from kube-system started at <nil> (0 container statuses recorded)
Mar  6 15:36:21.564: INFO: kube-flannel-7krsb from kube-system started at 2019-03-06 12:08:05 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:21.564: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 15:36:21.564: INFO: kube-dns-5575866df4-dkl5f from kube-system started at 2019-03-06 12:08:35 +0000 UTC (3 container statuses recorded)
Mar  6 15:36:21.564: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 15:36:21.564: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 15:36:21.564: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 15:36:21.564: INFO: 
Logging pods the kubelet thinks is on node sponde before test
Mar  6 15:36:21.569: INFO: haproxy-sponde from kube-system started at <nil> (0 container statuses recorded)
Mar  6 15:36:21.569: INFO: kube-dns-5575866df4-q92k8 from kube-system started at 2019-03-06 12:08:33 +0000 UTC (3 container statuses recorded)
Mar  6 15:36:21.569: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 15:36:21.569: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 15:36:21.569: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 15:36:21.569: INFO: tiller-deploy-df475665f-sxnwr from kube-system started at 2019-03-06 12:08:35 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:21.569: INFO: 	Container tiller ready: true, restart count 0
Mar  6 15:36:21.569: INFO: kube-flannel-hrd9n from kube-system started at 2019-03-06 12:08:03 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:21.569: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 15:36:21.569: INFO: 
Logging pods the kubelet thinks is on node themisto before test
Mar  6 15:36:21.573: INFO: kube-flannel-cc7kg from kube-system started at 2019-03-06 15:05:24 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:21.573: INFO: 	Container kube-flannel ready: true, restart count 2
Mar  6 15:36:21.573: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 15:12:49 +0000 UTC (1 container statuses recorded)
Mar  6 15:36:21.573: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 15:36:21.573: INFO: haproxy-themisto from kube-system started at <nil> (0 container statuses recorded)
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node kronos
STEP: verifying the node has the label node sponde
STEP: verifying the node has the label node themisto
Mar  6 15:36:21.608: INFO: Pod sonobuoy requesting resource cpu=0m on Node themisto
Mar  6 15:36:21.608: INFO: Pod haproxy-kronos requesting resource cpu=0m on Node kronos
Mar  6 15:36:21.608: INFO: Pod haproxy-sponde requesting resource cpu=0m on Node sponde
Mar  6 15:36:21.608: INFO: Pod haproxy-themisto requesting resource cpu=0m on Node themisto
Mar  6 15:36:21.608: INFO: Pod kube-dns-5575866df4-dkl5f requesting resource cpu=260m on Node kronos
Mar  6 15:36:21.608: INFO: Pod kube-dns-5575866df4-q92k8 requesting resource cpu=260m on Node sponde
Mar  6 15:36:21.608: INFO: Pod kube-flannel-7krsb requesting resource cpu=0m on Node kronos
Mar  6 15:36:21.608: INFO: Pod kube-flannel-cc7kg requesting resource cpu=0m on Node themisto
Mar  6 15:36:21.608: INFO: Pod kube-flannel-hrd9n requesting resource cpu=0m on Node sponde
Mar  6 15:36:21.608: INFO: Pod tiller-deploy-df475665f-sxnwr requesting resource cpu=0m on Node sponde
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007.1589689e875d6e2d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vmjdv/filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007 to kronos]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007.158968a0bb01452b], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-sched-pred-vmjdv.svc.cluster.local svc.cluster.local cluster.local qa.suse.cz suse.cz suse.de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007.158968a0cad973cf], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007.158968a0ccf0243c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca16e8-4025-11e9-9071-0a58ac100007.158968a0d070e852], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007.1589689e8981baf6], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vmjdv/filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007 to sponde]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007.158968a05673b9cc], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-sched-pred-vmjdv.svc.cluster.local svc.cluster.local cluster.local qa.suse.cz suse.cz suse.de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007.158968a064ab5346], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007.158968a06636a486], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cb22ed-4025-11e9-9071-0a58ac100007.158968a069b5c013], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007.1589689e897d3c08], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vmjdv/filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007 to themisto]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007.158968ac9c963989], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-sched-pred-vmjdv.svc.cluster.local svc.cluster.local cluster.local qa.suse.cz suse.cz suse.de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007.158968acacc32f45], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007.158968acadfc750f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97cc51c4-4025-11e9-9071-0a58ac100007.158968acb192493b], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1589689efff86bc5], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node kronos
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sponde
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node themisto
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:36:24.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vmjdv" for this suite.
Mar  6 15:36:30.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:36:30.717: INFO: namespace: e2e-tests-sched-pred-vmjdv, resource: bindings, ignored listing per whitelist
Mar  6 15:36:30.753: INFO: namespace e2e-tests-sched-pred-vmjdv deletion completed in 6.067862569s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.378 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:36:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-w6m77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  6 15:36:30.938: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-w6m77,SelfLink:/api/v1/namespaces/e2e-tests-watch-w6m77/configmaps/e2e-watch-test-watch-closed,UID:9d583e63-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54119,Generation:0,CreationTimestamp:2019-03-06 15:36:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 15:36:30.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-w6m77,SelfLink:/api/v1/namespaces/e2e-tests-watch-w6m77/configmaps/e2e-watch-test-watch-closed,UID:9d583e63-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54120,Generation:0,CreationTimestamp:2019-03-06 15:36:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  6 15:36:30.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-w6m77,SelfLink:/api/v1/namespaces/e2e-tests-watch-w6m77/configmaps/e2e-watch-test-watch-closed,UID:9d583e63-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54121,Generation:0,CreationTimestamp:2019-03-06 15:36:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 15:36:30.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-w6m77,SelfLink:/api/v1/namespaces/e2e-tests-watch-w6m77/configmaps/e2e-watch-test-watch-closed,UID:9d583e63-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54122,Generation:0,CreationTimestamp:2019-03-06 15:36:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:36:30.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-w6m77" for this suite.
Mar  6 15:36:36.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:36:37.025: INFO: namespace: e2e-tests-watch-w6m77, resource: bindings, ignored listing per whitelist
Mar  6 15:36:37.031: INFO: namespace e2e-tests-watch-w6m77 deletion completed in 6.078359244s

• [SLOW TEST:6.278 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:36:37.031: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-27t8q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:36:39.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-27t8q" for this suite.
Mar  6 15:37:27.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:37:27.295: INFO: namespace: e2e-tests-kubelet-test-27t8q, resource: bindings, ignored listing per whitelist
Mar  6 15:37:27.315: INFO: namespace e2e-tests-kubelet-test-27t8q deletion completed in 48.085093883s

• [SLOW TEST:50.283 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:37:27.315: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-82w2q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:37:27.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-82w2q'
Mar  6 15:37:27.580: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 15:37:27.580: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Mar  6 15:37:31.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-82w2q'
Mar  6 15:37:31.673: INFO: stderr: ""
Mar  6 15:37:31.673: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:37:31.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-82w2q" for this suite.
Mar  6 15:37:37.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:37:37.705: INFO: namespace: e2e-tests-kubectl-82w2q, resource: bindings, ignored listing per whitelist
Mar  6 15:37:37.755: INFO: namespace e2e-tests-kubectl-82w2q deletion completed in 6.07811635s

• [SLOW TEST:10.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:37:37.755: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-cb76r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-44qz
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 15:37:37.947: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-44qz" in namespace "e2e-tests-subpath-cb76r" to be "success or failure"
Mar  6 15:37:37.949: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822771ms
Mar  6 15:37:39.953: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 2.005207335s
Mar  6 15:37:41.955: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 4.007478368s
Mar  6 15:37:43.958: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 6.010609525s
Mar  6 15:37:45.961: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 8.01325635s
Mar  6 15:37:47.964: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 10.016462115s
Mar  6 15:37:49.967: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 12.019550862s
Mar  6 15:37:51.970: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 14.022238303s
Mar  6 15:37:53.972: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 16.025103086s
Mar  6 15:37:55.975: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 18.027887927s
Mar  6 15:37:57.978: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Running", Reason="", readiness=false. Elapsed: 20.030858883s
Mar  6 15:37:59.981: INFO: Pod "pod-subpath-test-secret-44qz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03362158s
STEP: Saw pod success
Mar  6 15:37:59.981: INFO: Pod "pod-subpath-test-secret-44qz" satisfied condition "success or failure"
Mar  6 15:37:59.983: INFO: Trying to get logs from node kronos pod pod-subpath-test-secret-44qz container test-container-subpath-secret-44qz: <nil>
STEP: delete the pod
Mar  6 15:38:00.002: INFO: Waiting for pod pod-subpath-test-secret-44qz to disappear
Mar  6 15:38:00.004: INFO: Pod pod-subpath-test-secret-44qz no longer exists
STEP: Deleting pod pod-subpath-test-secret-44qz
Mar  6 15:38:00.004: INFO: Deleting pod "pod-subpath-test-secret-44qz" in namespace "e2e-tests-subpath-cb76r"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:00.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-cb76r" for this suite.
Mar  6 15:38:06.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:38:06.049: INFO: namespace: e2e-tests-subpath-cb76r, resource: bindings, ignored listing per whitelist
Mar  6 15:38:06.085: INFO: namespace e2e-tests-subpath-cb76r deletion completed in 6.076353804s

• [SLOW TEST:28.330 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:38:06.085: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-mrcp5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:38:06.263: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  6 15:38:06.270: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  6 15:38:11.273: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 15:38:11.273: INFO: Creating deployment "test-rolling-update-deployment"
Mar  6 15:38:11.280: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  6 15:38:11.284: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  6 15:38:13.289: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  6 15:38:13.291: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 15:38:13.298: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-mrcp5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mrcp5/deployments/test-rolling-update-deployment,UID:d927d7a0-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54694,Generation:1,CreationTimestamp:2019-03-06 15:38:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-06 15:38:06 +0000 UTC 2019-03-06 15:38:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-06 15:38:07 +0000 UTC 2019-03-06 15:38:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 15:38:13.301: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-mrcp5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mrcp5/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:d92d0682-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54685,Generation:1,CreationTimestamp:2019-03-06 15:38:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d927d7a0-4025-11e9-b3f2-0cc47aaad1b4 0xc0025ffd67 0xc0025ffd68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 15:38:13.301: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  6 15:38:13.301: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-mrcp5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mrcp5/replicasets/test-rolling-update-controller,UID:d62b4cdd-4025-11e9-b3f2-0cc47aaad1b4,ResourceVersion:54693,Generation:2,CreationTimestamp:2019-03-06 15:38:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d927d7a0-4025-11e9-b3f2-0cc47aaad1b4 0xc0025ffca7 0xc0025ffca8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 15:38:13.303: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-kn2m9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-kn2m9,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-mrcp5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-mrcp5/pods/test-rolling-update-deployment-68b55d7bc6-kn2m9,UID:d63e92bf-4025-11e9-818a-0cc47aaa4380,ResourceVersion:54684,Generation:0,CreationTimestamp:2019-03-06 15:38:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 d92d0682-4025-11e9-b3f2-0cc47aaad1b4 0xc001b9a9e7 0xc001b9a9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-l6bsn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l6bsn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-l6bsn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9aa60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9aa80}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:38:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:38:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:38:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:37:56 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.218,StartTime:2019-03-06 15:38:15 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-06 15:38:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f83b0931af7622d9650f5da0c08edde7672e03b8530e8cb053b2a4464c4455b9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-mrcp5" for this suite.
Mar  6 15:38:19.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:38:19.329: INFO: namespace: e2e-tests-deployment-mrcp5, resource: bindings, ignored listing per whitelist
Mar  6 15:38:19.391: INFO: namespace e2e-tests-deployment-mrcp5 deletion completed in 6.083981842s

• [SLOW TEST:13.306 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:38:19.391: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-q8hsq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar  6 15:38:19.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 --namespace=e2e-tests-kubectl-q8hsq run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  6 15:38:20.608: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  6 15:38:20.608: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:22.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q8hsq" for this suite.
Mar  6 15:38:36.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:38:36.675: INFO: namespace: e2e-tests-kubectl-q8hsq, resource: bindings, ignored listing per whitelist
Mar  6 15:38:36.682: INFO: namespace e2e-tests-kubectl-q8hsq deletion completed in 14.065381186s

• [SLOW TEST:17.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:38:36.682: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cp4bq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 15:38:36.860: INFO: Waiting up to 5m0s for pod "downward-api-e866ef2f-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-cp4bq" to be "success or failure"
Mar  6 15:38:36.862: INFO: Pod "downward-api-e866ef2f-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.449403ms
Mar  6 15:38:38.865: INFO: Pod "downward-api-e866ef2f-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004382232s
STEP: Saw pod success
Mar  6 15:38:38.865: INFO: Pod "downward-api-e866ef2f-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:38:38.867: INFO: Trying to get logs from node kronos pod downward-api-e866ef2f-4025-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 15:38:38.885: INFO: Waiting for pod downward-api-e866ef2f-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:38:38.887: INFO: Pod downward-api-e866ef2f-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:38.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cp4bq" for this suite.
Mar  6 15:38:44.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:38:44.956: INFO: namespace: e2e-tests-downward-api-cp4bq, resource: bindings, ignored listing per whitelist
Mar  6 15:38:44.972: INFO: namespace e2e-tests-downward-api-cp4bq deletion completed in 6.081185335s

• [SLOW TEST:8.290 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:38:44.972: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jwx7w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:38:45.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-jwx7w" to be "success or failure"
Mar  6 15:38:45.161: INFO: Pod "downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691731ms
Mar  6 15:38:47.163: INFO: Pod "downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004142181s
STEP: Saw pod success
Mar  6 15:38:47.163: INFO: Pod "downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:38:47.165: INFO: Trying to get logs from node themisto pod downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:38:47.182: INFO: Waiting for pod downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:38:47.184: INFO: Pod downwardapi-volume-ed590869-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:47.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jwx7w" for this suite.
Mar  6 15:38:53.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:38:53.231: INFO: namespace: e2e-tests-projected-jwx7w, resource: bindings, ignored listing per whitelist
Mar  6 15:38:53.253: INFO: namespace e2e-tests-projected-jwx7w deletion completed in 6.066918788s

• [SLOW TEST:8.282 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:38:53.254: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qfgg6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-f24870e7-4025-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 15:38:53.442: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-qfgg6" to be "success or failure"
Mar  6 15:38:53.444: INFO: Pod "pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529485ms
Mar  6 15:38:55.447: INFO: Pod "pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004166852s
STEP: Saw pod success
Mar  6 15:38:55.447: INFO: Pod "pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:38:55.449: INFO: Trying to get logs from node sponde pod pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 15:38:55.468: INFO: Waiting for pod pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:38:55.469: INFO: Pod pod-projected-secrets-f2493c6a-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:38:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qfgg6" for this suite.
Mar  6 15:39:01.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:39:01.516: INFO: namespace: e2e-tests-projected-qfgg6, resource: bindings, ignored listing per whitelist
Mar  6 15:39:01.553: INFO: namespace e2e-tests-projected-qfgg6 deletion completed in 6.08095179s

• [SLOW TEST:8.300 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:39:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bvrwm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f73ad331-4025-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:39:01.743: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-bvrwm" to be "success or failure"
Mar  6 15:39:01.744: INFO: Pod "pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461843ms
Mar  6 15:39:03.747: INFO: Pod "pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004084389s
STEP: Saw pod success
Mar  6 15:39:03.747: INFO: Pod "pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:39:03.750: INFO: Trying to get logs from node themisto pod pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:39:03.769: INFO: Waiting for pod pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007 to disappear
Mar  6 15:39:03.770: INFO: Pod pod-projected-configmaps-f73ba229-4025-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:39:03.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bvrwm" for this suite.
Mar  6 15:39:09.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:39:09.814: INFO: namespace: e2e-tests-projected-bvrwm, resource: bindings, ignored listing per whitelist
Mar  6 15:39:09.848: INFO: namespace e2e-tests-projected-bvrwm deletion completed in 6.074403644s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:39:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-h8cl2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-h8cl2
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-h8cl2
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-h8cl2
Mar  6 15:39:10.037: INFO: Found 0 stateful pods, waiting for 1
Mar  6 15:39:20.040: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  6 15:39:20.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:39:20.282: INFO: stderr: ""
Mar  6 15:39:20.283: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:39:20.283: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 15:39:20.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 15:39:30.288: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 15:39:30.288: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:39:30.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999673s
Mar  6 15:39:31.303: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997960819s
Mar  6 15:39:32.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995279997s
Mar  6 15:39:33.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992505898s
Mar  6 15:39:34.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98988657s
Mar  6 15:39:35.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987047479s
Mar  6 15:39:36.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984221025s
Mar  6 15:39:37.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979915168s
Mar  6 15:39:38.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.977250013s
Mar  6 15:39:39.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.571293ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-h8cl2
Mar  6 15:39:40.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:39:40.545: INFO: stderr: ""
Mar  6 15:39:40.545: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:39:40.545: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 15:39:40.548: INFO: Found 1 stateful pods, waiting for 3
Mar  6 15:39:50.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:39:50.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 15:39:50.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  6 15:39:50.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:39:50.796: INFO: stderr: ""
Mar  6 15:39:50.796: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:39:50.796: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 15:39:50.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:39:51.006: INFO: stderr: ""
Mar  6 15:39:51.006: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:39:51.006: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 15:39:51.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 15:39:51.212: INFO: stderr: ""
Mar  6 15:39:51.212: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 15:39:51.212: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 15:39:51.212: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:39:51.214: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  6 15:40:01.219: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 15:40:01.219: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 15:40:01.219: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 15:40:01.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999731s
Mar  6 15:40:02.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997347964s
Mar  6 15:40:03.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99394086s
Mar  6 15:40:04.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990745599s
Mar  6 15:40:05.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986851606s
Mar  6 15:40:06.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983010001s
Mar  6 15:40:07.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979760896s
Mar  6 15:40:08.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976253987s
Mar  6 15:40:09.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.972764954s
Mar  6 15:40:10.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 969.309798ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-h8cl2
Mar  6 15:40:11.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:40:11.487: INFO: stderr: ""
Mar  6 15:40:11.487: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:40:11.487: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 15:40:11.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:40:11.699: INFO: stderr: ""
Mar  6 15:40:11.699: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:40:11.699: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 15:40:11.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-h8cl2 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 15:40:11.910: INFO: stderr: ""
Mar  6 15:40:11.910: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 15:40:11.910: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 15:40:11.910: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 15:40:41.920: INFO: Deleting all statefulset in ns e2e-tests-statefulset-h8cl2
Mar  6 15:40:41.922: INFO: Scaling statefulset ss to 0
Mar  6 15:40:41.929: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 15:40:41.931: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:40:41.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-h8cl2" for this suite.
Mar  6 15:40:47.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:40:47.968: INFO: namespace: e2e-tests-statefulset-h8cl2, resource: bindings, ignored listing per whitelist
Mar  6 15:40:48.015: INFO: namespace e2e-tests-statefulset-h8cl2 deletion completed in 6.068737701s

• [SLOW TEST:98.166 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:40:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-flgpf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:40:48.199: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-flgpf" to be "success or failure"
Mar  6 15:40:48.201: INFO: Pod "downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630125ms
Mar  6 15:40:50.204: INFO: Pod "downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004428413s
STEP: Saw pod success
Mar  6 15:40:50.204: INFO: Pod "downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:40:50.206: INFO: Trying to get logs from node themisto pod downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:40:50.225: INFO: Waiting for pod downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007 to disappear
Mar  6 15:40:50.227: INFO: Pod downwardapi-volume-36af87ec-4026-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:40:50.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-flgpf" for this suite.
Mar  6 15:40:56.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:40:56.277: INFO: namespace: e2e-tests-downward-api-flgpf, resource: bindings, ignored listing per whitelist
Mar  6 15:40:56.313: INFO: namespace e2e-tests-downward-api-flgpf deletion completed in 6.083156346s

• [SLOW TEST:8.298 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:40:56.313: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-6xszk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0306 15:41:06.553248      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 15:41:06.553: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:41:06.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6xszk" for this suite.
Mar  6 15:41:12.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:41:12.575: INFO: namespace: e2e-tests-gc-6xszk, resource: bindings, ignored listing per whitelist
Mar  6 15:41:12.621: INFO: namespace e2e-tests-gc-6xszk deletion completed in 6.064312216s

• [SLOW TEST:16.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:41:12.621: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-srdmd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-dlzzq
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Mar  6 15:41:22.148: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-rs8qn
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:41:39.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-srdmd" for this suite.
Mar  6 15:41:45.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:41:45.191: INFO: namespace: e2e-tests-namespaces-srdmd, resource: bindings, ignored listing per whitelist
Mar  6 15:41:45.206: INFO: namespace e2e-tests-namespaces-srdmd deletion completed in 6.067640485s
STEP: Destroying namespace "e2e-tests-nsdeletetest-dlzzq" for this suite.
Mar  6 15:41:45.208: INFO: Namespace e2e-tests-nsdeletetest-dlzzq was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-rs8qn" for this suite.
Mar  6 15:41:51.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:41:51.250: INFO: namespace: e2e-tests-nsdeletetest-rs8qn, resource: bindings, ignored listing per whitelist
Mar  6 15:41:51.283: INFO: namespace e2e-tests-nsdeletetest-rs8qn deletion completed in 6.075042808s

• [SLOW TEST:38.662 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:41:51.283: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qfqsq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 15:41:51.481: INFO: Waiting up to 5m0s for pod "pod-5c67a997-4026-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-qfqsq" to be "success or failure"
Mar  6 15:41:51.483: INFO: Pod "pod-5c67a997-4026-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677002ms
Mar  6 15:41:53.485: INFO: Pod "pod-5c67a997-4026-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004039131s
STEP: Saw pod success
Mar  6 15:41:53.485: INFO: Pod "pod-5c67a997-4026-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:41:53.487: INFO: Trying to get logs from node kronos pod pod-5c67a997-4026-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:41:53.504: INFO: Waiting for pod pod-5c67a997-4026-11e9-9071-0a58ac100007 to disappear
Mar  6 15:41:53.506: INFO: Pod pod-5c67a997-4026-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:41:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qfqsq" for this suite.
Mar  6 15:41:59.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:41:59.531: INFO: namespace: e2e-tests-emptydir-qfqsq, resource: bindings, ignored listing per whitelist
Mar  6 15:41:59.579: INFO: namespace e2e-tests-emptydir-qfqsq deletion completed in 6.06993366s

• [SLOW TEST:8.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:41:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-r6wcr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:41:59.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:41:59.837: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 15:41:59.837: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  6 15:41:59.840: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar  6 15:41:59.859: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  6 15:41:59.867: INFO: scanned /root for discovery docs: <nil>
Mar  6 15:41:59.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:42:15.605: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 15:42:15.605: INFO: stdout: "Created e2e-test-nginx-rc-b93adee829269864f44c861d3321620b\nScaling up e2e-test-nginx-rc-b93adee829269864f44c861d3321620b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b93adee829269864f44c861d3321620b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b93adee829269864f44c861d3321620b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  6 15:42:15.605: INFO: stdout: "Created e2e-test-nginx-rc-b93adee829269864f44c861d3321620b\nScaling up e2e-test-nginx-rc-b93adee829269864f44c861d3321620b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b93adee829269864f44c861d3321620b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b93adee829269864f44c861d3321620b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  6 15:42:15.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:42:15.686: INFO: stderr: ""
Mar  6 15:42:15.686: INFO: stdout: "e2e-test-nginx-rc-b93adee829269864f44c861d3321620b-8msc7 "
Mar  6 15:42:15.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods e2e-test-nginx-rc-b93adee829269864f44c861d3321620b-8msc7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:42:15.762: INFO: stderr: ""
Mar  6 15:42:15.762: INFO: stdout: "true"
Mar  6 15:42:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods e2e-test-nginx-rc-b93adee829269864f44c861d3321620b-8msc7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:42:15.835: INFO: stderr: ""
Mar  6 15:42:15.835: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  6 15:42:15.835: INFO: e2e-test-nginx-rc-b93adee829269864f44c861d3321620b-8msc7 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Mar  6 15:42:15.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r6wcr'
Mar  6 15:42:15.912: INFO: stderr: ""
Mar  6 15:42:15.912: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:42:15.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r6wcr" for this suite.
Mar  6 15:42:21.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:42:21.960: INFO: namespace: e2e-tests-kubectl-r6wcr, resource: bindings, ignored listing per whitelist
Mar  6 15:42:21.988: INFO: namespace e2e-tests-kubectl-r6wcr deletion completed in 6.072449803s

• [SLOW TEST:22.409 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:42:21.988: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xbf69
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 15:42:22.178: INFO: Waiting up to 5m0s for pod "pod-6eb3a36a-4026-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-xbf69" to be "success or failure"
Mar  6 15:42:22.179: INFO: Pod "pod-6eb3a36a-4026-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.444883ms
Mar  6 15:42:24.182: INFO: Pod "pod-6eb3a36a-4026-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004137427s
STEP: Saw pod success
Mar  6 15:42:24.182: INFO: Pod "pod-6eb3a36a-4026-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:42:24.184: INFO: Trying to get logs from node themisto pod pod-6eb3a36a-4026-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 15:42:24.202: INFO: Waiting for pod pod-6eb3a36a-4026-11e9-9071-0a58ac100007 to disappear
Mar  6 15:42:24.203: INFO: Pod pod-6eb3a36a-4026-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:42:24.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xbf69" for this suite.
Mar  6 15:42:30.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:42:30.266: INFO: namespace: e2e-tests-emptydir-xbf69, resource: bindings, ignored listing per whitelist
Mar  6 15:42:30.287: INFO: namespace e2e-tests-emptydir-xbf69 deletion completed in 6.080672506s

• [SLOW TEST:8.299 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:42:30.287: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-vzzwx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:42:30.492: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73a7cac7-4026-11e9-b3f2-0cc47aaad1b4", Controller:(*bool)(0xc0005992de), BlockOwnerDeletion:(*bool)(0xc0005992df)}}
Mar  6 15:42:30.504: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"73a59338-4026-11e9-b3f2-0cc47aaad1b4", Controller:(*bool)(0xc0014edd76), BlockOwnerDeletion:(*bool)(0xc0014edd77)}}
Mar  6 15:42:30.509: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"73a69d9c-4026-11e9-b3f2-0cc47aaad1b4", Controller:(*bool)(0xc000d1570e), BlockOwnerDeletion:(*bool)(0xc000d1570f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:42:35.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vzzwx" for this suite.
Mar  6 15:42:41.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:42:41.548: INFO: namespace: e2e-tests-gc-vzzwx, resource: bindings, ignored listing per whitelist
Mar  6 15:42:41.600: INFO: namespace e2e-tests-gc-vzzwx deletion completed in 6.07906169s

• [SLOW TEST:11.313 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:42:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rzw98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 15:42:41.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-rzw98'
Mar  6 15:42:41.879: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 15:42:41.879: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  6 15:42:41.884: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-5bkpx]
Mar  6 15:42:41.884: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-5bkpx" in namespace "e2e-tests-kubectl-rzw98" to be "running and ready"
Mar  6 15:42:41.886: INFO: Pod "e2e-test-nginx-rc-5bkpx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529779ms
Mar  6 15:42:43.890: INFO: Pod "e2e-test-nginx-rc-5bkpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005763355s
Mar  6 15:42:43.890: INFO: Pod "e2e-test-nginx-rc-5bkpx" satisfied condition "running and ready"
Mar  6 15:42:43.890: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-5bkpx]
Mar  6 15:42:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-rzw98'
Mar  6 15:42:44.024: INFO: stderr: ""
Mar  6 15:42:44.024: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Mar  6 15:42:44.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-rzw98'
Mar  6 15:42:44.099: INFO: stderr: ""
Mar  6 15:42:44.099: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:42:44.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rzw98" for this suite.
Mar  6 15:43:06.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:43:06.135: INFO: namespace: e2e-tests-kubectl-rzw98, resource: bindings, ignored listing per whitelist
Mar  6 15:43:06.173: INFO: namespace e2e-tests-kubectl-rzw98 deletion completed in 22.070470199s

• [SLOW TEST:24.572 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:43:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-b9tjv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar  6 15:43:06.351: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  6 15:43:06.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:06.509: INFO: stderr: ""
Mar  6 15:43:06.509: INFO: stdout: "service/redis-slave created\n"
Mar  6 15:43:06.509: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  6 15:43:06.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:06.651: INFO: stderr: ""
Mar  6 15:43:06.651: INFO: stdout: "service/redis-master created\n"
Mar  6 15:43:06.651: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  6 15:43:06.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:06.794: INFO: stderr: ""
Mar  6 15:43:06.794: INFO: stdout: "service/frontend created\n"
Mar  6 15:43:06.794: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  6 15:43:06.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:06.925: INFO: stderr: ""
Mar  6 15:43:06.925: INFO: stdout: "deployment.extensions/frontend created\n"
Mar  6 15:43:06.925: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  6 15:43:06.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:07.071: INFO: stderr: ""
Mar  6 15:43:07.071: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar  6 15:43:07.071: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  6 15:43:07.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:43:07.209: INFO: stderr: ""
Mar  6 15:43:07.209: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar  6 15:43:07.209: INFO: Waiting for all frontend pods to be Running.
Mar  6 15:44:37.262: INFO: Waiting for frontend to serve content.
Mar  6 15:44:37.279: INFO: Trying to add a new entry to the guestbook.
Mar  6 15:44:42.295: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-master:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-mas...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Str in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar  6 15:44:47.314: INFO: Verifying that added entry can be retrieved.
Mar  6 15:44:47.323: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  6 15:44:52.335: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  6 15:44:57.344: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  6 15:45:02.361: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  6 15:45:07.378: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Mar  6 15:45:12.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.550: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.550: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 15:45:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.640: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.640: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 15:45:12.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.732: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.732: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 15:45:12.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.809: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.809: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 15:45:12.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.889: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.889: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 15:45:12.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b9tjv'
Mar  6 15:45:12.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:45:12.962: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:45:12.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b9tjv" for this suite.
Mar  6 15:45:54.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:45:55.015: INFO: namespace: e2e-tests-kubectl-b9tjv, resource: bindings, ignored listing per whitelist
Mar  6 15:45:55.031: INFO: namespace e2e-tests-kubectl-b9tjv deletion completed in 42.066647345s

• [SLOW TEST:168.858 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:45:55.031: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-fg7wb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-fg7wb/configmap-test-edaeafa0-4026-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:45:55.220: INFO: Waiting up to 5m0s for pod "pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-fg7wb" to be "success or failure"
Mar  6 15:45:55.222: INFO: Pod "pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484936ms
Mar  6 15:45:57.224: INFO: Pod "pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004093045s
STEP: Saw pod success
Mar  6 15:45:57.224: INFO: Pod "pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:45:57.226: INFO: Trying to get logs from node kronos pod pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007 container env-test: <nil>
STEP: delete the pod
Mar  6 15:45:57.245: INFO: Waiting for pod pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007 to disappear
Mar  6 15:45:57.247: INFO: Pod pod-configmaps-edaf64d8-4026-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:45:57.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fg7wb" for this suite.
Mar  6 15:46:03.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:46:03.298: INFO: namespace: e2e-tests-configmap-fg7wb, resource: bindings, ignored listing per whitelist
Mar  6 15:46:03.326: INFO: namespace e2e-tests-configmap-fg7wb deletion completed in 6.074638095s

• [SLOW TEST:8.294 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:46:03.326: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vzms2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-f29fb7f9-4026-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 15:46:03.511: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-vzms2" to be "success or failure"
Mar  6 15:46:03.513: INFO: Pod "pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691563ms
Mar  6 15:46:05.516: INFO: Pod "pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004853963s
STEP: Saw pod success
Mar  6 15:46:05.516: INFO: Pod "pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:46:05.519: INFO: Trying to get logs from node sponde pod pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 15:46:05.539: INFO: Waiting for pod pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007 to disappear
Mar  6 15:46:05.541: INFO: Pod pod-projected-secrets-f2a077a5-4026-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:46:05.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vzms2" for this suite.
Mar  6 15:46:11.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:46:11.593: INFO: namespace: e2e-tests-projected-vzms2, resource: bindings, ignored listing per whitelist
Mar  6 15:46:11.611: INFO: namespace e2e-tests-projected-vzms2 deletion completed in 6.066171417s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:46:11.611: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-hwfsm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-hwfsm
Mar  6 15:46:13.795: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-hwfsm
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 15:46:13.797: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:50:14.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hwfsm" for this suite.
Mar  6 15:50:20.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:50:20.175: INFO: namespace: e2e-tests-container-probe-hwfsm, resource: bindings, ignored listing per whitelist
Mar  6 15:50:20.224: INFO: namespace e2e-tests-container-probe-hwfsm deletion completed in 6.080461483s

• [SLOW TEST:248.613 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:50:20.224: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l525l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:50:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 version --client'
Mar  6 15:50:20.441: INFO: stderr: ""
Mar  6 15:50:20.441: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar  6 15:50:20.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-l525l'
Mar  6 15:50:20.582: INFO: stderr: ""
Mar  6 15:50:20.583: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  6 15:50:20.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-l525l'
Mar  6 15:50:20.720: INFO: stderr: ""
Mar  6 15:50:20.720: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 15:50:21.728: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:50:21.728: INFO: Found 0 / 1
Mar  6 15:50:22.722: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:50:22.722: INFO: Found 1 / 1
Mar  6 15:50:22.722: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 15:50:22.725: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:50:22.725: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 15:50:22.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 describe pod redis-master-78qbh --namespace=e2e-tests-kubectl-l525l'
Mar  6 15:50:22.818: INFO: stderr: ""
Mar  6 15:50:22.818: INFO: stdout: "Name:           redis-master-78qbh\nNamespace:      e2e-tests-kubectl-l525l\nNode:           themisto/10.100.96.46\nStart Time:     Wed, 06 Mar 2019 15:51:15 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             172.16.10.40\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c1f9c6eac92a64363f0d9efd634de48091a63b6b3367bfcb4144300c21942fff\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 Mar 2019 15:51:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vkrwf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vkrwf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vkrwf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason            Age                            From               Message\n  ----     ------            ----                           ----               -------\n  Normal   Scheduled         7s                             default-scheduler  Successfully assigned e2e-tests-kubectl-l525l/redis-master-78qbh to themisto\n  Normal   Pulled            <invalid>                      kubelet, themisto  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal   Created           <invalid>                      kubelet, themisto  Created container\n  Normal   Started           <invalid>                      kubelet, themisto  Started container\n  Warning  DNSConfigForming  <invalid> (x3 over <invalid>)  kubelet, themisto  Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-kubectl-l525l.svc.cluster.local svc.cluster.local cluster.local qa.suse.cz suse.cz suse.de\n"
Mar  6 15:50:22.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 describe rc redis-master --namespace=e2e-tests-kubectl-l525l'
Mar  6 15:50:22.907: INFO: stderr: ""
Mar  6 15:50:22.907: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-l525l\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-78qbh\n"
Mar  6 15:50:22.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 describe service redis-master --namespace=e2e-tests-kubectl-l525l'
Mar  6 15:50:22.989: INFO: stderr: ""
Mar  6 15:50:22.989: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-l525l\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.24.206.106\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.10.40:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  6 15:50:22.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 describe node erlara'
Mar  6 15:50:23.084: INFO: stderr: ""
Mar  6 15:50:23.084: INFO: stdout: "Name:               erlara\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=erlara\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"46:01:73:ad:04:11\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.100.96.39\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Mar 2019 15:03:34 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 06 Mar 2019 15:50:03 +0000   Wed, 06 Mar 2019 15:03:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 06 Mar 2019 15:50:03 +0000   Wed, 06 Mar 2019 15:03:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 06 Mar 2019 15:50:03 +0000   Wed, 06 Mar 2019 15:03:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 06 Mar 2019 15:50:03 +0000   Wed, 06 Mar 2019 15:03:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.100.96.39\n  Hostname:    erlara\nCapacity:\n cpu:                32\n ephemeral-storage:  30Gi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             264025084Ki\n pods:               110\nAllocatable:\n cpu:                32\n ephemeral-storage:  28991029200\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             263922684Ki\n pods:               110\nSystem Info:\n Machine ID:                 420c657df5624baea364fafaea01960c\n System UUID:                00000000-0000-0000-0000-0CC47A6C40E2\n Boot ID:                    579a9c7c-1e1f-4b61-83b1-2e9e04cea63e\n Kernel Version:             4.12.14-25.28-default\n OS Image:                   SUSE CaaS Platform 4.0\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.1\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     172.16.8.0/23\nNon-terminated Pods:         (3 in total)\n  Namespace                  Name                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                    ------------  ----------  ---------------  -------------  ---\n  kube-system                dex-6dddd99454-9hmfk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                haproxy-erlara          0 (0%)        0 (0%)      128Mi (0%)       128Mi (0%)     45m\n  kube-system                kube-flannel-zmp4d      0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                0 (0%)      0 (0%)\n  memory             128Mi (0%)  128Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type     Reason                    Age                From                Message\n  ----     ------                    ----               ----                -------\n  Normal   Starting                  47m                kubelet, erlara     Starting kubelet.\n  Normal   NodeAllocatableEnforced   47m                kubelet, erlara     Updated Node Allocatable limit across pods\n  Normal   NodeHasNoDiskPressure     47m (x7 over 47m)  kubelet, erlara     Node erlara status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      47m (x7 over 47m)  kubelet, erlara     Node erlara status is now: NodeHasSufficientPID\n  Warning  CheckLimitsForResolvConf  47m                kubelet, erlara     Resolv.conf file '/etc/resolv.conf' contains search line consisting of more than 3 domains!\n  Normal   NodeHasSufficientMemory   47m (x8 over 47m)  kubelet, erlara     Node erlara status is now: NodeHasSufficientMemory\n  Normal   Starting                  45m                kube-proxy, erlara  Starting kube-proxy.\n"
Mar  6 15:50:23.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 describe namespace e2e-tests-kubectl-l525l'
Mar  6 15:50:23.159: INFO: stderr: ""
Mar  6 15:50:23.159: INFO: stdout: "Name:         e2e-tests-kubectl-l525l\nLabels:       e2e-framework=kubectl\n              e2e-run=315bbb91-4022-11e9-9071-0a58ac100007\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:50:23.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l525l" for this suite.
Mar  6 15:50:45.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:50:45.179: INFO: namespace: e2e-tests-kubectl-l525l, resource: bindings, ignored listing per whitelist
Mar  6 15:50:45.232: INFO: namespace e2e-tests-kubectl-l525l deletion completed in 22.070124115s

• [SLOW TEST:25.008 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:50:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-j97f4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:50:45.409: INFO: Creating deployment "nginx-deployment"
Mar  6 15:50:45.414: INFO: Waiting for observed generation 1
Mar  6 15:50:47.418: INFO: Waiting for all required pods to come up
Mar  6 15:50:47.421: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  6 15:50:47.421: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  6 15:50:47.425: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  6 15:50:47.432: INFO: Updating deployment nginx-deployment
Mar  6 15:50:47.432: INFO: Waiting for observed generation 2
Mar  6 15:50:49.436: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  6 15:50:49.437: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  6 15:50:49.438: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 15:50:49.444: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  6 15:50:49.444: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  6 15:50:49.445: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 15:50:49.448: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  6 15:50:49.448: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  6 15:50:49.454: INFO: Updating deployment nginx-deployment
Mar  6 15:50:49.454: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  6 15:50:49.461: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  6 15:50:49.463: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 15:50:51.468: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-j97f4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j97f4/deployments/nginx-deployment,UID:9aa7b6bb-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59278,Generation:3,CreationTimestamp:2019-03-06 15:50:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:12,UnavailableReplicas:21,Conditions:[{Available False 2019-03-06 15:50:44 +0000 UTC 2019-03-06 15:50:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-06 15:50:46 +0000 UTC 2019-03-06 15:50:40 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:12,CollisionCount:nil,},}

Mar  6 15:50:51.470: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-j97f4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j97f4/replicasets/nginx-deployment-65bbdb5f8,UID:92c56065-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59177,Generation:3,CreationTimestamp:2019-03-06 15:50:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9aa7b6bb-4027-11e9-b3f2-0cc47aaad1b4 0xc0022e0117 0xc0022e0118}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 15:50:51.470: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  6 15:50:51.471: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-j97f4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j97f4/replicasets/nginx-deployment-555b55d965,UID:9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59277,Generation:3,CreationTimestamp:2019-03-06 15:50:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9aa7b6bb-4027-11e9-b3f2-0cc47aaad1b4 0xc0022e0027 0xc0022e0028}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:12,AvailableReplicas:12,Conditions:[],},}
Mar  6 15:50:51.476: INFO: Pod "nginx-deployment-555b55d965-45vks" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-45vks,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-45vks,UID:93fe9b7a-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59237,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2c167 0xc001c2c168}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2c1e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2c3a0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.476: INFO: Pod "nginx-deployment-555b55d965-55ggx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-55ggx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-55ggx,UID:9a25b786-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59255,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2c557 0xc001c2c558}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2c5d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2c5f0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.235,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://27a869599b87210c41dc453b006a1474524428f7f2014dd02a208c0c669f05af}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.476: INFO: Pod "nginx-deployment-555b55d965-6sz6h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6sz6h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-6sz6h,UID:9aab303f-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:58963,Generation:0,CreationTimestamp:2019-03-06 15:50:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2c6a7 0xc001c2c6a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2c750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2c770}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:30 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.192,StartTime:2019-03-06 15:50:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ec8a940bc039d7de417376d7b348f9ba6849f201903422fc4765c3c2466b68c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.476: INFO: Pod "nginx-deployment-555b55d965-7rcxr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7rcxr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-7rcxr,UID:9a27da48-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59279,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2c837 0xc001c2c838}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2c8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2c8d0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-7x9d6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7x9d6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-7x9d6,UID:9a24094d-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59191,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2c9e7 0xc001c2c9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2ca60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2ca80}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-8lknl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8lknl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-8lknl,UID:94006986-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59267,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2cb27 0xc001c2cb28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2cba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2cbc0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.236,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://0d0cfceb56d73b6921b8c186d491a6d0b8f318002d1f99fe6a892d860aad0d1b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-8xlx5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8xlx5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-8xlx5,UID:9d143346-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59198,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2cd07 0xc001c2cd08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2cd80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2cda0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-c4csh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c4csh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-c4csh,UID:97c20369-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59005,Generation:0,CreationTimestamp:2019-03-06 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2cef7 0xc001c2cef8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2cf70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2cf90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:45 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.230,StartTime:2019-03-06 15:50:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7e26f22bd3c5778d4c65bb94846479699e89c52af3d6a49afa6936c93f559873}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-f8f9r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f8f9r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-f8f9r,UID:91992a45-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59018,Generation:0,CreationTimestamp:2019-03-06 15:50:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2d047 0xc001c2d048}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2d160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2d180}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:30 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.193,StartTime:2019-03-06 15:50:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6833bca7af696b926c1c0c268bba62cf7ded002399e490b08ca57b5e800acdc3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-ftnkw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ftnkw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-ftnkw,UID:9d17f7bc-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59272,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2d237 0xc001c2d238}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2d2c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2d360}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:34 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.200,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://3d7b1a8c61dd9ae337b0fa56f9135d52d25f9999f27ed18173cdd5037169a9d6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-gqqp2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gqqp2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-gqqp2,UID:9400724d-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59275,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2d417 0xc001c2d418}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2d490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2d4b0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:34 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.201,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://4245751bf37b3a867f190d03acf22a766d01f6908eb917260375ca6597563dd1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-k25b7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-k25b7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-k25b7,UID:9d15ecfa-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59196,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2d5d7 0xc001c2d5d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2d650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2d680}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-kjzc4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kjzc4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-kjzc4,UID:9197d5c3-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:58993,Generation:0,CreationTimestamp:2019-03-06 15:50:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc001c2d727 0xc001c2d728}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2d7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2d7c0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:172.16.10.41,StartTime:2019-03-06 15:51:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:51:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://147409a7fde7946740fd20945aac8b65e6f6c7f193f7f149edef7622588cbf01}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.477: INFO: Pod "nginx-deployment-555b55d965-lj9xd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lj9xd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-lj9xd,UID:9ab3748a-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59024,Generation:0,CreationTimestamp:2019-03-06 15:50:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d4127 0xc0011d4128}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d42e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d4300}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:30 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.195,StartTime:2019-03-06 15:50:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://031f8b4afc4b66cc0c1079a08baaa9aeac3c5bfb6e66ba9d73495ff7132b430b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-lzvpg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lzvpg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-lzvpg,UID:93fb1f24-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59159,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d44a7 0xc0011d44a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d45b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d45d0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-mbd75" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mbd75,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-mbd75,UID:9d17e362-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59261,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d4677 0xc0011d4678}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d4720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d4740}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-qq4lj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qq4lj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-qq4lj,UID:97be0d8e-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59008,Generation:0,CreationTimestamp:2019-03-06 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d4af7 0xc0011d4af8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d4b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d4b90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:45 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.229,StartTime:2019-03-06 15:50:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7ce833f68eb978bc8a7108b71f9e24db5d8ff8672dbb320c3879c424d8779afa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-rh77k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rh77k,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-rh77k,UID:93fe8e7d-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59182,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d4ce7 0xc0011d4ce8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d4d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d4db0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:34 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-sdsc2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sdsc2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-sdsc2,UID:9ab212bc-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59020,Generation:0,CreationTimestamp:2019-03-06 15:50:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d4f97 0xc0011d4f98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d5050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d5070}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.194,StartTime:2019-03-06 15:50:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://d26d7e157065f469366d89bb44cb6087b74b88c40c349c4b5f3557a84453b970}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-555b55d965-sgq9r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sgq9r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-555b55d965-sgq9r,UID:97c36635-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59001,Generation:0,CreationTimestamp:2019-03-06 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9aa97bc5-4027-11e9-b3f2-0cc47aaad1b4 0xc0011d5267 0xc0011d5268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d5310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d5330}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.231,StartTime:2019-03-06 15:50:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 15:50:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://11cfd0b6097c8a1508b673072e51954975be94b0a2c893fd8822ed79e978d857}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-65bbdb5f8-2pknh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2pknh,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-2pknh,UID:98eeb7ac-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59244,Generation:0,CreationTimestamp:2019-03-06 15:50:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc0011d5437 0xc0011d5438}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d54b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d55c0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:32 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:172.16.4.196,StartTime:2019-03-06 15:50:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-65bbdb5f8-4ds7m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4ds7m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-4ds7m,UID:9d177d24-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59190,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc0011d5690 0xc0011d5691}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011d5720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011d5740}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-65bbdb5f8-76nhw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-76nhw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-76nhw,UID:9be62804-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59081,Generation:0,CreationTimestamp:2019-03-06 15:50:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e62570 0xc001e62571}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e62610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e62630}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:32 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:,StartTime:2019-03-06 15:50:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.478: INFO: Pod "nginx-deployment-65bbdb5f8-8fl7v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8fl7v,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-8fl7v,UID:9a2764f0-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59251,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e62770 0xc001e62771}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e62a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e62ab0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-dsrl7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dsrl7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-dsrl7,UID:98f7bc88-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59082,Generation:0,CreationTimestamp:2019-03-06 15:50:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e62c00 0xc001e62c01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e62c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e62ca0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:32 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:42 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-g6d7t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-g6d7t,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-g6d7t,UID:93fe0f42-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59218,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e62e20 0xc001e62e21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e631c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e631e0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:34 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:,StartTime:2019-03-06 15:51:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-kf8bc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kf8bc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-kf8bc,UID:9a23a237-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59173,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e632a0 0xc001e632a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e63410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63430}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-kkcnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kkcnz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-kkcnz,UID:9bdefd45-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59245,Generation:0,CreationTimestamp:2019-03-06 15:50:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e634e0 0xc001e634e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e63750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63770}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:47 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:172.16.2.232,StartTime:2019-03-06 15:50:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-l4rnv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l4rnv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-l4rnv,UID:92c654ac-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59180,Generation:0,CreationTimestamp:2019-03-06 15:50:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e63840 0xc001e63841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e638c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63a50}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:51:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:47 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:172.16.10.44,StartTime:2019-03-06 15:51:42 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-lpbr8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lpbr8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-lpbr8,UID:940018d1-4027-11e9-aa7e-0cc47a6c40e2,ResourceVersion:59197,Generation:0,CreationTimestamp:2019-03-06 15:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e63b20 0xc001e63b21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sponde,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e63ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63bc0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:49 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.44,PodIP:,StartTime:2019-03-06 15:50:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-nvkwf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nvkwf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-nvkwf,UID:9a275c0c-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59206,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e63d30 0xc001e63d31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e63db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63dd0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:34 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-s8tgg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-s8tgg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-s8tgg,UID:9a295909-4027-11e9-818a-0cc47aaa4380,ResourceVersion:59229,Generation:0,CreationTimestamp:2019-03-06 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001e63e80 0xc001e63e81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e63f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e63f90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 15:50:51.479: INFO: Pod "nginx-deployment-65bbdb5f8-z928h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-z928h,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j97f4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j97f4/pods/nginx-deployment-65bbdb5f8-z928h,UID:9d15af8e-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59178,Generation:0,CreationTimestamp:2019-03-06 15:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 92c56065-4027-11e9-aa7e-0cc47a6c40e2 0xc001c36050 0xc001c36051}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xqdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xqdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xqdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c360d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c360f0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.42,PodIP:,StartTime:2019-03-06 15:50:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:50:51.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-j97f4" for this suite.
Mar  6 15:50:59.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:50:59.509: INFO: namespace: e2e-tests-deployment-j97f4, resource: bindings, ignored listing per whitelist
Mar  6 15:50:59.560: INFO: namespace e2e-tests-deployment-j97f4 deletion completed in 8.0779062s

• [SLOW TEST:14.328 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:50:59.560: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6lsjn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-a331b632-4027-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:50:59.747: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-6lsjn" to be "success or failure"
Mar  6 15:50:59.748: INFO: Pod "pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511881ms
Mar  6 15:51:01.751: INFO: Pod "pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004027978s
STEP: Saw pod success
Mar  6 15:51:01.751: INFO: Pod "pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:51:01.753: INFO: Trying to get logs from node themisto pod pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:51:01.776: INFO: Waiting for pod pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:51:01.777: INFO: Pod pod-projected-configmaps-a3326ddb-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:51:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6lsjn" for this suite.
Mar  6 15:51:07.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:07.853: INFO: namespace: e2e-tests-projected-6lsjn, resource: bindings, ignored listing per whitelist
Mar  6 15:51:07.856: INFO: namespace e2e-tests-projected-6lsjn deletion completed in 6.075959233s

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:51:07.856: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-6l6wk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  6 15:51:08.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59646,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 15:51:08.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59647,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 15:51:08.049: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59648,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  6 15:51:18.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59687,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 15:51:18.077: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59688,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  6 15:51:18.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6l6wk,SelfLink:/api/v1/namespaces/e2e-tests-watch-6l6wk/configmaps/e2e-watch-test-label-changed,UID:a823dc0c-4027-11e9-b3f2-0cc47aaad1b4,ResourceVersion:59689,Generation:0,CreationTimestamp:2019-03-06 15:51:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:51:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6l6wk" for this suite.
Mar  6 15:51:24.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:24.155: INFO: namespace: e2e-tests-watch-6l6wk, resource: bindings, ignored listing per whitelist
Mar  6 15:51:24.160: INFO: namespace e2e-tests-watch-6l6wk deletion completed in 6.078896501s

• [SLOW TEST:16.303 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:51:24.160: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rbkrq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-rbkrq/configmap-test-b1dc5a78-4027-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:51:24.354: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-rbkrq" to be "success or failure"
Mar  6 15:51:24.356: INFO: Pod "pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664372ms
Mar  6 15:51:26.358: INFO: Pod "pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004181426s
STEP: Saw pod success
Mar  6 15:51:26.358: INFO: Pod "pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:51:26.360: INFO: Trying to get logs from node sponde pod pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007 container env-test: <nil>
STEP: delete the pod
Mar  6 15:51:26.381: INFO: Waiting for pod pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:51:26.382: INFO: Pod pod-configmaps-b1dd25be-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:51:26.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rbkrq" for this suite.
Mar  6 15:51:32.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:32.407: INFO: namespace: e2e-tests-configmap-rbkrq, resource: bindings, ignored listing per whitelist
Mar  6 15:51:32.460: INFO: namespace e2e-tests-configmap-rbkrq deletion completed in 6.074383664s

• [SLOW TEST:8.301 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:51:32.461: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-r8lgd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b6ce51d8-4027-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:51:32.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-r8lgd" to be "success or failure"
Mar  6 15:51:32.651: INFO: Pod "pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434762ms
Mar  6 15:51:34.654: INFO: Pod "pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003970645s
STEP: Saw pod success
Mar  6 15:51:34.654: INFO: Pod "pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:51:34.656: INFO: Trying to get logs from node kronos pod pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:51:34.675: INFO: Waiting for pod pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:51:34.676: INFO: Pod pod-configmaps-b6cf0bd9-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:51:34.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-r8lgd" for this suite.
Mar  6 15:51:40.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:40.720: INFO: namespace: e2e-tests-configmap-r8lgd, resource: bindings, ignored listing per whitelist
Mar  6 15:51:40.759: INFO: namespace e2e-tests-configmap-r8lgd deletion completed in 6.079484754s

• [SLOW TEST:8.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:51:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-dcrpm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-n2jf6
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-rbsvq
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:51:47.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-dcrpm" for this suite.
Mar  6 15:51:53.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:53.327: INFO: namespace: e2e-tests-namespaces-dcrpm, resource: bindings, ignored listing per whitelist
Mar  6 15:51:53.357: INFO: namespace e2e-tests-namespaces-dcrpm deletion completed in 6.075480947s
STEP: Destroying namespace "e2e-tests-nsdeletetest-n2jf6" for this suite.
Mar  6 15:51:53.359: INFO: Namespace e2e-tests-nsdeletetest-n2jf6 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-rbsvq" for this suite.
Mar  6 15:51:59.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:51:59.435: INFO: namespace: e2e-tests-nsdeletetest-rbsvq, resource: bindings, ignored listing per whitelist
Mar  6 15:51:59.439: INFO: namespace e2e-tests-nsdeletetest-rbsvq deletion completed in 6.079795297s

• [SLOW TEST:18.679 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:51:59.439: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jspjp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:51:59.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-jspjp" to be "success or failure"
Mar  6 15:51:59.623: INFO: Pod "downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.620246ms
Mar  6 15:52:01.626: INFO: Pod "downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004512093s
STEP: Saw pod success
Mar  6 15:52:01.626: INFO: Pod "downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:52:01.629: INFO: Trying to get logs from node kronos pod downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:52:01.649: INFO: Waiting for pod downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:52:01.651: INFO: Pod downwardapi-volume-c6e2981f-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:52:01.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jspjp" for this suite.
Mar  6 15:52:07.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:52:07.672: INFO: namespace: e2e-tests-projected-jspjp, resource: bindings, ignored listing per whitelist
Mar  6 15:52:07.728: INFO: namespace e2e-tests-projected-jspjp deletion completed in 6.073488917s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:52:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qjrml
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-cbd420de-4027-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:52:07.920: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-qjrml" to be "success or failure"
Mar  6 15:52:07.922: INFO: Pod "pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459542ms
Mar  6 15:52:09.924: INFO: Pod "pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00401044s
STEP: Saw pod success
Mar  6 15:52:09.924: INFO: Pod "pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:52:09.927: INFO: Trying to get logs from node sponde pod pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:52:09.947: INFO: Waiting for pod pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:52:09.949: INFO: Pod pod-projected-configmaps-cbd4e7bb-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:52:09.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qjrml" for this suite.
Mar  6 15:52:15.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:52:15.999: INFO: namespace: e2e-tests-projected-qjrml, resource: bindings, ignored listing per whitelist
Mar  6 15:52:16.028: INFO: namespace e2e-tests-projected-qjrml deletion completed in 6.074390458s

• [SLOW TEST:8.300 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:52:16.028: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lgl4h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 15:52:16.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-lgl4h" to be "success or failure"
Mar  6 15:52:16.209: INFO: Pod "downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.681901ms
Mar  6 15:52:18.212: INFO: Pod "downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004305882s
STEP: Saw pod success
Mar  6 15:52:18.212: INFO: Pod "downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:52:18.214: INFO: Trying to get logs from node themisto pod downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 15:52:18.233: INFO: Waiting for pod downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:52:18.235: INFO: Pod downwardapi-volume-d0c5393e-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:52:18.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lgl4h" for this suite.
Mar  6 15:52:24.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:52:24.279: INFO: namespace: e2e-tests-projected-lgl4h, resource: bindings, ignored listing per whitelist
Mar  6 15:52:24.317: INFO: namespace e2e-tests-projected-lgl4h deletion completed in 6.079311222s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:52:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8khww
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 15:52:24.506: INFO: Waiting up to 5m0s for pod "downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-8khww" to be "success or failure"
Mar  6 15:52:24.508: INFO: Pod "downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70638ms
Mar  6 15:52:26.511: INFO: Pod "downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00438339s
STEP: Saw pod success
Mar  6 15:52:26.511: INFO: Pod "downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:52:26.513: INFO: Trying to get logs from node themisto pod downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 15:52:26.532: INFO: Waiting for pod downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007 to disappear
Mar  6 15:52:26.534: INFO: Pod downward-api-d5b78aa5-4027-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:52:26.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8khww" for this suite.
Mar  6 15:52:32.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:52:32.588: INFO: namespace: e2e-tests-downward-api-8khww, resource: bindings, ignored listing per whitelist
Mar  6 15:52:32.620: INFO: namespace e2e-tests-downward-api-8khww deletion completed in 6.083173367s

• [SLOW TEST:8.303 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:52:32.621: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-46zlk
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-daa9ed8c-4027-11e9-9071-0a58ac100007
STEP: Creating configMap with name cm-test-opt-upd-daa9edd5-4027-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-daa9ed8c-4027-11e9-9071-0a58ac100007
STEP: Updating configmap cm-test-opt-upd-daa9edd5-4027-11e9-9071-0a58ac100007
STEP: Creating configMap with name cm-test-opt-create-daa9edf8-4027-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:52:36.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-46zlk" for this suite.
Mar  6 15:52:58.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:52:58.954: INFO: namespace: e2e-tests-projected-46zlk, resource: bindings, ignored listing per whitelist
Mar  6 15:52:58.957: INFO: namespace e2e-tests-projected-46zlk deletion completed in 22.070389636s

• [SLOW TEST:26.336 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:52:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-f7vpr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-lsw7
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 15:52:59.153: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lsw7" in namespace "e2e-tests-subpath-f7vpr" to be "success or failure"
Mar  6 15:52:59.154: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455295ms
Mar  6 15:53:01.157: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 2.004130177s
Mar  6 15:53:03.160: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 4.007235514s
Mar  6 15:53:05.163: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 6.010123843s
Mar  6 15:53:07.166: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 8.013055578s
Mar  6 15:53:09.169: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 10.016162106s
Mar  6 15:53:11.172: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 12.019222473s
Mar  6 15:53:13.175: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 14.022357199s
Mar  6 15:53:15.178: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 16.025316824s
Mar  6 15:53:17.181: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 18.028096191s
Mar  6 15:53:19.184: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Running", Reason="", readiness=false. Elapsed: 20.03108329s
Mar  6 15:53:21.187: INFO: Pod "pod-subpath-test-downwardapi-lsw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034000586s
STEP: Saw pod success
Mar  6 15:53:21.187: INFO: Pod "pod-subpath-test-downwardapi-lsw7" satisfied condition "success or failure"
Mar  6 15:53:21.189: INFO: Trying to get logs from node themisto pod pod-subpath-test-downwardapi-lsw7 container test-container-subpath-downwardapi-lsw7: <nil>
STEP: delete the pod
Mar  6 15:53:21.207: INFO: Waiting for pod pod-subpath-test-downwardapi-lsw7 to disappear
Mar  6 15:53:21.209: INFO: Pod pod-subpath-test-downwardapi-lsw7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lsw7
Mar  6 15:53:21.209: INFO: Deleting pod "pod-subpath-test-downwardapi-lsw7" in namespace "e2e-tests-subpath-f7vpr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:53:21.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-f7vpr" for this suite.
Mar  6 15:53:27.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:53:27.237: INFO: namespace: e2e-tests-subpath-f7vpr, resource: bindings, ignored listing per whitelist
Mar  6 15:53:27.288: INFO: namespace e2e-tests-subpath-f7vpr deletion completed in 6.074175858s

• [SLOW TEST:28.331 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:53:27.289: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-t2d7j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 15:53:31.503: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:31.505: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:33.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:33.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:35.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:35.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:37.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:37.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:39.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:39.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:41.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:41.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:43.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:43.508: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 15:53:45.505: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 15:53:45.507: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:53:45.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-t2d7j" for this suite.
Mar  6 15:54:07.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:54:07.558: INFO: namespace: e2e-tests-container-lifecycle-hook-t2d7j, resource: bindings, ignored listing per whitelist
Mar  6 15:54:07.587: INFO: namespace e2e-tests-container-lifecycle-hook-t2d7j deletion completed in 22.068360926s

• [SLOW TEST:40.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:54:07.587: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xjxv9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0306 15:54:17.783763      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 15:54:17.783: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:54:17.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xjxv9" for this suite.
Mar  6 15:54:23.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:54:23.817: INFO: namespace: e2e-tests-gc-xjxv9, resource: bindings, ignored listing per whitelist
Mar  6 15:54:23.858: INFO: namespace e2e-tests-gc-xjxv9 deletion completed in 6.071944812s

• [SLOW TEST:16.271 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:54:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-hf8wj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:54:46.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-hf8wj" for this suite.
Mar  6 15:54:52.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:54:52.212: INFO: namespace: e2e-tests-container-runtime-hf8wj, resource: bindings, ignored listing per whitelist
Mar  6 15:54:52.253: INFO: namespace e2e-tests-container-runtime-hf8wj deletion completed in 6.069962792s

• [SLOW TEST:28.395 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:54:52.253: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-wvh7d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:54:52.429: INFO: Creating ReplicaSet my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007
Mar  6 15:54:52.436: INFO: Pod name my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007: Found 0 pods out of 1
Mar  6 15:54:57.438: INFO: Pod name my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007: Found 1 pods out of 1
Mar  6 15:54:57.438: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007" is running
Mar  6 15:54:57.440: INFO: Pod "my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007-fxqcb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:55:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:55:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:55:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 15:54:52 +0000 UTC Reason: Message:}])
Mar  6 15:54:57.440: INFO: Trying to dial the pod
Mar  6 15:55:02.449: INFO: Controller my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007: Got expected result from replica 1 [my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007-fxqcb]: "my-hostname-basic-2de3ea0b-4028-11e9-9071-0a58ac100007-fxqcb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:55:02.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-wvh7d" for this suite.
Mar  6 15:55:08.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:55:08.527: INFO: namespace: e2e-tests-replicaset-wvh7d, resource: bindings, ignored listing per whitelist
Mar  6 15:55:08.529: INFO: namespace e2e-tests-replicaset-wvh7d deletion completed in 6.076966545s

• [SLOW TEST:16.277 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:55:08.530: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bqnhn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-37978b9d-4028-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:55:08.718: INFO: Waiting up to 5m0s for pod "pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-bqnhn" to be "success or failure"
Mar  6 15:55:08.719: INFO: Pod "pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.485593ms
Mar  6 15:55:10.722: INFO: Pod "pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004198375s
STEP: Saw pod success
Mar  6 15:55:10.722: INFO: Pod "pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:55:10.724: INFO: Trying to get logs from node sponde pod pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:55:10.743: INFO: Waiting for pod pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007 to disappear
Mar  6 15:55:10.745: INFO: Pod pod-configmaps-379846fc-4028-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:55:10.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bqnhn" for this suite.
Mar  6 15:55:16.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:55:16.824: INFO: namespace: e2e-tests-configmap-bqnhn, resource: bindings, ignored listing per whitelist
Mar  6 15:55:16.828: INFO: namespace e2e-tests-configmap-bqnhn deletion completed in 6.0790214s

• [SLOW TEST:8.298 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:55:16.828: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vpw22
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:55:17.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vpw22" for this suite.
Mar  6 15:55:39.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:55:39.082: INFO: namespace: e2e-tests-pods-vpw22, resource: bindings, ignored listing per whitelist
Mar  6 15:55:39.095: INFO: namespace e2e-tests-pods-vpw22 deletion completed in 22.072185101s

• [SLOW TEST:22.267 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:55:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-w8dt7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  6 15:55:39.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-w8dt7'
Mar  6 15:55:39.488: INFO: stderr: ""
Mar  6 15:55:39.488: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 15:55:40.491: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:55:40.491: INFO: Found 0 / 1
Mar  6 15:55:41.491: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:55:41.491: INFO: Found 1 / 1
Mar  6 15:55:41.491: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  6 15:55:41.494: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:55:41.494: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 15:55:41.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 patch pod redis-master-q2fcz --namespace=e2e-tests-kubectl-w8dt7 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  6 15:55:41.579: INFO: stderr: ""
Mar  6 15:55:41.579: INFO: stdout: "pod/redis-master-q2fcz patched\n"
STEP: checking annotations
Mar  6 15:55:41.582: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 15:55:41.582: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:55:41.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w8dt7" for this suite.
Mar  6 15:56:03.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:56:03.659: INFO: namespace: e2e-tests-kubectl-w8dt7, resource: bindings, ignored listing per whitelist
Mar  6 15:56:03.659: INFO: namespace e2e-tests-kubectl-w8dt7 deletion completed in 22.074103513s

• [SLOW TEST:24.564 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:56:03.659: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-fpdtd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-587396e8-4028-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 15:56:03.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-fpdtd" to be "success or failure"
Mar  6 15:56:03.848: INFO: Pod "pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.427424ms
Mar  6 15:56:05.852: INFO: Pod "pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00600711s
STEP: Saw pod success
Mar  6 15:56:05.852: INFO: Pod "pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 15:56:05.855: INFO: Trying to get logs from node sponde pod pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 15:56:05.876: INFO: Waiting for pod pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007 to disappear
Mar  6 15:56:05.878: INFO: Pod pod-configmaps-58745c4f-4028-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:56:05.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fpdtd" for this suite.
Mar  6 15:56:11.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:56:11.911: INFO: namespace: e2e-tests-configmap-fpdtd, resource: bindings, ignored listing per whitelist
Mar  6 15:56:11.967: INFO: namespace e2e-tests-configmap-fpdtd deletion completed in 6.085852737s

• [SLOW TEST:8.308 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:56:11.967: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qh5g4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qh5g4
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 15:56:12.144: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 15:56:34.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.215:8080/dial?request=hostName&protocol=http&host=172.16.4.214&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qh5g4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:56:34.215: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:56:34.351: INFO: Waiting for endpoints: map[]
Mar  6 15:56:34.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.215:8080/dial?request=hostName&protocol=http&host=172.16.2.246&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qh5g4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:56:34.353: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:56:34.486: INFO: Waiting for endpoints: map[]
Mar  6 15:56:34.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.215:8080/dial?request=hostName&protocol=http&host=172.16.10.62&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qh5g4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 15:56:34.488: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 15:56:34.609: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:56:34.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qh5g4" for this suite.
Mar  6 15:56:56.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:56:56.651: INFO: namespace: e2e-tests-pod-network-test-qh5g4, resource: bindings, ignored listing per whitelist
Mar  6 15:56:56.683: INFO: namespace e2e-tests-pod-network-test-qh5g4 deletion completed in 22.07044002s

• [SLOW TEST:44.716 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:56:56.683: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fwz8m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  6 15:56:56.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:56:56.991: INFO: stderr: ""
Mar  6 15:56:56.991: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 15:56:56.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:56:57.068: INFO: stderr: ""
Mar  6 15:56:57.068: INFO: stdout: "update-demo-nautilus-s85sv update-demo-nautilus-x9cgq "
Mar  6 15:56:57.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-s85sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:56:57.146: INFO: stderr: ""
Mar  6 15:56:57.146: INFO: stdout: ""
Mar  6 15:56:57.146: INFO: update-demo-nautilus-s85sv is created but not running
Mar  6 15:57:02.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.239: INFO: stderr: ""
Mar  6 15:57:02.239: INFO: stdout: "update-demo-nautilus-s85sv update-demo-nautilus-x9cgq "
Mar  6 15:57:02.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-s85sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.312: INFO: stderr: ""
Mar  6 15:57:02.312: INFO: stdout: "true"
Mar  6 15:57:02.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-s85sv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.380: INFO: stderr: ""
Mar  6 15:57:02.380: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 15:57:02.380: INFO: validating pod update-demo-nautilus-s85sv
Mar  6 15:57:02.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 15:57:02.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 15:57:02.384: INFO: update-demo-nautilus-s85sv is verified up and running
Mar  6 15:57:02.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-x9cgq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.451: INFO: stderr: ""
Mar  6 15:57:02.451: INFO: stdout: "true"
Mar  6 15:57:02.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-x9cgq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.529: INFO: stderr: ""
Mar  6 15:57:02.529: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 15:57:02.529: INFO: validating pod update-demo-nautilus-x9cgq
Mar  6 15:57:02.533: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 15:57:02.533: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 15:57:02.533: INFO: update-demo-nautilus-x9cgq is verified up and running
STEP: using delete to clean up resources
Mar  6 15:57:02.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 15:57:02.605: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 15:57:02.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-fwz8m'
Mar  6 15:57:02.678: INFO: stderr: "No resources found.\n"
Mar  6 15:57:02.678: INFO: stdout: ""
Mar  6 15:57:02.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -l name=update-demo --namespace=e2e-tests-kubectl-fwz8m -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 15:57:02.754: INFO: stderr: ""
Mar  6 15:57:02.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:57:02.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fwz8m" for this suite.
Mar  6 15:57:24.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:57:24.802: INFO: namespace: e2e-tests-kubectl-fwz8m, resource: bindings, ignored listing per whitelist
Mar  6 15:57:24.822: INFO: namespace e2e-tests-kubectl-fwz8m deletion completed in 22.065373638s

• [SLOW TEST:28.139 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:57:24.822: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-bbzgc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:57:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-bbzgc" for this suite.
Mar  6 15:57:52.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:57:52.073: INFO: namespace: e2e-tests-replication-controller-bbzgc, resource: bindings, ignored listing per whitelist
Mar  6 15:57:52.099: INFO: namespace e2e-tests-replication-controller-bbzgc deletion completed in 22.077449367s

• [SLOW TEST:27.277 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:57:52.100: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gs6f6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:58:14.292: INFO: Container started at 2019-03-06 15:58:48 +0000 UTC, pod became ready at 2019-03-06 15:59:07 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:58:14.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gs6f6" for this suite.
Mar  6 15:58:36.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:58:36.347: INFO: namespace: e2e-tests-container-probe-gs6f6, resource: bindings, ignored listing per whitelist
Mar  6 15:58:36.367: INFO: namespace e2e-tests-container-probe-gs6f6 deletion completed in 22.070563783s

• [SLOW TEST:44.267 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:58:36.367: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-x74tn
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-b3792dfa-4028-11e9-9071-0a58ac100007
STEP: Creating secret with name s-test-opt-upd-b3792e2f-4028-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b3792dfa-4028-11e9-9071-0a58ac100007
STEP: Updating secret s-test-opt-upd-b3792e2f-4028-11e9-9071-0a58ac100007
STEP: Creating secret with name s-test-opt-create-b3792e5a-4028-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:58:40.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x74tn" for this suite.
Mar  6 15:59:02.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:59:02.708: INFO: namespace: e2e-tests-secrets-x74tn, resource: bindings, ignored listing per whitelist
Mar  6 15:59:02.708: INFO: namespace e2e-tests-secrets-x74tn deletion completed in 22.069817763s

• [SLOW TEST:26.341 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:59:02.708: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-gtc8g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 15:59:02.880: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 15:59:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gtc8g" for this suite.
Mar  6 15:59:44.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 15:59:44.989: INFO: namespace: e2e-tests-pods-gtc8g, resource: bindings, ignored listing per whitelist
Mar  6 15:59:45.003: INFO: namespace e2e-tests-pods-gtc8g deletion completed in 40.078723294s

• [SLOW TEST:42.295 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 15:59:45.004: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-hsp9z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  6 15:59:45.416: INFO: Pod name wrapped-volume-race-dc848f0a-4028-11e9-9071-0a58ac100007: Found 0 pods out of 5
Mar  6 15:59:50.420: INFO: Pod name wrapped-volume-race-dc848f0a-4028-11e9-9071-0a58ac100007: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dc848f0a-4028-11e9-9071-0a58ac100007 in namespace e2e-tests-emptydir-wrapper-hsp9z, will wait for the garbage collector to delete the pods
Mar  6 16:00:00.503: INFO: Deleting ReplicationController wrapped-volume-race-dc848f0a-4028-11e9-9071-0a58ac100007 took: 8.896492ms
Mar  6 16:00:00.603: INFO: Terminating ReplicationController wrapped-volume-race-dc848f0a-4028-11e9-9071-0a58ac100007 pods took: 100.200181ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 16:00:43.618: INFO: Pod name wrapped-volume-race-ff34ea68-4028-11e9-9071-0a58ac100007: Found 0 pods out of 5
Mar  6 16:00:48.624: INFO: Pod name wrapped-volume-race-ff34ea68-4028-11e9-9071-0a58ac100007: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ff34ea68-4028-11e9-9071-0a58ac100007 in namespace e2e-tests-emptydir-wrapper-hsp9z, will wait for the garbage collector to delete the pods
Mar  6 16:00:58.701: INFO: Deleting ReplicationController wrapped-volume-race-ff34ea68-4028-11e9-9071-0a58ac100007 took: 7.842616ms
Mar  6 16:00:58.802: INFO: Terminating ReplicationController wrapped-volume-race-ff34ea68-4028-11e9-9071-0a58ac100007 pods took: 100.184437ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 16:01:43.617: INFO: Pod name wrapped-volume-race-22f7f118-4029-11e9-9071-0a58ac100007: Found 0 pods out of 5
Mar  6 16:01:48.622: INFO: Pod name wrapped-volume-race-22f7f118-4029-11e9-9071-0a58ac100007: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-22f7f118-4029-11e9-9071-0a58ac100007 in namespace e2e-tests-emptydir-wrapper-hsp9z, will wait for the garbage collector to delete the pods
Mar  6 16:01:58.698: INFO: Deleting ReplicationController wrapped-volume-race-22f7f118-4029-11e9-9071-0a58ac100007 took: 7.928303ms
Mar  6 16:01:58.798: INFO: Terminating ReplicationController wrapped-volume-race-22f7f118-4029-11e9-9071-0a58ac100007 pods took: 100.258893ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:02:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-hsp9z" for this suite.
Mar  6 16:02:52.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:02:52.257: INFO: namespace: e2e-tests-emptydir-wrapper-hsp9z, resource: bindings, ignored listing per whitelist
Mar  6 16:02:52.266: INFO: namespace e2e-tests-emptydir-wrapper-hsp9z deletion completed in 8.078253738s

• [SLOW TEST:187.263 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:02:52.266: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-pstvv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-pstvv
I0306 16:02:52.452559      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-pstvv, replica count: 1
I0306 16:02:53.502924      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 16:02:53.614: INFO: Created: latency-svc-f8zkh
Mar  6 16:02:53.624: INFO: Got endpoints: latency-svc-f8zkh [21.244596ms]
Mar  6 16:02:53.633: INFO: Created: latency-svc-c4fw2
Mar  6 16:02:53.641: INFO: Created: latency-svc-ln42r
Mar  6 16:02:53.648: INFO: Got endpoints: latency-svc-c4fw2 [23.795421ms]
Mar  6 16:02:53.652: INFO: Created: latency-svc-2f6t8
Mar  6 16:02:53.659: INFO: Got endpoints: latency-svc-ln42r [35.528581ms]
Mar  6 16:02:53.662: INFO: Got endpoints: latency-svc-2f6t8 [38.204965ms]
Mar  6 16:02:53.663: INFO: Created: latency-svc-xrwhk
Mar  6 16:02:53.669: INFO: Got endpoints: latency-svc-xrwhk [45.019493ms]
Mar  6 16:02:53.673: INFO: Created: latency-svc-n2ln2
Mar  6 16:02:53.679: INFO: Got endpoints: latency-svc-n2ln2 [55.095703ms]
Mar  6 16:02:53.682: INFO: Created: latency-svc-87m28
Mar  6 16:02:53.688: INFO: Got endpoints: latency-svc-87m28 [64.397ms]
Mar  6 16:02:53.692: INFO: Created: latency-svc-mth7c
Mar  6 16:02:53.698: INFO: Got endpoints: latency-svc-mth7c [74.076222ms]
Mar  6 16:02:53.701: INFO: Created: latency-svc-ktrqb
Mar  6 16:02:53.707: INFO: Got endpoints: latency-svc-ktrqb [83.274108ms]
Mar  6 16:02:53.711: INFO: Created: latency-svc-bsbr6
Mar  6 16:02:53.717: INFO: Got endpoints: latency-svc-bsbr6 [92.600771ms]
Mar  6 16:02:53.720: INFO: Created: latency-svc-sjqrz
Mar  6 16:02:53.726: INFO: Got endpoints: latency-svc-sjqrz [101.780827ms]
Mar  6 16:02:53.729: INFO: Created: latency-svc-5695m
Mar  6 16:02:53.735: INFO: Got endpoints: latency-svc-5695m [111.306735ms]
Mar  6 16:02:53.739: INFO: Created: latency-svc-mp7bj
Mar  6 16:02:53.745: INFO: Got endpoints: latency-svc-mp7bj [120.472008ms]
Mar  6 16:02:53.748: INFO: Created: latency-svc-vv47b
Mar  6 16:02:53.754: INFO: Got endpoints: latency-svc-vv47b [130.373853ms]
Mar  6 16:02:53.759: INFO: Created: latency-svc-9q7bq
Mar  6 16:02:53.768: INFO: Got endpoints: latency-svc-9q7bq [144.320929ms]
Mar  6 16:02:53.769: INFO: Created: latency-svc-lxcqb
Mar  6 16:02:53.779: INFO: Got endpoints: latency-svc-lxcqb [155.289222ms]
Mar  6 16:02:53.780: INFO: Created: latency-svc-nlb2n
Mar  6 16:02:53.786: INFO: Got endpoints: latency-svc-nlb2n [138.636861ms]
Mar  6 16:02:53.790: INFO: Created: latency-svc-5qdkw
Mar  6 16:02:53.797: INFO: Got endpoints: latency-svc-5qdkw [137.076418ms]
Mar  6 16:02:53.800: INFO: Created: latency-svc-k2jmr
Mar  6 16:02:53.805: INFO: Got endpoints: latency-svc-k2jmr [143.077735ms]
Mar  6 16:02:53.809: INFO: Created: latency-svc-8pdxz
Mar  6 16:02:53.815: INFO: Got endpoints: latency-svc-8pdxz [146.342132ms]
Mar  6 16:02:53.819: INFO: Created: latency-svc-z4pg2
Mar  6 16:02:53.825: INFO: Got endpoints: latency-svc-z4pg2 [146.044949ms]
Mar  6 16:02:53.829: INFO: Created: latency-svc-95pk8
Mar  6 16:02:53.835: INFO: Got endpoints: latency-svc-95pk8 [146.075825ms]
Mar  6 16:02:53.838: INFO: Created: latency-svc-4vz97
Mar  6 16:02:53.844: INFO: Got endpoints: latency-svc-4vz97 [145.962104ms]
Mar  6 16:02:53.848: INFO: Created: latency-svc-zkvz2
Mar  6 16:02:53.854: INFO: Got endpoints: latency-svc-zkvz2 [146.497683ms]
Mar  6 16:02:53.858: INFO: Created: latency-svc-6g48w
Mar  6 16:02:53.867: INFO: Got endpoints: latency-svc-6g48w [150.437957ms]
Mar  6 16:02:53.867: INFO: Created: latency-svc-9dqtj
Mar  6 16:02:53.874: INFO: Got endpoints: latency-svc-9dqtj [147.784384ms]
Mar  6 16:02:53.878: INFO: Created: latency-svc-s5fmw
Mar  6 16:02:53.886: INFO: Got endpoints: latency-svc-s5fmw [150.82003ms]
Mar  6 16:02:53.887: INFO: Created: latency-svc-qc4cb
Mar  6 16:02:53.893: INFO: Got endpoints: latency-svc-qc4cb [148.131307ms]
Mar  6 16:02:53.896: INFO: Created: latency-svc-f7cv4
Mar  6 16:02:53.903: INFO: Got endpoints: latency-svc-f7cv4 [148.337808ms]
Mar  6 16:02:53.907: INFO: Created: latency-svc-hc4vs
Mar  6 16:02:53.914: INFO: Got endpoints: latency-svc-hc4vs [145.219282ms]
Mar  6 16:02:53.920: INFO: Created: latency-svc-58wsn
Mar  6 16:02:53.926: INFO: Got endpoints: latency-svc-58wsn [146.583988ms]
Mar  6 16:02:53.930: INFO: Created: latency-svc-pwp9k
Mar  6 16:02:53.937: INFO: Got endpoints: latency-svc-pwp9k [151.05079ms]
Mar  6 16:02:53.940: INFO: Created: latency-svc-mztv7
Mar  6 16:02:53.947: INFO: Got endpoints: latency-svc-mztv7 [150.020383ms]
Mar  6 16:02:53.950: INFO: Created: latency-svc-btsjd
Mar  6 16:02:53.957: INFO: Got endpoints: latency-svc-btsjd [151.338888ms]
Mar  6 16:02:53.961: INFO: Created: latency-svc-7pzjq
Mar  6 16:02:53.968: INFO: Got endpoints: latency-svc-7pzjq [152.915342ms]
Mar  6 16:02:53.973: INFO: Created: latency-svc-4vtbp
Mar  6 16:02:53.979: INFO: Got endpoints: latency-svc-4vtbp [154.070075ms]
Mar  6 16:02:53.983: INFO: Created: latency-svc-n6jq4
Mar  6 16:02:53.989: INFO: Got endpoints: latency-svc-n6jq4 [154.385476ms]
Mar  6 16:02:53.992: INFO: Created: latency-svc-wsxf9
Mar  6 16:02:53.999: INFO: Created: latency-svc-j69h5
Mar  6 16:02:54.007: INFO: Created: latency-svc-bdgkg
Mar  6 16:02:54.014: INFO: Created: latency-svc-2w7h8
Mar  6 16:02:54.023: INFO: Got endpoints: latency-svc-wsxf9 [179.395792ms]
Mar  6 16:02:54.024: INFO: Created: latency-svc-wvpj9
Mar  6 16:02:54.031: INFO: Created: latency-svc-gnwjx
Mar  6 16:02:54.038: INFO: Created: latency-svc-v7b2g
Mar  6 16:02:54.046: INFO: Created: latency-svc-nq4nk
Mar  6 16:02:54.053: INFO: Created: latency-svc-vpx9n
Mar  6 16:02:54.065: INFO: Created: latency-svc-9mzxr
Mar  6 16:02:54.072: INFO: Got endpoints: latency-svc-j69h5 [218.441623ms]
Mar  6 16:02:54.076: INFO: Created: latency-svc-qxhwg
Mar  6 16:02:54.083: INFO: Created: latency-svc-7q28g
Mar  6 16:02:54.090: INFO: Created: latency-svc-mq8tw
Mar  6 16:02:54.097: INFO: Created: latency-svc-rbfhs
Mar  6 16:02:54.105: INFO: Created: latency-svc-tgn9n
Mar  6 16:02:54.112: INFO: Created: latency-svc-wjvqx
Mar  6 16:02:54.122: INFO: Got endpoints: latency-svc-bdgkg [255.001328ms]
Mar  6 16:02:54.122: INFO: Created: latency-svc-c457l
Mar  6 16:02:54.131: INFO: Created: latency-svc-w2mqk
Mar  6 16:02:54.172: INFO: Got endpoints: latency-svc-2w7h8 [298.107244ms]
Mar  6 16:02:54.181: INFO: Created: latency-svc-nbhcf
Mar  6 16:02:54.221: INFO: Got endpoints: latency-svc-wvpj9 [334.761491ms]
Mar  6 16:02:54.230: INFO: Created: latency-svc-lhxhm
Mar  6 16:02:54.271: INFO: Got endpoints: latency-svc-gnwjx [377.774094ms]
Mar  6 16:02:54.280: INFO: Created: latency-svc-2sr48
Mar  6 16:02:54.321: INFO: Got endpoints: latency-svc-v7b2g [418.577453ms]
Mar  6 16:02:54.330: INFO: Created: latency-svc-9txqt
Mar  6 16:02:54.371: INFO: Got endpoints: latency-svc-nq4nk [457.447322ms]
Mar  6 16:02:54.381: INFO: Created: latency-svc-xpwpf
Mar  6 16:02:54.420: INFO: Got endpoints: latency-svc-vpx9n [494.276178ms]
Mar  6 16:02:54.430: INFO: Created: latency-svc-bcfct
Mar  6 16:02:54.471: INFO: Got endpoints: latency-svc-9mzxr [533.366278ms]
Mar  6 16:02:54.481: INFO: Created: latency-svc-nclmq
Mar  6 16:02:54.521: INFO: Got endpoints: latency-svc-qxhwg [574.317493ms]
Mar  6 16:02:54.530: INFO: Created: latency-svc-bht6s
Mar  6 16:02:54.571: INFO: Got endpoints: latency-svc-7q28g [614.080798ms]
Mar  6 16:02:54.581: INFO: Created: latency-svc-f4xjt
Mar  6 16:02:54.621: INFO: Got endpoints: latency-svc-mq8tw [652.678015ms]
Mar  6 16:02:54.631: INFO: Created: latency-svc-2kznk
Mar  6 16:02:54.672: INFO: Got endpoints: latency-svc-rbfhs [692.357931ms]
Mar  6 16:02:54.682: INFO: Created: latency-svc-v7ggg
Mar  6 16:02:54.721: INFO: Got endpoints: latency-svc-tgn9n [731.94824ms]
Mar  6 16:02:54.731: INFO: Created: latency-svc-nwnm9
Mar  6 16:02:54.770: INFO: Got endpoints: latency-svc-wjvqx [746.992074ms]
Mar  6 16:02:54.781: INFO: Created: latency-svc-r6sgz
Mar  6 16:02:54.820: INFO: Got endpoints: latency-svc-c457l [748.084103ms]
Mar  6 16:02:54.830: INFO: Created: latency-svc-b6dg4
Mar  6 16:02:54.872: INFO: Got endpoints: latency-svc-w2mqk [749.546106ms]
Mar  6 16:02:54.881: INFO: Created: latency-svc-82246
Mar  6 16:02:54.921: INFO: Got endpoints: latency-svc-nbhcf [749.274231ms]
Mar  6 16:02:54.931: INFO: Created: latency-svc-zkjk7
Mar  6 16:02:54.971: INFO: Got endpoints: latency-svc-lhxhm [749.784969ms]
Mar  6 16:02:54.981: INFO: Created: latency-svc-7xf5z
Mar  6 16:02:55.021: INFO: Got endpoints: latency-svc-2sr48 [750.745496ms]
Mar  6 16:02:55.031: INFO: Created: latency-svc-fc8n4
Mar  6 16:02:55.071: INFO: Got endpoints: latency-svc-9txqt [749.171482ms]
Mar  6 16:02:55.080: INFO: Created: latency-svc-gr2v2
Mar  6 16:02:55.121: INFO: Got endpoints: latency-svc-xpwpf [750.30679ms]
Mar  6 16:02:55.131: INFO: Created: latency-svc-gf9fq
Mar  6 16:02:55.170: INFO: Got endpoints: latency-svc-bcfct [749.313745ms]
Mar  6 16:02:55.179: INFO: Created: latency-svc-hkxgl
Mar  6 16:02:55.221: INFO: Got endpoints: latency-svc-nclmq [749.984379ms]
Mar  6 16:02:55.230: INFO: Created: latency-svc-ns4xr
Mar  6 16:02:55.271: INFO: Got endpoints: latency-svc-bht6s [749.952742ms]
Mar  6 16:02:55.280: INFO: Created: latency-svc-nxwqk
Mar  6 16:02:55.321: INFO: Got endpoints: latency-svc-f4xjt [749.841681ms]
Mar  6 16:02:55.331: INFO: Created: latency-svc-h8njb
Mar  6 16:02:55.371: INFO: Got endpoints: latency-svc-2kznk [749.675924ms]
Mar  6 16:02:55.380: INFO: Created: latency-svc-7znxd
Mar  6 16:02:55.421: INFO: Got endpoints: latency-svc-v7ggg [749.595308ms]
Mar  6 16:02:55.431: INFO: Created: latency-svc-9jsb7
Mar  6 16:02:55.471: INFO: Got endpoints: latency-svc-nwnm9 [749.765685ms]
Mar  6 16:02:55.480: INFO: Created: latency-svc-4bjns
Mar  6 16:02:55.521: INFO: Got endpoints: latency-svc-r6sgz [750.978673ms]
Mar  6 16:02:55.531: INFO: Created: latency-svc-bfgtp
Mar  6 16:02:55.571: INFO: Got endpoints: latency-svc-b6dg4 [750.31527ms]
Mar  6 16:02:55.580: INFO: Created: latency-svc-xsxrq
Mar  6 16:02:55.621: INFO: Got endpoints: latency-svc-82246 [749.081098ms]
Mar  6 16:02:55.630: INFO: Created: latency-svc-96rh2
Mar  6 16:02:55.671: INFO: Got endpoints: latency-svc-zkjk7 [749.684104ms]
Mar  6 16:02:55.681: INFO: Created: latency-svc-6hnh4
Mar  6 16:02:55.721: INFO: Got endpoints: latency-svc-7xf5z [749.880975ms]
Mar  6 16:02:55.730: INFO: Created: latency-svc-xv67g
Mar  6 16:02:55.771: INFO: Got endpoints: latency-svc-fc8n4 [749.445692ms]
Mar  6 16:02:55.780: INFO: Created: latency-svc-r7md5
Mar  6 16:02:55.821: INFO: Got endpoints: latency-svc-gr2v2 [750.315281ms]
Mar  6 16:02:55.831: INFO: Created: latency-svc-q6rhw
Mar  6 16:02:55.871: INFO: Got endpoints: latency-svc-gf9fq [749.326453ms]
Mar  6 16:02:55.880: INFO: Created: latency-svc-cqxrm
Mar  6 16:02:55.921: INFO: Got endpoints: latency-svc-hkxgl [751.313122ms]
Mar  6 16:02:55.930: INFO: Created: latency-svc-wc97k
Mar  6 16:02:55.971: INFO: Got endpoints: latency-svc-ns4xr [750.47313ms]
Mar  6 16:02:55.981: INFO: Created: latency-svc-wpndg
Mar  6 16:02:56.021: INFO: Got endpoints: latency-svc-nxwqk [750.132682ms]
Mar  6 16:02:56.031: INFO: Created: latency-svc-l2s8g
Mar  6 16:02:56.071: INFO: Got endpoints: latency-svc-h8njb [749.905914ms]
Mar  6 16:02:56.080: INFO: Created: latency-svc-8mfkt
Mar  6 16:02:56.121: INFO: Got endpoints: latency-svc-7znxd [750.432847ms]
Mar  6 16:02:56.130: INFO: Created: latency-svc-xgslq
Mar  6 16:02:56.171: INFO: Got endpoints: latency-svc-9jsb7 [749.622076ms]
Mar  6 16:02:56.180: INFO: Created: latency-svc-zqbfx
Mar  6 16:02:56.221: INFO: Got endpoints: latency-svc-4bjns [749.751394ms]
Mar  6 16:02:56.230: INFO: Created: latency-svc-lwbvj
Mar  6 16:02:56.271: INFO: Got endpoints: latency-svc-bfgtp [749.925252ms]
Mar  6 16:02:56.281: INFO: Created: latency-svc-n7kxp
Mar  6 16:02:56.321: INFO: Got endpoints: latency-svc-xsxrq [750.359249ms]
Mar  6 16:02:56.331: INFO: Created: latency-svc-m2k7t
Mar  6 16:02:56.370: INFO: Got endpoints: latency-svc-96rh2 [749.617796ms]
Mar  6 16:02:56.380: INFO: Created: latency-svc-7vpg4
Mar  6 16:02:56.422: INFO: Got endpoints: latency-svc-6hnh4 [750.764736ms]
Mar  6 16:02:56.431: INFO: Created: latency-svc-hccdv
Mar  6 16:02:56.471: INFO: Got endpoints: latency-svc-xv67g [749.732894ms]
Mar  6 16:02:56.480: INFO: Created: latency-svc-qtf94
Mar  6 16:02:56.520: INFO: Got endpoints: latency-svc-r7md5 [749.453229ms]
Mar  6 16:02:56.530: INFO: Created: latency-svc-gvc6s
Mar  6 16:02:56.571: INFO: Got endpoints: latency-svc-q6rhw [750.24104ms]
Mar  6 16:02:56.581: INFO: Created: latency-svc-zcxt4
Mar  6 16:02:56.621: INFO: Got endpoints: latency-svc-cqxrm [750.112998ms]
Mar  6 16:02:56.630: INFO: Created: latency-svc-9clpl
Mar  6 16:02:56.671: INFO: Got endpoints: latency-svc-wc97k [750.025479ms]
Mar  6 16:02:56.682: INFO: Created: latency-svc-r4ks5
Mar  6 16:02:56.721: INFO: Got endpoints: latency-svc-wpndg [749.500944ms]
Mar  6 16:02:56.730: INFO: Created: latency-svc-j6lgc
Mar  6 16:02:56.770: INFO: Got endpoints: latency-svc-l2s8g [749.176986ms]
Mar  6 16:02:56.779: INFO: Created: latency-svc-pqhtz
Mar  6 16:02:56.821: INFO: Got endpoints: latency-svc-8mfkt [750.415594ms]
Mar  6 16:02:56.831: INFO: Created: latency-svc-c2pn6
Mar  6 16:02:56.871: INFO: Got endpoints: latency-svc-xgslq [749.583389ms]
Mar  6 16:02:56.880: INFO: Created: latency-svc-m8299
Mar  6 16:02:56.920: INFO: Got endpoints: latency-svc-zqbfx [749.210936ms]
Mar  6 16:02:56.930: INFO: Created: latency-svc-rqtc9
Mar  6 16:02:56.971: INFO: Got endpoints: latency-svc-lwbvj [750.763055ms]
Mar  6 16:02:56.981: INFO: Created: latency-svc-xfkmg
Mar  6 16:02:57.021: INFO: Got endpoints: latency-svc-n7kxp [749.433551ms]
Mar  6 16:02:57.030: INFO: Created: latency-svc-bbnjg
Mar  6 16:02:57.071: INFO: Got endpoints: latency-svc-m2k7t [749.562022ms]
Mar  6 16:02:57.080: INFO: Created: latency-svc-v25pq
Mar  6 16:02:57.121: INFO: Got endpoints: latency-svc-7vpg4 [750.25297ms]
Mar  6 16:02:57.131: INFO: Created: latency-svc-pzxp6
Mar  6 16:02:57.171: INFO: Got endpoints: latency-svc-hccdv [749.378395ms]
Mar  6 16:02:57.181: INFO: Created: latency-svc-69p22
Mar  6 16:02:57.221: INFO: Got endpoints: latency-svc-qtf94 [750.602237ms]
Mar  6 16:02:57.231: INFO: Created: latency-svc-2vlcj
Mar  6 16:02:57.272: INFO: Got endpoints: latency-svc-gvc6s [751.269786ms]
Mar  6 16:02:57.281: INFO: Created: latency-svc-gfnpm
Mar  6 16:02:57.321: INFO: Got endpoints: latency-svc-zcxt4 [749.478186ms]
Mar  6 16:02:57.331: INFO: Created: latency-svc-dlc27
Mar  6 16:02:57.371: INFO: Got endpoints: latency-svc-9clpl [750.068341ms]
Mar  6 16:02:57.381: INFO: Created: latency-svc-llfnb
Mar  6 16:02:57.421: INFO: Got endpoints: latency-svc-r4ks5 [750.06821ms]
Mar  6 16:02:57.431: INFO: Created: latency-svc-p985c
Mar  6 16:02:57.470: INFO: Got endpoints: latency-svc-j6lgc [749.497366ms]
Mar  6 16:02:57.480: INFO: Created: latency-svc-pb7wc
Mar  6 16:02:57.521: INFO: Got endpoints: latency-svc-pqhtz [750.348681ms]
Mar  6 16:02:57.530: INFO: Created: latency-svc-fbhzq
Mar  6 16:02:57.571: INFO: Got endpoints: latency-svc-c2pn6 [750.199823ms]
Mar  6 16:02:57.581: INFO: Created: latency-svc-hpvbf
Mar  6 16:02:57.621: INFO: Got endpoints: latency-svc-m8299 [750.500819ms]
Mar  6 16:02:57.631: INFO: Created: latency-svc-sgl4f
Mar  6 16:02:57.671: INFO: Got endpoints: latency-svc-rqtc9 [750.597874ms]
Mar  6 16:02:57.681: INFO: Created: latency-svc-cg2dq
Mar  6 16:02:57.721: INFO: Got endpoints: latency-svc-xfkmg [749.874705ms]
Mar  6 16:02:57.731: INFO: Created: latency-svc-jfb2f
Mar  6 16:02:57.772: INFO: Got endpoints: latency-svc-bbnjg [751.46783ms]
Mar  6 16:02:57.782: INFO: Created: latency-svc-5wq8t
Mar  6 16:02:57.821: INFO: Got endpoints: latency-svc-v25pq [750.010348ms]
Mar  6 16:02:57.830: INFO: Created: latency-svc-2jv6t
Mar  6 16:02:57.871: INFO: Got endpoints: latency-svc-pzxp6 [750.278171ms]
Mar  6 16:02:57.881: INFO: Created: latency-svc-8knfn
Mar  6 16:02:57.921: INFO: Got endpoints: latency-svc-69p22 [750.392544ms]
Mar  6 16:02:57.931: INFO: Created: latency-svc-2894g
Mar  6 16:02:57.971: INFO: Got endpoints: latency-svc-2vlcj [750.0321ms]
Mar  6 16:02:57.981: INFO: Created: latency-svc-wzkq9
Mar  6 16:02:58.022: INFO: Got endpoints: latency-svc-gfnpm [749.970759ms]
Mar  6 16:02:58.031: INFO: Created: latency-svc-gl6f2
Mar  6 16:02:58.071: INFO: Got endpoints: latency-svc-dlc27 [750.506473ms]
Mar  6 16:02:58.081: INFO: Created: latency-svc-td7ph
Mar  6 16:02:58.121: INFO: Got endpoints: latency-svc-llfnb [750.300194ms]
Mar  6 16:02:58.131: INFO: Created: latency-svc-75x5h
Mar  6 16:02:58.171: INFO: Got endpoints: latency-svc-p985c [749.493694ms]
Mar  6 16:02:58.180: INFO: Created: latency-svc-d6vgf
Mar  6 16:02:58.222: INFO: Got endpoints: latency-svc-pb7wc [751.170086ms]
Mar  6 16:02:58.231: INFO: Created: latency-svc-tjlql
Mar  6 16:02:58.271: INFO: Got endpoints: latency-svc-fbhzq [749.946748ms]
Mar  6 16:02:58.280: INFO: Created: latency-svc-c7gfj
Mar  6 16:02:58.321: INFO: Got endpoints: latency-svc-hpvbf [750.034654ms]
Mar  6 16:02:58.331: INFO: Created: latency-svc-pt2gf
Mar  6 16:02:58.371: INFO: Got endpoints: latency-svc-sgl4f [749.742767ms]
Mar  6 16:02:58.381: INFO: Created: latency-svc-sbr28
Mar  6 16:02:58.422: INFO: Got endpoints: latency-svc-cg2dq [750.740644ms]
Mar  6 16:02:58.431: INFO: Created: latency-svc-bsngs
Mar  6 16:02:58.471: INFO: Got endpoints: latency-svc-jfb2f [749.503746ms]
Mar  6 16:02:58.480: INFO: Created: latency-svc-hqjg2
Mar  6 16:02:58.522: INFO: Got endpoints: latency-svc-5wq8t [749.2264ms]
Mar  6 16:02:58.531: INFO: Created: latency-svc-s4hww
Mar  6 16:02:58.571: INFO: Got endpoints: latency-svc-2jv6t [750.485317ms]
Mar  6 16:02:58.581: INFO: Created: latency-svc-mljcj
Mar  6 16:02:58.621: INFO: Got endpoints: latency-svc-8knfn [750.252049ms]
Mar  6 16:02:58.631: INFO: Created: latency-svc-v4kkh
Mar  6 16:02:58.671: INFO: Got endpoints: latency-svc-2894g [749.596508ms]
Mar  6 16:02:58.681: INFO: Created: latency-svc-svkzq
Mar  6 16:02:58.722: INFO: Got endpoints: latency-svc-wzkq9 [750.340613ms]
Mar  6 16:02:58.731: INFO: Created: latency-svc-tv7dx
Mar  6 16:02:58.771: INFO: Got endpoints: latency-svc-gl6f2 [749.022925ms]
Mar  6 16:02:58.780: INFO: Created: latency-svc-hncp8
Mar  6 16:02:58.821: INFO: Got endpoints: latency-svc-td7ph [750.148476ms]
Mar  6 16:02:58.831: INFO: Created: latency-svc-zj2ls
Mar  6 16:02:58.871: INFO: Got endpoints: latency-svc-75x5h [749.27644ms]
Mar  6 16:02:58.880: INFO: Created: latency-svc-j8hp8
Mar  6 16:02:58.922: INFO: Got endpoints: latency-svc-d6vgf [751.074306ms]
Mar  6 16:02:58.932: INFO: Created: latency-svc-wwgqr
Mar  6 16:02:58.971: INFO: Got endpoints: latency-svc-tjlql [749.538578ms]
Mar  6 16:02:58.981: INFO: Created: latency-svc-f9n84
Mar  6 16:02:59.021: INFO: Got endpoints: latency-svc-c7gfj [750.34976ms]
Mar  6 16:02:59.031: INFO: Created: latency-svc-9f4l9
Mar  6 16:02:59.071: INFO: Got endpoints: latency-svc-pt2gf [750.065321ms]
Mar  6 16:02:59.081: INFO: Created: latency-svc-m4nw4
Mar  6 16:02:59.122: INFO: Got endpoints: latency-svc-sbr28 [750.524218ms]
Mar  6 16:02:59.131: INFO: Created: latency-svc-ktwpr
Mar  6 16:02:59.171: INFO: Got endpoints: latency-svc-bsngs [749.194928ms]
Mar  6 16:02:59.180: INFO: Created: latency-svc-zv5v6
Mar  6 16:02:59.222: INFO: Got endpoints: latency-svc-hqjg2 [751.002701ms]
Mar  6 16:02:59.232: INFO: Created: latency-svc-z76wz
Mar  6 16:02:59.272: INFO: Got endpoints: latency-svc-s4hww [750.436152ms]
Mar  6 16:02:59.282: INFO: Created: latency-svc-hfs44
Mar  6 16:02:59.321: INFO: Got endpoints: latency-svc-mljcj [749.296612ms]
Mar  6 16:02:59.330: INFO: Created: latency-svc-mxt9b
Mar  6 16:02:59.371: INFO: Got endpoints: latency-svc-v4kkh [749.997796ms]
Mar  6 16:02:59.381: INFO: Created: latency-svc-g82zd
Mar  6 16:02:59.421: INFO: Got endpoints: latency-svc-svkzq [750.040671ms]
Mar  6 16:02:59.431: INFO: Created: latency-svc-gnv58
Mar  6 16:02:59.470: INFO: Got endpoints: latency-svc-tv7dx [748.637335ms]
Mar  6 16:02:59.479: INFO: Created: latency-svc-vtzph
Mar  6 16:02:59.521: INFO: Got endpoints: latency-svc-hncp8 [750.364815ms]
Mar  6 16:02:59.530: INFO: Created: latency-svc-48qnr
Mar  6 16:02:59.571: INFO: Got endpoints: latency-svc-zj2ls [749.697347ms]
Mar  6 16:02:59.581: INFO: Created: latency-svc-m9f68
Mar  6 16:02:59.621: INFO: Got endpoints: latency-svc-j8hp8 [750.2437ms]
Mar  6 16:02:59.630: INFO: Created: latency-svc-jkt87
Mar  6 16:02:59.671: INFO: Got endpoints: latency-svc-wwgqr [749.401324ms]
Mar  6 16:02:59.681: INFO: Created: latency-svc-bf87m
Mar  6 16:02:59.721: INFO: Got endpoints: latency-svc-f9n84 [749.690369ms]
Mar  6 16:02:59.730: INFO: Created: latency-svc-4gfjw
Mar  6 16:02:59.771: INFO: Got endpoints: latency-svc-9f4l9 [749.691118ms]
Mar  6 16:02:59.784: INFO: Created: latency-svc-vkggz
Mar  6 16:02:59.823: INFO: Got endpoints: latency-svc-m4nw4 [751.932331ms]
Mar  6 16:02:59.833: INFO: Created: latency-svc-pwwdf
Mar  6 16:02:59.871: INFO: Got endpoints: latency-svc-ktwpr [748.993798ms]
Mar  6 16:02:59.880: INFO: Created: latency-svc-pfgns
Mar  6 16:02:59.922: INFO: Got endpoints: latency-svc-zv5v6 [751.030957ms]
Mar  6 16:02:59.931: INFO: Created: latency-svc-hrwj4
Mar  6 16:02:59.971: INFO: Got endpoints: latency-svc-z76wz [749.437833ms]
Mar  6 16:02:59.980: INFO: Created: latency-svc-72vdk
Mar  6 16:03:00.022: INFO: Got endpoints: latency-svc-hfs44 [749.694572ms]
Mar  6 16:03:00.032: INFO: Created: latency-svc-lhv9n
Mar  6 16:03:00.071: INFO: Got endpoints: latency-svc-mxt9b [750.139084ms]
Mar  6 16:03:00.081: INFO: Created: latency-svc-dgxxq
Mar  6 16:03:00.121: INFO: Got endpoints: latency-svc-g82zd [749.9979ms]
Mar  6 16:03:00.131: INFO: Created: latency-svc-dbb4h
Mar  6 16:03:00.171: INFO: Got endpoints: latency-svc-gnv58 [749.714936ms]
Mar  6 16:03:00.180: INFO: Created: latency-svc-d5lzr
Mar  6 16:03:00.220: INFO: Got endpoints: latency-svc-vtzph [749.953349ms]
Mar  6 16:03:00.230: INFO: Created: latency-svc-j9s7n
Mar  6 16:03:00.271: INFO: Got endpoints: latency-svc-48qnr [749.888105ms]
Mar  6 16:03:00.280: INFO: Created: latency-svc-l7s97
Mar  6 16:03:00.321: INFO: Got endpoints: latency-svc-m9f68 [750.023845ms]
Mar  6 16:03:00.331: INFO: Created: latency-svc-wt6lj
Mar  6 16:03:00.371: INFO: Got endpoints: latency-svc-jkt87 [750.053406ms]
Mar  6 16:03:00.381: INFO: Created: latency-svc-g9fgx
Mar  6 16:03:00.421: INFO: Got endpoints: latency-svc-bf87m [749.964817ms]
Mar  6 16:03:00.431: INFO: Created: latency-svc-76h4l
Mar  6 16:03:00.471: INFO: Got endpoints: latency-svc-4gfjw [749.756525ms]
Mar  6 16:03:00.480: INFO: Created: latency-svc-kbwp6
Mar  6 16:03:00.520: INFO: Got endpoints: latency-svc-vkggz [749.435657ms]
Mar  6 16:03:00.530: INFO: Created: latency-svc-ndpcl
Mar  6 16:03:00.571: INFO: Got endpoints: latency-svc-pwwdf [747.079311ms]
Mar  6 16:03:00.580: INFO: Created: latency-svc-b66qn
Mar  6 16:03:00.621: INFO: Got endpoints: latency-svc-pfgns [750.427682ms]
Mar  6 16:03:00.633: INFO: Created: latency-svc-jzwj2
Mar  6 16:03:00.671: INFO: Got endpoints: latency-svc-hrwj4 [749.077034ms]
Mar  6 16:03:00.681: INFO: Created: latency-svc-b8wwz
Mar  6 16:03:00.721: INFO: Got endpoints: latency-svc-72vdk [749.987896ms]
Mar  6 16:03:00.731: INFO: Created: latency-svc-hr5vj
Mar  6 16:03:00.771: INFO: Got endpoints: latency-svc-lhv9n [749.149627ms]
Mar  6 16:03:00.783: INFO: Created: latency-svc-j9cdt
Mar  6 16:03:00.821: INFO: Got endpoints: latency-svc-dgxxq [750.112118ms]
Mar  6 16:03:00.831: INFO: Created: latency-svc-tct7h
Mar  6 16:03:00.872: INFO: Got endpoints: latency-svc-dbb4h [750.518736ms]
Mar  6 16:03:00.881: INFO: Created: latency-svc-42hw2
Mar  6 16:03:00.921: INFO: Got endpoints: latency-svc-d5lzr [749.831587ms]
Mar  6 16:03:00.930: INFO: Created: latency-svc-mm7hj
Mar  6 16:03:00.971: INFO: Got endpoints: latency-svc-j9s7n [750.990273ms]
Mar  6 16:03:00.983: INFO: Created: latency-svc-fc6lf
Mar  6 16:03:01.022: INFO: Got endpoints: latency-svc-l7s97 [751.376993ms]
Mar  6 16:03:01.032: INFO: Created: latency-svc-vssjb
Mar  6 16:03:01.071: INFO: Got endpoints: latency-svc-wt6lj [749.572412ms]
Mar  6 16:03:01.080: INFO: Created: latency-svc-rnnwm
Mar  6 16:03:01.121: INFO: Got endpoints: latency-svc-g9fgx [750.340536ms]
Mar  6 16:03:01.131: INFO: Created: latency-svc-j6j69
Mar  6 16:03:01.172: INFO: Got endpoints: latency-svc-76h4l [750.560676ms]
Mar  6 16:03:01.182: INFO: Created: latency-svc-8pjdh
Mar  6 16:03:01.220: INFO: Got endpoints: latency-svc-kbwp6 [749.724512ms]
Mar  6 16:03:01.230: INFO: Created: latency-svc-zm5d7
Mar  6 16:03:01.271: INFO: Got endpoints: latency-svc-ndpcl [751.166028ms]
Mar  6 16:03:01.281: INFO: Created: latency-svc-59p68
Mar  6 16:03:01.321: INFO: Got endpoints: latency-svc-b66qn [750.518153ms]
Mar  6 16:03:01.331: INFO: Created: latency-svc-6m5xs
Mar  6 16:03:01.371: INFO: Got endpoints: latency-svc-jzwj2 [749.702745ms]
Mar  6 16:03:01.381: INFO: Created: latency-svc-w5v7p
Mar  6 16:03:01.421: INFO: Got endpoints: latency-svc-b8wwz [750.220288ms]
Mar  6 16:03:01.431: INFO: Created: latency-svc-jxqwq
Mar  6 16:03:01.471: INFO: Got endpoints: latency-svc-hr5vj [749.861912ms]
Mar  6 16:03:01.521: INFO: Got endpoints: latency-svc-j9cdt [749.598251ms]
Mar  6 16:03:01.573: INFO: Got endpoints: latency-svc-tct7h [751.532094ms]
Mar  6 16:03:01.621: INFO: Got endpoints: latency-svc-42hw2 [748.930263ms]
Mar  6 16:03:01.671: INFO: Got endpoints: latency-svc-mm7hj [750.492799ms]
Mar  6 16:03:01.722: INFO: Got endpoints: latency-svc-fc6lf [750.474969ms]
Mar  6 16:03:01.772: INFO: Got endpoints: latency-svc-vssjb [749.462391ms]
Mar  6 16:03:01.821: INFO: Got endpoints: latency-svc-rnnwm [749.718052ms]
Mar  6 16:03:01.871: INFO: Got endpoints: latency-svc-j6j69 [750.110008ms]
Mar  6 16:03:01.922: INFO: Got endpoints: latency-svc-8pjdh [749.772388ms]
Mar  6 16:03:01.971: INFO: Got endpoints: latency-svc-zm5d7 [750.772916ms]
Mar  6 16:03:02.021: INFO: Got endpoints: latency-svc-59p68 [749.604132ms]
Mar  6 16:03:02.071: INFO: Got endpoints: latency-svc-6m5xs [750.163468ms]
Mar  6 16:03:02.121: INFO: Got endpoints: latency-svc-w5v7p [749.575098ms]
Mar  6 16:03:02.172: INFO: Got endpoints: latency-svc-jxqwq [750.794424ms]
Mar  6 16:03:02.172: INFO: Latencies: [23.795421ms 35.528581ms 38.204965ms 45.019493ms 55.095703ms 64.397ms 74.076222ms 83.274108ms 92.600771ms 101.780827ms 111.306735ms 120.472008ms 130.373853ms 137.076418ms 138.636861ms 143.077735ms 144.320929ms 145.219282ms 145.962104ms 146.044949ms 146.075825ms 146.342132ms 146.497683ms 146.583988ms 147.784384ms 148.131307ms 148.337808ms 150.020383ms 150.437957ms 150.82003ms 151.05079ms 151.338888ms 152.915342ms 154.070075ms 154.385476ms 155.289222ms 179.395792ms 218.441623ms 255.001328ms 298.107244ms 334.761491ms 377.774094ms 418.577453ms 457.447322ms 494.276178ms 533.366278ms 574.317493ms 614.080798ms 652.678015ms 692.357931ms 731.94824ms 746.992074ms 747.079311ms 748.084103ms 748.637335ms 748.930263ms 748.993798ms 749.022925ms 749.077034ms 749.081098ms 749.149627ms 749.171482ms 749.176986ms 749.194928ms 749.210936ms 749.2264ms 749.274231ms 749.27644ms 749.296612ms 749.313745ms 749.326453ms 749.378395ms 749.401324ms 749.433551ms 749.435657ms 749.437833ms 749.445692ms 749.453229ms 749.462391ms 749.478186ms 749.493694ms 749.497366ms 749.500944ms 749.503746ms 749.538578ms 749.546106ms 749.562022ms 749.572412ms 749.575098ms 749.583389ms 749.595308ms 749.596508ms 749.598251ms 749.604132ms 749.617796ms 749.622076ms 749.675924ms 749.684104ms 749.690369ms 749.691118ms 749.694572ms 749.697347ms 749.702745ms 749.714936ms 749.718052ms 749.724512ms 749.732894ms 749.742767ms 749.751394ms 749.756525ms 749.765685ms 749.772388ms 749.784969ms 749.831587ms 749.841681ms 749.861912ms 749.874705ms 749.880975ms 749.888105ms 749.905914ms 749.925252ms 749.946748ms 749.952742ms 749.953349ms 749.964817ms 749.970759ms 749.984379ms 749.987896ms 749.997796ms 749.9979ms 750.010348ms 750.023845ms 750.025479ms 750.0321ms 750.034654ms 750.040671ms 750.053406ms 750.065321ms 750.06821ms 750.068341ms 750.110008ms 750.112118ms 750.112998ms 750.132682ms 750.139084ms 750.148476ms 750.163468ms 750.199823ms 750.220288ms 750.24104ms 750.2437ms 750.252049ms 750.25297ms 750.278171ms 750.300194ms 750.30679ms 750.31527ms 750.315281ms 750.340536ms 750.340613ms 750.348681ms 750.34976ms 750.359249ms 750.364815ms 750.392544ms 750.415594ms 750.427682ms 750.432847ms 750.436152ms 750.47313ms 750.474969ms 750.485317ms 750.492799ms 750.500819ms 750.506473ms 750.518153ms 750.518736ms 750.524218ms 750.560676ms 750.597874ms 750.602237ms 750.740644ms 750.745496ms 750.763055ms 750.764736ms 750.772916ms 750.794424ms 750.978673ms 750.990273ms 751.002701ms 751.030957ms 751.074306ms 751.166028ms 751.170086ms 751.269786ms 751.313122ms 751.376993ms 751.46783ms 751.532094ms 751.932331ms]
Mar  6 16:03:02.172: INFO: 50 %ile: 749.694572ms
Mar  6 16:03:02.172: INFO: 90 %ile: 750.602237ms
Mar  6 16:03:02.172: INFO: 99 %ile: 751.532094ms
Mar  6 16:03:02.172: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:03:02.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-pstvv" for this suite.
Mar  6 16:03:14.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:03:14.201: INFO: namespace: e2e-tests-svc-latency-pstvv, resource: bindings, ignored listing per whitelist
Mar  6 16:03:14.246: INFO: namespace e2e-tests-svc-latency-pstvv deletion completed in 12.06928277s

• [SLOW TEST:21.980 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:03:14.246: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vtkhr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar  6 16:03:14.426: INFO: Waiting up to 5m0s for pod "client-containers-591988c9-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-containers-vtkhr" to be "success or failure"
Mar  6 16:03:14.429: INFO: Pod "client-containers-591988c9-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275142ms
Mar  6 16:03:16.431: INFO: Pod "client-containers-591988c9-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004827492s
Mar  6 16:03:18.434: INFO: Pod "client-containers-591988c9-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007481556s
STEP: Saw pod success
Mar  6 16:03:18.434: INFO: Pod "client-containers-591988c9-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:03:18.436: INFO: Trying to get logs from node themisto pod client-containers-591988c9-4029-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:03:18.454: INFO: Waiting for pod client-containers-591988c9-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:03:18.456: INFO: Pod client-containers-591988c9-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:03:18.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vtkhr" for this suite.
Mar  6 16:03:24.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:03:24.501: INFO: namespace: e2e-tests-containers-vtkhr, resource: bindings, ignored listing per whitelist
Mar  6 16:03:24.535: INFO: namespace e2e-tests-containers-vtkhr deletion completed in 6.075981601s

• [SLOW TEST:10.289 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:03:24.535: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-7nfzh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  6 16:03:29.738: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:03:30.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-7nfzh" for this suite.
Mar  6 16:03:52.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:03:52.783: INFO: namespace: e2e-tests-replicaset-7nfzh, resource: bindings, ignored listing per whitelist
Mar  6 16:03:52.826: INFO: namespace e2e-tests-replicaset-7nfzh deletion completed in 22.071618668s

• [SLOW TEST:28.291 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:03:52.826: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7jp85
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-70190f75-4029-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:03:53.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-7jp85" to be "success or failure"
Mar  6 16:03:53.016: INFO: Pod "pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341945ms
Mar  6 16:03:55.019: INFO: Pod "pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003826254s
STEP: Saw pod success
Mar  6 16:03:55.019: INFO: Pod "pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:03:55.021: INFO: Trying to get logs from node themisto pod pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:03:55.039: INFO: Waiting for pod pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:03:55.040: INFO: Pod pod-projected-secrets-7019ccf1-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:03:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7jp85" for this suite.
Mar  6 16:04:01.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:04:01.101: INFO: namespace: e2e-tests-projected-7jp85, resource: bindings, ignored listing per whitelist
Mar  6 16:04:01.121: INFO: namespace e2e-tests-projected-7jp85 deletion completed in 6.077155258s

• [SLOW TEST:8.294 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:04:01.121: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-g5fmn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0306 16:04:41.336032      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 16:04:41.336: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:04:41.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-g5fmn" for this suite.
Mar  6 16:04:47.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:04:47.359: INFO: namespace: e2e-tests-gc-g5fmn, resource: bindings, ignored listing per whitelist
Mar  6 16:04:47.405: INFO: namespace e2e-tests-gc-g5fmn deletion completed in 6.066685735s

• [SLOW TEST:46.285 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:04:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wbrcm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 16:04:47.588: INFO: Waiting up to 5m0s for pod "downward-api-90a0d955-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-wbrcm" to be "success or failure"
Mar  6 16:04:47.589: INFO: Pod "downward-api-90a0d955-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59826ms
Mar  6 16:04:49.592: INFO: Pod "downward-api-90a0d955-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004364152s
STEP: Saw pod success
Mar  6 16:04:49.592: INFO: Pod "downward-api-90a0d955-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:04:49.595: INFO: Trying to get logs from node themisto pod downward-api-90a0d955-4029-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 16:04:49.614: INFO: Waiting for pod downward-api-90a0d955-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:04:49.616: INFO: Pod downward-api-90a0d955-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:04:49.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wbrcm" for this suite.
Mar  6 16:04:55.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:04:55.658: INFO: namespace: e2e-tests-downward-api-wbrcm, resource: bindings, ignored listing per whitelist
Mar  6 16:04:55.693: INFO: namespace e2e-tests-downward-api-wbrcm deletion completed in 6.072749126s

• [SLOW TEST:8.288 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:04:55.693: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-k9qgl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-k9qgl/secret-test-95921204-4029-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:04:55.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-k9qgl" to be "success or failure"
Mar  6 16:04:55.885: INFO: Pod "pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.27544ms
Mar  6 16:04:57.887: INFO: Pod "pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00348755s
STEP: Saw pod success
Mar  6 16:04:57.887: INFO: Pod "pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:04:57.889: INFO: Trying to get logs from node themisto pod pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007 container env-test: <nil>
STEP: delete the pod
Mar  6 16:04:57.906: INFO: Waiting for pod pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:04:57.908: INFO: Pod pod-configmaps-9592db2f-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:04:57.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-k9qgl" for this suite.
Mar  6 16:05:03.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:05:03.942: INFO: namespace: e2e-tests-secrets-k9qgl, resource: bindings, ignored listing per whitelist
Mar  6 16:05:03.996: INFO: namespace e2e-tests-secrets-k9qgl deletion completed in 6.084515023s

• [SLOW TEST:8.303 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:05:03.996: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-z7svj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:05:04.167: INFO: Creating deployment "test-recreate-deployment"
Mar  6 16:05:04.173: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  6 16:05:04.176: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  6 16:05:06.182: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  6 16:05:06.184: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  6 16:05:06.192: INFO: Updating deployment test-recreate-deployment
Mar  6 16:05:06.192: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 16:05:06.281: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-z7svj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7svj/deployments/test-recreate-deployment,UID:9a83ee48-4029-11e9-b3f2-0cc47aaad1b4,ResourceVersion:66467,Generation:2,CreationTimestamp:2019-03-06 16:05:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-06 16:05:01 +0000 UTC 2019-03-06 16:05:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-06 16:05:01 +0000 UTC 2019-03-06 16:04:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 16:05:06.283: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-z7svj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7svj/replicasets/test-recreate-deployment-697fbf54bf,UID:9bbf600d-4029-11e9-b3f2-0cc47aaad1b4,ResourceVersion:66466,Generation:1,CreationTimestamp:2019-03-06 16:05:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9a83ee48-4029-11e9-b3f2-0cc47aaad1b4 0xc001b03b67 0xc001b03b68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 16:05:06.283: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  6 16:05:06.283: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-z7svj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7svj/replicasets/test-recreate-deployment-5dfdcc846d,UID:916d3aa1-4029-11e9-aa7e-0cc47a6c40e2,ResourceVersion:66456,Generation:2,CreationTimestamp:2019-03-06 16:04:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9a83ee48-4029-11e9-b3f2-0cc47aaad1b4 0xc001b03aa7 0xc001b03aa8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 16:05:06.285: INFO: Pod "test-recreate-deployment-697fbf54bf-d5tgz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-d5tgz,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-z7svj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z7svj/pods/test-recreate-deployment-697fbf54bf-d5tgz,UID:9bc07f17-4029-11e9-b3f2-0cc47aaad1b4,ResourceVersion:66461,Generation:0,CreationTimestamp:2019-03-06 16:05:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 9bbf600d-4029-11e9-b3f2-0cc47aaad1b4 0xc0029f0b97 0xc0029f0b98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5n4db {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5n4db,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5n4db true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kronos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029f1150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029f1170}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:05:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:05:06.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-z7svj" for this suite.
Mar  6 16:05:12.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:05:12.354: INFO: namespace: e2e-tests-deployment-z7svj, resource: bindings, ignored listing per whitelist
Mar  6 16:05:12.364: INFO: namespace e2e-tests-deployment-z7svj deletion completed in 6.076369424s

• [SLOW TEST:8.368 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:05:12.365: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-r7gj6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-9f80f55e-4029-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:05:12.549: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-r7gj6" to be "success or failure"
Mar  6 16:05:12.551: INFO: Pod "pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.528631ms
Mar  6 16:05:14.553: INFO: Pod "pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004091186s
STEP: Saw pod success
Mar  6 16:05:14.553: INFO: Pod "pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:05:14.555: INFO: Trying to get logs from node themisto pod pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:05:14.574: INFO: Waiting for pod pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:05:14.576: INFO: Pod pod-projected-secrets-9f81b580-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:05:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r7gj6" for this suite.
Mar  6 16:05:20.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:05:20.615: INFO: namespace: e2e-tests-projected-r7gj6, resource: bindings, ignored listing per whitelist
Mar  6 16:05:20.655: INFO: namespace e2e-tests-projected-r7gj6 deletion completed in 6.076727112s

• [SLOW TEST:8.291 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:05:20.656: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-hbpvc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hbpvc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 16:05:20.834: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 16:05:40.904: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.226:8080/dial?request=hostName&protocol=udp&host=172.16.10.73&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hbpvc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:05:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:05:41.032: INFO: Waiting for endpoints: map[]
Mar  6 16:05:41.034: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.226:8080/dial?request=hostName&protocol=udp&host=172.16.4.225&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hbpvc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:05:41.034: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:05:41.165: INFO: Waiting for endpoints: map[]
Mar  6 16:05:41.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.4.226:8080/dial?request=hostName&protocol=udp&host=172.16.3.14&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hbpvc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:05:41.167: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:05:41.276: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:05:41.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hbpvc" for this suite.
Mar  6 16:06:03.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:06:03.352: INFO: namespace: e2e-tests-pod-network-test-hbpvc, resource: bindings, ignored listing per whitelist
Mar  6 16:06:03.356: INFO: namespace e2e-tests-pod-network-test-hbpvc deletion completed in 22.076038593s

• [SLOW TEST:42.700 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:06:03.356: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-wdk76
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar  6 16:06:04.047: INFO: Waiting up to 5m0s for pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d" in namespace "e2e-tests-svcaccounts-wdk76" to be "success or failure"
Mar  6 16:06:04.049: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.797595ms
Mar  6 16:06:06.051: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004314066s
STEP: Saw pod success
Mar  6 16:06:06.051: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d" satisfied condition "success or failure"
Mar  6 16:06:06.053: INFO: Trying to get logs from node kronos pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d container token-test: <nil>
STEP: delete the pod
Mar  6 16:06:06.071: INFO: Waiting for pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d to disappear
Mar  6 16:06:06.073: INFO: Pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-2gv2d no longer exists
STEP: Creating a pod to test consume service account root CA
Mar  6 16:06:06.078: INFO: Waiting up to 5m0s for pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7" in namespace "e2e-tests-svcaccounts-wdk76" to be "success or failure"
Mar  6 16:06:06.081: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.46278ms
Mar  6 16:06:08.084: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005173535s
STEP: Saw pod success
Mar  6 16:06:08.084: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7" satisfied condition "success or failure"
Mar  6 16:06:08.086: INFO: Trying to get logs from node themisto pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7 container root-ca-test: <nil>
STEP: delete the pod
Mar  6 16:06:08.108: INFO: Waiting for pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7 to disappear
Mar  6 16:06:08.109: INFO: Pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-49hd7 no longer exists
STEP: Creating a pod to test consume service account namespace
Mar  6 16:06:08.115: INFO: Waiting up to 5m0s for pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28" in namespace "e2e-tests-svcaccounts-wdk76" to be "success or failure"
Mar  6 16:06:08.117: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668543ms
Mar  6 16:06:10.120: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004358541s
STEP: Saw pod success
Mar  6 16:06:10.120: INFO: Pod "pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28" satisfied condition "success or failure"
Mar  6 16:06:10.122: INFO: Trying to get logs from node kronos pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28 container namespace-test: <nil>
STEP: delete the pod
Mar  6 16:06:10.141: INFO: Waiting for pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28 to disappear
Mar  6 16:06:10.142: INFO: Pod pod-service-account-be33a265-4029-11e9-9071-0a58ac100007-txj28 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:06:10.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-wdk76" for this suite.
Mar  6 16:06:16.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:06:16.170: INFO: namespace: e2e-tests-svcaccounts-wdk76, resource: bindings, ignored listing per whitelist
Mar  6 16:06:16.214: INFO: namespace e2e-tests-svcaccounts-wdk76 deletion completed in 6.068253054s

• [SLOW TEST:12.858 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:06:16.214: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dlcjn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar  6 16:06:18.402: INFO: Pod pod-hostip-c58f8f6f-4029-11e9-9071-0a58ac100007 has hostIP: 10.100.96.46
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:06:18.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dlcjn" for this suite.
Mar  6 16:06:40.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:06:40.470: INFO: namespace: e2e-tests-pods-dlcjn, resource: bindings, ignored listing per whitelist
Mar  6 16:06:40.482: INFO: namespace e2e-tests-pods-dlcjn deletion completed in 22.07226311s

• [SLOW TEST:24.268 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:06:40.482: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6f96c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  6 16:06:40.660: INFO: namespace e2e-tests-kubectl-6f96c
Mar  6 16:06:40.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-6f96c'
Mar  6 16:06:40.880: INFO: stderr: ""
Mar  6 16:06:40.880: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 16:06:41.883: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 16:06:41.883: INFO: Found 0 / 1
Mar  6 16:06:42.883: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 16:06:42.883: INFO: Found 1 / 1
Mar  6 16:06:42.883: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 16:06:42.886: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 16:06:42.886: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 16:06:42.886: INFO: wait on redis-master startup in e2e-tests-kubectl-6f96c 
Mar  6 16:06:42.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 logs redis-master-8t4bs redis-master --namespace=e2e-tests-kubectl-6f96c'
Mar  6 16:06:42.995: INFO: stderr: ""
Mar  6 16:06:42.995: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 16:06:44.151 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 16:06:44.152 # Server started, Redis version 3.2.12\n1:M 06 Mar 16:06:44.152 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 16:06:44.152 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  6 16:06:42.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-6f96c'
Mar  6 16:06:43.084: INFO: stderr: ""
Mar  6 16:06:43.084: INFO: stdout: "service/rm2 exposed\n"
Mar  6 16:06:43.086: INFO: Service rm2 in namespace e2e-tests-kubectl-6f96c found.
STEP: exposing service
Mar  6 16:06:45.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-6f96c'
Mar  6 16:06:45.184: INFO: stderr: ""
Mar  6 16:06:45.184: INFO: stdout: "service/rm3 exposed\n"
Mar  6 16:06:45.186: INFO: Service rm3 in namespace e2e-tests-kubectl-6f96c found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:06:47.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6f96c" for this suite.
Mar  6 16:07:09.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:07:09.223: INFO: namespace: e2e-tests-kubectl-6f96c, resource: bindings, ignored listing per whitelist
Mar  6 16:07:09.261: INFO: namespace e2e-tests-kubectl-6f96c deletion completed in 22.066347047s

• [SLOW TEST:28.779 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:07:09.261: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-db7c4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:07:09.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-db7c4" to be "success or failure"
Mar  6 16:07:09.447: INFO: Pod "downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556794ms
Mar  6 16:07:11.449: INFO: Pod "downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003747112s
STEP: Saw pod success
Mar  6 16:07:11.449: INFO: Pod "downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:07:11.451: INFO: Trying to get logs from node themisto pod downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:07:11.469: INFO: Waiting for pod downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007 to disappear
Mar  6 16:07:11.470: INFO: Pod downwardapi-volume-e52e8171-4029-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:07:11.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-db7c4" for this suite.
Mar  6 16:07:17.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:07:17.520: INFO: namespace: e2e-tests-projected-db7c4, resource: bindings, ignored listing per whitelist
Mar  6 16:07:17.554: INFO: namespace e2e-tests-projected-db7c4 deletion completed in 6.080651555s

• [SLOW TEST:8.292 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:07:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-7vwt6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 16:07:21.775: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:21.778: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:23.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:23.785: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:25.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:25.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:27.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:27.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:29.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:29.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:31.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:31.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:33.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:33.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:35.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:35.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:37.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:37.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:39.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:39.781: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 16:07:41.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 16:07:41.781: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:07:41.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7vwt6" for this suite.
Mar  6 16:08:03.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:08:03.853: INFO: namespace: e2e-tests-container-lifecycle-hook-7vwt6, resource: bindings, ignored listing per whitelist
Mar  6 16:08:03.857: INFO: namespace e2e-tests-container-lifecycle-hook-7vwt6 deletion completed in 22.071246771s

• [SLOW TEST:46.303 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:08:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-lmkqj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-lmkqj
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 16:08:04.030: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 16:08:24.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.4.229:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lmkqj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:08:24.097: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:08:24.241: INFO: Found all expected endpoints: [netserver-0]
Mar  6 16:08:24.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.10.78:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lmkqj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:08:24.243: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:08:24.356: INFO: Found all expected endpoints: [netserver-1]
Mar  6 16:08:24.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.3.17:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lmkqj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 16:08:24.358: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
Mar  6 16:08:24.473: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:08:24.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-lmkqj" for this suite.
Mar  6 16:08:46.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:08:46.532: INFO: namespace: e2e-tests-pod-network-test-lmkqj, resource: bindings, ignored listing per whitelist
Mar  6 16:08:46.551: INFO: namespace e2e-tests-pod-network-test-lmkqj deletion completed in 22.073881743s

• [SLOW TEST:42.695 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:08:46.552: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8mwxt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:08:46.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-8mwxt" to be "success or failure"
Mar  6 16:08:46.737: INFO: Pod "downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591232ms
Mar  6 16:08:48.739: INFO: Pod "downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004332128s
STEP: Saw pod success
Mar  6 16:08:48.739: INFO: Pod "downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:08:48.742: INFO: Trying to get logs from node sponde pod downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:08:48.760: INFO: Waiting for pod downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:08:48.762: INFO: Pod downwardapi-volume-1f2bdbfa-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:08:48.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8mwxt" for this suite.
Mar  6 16:08:54.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:08:54.799: INFO: namespace: e2e-tests-downward-api-8mwxt, resource: bindings, ignored listing per whitelist
Mar  6 16:08:54.845: INFO: namespace e2e-tests-downward-api-8mwxt deletion completed in 6.079286332s

• [SLOW TEST:8.293 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:08:54.845: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jtg9s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 16:08:55.030: INFO: Waiting up to 5m0s for pod "pod-241d7b1a-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-jtg9s" to be "success or failure"
Mar  6 16:08:55.032: INFO: Pod "pod-241d7b1a-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788314ms
Mar  6 16:08:57.035: INFO: Pod "pod-241d7b1a-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004489062s
STEP: Saw pod success
Mar  6 16:08:57.035: INFO: Pod "pod-241d7b1a-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:08:57.037: INFO: Trying to get logs from node kronos pod pod-241d7b1a-402a-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:08:57.055: INFO: Waiting for pod pod-241d7b1a-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:08:57.057: INFO: Pod pod-241d7b1a-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:08:57.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jtg9s" for this suite.
Mar  6 16:09:03.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:09:03.111: INFO: namespace: e2e-tests-emptydir-jtg9s, resource: bindings, ignored listing per whitelist
Mar  6 16:09:03.137: INFO: namespace e2e-tests-emptydir-jtg9s deletion completed in 6.076651946s

• [SLOW TEST:8.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:09:03.138: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-pcjh4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  6 16:09:03.325: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  6 16:09:08.328: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:09:09.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-pcjh4" for this suite.
Mar  6 16:09:15.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:09:15.374: INFO: namespace: e2e-tests-replication-controller-pcjh4, resource: bindings, ignored listing per whitelist
Mar  6 16:09:15.416: INFO: namespace e2e-tests-replication-controller-pcjh4 deletion completed in 6.069792932s

• [SLOW TEST:12.278 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:09:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6mtlh
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3060c023-402a-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3060c023-402a-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:09:19.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6mtlh" for this suite.
Mar  6 16:09:41.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:09:41.668: INFO: namespace: e2e-tests-projected-6mtlh, resource: bindings, ignored listing per whitelist
Mar  6 16:09:41.716: INFO: namespace e2e-tests-projected-6mtlh deletion completed in 22.070185907s

• [SLOW TEST:26.300 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:09:41.716: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-8whdg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-8whdg
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-8whdg
STEP: Deleting pre-stop pod
Mar  6 16:09:50.926: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:09:50.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-8whdg" for this suite.
Mar  6 16:10:30.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:10:31.012: INFO: namespace: e2e-tests-prestop-8whdg, resource: bindings, ignored listing per whitelist
Mar  6 16:10:31.022: INFO: namespace e2e-tests-prestop-8whdg deletion completed in 40.085027852s

• [SLOW TEST:49.306 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:10:31.022: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-lgssh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 16:10:31.198: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:10:33.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-lgssh" for this suite.
Mar  6 16:10:39.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:10:39.653: INFO: namespace: e2e-tests-init-container-lgssh, resource: bindings, ignored listing per whitelist
Mar  6 16:10:39.665: INFO: namespace e2e-tests-init-container-lgssh deletion completed in 6.073473096s

• [SLOW TEST:8.643 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:10:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qnmr2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:10:39.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-qnmr2" to be "success or failure"
Mar  6 16:10:39.887: INFO: Pod "downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127993ms
Mar  6 16:10:41.890: INFO: Pod "downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004813303s
STEP: Saw pod success
Mar  6 16:10:41.890: INFO: Pod "downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:10:41.892: INFO: Trying to get logs from node themisto pod downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:10:41.911: INFO: Waiting for pod downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:10:41.912: INFO: Pod downwardapi-volume-629d3d42-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:10:41.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qnmr2" for this suite.
Mar  6 16:10:47.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:10:47.946: INFO: namespace: e2e-tests-downward-api-qnmr2, resource: bindings, ignored listing per whitelist
Mar  6 16:10:47.985: INFO: namespace e2e-tests-downward-api-qnmr2 deletion completed in 6.069834075s

• [SLOW TEST:8.320 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:10:47.986: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-k96wh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:10:48.176: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 16:10:48.184: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:48.184: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:48.184: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:48.185: INFO: Number of nodes with available pods: 0
Mar  6 16:10:48.186: INFO: Node kronos is running more than one daemon pod
Mar  6 16:10:49.189: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:49.189: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:49.189: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:49.191: INFO: Number of nodes with available pods: 1
Mar  6 16:10:49.191: INFO: Node kronos is running more than one daemon pod
Mar  6 16:10:50.190: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:50.190: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:50.190: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:50.193: INFO: Number of nodes with available pods: 3
Mar  6 16:10:50.193: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  6 16:10:50.212: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:50.212: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:50.212: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:50.215: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:50.215: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:50.215: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:51.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:51.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:51.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:51.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:51.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:51.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:52.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:52.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:52.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:52.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:52.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:52.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:53.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:53.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:53.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:53.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:53.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:53.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:54.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:54.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:54.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:54.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:54.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:54.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:55.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:55.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:55.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:55.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:55.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:55.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:56.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:56.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:56.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:56.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:56.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:56.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:57.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:57.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:57.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:57.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:57.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:57.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:58.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:58.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:58.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:58.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:58.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:58.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:59.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:59.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:59.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:10:59.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:59.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:10:59.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:00.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:00.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:00.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:00.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:00.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:00.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:01.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:01.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:01.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:01.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:01.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:01.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:02.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:02.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:02.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:02.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:02.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:02.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:03.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:03.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:03.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:03.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:03.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:03.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:04.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:04.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:04.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:04.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:04.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:04.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:05.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:05.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:05.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:05.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:05.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:05.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:06.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:06.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:06.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:06.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:06.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:06.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:07.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:07.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:07.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:07.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:07.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:07.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:08.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:08.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:08.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:08.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:08.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:08.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:09.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:09.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:09.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:09.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:09.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:09.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:10.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:10.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:10.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:10.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:10.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:10.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:11.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:11.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:11.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:11.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:11.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:11.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:12.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:12.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:12.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:12.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:12.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:12.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:13.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:13.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:13.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:13.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:13.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:13.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:14.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:14.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:14.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:14.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:14.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:14.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:15.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:15.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:15.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:15.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:15.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:15.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:16.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:16.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:16.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:16.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:16.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:16.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:17.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:17.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:17.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:17.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:17.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:17.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:18.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:18.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:18.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:18.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:18.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:18.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:19.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:19.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:19.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:19.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:19.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:19.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:20.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:20.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:20.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:20.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:20.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:20.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:21.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:21.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:21.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:21.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:21.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:21.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:22.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:22.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:22.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:22.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:22.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:22.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:23.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:23.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:23.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:23.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:23.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:23.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:23.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:24.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:24.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:24.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:24.219: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:24.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:24.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:24.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:25.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:25.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:25.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:25.219: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:25.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:25.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:25.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:26.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:26.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:26.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:26.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:26.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:26.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:26.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:27.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:27.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:27.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:27.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:27.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:27.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:27.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:28.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:28.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:28.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:28.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:28.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:28.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:28.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:29.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:29.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:29.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:29.219: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:29.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:29.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:29.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:30.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:30.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:30.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:30.219: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:30.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:30.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:30.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:31.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:31.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:31.219: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:31.219: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:31.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:31.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:31.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:32.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:32.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:32.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:32.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:32.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:32.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:32.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:33.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:33.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:33.218: INFO: Wrong image for pod: daemon-set-w4zv5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:33.218: INFO: Pod daemon-set-w4zv5 is not available
Mar  6 16:11:33.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:33.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:33.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:34.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:34.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:34.219: INFO: Pod daemon-set-p7g4d is not available
Mar  6 16:11:34.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:34.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:34.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:35.220: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:35.220: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:35.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:35.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:35.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:36.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:36.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:36.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:36.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:36.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:37.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:37.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:37.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:37.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:37.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:38.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:38.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:38.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:38.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:38.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:39.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:39.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:39.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:39.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:39.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:40.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:40.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:40.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:40.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:40.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:41.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:41.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:41.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:41.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:41.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:42.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:42.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:42.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:42.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:42.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:43.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:43.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:43.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:43.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:43.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:44.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:44.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:44.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:44.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:44.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:45.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:45.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:45.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:45.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:45.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:46.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:46.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:46.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:46.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:46.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:47.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:47.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:47.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:47.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:47.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:48.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:48.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:48.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:48.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:48.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:49.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:49.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:49.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:49.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:49.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:50.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:50.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:50.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:50.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:50.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:51.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:51.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:51.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:51.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:51.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:52.220: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:52.220: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:52.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:52.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:52.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:53.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:53.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:53.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:53.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:53.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:54.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:54.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:54.226: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:54.226: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:54.226: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:55.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:55.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:55.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:55.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:55.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:56.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:56.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:56.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:56.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:56.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:57.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:57.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:57.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:57.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:57.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:58.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:58.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:58.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:58.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:58.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:59.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:59.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:11:59.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:59.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:11:59.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:00.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:00.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:00.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:00.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:00.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:01.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:01.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:01.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:01.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:01.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:02.220: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:02.220: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:02.226: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:02.227: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:02.227: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:03.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:03.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:03.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:03.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:03.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:04.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:04.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:04.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:04.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:04.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:05.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:05.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:05.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:05.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:05.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:06.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:06.218: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:06.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:06.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:06.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:06.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:07.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:07.219: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:07.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:07.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:07.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:07.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:08.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:08.218: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:08.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:08.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:08.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:08.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:09.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:09.218: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:09.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:09.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:09.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:09.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:10.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:10.219: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:10.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:10.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:10.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:10.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:11.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:11.218: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:11.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:11.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:11.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:11.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:12.219: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:12.219: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:12.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:12.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:12.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:12.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:13.218: INFO: Wrong image for pod: daemon-set-2zn7s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:13.218: INFO: Pod daemon-set-2zn7s is not available
Mar  6 16:12:13.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:13.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:13.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:13.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:14.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:14.218: INFO: Pod daemon-set-p2jtr is not available
Mar  6 16:12:14.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:14.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:14.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:15.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:15.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:15.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:15.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:16.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:16.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:16.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:16.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:17.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:17.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:17.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:17.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:18.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:18.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:18.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:18.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:19.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:19.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:19.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:19.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:20.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:20.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:20.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:20.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:21.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:21.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:21.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:21.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:22.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:22.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:22.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:22.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:23.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:23.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:23.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:23.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:24.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:24.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:24.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:24.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:25.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:25.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:25.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:25.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:26.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:26.226: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:26.226: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:26.227: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:27.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:27.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:27.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:27.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:28.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:28.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:28.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:28.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:29.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:29.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:29.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:29.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:30.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:30.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:30.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:30.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:31.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:31.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:31.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:31.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:32.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:32.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:32.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:32.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:33.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:33.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:33.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:33.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:34.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:34.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:34.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:34.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:35.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:35.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:35.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:35.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:36.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:36.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:36.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:36.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:37.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:37.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:37.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:37.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:38.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:38.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:38.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:38.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:39.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:39.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:39.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:39.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:40.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:40.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:40.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:40.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:41.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:41.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:41.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:41.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:42.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:42.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:42.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:42.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:43.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:43.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:43.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:43.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:44.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:44.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:44.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:44.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:45.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:45.222: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:45.222: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:45.222: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:46.219: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:46.219: INFO: Pod daemon-set-nbbnk is not available
Mar  6 16:12:46.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:46.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:46.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:47.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:47.218: INFO: Pod daemon-set-nbbnk is not available
Mar  6 16:12:47.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:47.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:47.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:48.218: INFO: Wrong image for pod: daemon-set-nbbnk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 16:12:48.218: INFO: Pod daemon-set-nbbnk is not available
Mar  6 16:12:48.223: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:48.223: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:48.223: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.218: INFO: Pod daemon-set-qndgz is not available
Mar  6 16:12:49.221: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.221: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.221: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  6 16:12:49.224: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.224: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.224: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:49.227: INFO: Number of nodes with available pods: 2
Mar  6 16:12:49.227: INFO: Node themisto is running more than one daemon pod
Mar  6 16:12:50.231: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:50.231: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:50.231: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:50.233: INFO: Number of nodes with available pods: 2
Mar  6 16:12:50.233: INFO: Node themisto is running more than one daemon pod
Mar  6 16:12:51.231: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:51.231: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:51.231: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:12:51.234: INFO: Number of nodes with available pods: 3
Mar  6 16:12:51.234: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-k96wh, will wait for the garbage collector to delete the pods
Mar  6 16:12:51.307: INFO: Deleting DaemonSet.extensions daemon-set took: 8.541503ms
Mar  6 16:12:51.408: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.25883ms
Mar  6 16:13:03.610: INFO: Number of nodes with available pods: 0
Mar  6 16:13:03.610: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 16:13:03.612: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k96wh/daemonsets","resourceVersion":"69242"},"items":null}

Mar  6 16:13:03.614: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k96wh/pods","resourceVersion":"69242"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k96wh" for this suite.
Mar  6 16:13:09.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:09.668: INFO: namespace: e2e-tests-daemonsets-k96wh, resource: bindings, ignored listing per whitelist
Mar  6 16:13:09.698: INFO: namespace e2e-tests-daemonsets-k96wh deletion completed in 6.071308344s

• [SLOW TEST:141.712 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8zv7v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:13:09.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-8zv7v" to be "success or failure"
Mar  6 16:13:09.880: INFO: Pod "downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87321ms
Mar  6 16:13:11.882: INFO: Pod "downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004485177s
STEP: Saw pod success
Mar  6 16:13:11.882: INFO: Pod "downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:13:11.884: INFO: Trying to get logs from node sponde pod downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:13:11.904: INFO: Waiting for pod downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:13:11.906: INFO: Pod downwardapi-volume-bc04449d-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8zv7v" for this suite.
Mar  6 16:13:17.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:17.967: INFO: namespace: e2e-tests-downward-api-8zv7v, resource: bindings, ignored listing per whitelist
Mar  6 16:13:17.993: INFO: namespace e2e-tests-downward-api-8zv7v deletion completed in 6.083182938s

• [SLOW TEST:8.295 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:17.993: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-x9lw5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  6 16:13:18.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-x9lw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-x9lw5/configmaps/e2e-watch-test-resource-version,UID:c0f66ebd-402a-11e9-b3f2-0cc47aaad1b4,ResourceVersion:69382,Generation:0,CreationTimestamp:2019-03-06 16:13:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 16:13:18.193: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-x9lw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-x9lw5/configmaps/e2e-watch-test-resource-version,UID:c0f66ebd-402a-11e9-b3f2-0cc47aaad1b4,ResourceVersion:69383,Generation:0,CreationTimestamp:2019-03-06 16:13:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:18.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-x9lw5" for this suite.
Mar  6 16:13:24.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:24.237: INFO: namespace: e2e-tests-watch-x9lw5, resource: bindings, ignored listing per whitelist
Mar  6 16:13:24.272: INFO: namespace e2e-tests-watch-x9lw5 deletion completed in 6.076712033s

• [SLOW TEST:6.280 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:24.273: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-zm9km
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-gm44h
STEP: Creating secret with name secret-test-c4b4c6a1-402a-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:13:24.615: INFO: Waiting up to 5m0s for pod "pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-zm9km" to be "success or failure"
Mar  6 16:13:24.616: INFO: Pod "pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.460956ms
Mar  6 16:13:26.619: INFO: Pod "pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003914701s
STEP: Saw pod success
Mar  6 16:13:26.619: INFO: Pod "pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:13:26.621: INFO: Trying to get logs from node kronos pod pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:13:26.641: INFO: Waiting for pod pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:13:26.643: INFO: Pod pod-secrets-c4cd05a1-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zm9km" for this suite.
Mar  6 16:13:32.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:32.688: INFO: namespace: e2e-tests-secrets-zm9km, resource: bindings, ignored listing per whitelist
Mar  6 16:13:32.717: INFO: namespace e2e-tests-secrets-zm9km deletion completed in 6.07065835s
STEP: Destroying namespace "e2e-tests-secret-namespace-gm44h" for this suite.
Mar  6 16:13:38.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:38.731: INFO: namespace: e2e-tests-secret-namespace-gm44h, resource: bindings, ignored listing per whitelist
Mar  6 16:13:38.782: INFO: namespace e2e-tests-secret-namespace-gm44h deletion completed in 6.065103287s

• [SLOW TEST:14.509 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5qpnp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:13:38.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-5qpnp" to be "success or failure"
Mar  6 16:13:38.965: INFO: Pod "downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495876ms
Mar  6 16:13:40.967: INFO: Pod "downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003687198s
STEP: Saw pod success
Mar  6 16:13:40.967: INFO: Pod "downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:13:40.969: INFO: Trying to get logs from node themisto pod downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:13:40.993: INFO: Waiting for pod downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:13:40.994: INFO: Pod downwardapi-volume-cd5a76b0-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:40.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5qpnp" for this suite.
Mar  6 16:13:47.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:47.071: INFO: namespace: e2e-tests-projected-5qpnp, resource: bindings, ignored listing per whitelist
Mar  6 16:13:47.077: INFO: namespace e2e-tests-projected-5qpnp deletion completed in 6.079970948s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:47.077: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-lszgj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:13:47.260: INFO: (0) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.320302ms)
Mar  6 16:13:47.263: INFO: (1) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.270695ms)
Mar  6 16:13:47.266: INFO: (2) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.898886ms)
Mar  6 16:13:47.269: INFO: (3) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.840382ms)
Mar  6 16:13:47.272: INFO: (4) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.922869ms)
Mar  6 16:13:47.275: INFO: (5) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.881712ms)
Mar  6 16:13:47.278: INFO: (6) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.066791ms)
Mar  6 16:13:47.281: INFO: (7) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.951514ms)
Mar  6 16:13:47.284: INFO: (8) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.941666ms)
Mar  6 16:13:47.287: INFO: (9) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.054693ms)
Mar  6 16:13:47.290: INFO: (10) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.943652ms)
Mar  6 16:13:47.292: INFO: (11) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.507823ms)
Mar  6 16:13:47.295: INFO: (12) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.914737ms)
Mar  6 16:13:47.298: INFO: (13) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.090207ms)
Mar  6 16:13:47.301: INFO: (14) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.637663ms)
Mar  6 16:13:47.304: INFO: (15) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.688736ms)
Mar  6 16:13:47.309: INFO: (16) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.49643ms)
Mar  6 16:13:47.312: INFO: (17) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.639546ms)
Mar  6 16:13:47.316: INFO: (18) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 3.778122ms)
Mar  6 16:13:47.318: INFO: (19) /api/v1/nodes/kronos:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 2.51566ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:13:47.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-lszgj" for this suite.
Mar  6 16:13:53.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:13:53.340: INFO: namespace: e2e-tests-proxy-lszgj, resource: bindings, ignored listing per whitelist
Mar  6 16:13:53.403: INFO: namespace e2e-tests-proxy-lszgj deletion completed in 6.081387376s

• [SLOW TEST:6.325 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:13:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ztf7v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-ztf7v
Mar  6 16:13:55.591: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-ztf7v
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 16:13:55.593: INFO: Initial restart count of pod liveness-exec is 0
Mar  6 16:14:47.670: INFO: Restart count of pod e2e-tests-container-probe-ztf7v/liveness-exec is now 1 (52.076977854s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:14:47.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ztf7v" for this suite.
Mar  6 16:14:53.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:14:53.748: INFO: namespace: e2e-tests-container-probe-ztf7v, resource: bindings, ignored listing per whitelist
Mar  6 16:14:53.766: INFO: namespace e2e-tests-container-probe-ztf7v deletion completed in 6.07923108s

• [SLOW TEST:60.363 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:14:53.766: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s7wxz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-fa0c743a-402a-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 16:14:53.955: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-s7wxz" to be "success or failure"
Mar  6 16:14:53.957: INFO: Pod "pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625082ms
Mar  6 16:14:55.960: INFO: Pod "pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00440254s
STEP: Saw pod success
Mar  6 16:14:55.960: INFO: Pod "pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:14:55.962: INFO: Trying to get logs from node themisto pod pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 16:14:55.980: INFO: Waiting for pod pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:14:55.982: INFO: Pod pod-projected-configmaps-fa0d3815-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:14:55.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s7wxz" for this suite.
Mar  6 16:15:01.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:15:02.027: INFO: namespace: e2e-tests-projected-s7wxz, resource: bindings, ignored listing per whitelist
Mar  6 16:15:02.066: INFO: namespace e2e-tests-projected-s7wxz deletion completed in 6.08073783s

• [SLOW TEST:8.300 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:15:02.067: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7xb2g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:15:02.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-7xb2g" to be "success or failure"
Mar  6 16:15:02.254: INFO: Pod "downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510435ms
Mar  6 16:15:04.256: INFO: Pod "downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004310098s
STEP: Saw pod success
Mar  6 16:15:04.256: INFO: Pod "downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:15:04.259: INFO: Trying to get logs from node themisto pod downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:15:04.279: INFO: Waiting for pod downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007 to disappear
Mar  6 16:15:04.281: INFO: Pod downwardapi-volume-feff1788-402a-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:15:04.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7xb2g" for this suite.
Mar  6 16:15:10.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:15:10.335: INFO: namespace: e2e-tests-downward-api-7xb2g, resource: bindings, ignored listing per whitelist
Mar  6 16:15:10.359: INFO: namespace e2e-tests-downward-api-7xb2g deletion completed in 6.074433527s

• [SLOW TEST:8.293 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:15:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-chgmj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:15:10.537: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  6 16:15:15.540: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 16:15:15.540: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  6 16:15:17.543: INFO: Creating deployment "test-rollover-deployment"
Mar  6 16:15:17.551: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  6 16:15:19.556: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  6 16:15:19.561: INFO: Ensure that both replica sets have 1 created replica
Mar  6 16:15:19.565: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  6 16:15:19.576: INFO: Updating deployment test-rollover-deployment
Mar  6 16:15:19.576: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  6 16:15:21.580: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  6 16:15:21.585: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  6 16:15:21.591: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:21.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:23.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:23.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:25.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:25.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:27.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:27.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:29.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:29.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:31.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:31.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:33.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:33.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:35.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:35.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:37.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:37.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:39.596: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:39.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:41.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:41.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:43.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:43.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:45.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:45.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:47.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:47.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:49.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:49.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:51.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:51.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:53.600: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:53.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:55.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:55.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:57.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:57.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:15:59.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:15:59.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:01.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:01.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:03.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:03.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:05.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:05.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:07.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:07.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:09.596: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:09.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:11.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:11.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:13.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:13.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:15.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:15.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:17.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:17.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:19.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:19.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:21.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:21.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:23.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:23.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:25.598: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:25.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:27.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:27.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:29.597: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 16:16:29.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485715, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687485712, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 16:16:31.597: INFO: 
Mar  6 16:16:31.597: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 16:16:31.603: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-chgmj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-chgmj/deployments/test-rollover-deployment,UID:081da7e8-402b-11e9-b3f2-0cc47aaad1b4,ResourceVersion:70391,Generation:2,CreationTimestamp:2019-03-06 16:15:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-06 16:15:12 +0000 UTC 2019-03-06 16:15:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-06 16:16:25 +0000 UTC 2019-03-06 16:15:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 16:16:31.605: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-chgmj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-chgmj/replicasets/test-rollover-deployment-6b7f9d6597,UID:095422dc-402b-11e9-b3f2-0cc47aaad1b4,ResourceVersion:70381,Generation:2,CreationTimestamp:2019-03-06 16:15:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 081da7e8-402b-11e9-b3f2-0cc47aaad1b4 0xc0021df087 0xc0021df088}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 16:16:31.605: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  6 16:16:31.606: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-chgmj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-chgmj/replicasets/test-rollover-deployment-6586df867b,UID:ff076651-402a-11e9-aa7e-0cc47a6c40e2,ResourceVersion:70098,Generation:2,CreationTimestamp:2019-03-06 16:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 081da7e8-402b-11e9-b3f2-0cc47aaad1b4 0xc0021defb7 0xc0021defb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 16:16:31.606: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-chgmj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-chgmj/replicasets/test-rollover-controller,UID:03ef71dd-402b-11e9-b3f2-0cc47aaad1b4,ResourceVersion:70389,Generation:2,CreationTimestamp:2019-03-06 16:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 081da7e8-402b-11e9-b3f2-0cc47aaad1b4 0xc0021deef7 0xc0021deef8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 16:16:31.608: INFO: Pod "test-rollover-deployment-6b7f9d6597-z4lh8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-z4lh8,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-chgmj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-chgmj/pods/test-rollover-deployment-6b7f9d6597-z4lh8,UID:00413164-402b-11e9-aa7e-0cc47a6c40e2,ResourceVersion:70117,Generation:0,CreationTimestamp:2019-03-06 16:15:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 095422dc-402b-11e9-b3f2-0cc47aaad1b4 0xc0021dfdb7 0xc0021dfdb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jd9xj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jd9xj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jd9xj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021dfe30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021dfe50}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:16:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:16:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:16:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:15:19 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:172.16.10.86,StartTime:2019-03-06 16:16:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-06 16:16:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7cff3bbf3dfcba79ea49540d26adb25a876a0a07ea00d731c220662af7fbce69}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:16:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-chgmj" for this suite.
Mar  6 16:16:37.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:16:37.645: INFO: namespace: e2e-tests-deployment-chgmj, resource: bindings, ignored listing per whitelist
Mar  6 16:16:37.682: INFO: namespace e2e-tests-deployment-chgmj deletion completed in 6.070137285s

• [SLOW TEST:87.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:16:37.682: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2pv8w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-37fcce46-402b-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:16:37.871: INFO: Waiting up to 5m0s for pod "pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-2pv8w" to be "success or failure"
Mar  6 16:16:37.872: INFO: Pod "pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.404919ms
Mar  6 16:16:39.875: INFO: Pod "pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004362597s
STEP: Saw pod success
Mar  6 16:16:39.875: INFO: Pod "pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:16:39.878: INFO: Trying to get logs from node sponde pod pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:16:39.897: INFO: Waiting for pod pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:16:39.898: INFO: Pod pod-secrets-37fd9a29-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:16:39.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2pv8w" for this suite.
Mar  6 16:16:45.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:16:45.922: INFO: namespace: e2e-tests-secrets-2pv8w, resource: bindings, ignored listing per whitelist
Mar  6 16:16:45.977: INFO: namespace e2e-tests-secrets-2pv8w deletion completed in 6.075518405s

• [SLOW TEST:8.296 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:16:45.978: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-vhh5f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:16:46.168: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar  6 16:16:46.171: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-vhh5f/daemonsets","resourceVersion":"70529"},"items":null}

Mar  6 16:16:46.173: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-vhh5f/pods","resourceVersion":"70529"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:16:46.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-vhh5f" for this suite.
Mar  6 16:16:52.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:16:52.201: INFO: namespace: e2e-tests-daemonsets-vhh5f, resource: bindings, ignored listing per whitelist
Mar  6 16:16:52.259: INFO: namespace e2e-tests-daemonsets-vhh5f deletion completed in 6.07549586s

S [SKIPPING] [6.282 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar  6 16:16:46.168: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:16:52.259: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kp2fn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:16:52.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-kp2fn" to be "success or failure"
Mar  6 16:16:52.444: INFO: Pod "downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.290386ms
Mar  6 16:16:54.446: INFO: Pod "downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005010117s
STEP: Saw pod success
Mar  6 16:16:54.446: INFO: Pod "downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:16:54.448: INFO: Trying to get logs from node sponde pod downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:16:54.467: INFO: Waiting for pod downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:16:54.468: INFO: Pod downwardapi-volume-40aca157-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:16:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kp2fn" for this suite.
Mar  6 16:17:00.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:00.508: INFO: namespace: e2e-tests-projected-kp2fn, resource: bindings, ignored listing per whitelist
Mar  6 16:17:00.546: INFO: namespace e2e-tests-projected-kp2fn deletion completed in 6.074620603s

• [SLOW TEST:8.287 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:00.546: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mf6pp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:17:00.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-mf6pp" to be "success or failure"
Mar  6 16:17:00.728: INFO: Pod "downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518689ms
Mar  6 16:17:02.731: INFO: Pod "downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004856164s
STEP: Saw pod success
Mar  6 16:17:02.731: INFO: Pod "downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:02.733: INFO: Trying to get logs from node themisto pod downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:17:02.753: INFO: Waiting for pod downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:02.755: INFO: Pod downwardapi-volume-459cf689-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:02.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mf6pp" for this suite.
Mar  6 16:17:08.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:08.786: INFO: namespace: e2e-tests-projected-mf6pp, resource: bindings, ignored listing per whitelist
Mar  6 16:17:08.840: INFO: namespace e2e-tests-projected-mf6pp deletion completed in 6.081215605s

• [SLOW TEST:8.294 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:08.840: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hr5qd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 16:17:09.032: INFO: Waiting up to 5m0s for pod "downward-api-4a904bc7-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-hr5qd" to be "success or failure"
Mar  6 16:17:09.034: INFO: Pod "downward-api-4a904bc7-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642701ms
Mar  6 16:17:11.036: INFO: Pod "downward-api-4a904bc7-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003941938s
STEP: Saw pod success
Mar  6 16:17:11.036: INFO: Pod "downward-api-4a904bc7-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:11.038: INFO: Trying to get logs from node themisto pod downward-api-4a904bc7-402b-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 16:17:11.056: INFO: Waiting for pod downward-api-4a904bc7-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:11.058: INFO: Pod downward-api-4a904bc7-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:11.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hr5qd" for this suite.
Mar  6 16:17:17.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:17.114: INFO: namespace: e2e-tests-downward-api-hr5qd, resource: bindings, ignored listing per whitelist
Mar  6 16:17:17.143: INFO: namespace e2e-tests-downward-api-hr5qd deletion completed in 6.082052514s

• [SLOW TEST:8.303 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:17.143: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7bq67
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4f821e25-402b-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:17:17.332: INFO: Waiting up to 5m0s for pod "pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-7bq67" to be "success or failure"
Mar  6 16:17:17.333: INFO: Pod "pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494527ms
Mar  6 16:17:19.336: INFO: Pod "pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004401126s
STEP: Saw pod success
Mar  6 16:17:19.336: INFO: Pod "pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:19.338: INFO: Trying to get logs from node sponde pod pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:17:19.357: INFO: Waiting for pod pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:19.358: INFO: Pod pod-secrets-4f82d46d-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7bq67" for this suite.
Mar  6 16:17:25.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:25.390: INFO: namespace: e2e-tests-secrets-7bq67, resource: bindings, ignored listing per whitelist
Mar  6 16:17:25.444: INFO: namespace e2e-tests-secrets-7bq67 deletion completed in 6.082851071s

• [SLOW TEST:8.301 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:25.444: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dlpgk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-5474642b-402b-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:17:25.631: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-dlpgk" to be "success or failure"
Mar  6 16:17:25.633: INFO: Pod "pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59671ms
Mar  6 16:17:27.635: INFO: Pod "pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003936843s
STEP: Saw pod success
Mar  6 16:17:27.635: INFO: Pod "pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:27.637: INFO: Trying to get logs from node kronos pod pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:17:27.654: INFO: Waiting for pod pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:27.656: INFO: Pod pod-projected-secrets-547523e6-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:27.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dlpgk" for this suite.
Mar  6 16:17:33.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:33.715: INFO: namespace: e2e-tests-projected-dlpgk, resource: bindings, ignored listing per whitelist
Mar  6 16:17:33.739: INFO: namespace e2e-tests-projected-dlpgk deletion completed in 6.081199928s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-8xtx4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 16:17:33.914: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 16:17:33.920: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 16:17:33.922: INFO: 
Logging pods the kubelet thinks is on node kronos before test
Mar  6 16:17:33.927: INFO: kube-flannel-7krsb from kube-system started at 2019-03-06 12:08:05 +0000 UTC (1 container statuses recorded)
Mar  6 16:17:33.927: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 16:17:33.927: INFO: kube-dns-5575866df4-dkl5f from kube-system started at 2019-03-06 12:08:35 +0000 UTC (3 container statuses recorded)
Mar  6 16:17:33.927: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 16:17:33.927: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 16:17:33.927: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 16:17:33.927: INFO: haproxy-kronos from kube-system started at <nil> (0 container statuses recorded)
Mar  6 16:17:33.927: INFO: 
Logging pods the kubelet thinks is on node sponde before test
Mar  6 16:17:33.930: INFO: haproxy-sponde from kube-system started at <nil> (0 container statuses recorded)
Mar  6 16:17:33.930: INFO: kube-dns-5575866df4-q92k8 from kube-system started at 2019-03-06 12:08:33 +0000 UTC (3 container statuses recorded)
Mar  6 16:17:33.930: INFO: 	Container dnsmasq ready: true, restart count 0
Mar  6 16:17:33.930: INFO: 	Container kubedns ready: true, restart count 0
Mar  6 16:17:33.930: INFO: 	Container sidecar ready: true, restart count 0
Mar  6 16:17:33.930: INFO: kube-flannel-hrd9n from kube-system started at 2019-03-06 12:08:03 +0000 UTC (1 container statuses recorded)
Mar  6 16:17:33.931: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 16:17:33.931: INFO: tiller-deploy-df475665f-sxnwr from kube-system started at 2019-03-06 12:08:35 +0000 UTC (1 container statuses recorded)
Mar  6 16:17:33.931: INFO: 	Container tiller ready: true, restart count 0
Mar  6 16:17:33.931: INFO: 
Logging pods the kubelet thinks is on node themisto before test
Mar  6 16:17:33.935: INFO: kube-flannel-cc7kg from kube-system started at 2019-03-06 15:05:24 +0000 UTC (1 container statuses recorded)
Mar  6 16:17:33.935: INFO: 	Container kube-flannel ready: true, restart count 2
Mar  6 16:17:33.935: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 15:12:49 +0000 UTC (1 container statuses recorded)
Mar  6 16:17:33.935: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 16:17:33.935: INFO: haproxy-themisto from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15896ade2a2d32ba], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:34.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-8xtx4" for this suite.
Mar  6 16:17:40.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:41.014: INFO: namespace: e2e-tests-sched-pred-8xtx4, resource: bindings, ignored listing per whitelist
Mar  6 16:17:41.038: INFO: namespace e2e-tests-sched-pred-8xtx4 deletion completed in 6.081013346s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.299 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:41.039: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5ftbt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 16:17:41.221: INFO: Waiting up to 5m0s for pod "pod-5dbff079-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-5ftbt" to be "success or failure"
Mar  6 16:17:41.223: INFO: Pod "pod-5dbff079-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636714ms
Mar  6 16:17:43.226: INFO: Pod "pod-5dbff079-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004529784s
STEP: Saw pod success
Mar  6 16:17:43.226: INFO: Pod "pod-5dbff079-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:43.228: INFO: Trying to get logs from node kronos pod pod-5dbff079-402b-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:17:43.248: INFO: Waiting for pod pod-5dbff079-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:43.250: INFO: Pod pod-5dbff079-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:43.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5ftbt" for this suite.
Mar  6 16:17:49.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:49.333: INFO: namespace: e2e-tests-emptydir-5ftbt, resource: bindings, ignored listing per whitelist
Mar  6 16:17:49.333: INFO: namespace e2e-tests-emptydir-5ftbt deletion completed in 6.079186153s

• [SLOW TEST:8.294 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:49.333: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jsjlq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar  6 16:17:49.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 api-versions'
Mar  6 16:17:49.573: INFO: stderr: ""
Mar  6 16:17:49.573: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ndex.coreos.com/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jsjlq" for this suite.
Mar  6 16:17:55.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:17:55.644: INFO: namespace: e2e-tests-kubectl-jsjlq, resource: bindings, ignored listing per whitelist
Mar  6 16:17:55.656: INFO: namespace e2e-tests-kubectl-jsjlq deletion completed in 6.080298248s

• [SLOW TEST:6.323 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:17:55.657: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-pjqn5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-66771d92-402b-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 16:17:55.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-pjqn5" to be "success or failure"
Mar  6 16:17:55.856: INFO: Pod "pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 6.535255ms
Mar  6 16:17:57.859: INFO: Pod "pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009359977s
STEP: Saw pod success
Mar  6 16:17:57.859: INFO: Pod "pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:17:57.862: INFO: Trying to get logs from node kronos pod pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 16:17:57.881: INFO: Waiting for pod pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:17:57.882: INFO: Pod pod-configmaps-6677e356-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:17:57.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pjqn5" for this suite.
Mar  6 16:18:03.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:18:03.902: INFO: namespace: e2e-tests-configmap-pjqn5, resource: bindings, ignored listing per whitelist
Mar  6 16:18:03.953: INFO: namespace e2e-tests-configmap-pjqn5 deletion completed in 6.067259255s

• [SLOW TEST:8.296 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:18:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dtrnm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:18:06.158: INFO: Waiting up to 5m0s for pod "client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-pods-dtrnm" to be "success or failure"
Mar  6 16:18:06.160: INFO: Pod "client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867372ms
Mar  6 16:18:08.165: INFO: Pod "client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006471368s
STEP: Saw pod success
Mar  6 16:18:08.165: INFO: Pod "client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:18:08.167: INFO: Trying to get logs from node sponde pod client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007 container env3cont: <nil>
STEP: delete the pod
Mar  6 16:18:08.184: INFO: Waiting for pod client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:18:08.186: INFO: Pod client-envvars-6c9d6042-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:18:08.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dtrnm" for this suite.
Mar  6 16:18:50.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:18:50.248: INFO: namespace: e2e-tests-pods-dtrnm, resource: bindings, ignored listing per whitelist
Mar  6 16:18:50.267: INFO: namespace e2e-tests-pods-dtrnm deletion completed in 42.077690403s

• [SLOW TEST:46.313 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:18:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-zfqhh
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  6 16:18:50.452: INFO: Waiting up to 5m0s for pod "pod-8703d2b0-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-zfqhh" to be "success or failure"
Mar  6 16:18:50.454: INFO: Pod "pod-8703d2b0-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.646489ms
Mar  6 16:18:52.456: INFO: Pod "pod-8703d2b0-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004357679s
STEP: Saw pod success
Mar  6 16:18:52.456: INFO: Pod "pod-8703d2b0-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:18:52.458: INFO: Trying to get logs from node kronos pod pod-8703d2b0-402b-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:18:52.476: INFO: Waiting for pod pod-8703d2b0-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:18:52.477: INFO: Pod pod-8703d2b0-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:18:52.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zfqhh" for this suite.
Mar  6 16:18:58.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:18:58.493: INFO: namespace: e2e-tests-emptydir-zfqhh, resource: bindings, ignored listing per whitelist
Mar  6 16:18:58.550: INFO: namespace e2e-tests-emptydir-zfqhh deletion completed in 6.070121747s

• [SLOW TEST:8.284 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:18:58.550: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-mtq2z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  6 16:19:00.740: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8bf334e2-402b-11e9-9071-0a58ac100007,GenerateName:,Namespace:e2e-tests-events-mtq2z,SelfLink:/api/v1/namespaces/e2e-tests-events-mtq2z/pods/send-events-8bf334e2-402b-11e9-9071-0a58ac100007,UID:8bf388ee-402b-11e9-b3f2-0cc47aaad1b4,ResourceVersion:71390,Generation:0,CreationTimestamp:2019-03-06 16:18:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 725405118,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wwwqk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wwwqk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-wwwqk true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:themisto,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00291b400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00291b420}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:19:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:18:53 +0000 UTC  }],Message:,Reason:,HostIP:10.100.96.46,PodIP:172.16.10.90,StartTime:2019-03-06 16:19:53 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-06 16:19:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://0c547f907253602bce3f34bafd8a5cc1ac3ae1ff61ce1028e4bd56a2aac63ac8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  6 16:19:02.743: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  6 16:19:04.747: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:19:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-mtq2z" for this suite.
Mar  6 16:19:44.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:19:44.780: INFO: namespace: e2e-tests-events-mtq2z, resource: bindings, ignored listing per whitelist
Mar  6 16:19:44.840: INFO: namespace e2e-tests-events-mtq2z deletion completed in 40.080814546s

• [SLOW TEST:46.290 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:19:44.841: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bk7vn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 16:19:45.038: INFO: Waiting up to 5m0s for pod "downward-api-a78cabda-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-bk7vn" to be "success or failure"
Mar  6 16:19:45.039: INFO: Pod "downward-api-a78cabda-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.911958ms
Mar  6 16:19:47.042: INFO: Pod "downward-api-a78cabda-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004615427s
STEP: Saw pod success
Mar  6 16:19:47.042: INFO: Pod "downward-api-a78cabda-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:19:47.044: INFO: Trying to get logs from node sponde pod downward-api-a78cabda-402b-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 16:19:47.064: INFO: Waiting for pod downward-api-a78cabda-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:19:47.066: INFO: Pod downward-api-a78cabda-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:19:47.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bk7vn" for this suite.
Mar  6 16:19:53.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:19:53.134: INFO: namespace: e2e-tests-downward-api-bk7vn, resource: bindings, ignored listing per whitelist
Mar  6 16:19:53.152: INFO: namespace e2e-tests-downward-api-bk7vn deletion completed in 6.08250509s

• [SLOW TEST:8.312 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:19:53.152: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wvmzw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:19:53.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-wvmzw" to be "success or failure"
Mar  6 16:19:53.340: INFO: Pod "downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.404726ms
Mar  6 16:19:55.342: INFO: Pod "downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00394327s
STEP: Saw pod success
Mar  6 16:19:55.342: INFO: Pod "downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:19:55.344: INFO: Trying to get logs from node themisto pod downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:19:55.363: INFO: Waiting for pod downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:19:55.365: INFO: Pod downwardapi-volume-ac7f756c-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:19:55.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wvmzw" for this suite.
Mar  6 16:20:01.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:20:01.403: INFO: namespace: e2e-tests-downward-api-wvmzw, resource: bindings, ignored listing per whitelist
Mar  6 16:20:01.440: INFO: namespace e2e-tests-downward-api-wvmzw deletion completed in 6.072177609s

• [SLOW TEST:8.287 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:20:01.440: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mvdgb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:20:01.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-downward-api-mvdgb" to be "success or failure"
Mar  6 16:20:01.629: INFO: Pod "downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618514ms
Mar  6 16:20:03.632: INFO: Pod "downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00451389s
STEP: Saw pod success
Mar  6 16:20:03.632: INFO: Pod "downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:20:03.635: INFO: Trying to get logs from node themisto pod downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:20:03.655: INFO: Waiting for pod downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:20:03.656: INFO: Pod downwardapi-volume-b1704228-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:20:03.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mvdgb" for this suite.
Mar  6 16:20:09.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:20:09.679: INFO: namespace: e2e-tests-downward-api-mvdgb, resource: bindings, ignored listing per whitelist
Mar  6 16:20:09.737: INFO: namespace e2e-tests-downward-api-mvdgb deletion completed in 6.077165758s

• [SLOW TEST:8.297 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:20:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z65s5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 16:20:09.921: INFO: Waiting up to 5m0s for pod "pod-b661cc13-402b-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-z65s5" to be "success or failure"
Mar  6 16:20:09.922: INFO: Pod "pod-b661cc13-402b-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581183ms
Mar  6 16:20:11.925: INFO: Pod "pod-b661cc13-402b-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004348089s
STEP: Saw pod success
Mar  6 16:20:11.925: INFO: Pod "pod-b661cc13-402b-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:20:11.927: INFO: Trying to get logs from node sponde pod pod-b661cc13-402b-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:20:11.950: INFO: Waiting for pod pod-b661cc13-402b-11e9-9071-0a58ac100007 to disappear
Mar  6 16:20:11.952: INFO: Pod pod-b661cc13-402b-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:20:11.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z65s5" for this suite.
Mar  6 16:20:17.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:20:17.985: INFO: namespace: e2e-tests-emptydir-z65s5, resource: bindings, ignored listing per whitelist
Mar  6 16:20:18.037: INFO: namespace e2e-tests-emptydir-z65s5 deletion completed in 6.080524301s

• [SLOW TEST:8.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:20:18.037: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-kc5fp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 16:20:18.239: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:18.239: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:18.239: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:18.240: INFO: Number of nodes with available pods: 0
Mar  6 16:20:18.240: INFO: Node kronos is running more than one daemon pod
Mar  6 16:20:19.244: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:19.244: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:19.244: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:19.246: INFO: Number of nodes with available pods: 0
Mar  6 16:20:19.246: INFO: Node kronos is running more than one daemon pod
Mar  6 16:20:20.245: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.245: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.245: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.248: INFO: Number of nodes with available pods: 3
Mar  6 16:20:20.248: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  6 16:20:20.262: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.262: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.262: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:20:20.268: INFO: Number of nodes with available pods: 3
Mar  6 16:20:20.268: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-kc5fp, will wait for the garbage collector to delete the pods
Mar  6 16:20:21.336: INFO: Deleting DaemonSet.extensions daemon-set took: 6.919275ms
Mar  6 16:20:21.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.151714ms
Mar  6 16:21:03.539: INFO: Number of nodes with available pods: 0
Mar  6 16:21:03.539: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 16:21:03.541: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-kc5fp/daemonsets","resourceVersion":"72079"},"items":null}

Mar  6 16:21:03.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-kc5fp/pods","resourceVersion":"72079"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:21:03.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-kc5fp" for this suite.
Mar  6 16:21:09.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:21:09.573: INFO: namespace: e2e-tests-daemonsets-kc5fp, resource: bindings, ignored listing per whitelist
Mar  6 16:21:09.627: INFO: namespace e2e-tests-daemonsets-kc5fp deletion completed in 6.070977319s

• [SLOW TEST:51.590 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:21:09.627: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-w8gc9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t4tfq in namespace e2e-tests-proxy-w8gc9
I0306 16:21:09.820355      18 runners.go:184] Created replication controller with name: proxy-service-t4tfq, namespace: e2e-tests-proxy-w8gc9, replica count: 1
I0306 16:21:10.870711      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 16:21:11.870911      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 16:21:12.871179      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:13.871414      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:14.871627      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:15.871878      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:16.872116      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:17.872335      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 16:21:18.872508      18 runners.go:184] proxy-service-t4tfq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 16:21:18.876: INFO: setup took 9.072756806s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 7.036191ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 7.289414ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 7.335954ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 7.337587ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 7.298325ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 7.354597ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 7.374567ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 7.447847ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 7.540907ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 7.436543ms)
Mar  6 16:21:18.883: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 7.466704ms)
Mar  6 16:21:18.885: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 9.192706ms)
Mar  6 16:21:18.888: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 12.293983ms)
Mar  6 16:21:18.888: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 12.627243ms)
Mar  6 16:21:18.889: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 13.102817ms)
Mar  6 16:21:18.889: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 13.0384ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 3.071619ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 3.148936ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.163993ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 3.224475ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 3.223236ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.29952ms)
Mar  6 16:21:18.892: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 3.321168ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.567105ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.574516ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.538492ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 3.485345ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.685183ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 4.183601ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 4.171137ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 4.187662ms)
Mar  6 16:21:18.893: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 4.214907ms)
Mar  6 16:21:18.895: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.095674ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.475978ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.429911ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.46565ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.518343ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.663219ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.703579ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.776992ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.945804ms)
Mar  6 16:21:18.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.044535ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.40866ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.38815ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.880814ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.886112ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.963358ms)
Mar  6 16:21:18.897: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.961306ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.218371ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.427068ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.348178ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.332629ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.709183ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.948193ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.940451ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.975611ms)
Mar  6 16:21:18.900: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.971761ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.155549ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.124283ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.350166ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.335204ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.497963ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.617376ms)
Mar  6 16:21:18.901: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.686516ms)
Mar  6 16:21:18.904: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.510788ms)
Mar  6 16:21:18.904: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.974864ms)
Mar  6 16:21:18.904: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.250703ms)
Mar  6 16:21:18.904: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.175383ms)
Mar  6 16:21:18.904: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 3.25986ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 3.25752ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.355934ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.358813ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 3.36669ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 3.378813ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.436155ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.571825ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.604516ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.832018ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.921406ms)
Mar  6 16:21:18.905: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.937384ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.377918ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.444378ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.480894ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.437969ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.628257ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.795343ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.786889ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.850708ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.782177ms)
Mar  6 16:21:18.908: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.814647ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.260497ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.258995ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.566805ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.757069ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.732203ms)
Mar  6 16:21:18.909: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.898218ms)
Mar  6 16:21:18.911: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.221793ms)
Mar  6 16:21:18.911: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.281392ms)
Mar  6 16:21:18.911: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.331846ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.416004ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.662993ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.708943ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.709446ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.756341ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.752922ms)
Mar  6 16:21:18.912: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.748827ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.447024ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.493709ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.518254ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.674078ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 4.044531ms)
Mar  6 16:21:18.913: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.988303ms)
Mar  6 16:21:18.915: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.079416ms)
Mar  6 16:21:18.915: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.075125ms)
Mar  6 16:21:18.915: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.176316ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.523184ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.592317ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.788198ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.772407ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.851979ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.878446ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.953625ms)
Mar  6 16:21:18.916: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 2.980581ms)
Mar  6 16:21:18.917: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.276126ms)
Mar  6 16:21:18.917: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.373405ms)
Mar  6 16:21:18.917: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.427242ms)
Mar  6 16:21:18.917: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.413023ms)
Mar  6 16:21:18.917: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.717508ms)
Mar  6 16:21:18.919: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 1.847839ms)
Mar  6 16:21:18.919: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.297572ms)
Mar  6 16:21:18.919: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.316104ms)
Mar  6 16:21:18.919: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.288957ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.763468ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.866584ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.829466ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.814451ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.857426ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.904267ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 2.859142ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.345547ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.388687ms)
Mar  6 16:21:18.920: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.402927ms)
Mar  6 16:21:18.921: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.613902ms)
Mar  6 16:21:18.921: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.637906ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.065748ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.469211ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.448907ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.4742ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.572012ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.569686ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.592224ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.562932ms)
Mar  6 16:21:18.923: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.678076ms)
Mar  6 16:21:18.924: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.73936ms)
Mar  6 16:21:18.924: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.396296ms)
Mar  6 16:21:18.924: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.661959ms)
Mar  6 16:21:18.925: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.696789ms)
Mar  6 16:21:18.925: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.817915ms)
Mar  6 16:21:18.925: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.879394ms)
Mar  6 16:21:18.925: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.867051ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.154605ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.209065ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.210408ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.248931ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.410658ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.765886ms)
Mar  6 16:21:18.927: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.794925ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.786915ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.877904ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.89535ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 2.919981ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.15125ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.371054ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.420015ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.431188ms)
Mar  6 16:21:18.928: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.557553ms)
Mar  6 16:21:18.930: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.204589ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.278714ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.290891ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.349002ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.445977ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.442888ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.614743ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.687706ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.736028ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.713564ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.088222ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.089528ms)
Mar  6 16:21:18.931: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.167061ms)
Mar  6 16:21:18.932: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.247137ms)
Mar  6 16:21:18.932: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.298756ms)
Mar  6 16:21:18.932: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.397544ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.000108ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.229499ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.323582ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.413754ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.36961ms)
Mar  6 16:21:18.934: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.444232ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 3.089784ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 3.035367ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 3.194167ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 3.241291ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.263955ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.494737ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.457182ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.439237ms)
Mar  6 16:21:18.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.633104ms)
Mar  6 16:21:18.936: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.703249ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.288606ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.548103ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.556498ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.661366ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.625395ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.770372ms)
Mar  6 16:21:18.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.78963ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.970006ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.004998ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 3.180489ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 3.206291ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.17261ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.179843ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.469869ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.476043ms)
Mar  6 16:21:18.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.518519ms)
Mar  6 16:21:18.941: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.247099ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.363138ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.470039ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.63023ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.599584ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.628831ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.687499ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.827878ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 2.876373ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.875888ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.123025ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.167002ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.128622ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 3.127646ms)
Mar  6 16:21:18.942: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.174969ms)
Mar  6 16:21:18.943: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.333786ms)
Mar  6 16:21:18.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 1.892703ms)
Mar  6 16:21:18.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.093752ms)
Mar  6 16:21:18.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.149325ms)
Mar  6 16:21:18.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.186851ms)
Mar  6 16:21:18.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.75013ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.852856ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.814525ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.834741ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.930633ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.153003ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.176081ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 3.247991ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.230868ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.387079ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.483262ms)
Mar  6 16:21:18.946: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.509945ms)
Mar  6 16:21:18.948: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 1.934786ms)
Mar  6 16:21:18.948: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.097581ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.248552ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.230317ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.3222ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.301565ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.410413ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.544555ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.725168ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.740951ms)
Mar  6 16:21:18.949: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 2.912466ms)
Mar  6 16:21:18.950: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.448621ms)
Mar  6 16:21:18.950: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.422908ms)
Mar  6 16:21:18.950: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.454983ms)
Mar  6 16:21:18.950: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.490238ms)
Mar  6 16:21:18.950: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.486089ms)
Mar  6 16:21:18.952: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.079579ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.293938ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.388729ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.222241ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.344145ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.530757ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.554127ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.151164ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.369088ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.36833ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.15187ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 2.966331ms)
Mar  6 16:21:18.953: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 2.906295ms)
Mar  6 16:21:18.954: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.670103ms)
Mar  6 16:21:18.954: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.666172ms)
Mar  6 16:21:18.954: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 2.987977ms)
Mar  6 16:21:18.956: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.294513ms)
Mar  6 16:21:18.956: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.565348ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.709322ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.762576ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.742247ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.759837ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.772541ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.719675ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.782505ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.79576ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 2.925644ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.156078ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.438633ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.466297ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.623687ms)
Mar  6 16:21:18.957: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.686354ms)
Mar  6 16:21:18.959: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 1.909053ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:162/proxy/: bar (200; 2.090498ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.189833ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:460/proxy/: tls baz (200; 2.269923ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:443/proxy/... (200; 2.288046ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:1080/proxy/rewri... (200; 2.316867ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/http:proxy-service-t4tfq-mbmpm:1080/proxy/... (200; 2.395961ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/https:proxy-service-t4tfq-mbmpm:462/proxy/: tls qux (200; 2.517593ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm:160/proxy/: foo (200; 2.735879ms)
Mar  6 16:21:18.960: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-w8gc9/pods/proxy-service-t4tfq-mbmpm/proxy/rewriteme"... (200; 2.85484ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname2/proxy/: bar (200; 3.007356ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname1/proxy/: foo (200; 3.477668ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/http:proxy-service-t4tfq:portname1/proxy/: foo (200; 3.520161ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname2/proxy/: tls qux (200; 3.562751ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/proxy-service-t4tfq:portname2/proxy/: bar (200; 3.683578ms)
Mar  6 16:21:18.961: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-w8gc9/services/https:proxy-service-t4tfq:tlsportname1/proxy/: tls baz (200; 3.766822ms)
STEP: deleting ReplicationController proxy-service-t4tfq in namespace e2e-tests-proxy-w8gc9, will wait for the garbage collector to delete the pods
Mar  6 16:21:19.020: INFO: Deleting ReplicationController proxy-service-t4tfq took: 7.238696ms
Mar  6 16:21:19.121: INFO: Terminating ReplicationController proxy-service-t4tfq pods took: 100.190789ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:21:29.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-w8gc9" for this suite.
Mar  6 16:21:35.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:21:35.273: INFO: namespace: e2e-tests-proxy-w8gc9, resource: bindings, ignored listing per whitelist
Mar  6 16:21:35.297: INFO: namespace e2e-tests-proxy-w8gc9 deletion completed in 6.072523381s

• [SLOW TEST:25.671 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:21:35.298: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-md8xk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 16:21:35.474: INFO: PodSpec: initContainers in spec.initContainers
Mar  6 16:22:21.965: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e9613990-402b-11e9-9071-0a58ac100007", GenerateName:"", Namespace:"e2e-tests-init-container-md8xk", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-md8xk/pods/pod-init-e9613990-402b-11e9-9071-0a58ac100007", UID:"e9618d5d-402b-11e9-b3f2-0cc47aaad1b4", ResourceVersion:"72498", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687486095, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"474526854"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ckjjn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001d72500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckjjn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckjjn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ckjjn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c58878), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"themisto", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b38b40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c58900)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c58920)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c58928)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486150, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486150, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486150, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486090, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.100.96.46", PodIP:"172.16.10.95", StartTime:(*v1.Time)(0xc001e159e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002936b60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002936c40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://794e8cc3e93ce4507300e12dcb6b3d88f6f7c803d2bfbadcd8a66f0a4a9e629c"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e15a20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e15a00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:22:21.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-md8xk" for this suite.
Mar  6 16:22:43.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:22:44.015: INFO: namespace: e2e-tests-init-container-md8xk, resource: bindings, ignored listing per whitelist
Mar  6 16:22:44.046: INFO: namespace e2e-tests-init-container-md8xk deletion completed in 22.075111877s

• [SLOW TEST:68.748 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:22:44.046: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hg7f4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Mar  6 16:22:44.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-hg7f4'
Mar  6 16:22:44.449: INFO: stderr: ""
Mar  6 16:22:44.449: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar  6 16:22:45.452: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 16:22:45.452: INFO: Found 1 / 1
Mar  6 16:22:45.452: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 16:22:45.455: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 16:22:45.455: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  6 16:22:45.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 logs redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4'
Mar  6 16:22:45.548: INFO: stderr: ""
Mar  6 16:22:45.548: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 16:22:47.640 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 16:22:47.640 # Server started, Redis version 3.2.12\n1:M 06 Mar 16:22:47.640 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 16:22:47.640 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  6 16:22:45.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 log redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4 --tail=1'
Mar  6 16:22:45.640: INFO: stderr: ""
Mar  6 16:22:45.640: INFO: stdout: "1:M 06 Mar 16:22:47.640 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  6 16:22:45.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 log redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4 --limit-bytes=1'
Mar  6 16:22:45.716: INFO: stderr: ""
Mar  6 16:22:45.716: INFO: stdout: " "
STEP: exposing timestamps
Mar  6 16:22:45.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 log redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4 --tail=1 --timestamps'
Mar  6 16:22:45.793: INFO: stderr: ""
Mar  6 16:22:45.793: INFO: stdout: "2019-03-06T16:22:47.640235587Z 1:M 06 Mar 16:22:47.640 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  6 16:22:48.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 log redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4 --since=1s'
Mar  6 16:22:48.383: INFO: stderr: ""
Mar  6 16:22:48.383: INFO: stdout: ""
Mar  6 16:22:48.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 log redis-master-2b6kh redis-master --namespace=e2e-tests-kubectl-hg7f4 --since=24h'
Mar  6 16:22:48.505: INFO: stderr: ""
Mar  6 16:22:48.505: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 16:22:47.640 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 16:22:47.640 # Server started, Redis version 3.2.12\n1:M 06 Mar 16:22:47.640 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 16:22:47.640 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Mar  6 16:22:48.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-hg7f4'
Mar  6 16:22:48.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 16:22:48.585: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  6 16:22:48.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-hg7f4'
Mar  6 16:22:48.661: INFO: stderr: "No resources found.\n"
Mar  6 16:22:48.661: INFO: stdout: ""
Mar  6 16:22:48.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -l name=nginx --namespace=e2e-tests-kubectl-hg7f4 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 16:22:48.730: INFO: stderr: ""
Mar  6 16:22:48.730: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:22:48.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hg7f4" for this suite.
Mar  6 16:22:54.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:22:54.810: INFO: namespace: e2e-tests-kubectl-hg7f4, resource: bindings, ignored listing per whitelist
Mar  6 16:22:54.813: INFO: namespace e2e-tests-kubectl-hg7f4 deletion completed in 6.079633971s

• [SLOW TEST:10.767 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:22:54.813: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-pxljv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 16:22:54.991: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:22:58.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-pxljv" for this suite.
Mar  6 16:23:20.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:23:20.321: INFO: namespace: e2e-tests-init-container-pxljv, resource: bindings, ignored listing per whitelist
Mar  6 16:23:20.330: INFO: namespace e2e-tests-init-container-pxljv deletion completed in 22.071766129s

• [SLOW TEST:25.517 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:23:20.330: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-t4twt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-27fc78bc-402c-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:23:20.525: INFO: Waiting up to 5m0s for pod "pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-t4twt" to be "success or failure"
Mar  6 16:23:20.526: INFO: Pod "pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436397ms
Mar  6 16:23:22.529: INFO: Pod "pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004343173s
STEP: Saw pod success
Mar  6 16:23:22.529: INFO: Pod "pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:23:22.531: INFO: Trying to get logs from node themisto pod pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:23:22.549: INFO: Waiting for pod pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:23:22.550: INFO: Pod pod-secrets-27fdb201-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:23:22.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t4twt" for this suite.
Mar  6 16:23:28.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:23:28.603: INFO: namespace: e2e-tests-secrets-t4twt, resource: bindings, ignored listing per whitelist
Mar  6 16:23:28.628: INFO: namespace e2e-tests-secrets-t4twt deletion completed in 6.07476298s

• [SLOW TEST:8.298 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:23:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xpxnv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  6 16:23:28.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:28.953: INFO: stderr: ""
Mar  6 16:23:28.953: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 16:23:28.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:29.032: INFO: stderr: ""
Mar  6 16:23:29.032: INFO: stdout: "update-demo-nautilus-l2858 update-demo-nautilus-ngjc6 "
Mar  6 16:23:29.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:29.101: INFO: stderr: ""
Mar  6 16:23:29.101: INFO: stdout: ""
Mar  6 16:23:29.101: INFO: update-demo-nautilus-l2858 is created but not running
Mar  6 16:23:34.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:34.192: INFO: stderr: ""
Mar  6 16:23:34.192: INFO: stdout: "update-demo-nautilus-l2858 update-demo-nautilus-ngjc6 "
Mar  6 16:23:34.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:34.263: INFO: stderr: ""
Mar  6 16:23:34.263: INFO: stdout: "true"
Mar  6 16:23:34.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:34.330: INFO: stderr: ""
Mar  6 16:23:34.330: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:23:34.330: INFO: validating pod update-demo-nautilus-l2858
Mar  6 16:23:34.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:23:34.334: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:23:34.334: INFO: update-demo-nautilus-l2858 is verified up and running
Mar  6 16:23:34.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-ngjc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:34.410: INFO: stderr: ""
Mar  6 16:23:34.410: INFO: stdout: "true"
Mar  6 16:23:34.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-ngjc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:34.477: INFO: stderr: ""
Mar  6 16:23:34.477: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:23:34.477: INFO: validating pod update-demo-nautilus-ngjc6
Mar  6 16:23:34.481: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:23:34.481: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:23:34.481: INFO: update-demo-nautilus-ngjc6 is verified up and running
STEP: scaling down the replication controller
Mar  6 16:23:34.482: INFO: scanned /root for discovery docs: <nil>
Mar  6 16:23:34.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:35.583: INFO: stderr: ""
Mar  6 16:23:35.583: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 16:23:35.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:35.660: INFO: stderr: ""
Mar  6 16:23:35.660: INFO: stdout: "update-demo-nautilus-l2858 update-demo-nautilus-ngjc6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  6 16:23:40.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:40.746: INFO: stderr: ""
Mar  6 16:23:40.746: INFO: stdout: "update-demo-nautilus-l2858 "
Mar  6 16:23:40.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:40.823: INFO: stderr: ""
Mar  6 16:23:40.823: INFO: stdout: "true"
Mar  6 16:23:40.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:40.901: INFO: stderr: ""
Mar  6 16:23:40.901: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:23:40.901: INFO: validating pod update-demo-nautilus-l2858
Mar  6 16:23:40.904: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:23:40.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:23:40.905: INFO: update-demo-nautilus-l2858 is verified up and running
STEP: scaling up the replication controller
Mar  6 16:23:40.905: INFO: scanned /root for discovery docs: <nil>
Mar  6 16:23:40.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:41.997: INFO: stderr: ""
Mar  6 16:23:41.997: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 16:23:41.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.071: INFO: stderr: ""
Mar  6 16:23:42.071: INFO: stdout: "update-demo-nautilus-jn5gn update-demo-nautilus-l2858 "
Mar  6 16:23:42.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-jn5gn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.148: INFO: stderr: ""
Mar  6 16:23:42.148: INFO: stdout: "true"
Mar  6 16:23:42.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-jn5gn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.219: INFO: stderr: ""
Mar  6 16:23:42.219: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:23:42.219: INFO: validating pod update-demo-nautilus-jn5gn
Mar  6 16:23:42.223: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:23:42.223: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:23:42.223: INFO: update-demo-nautilus-jn5gn is verified up and running
Mar  6 16:23:42.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.299: INFO: stderr: ""
Mar  6 16:23:42.299: INFO: stdout: "true"
Mar  6 16:23:42.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-l2858 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.370: INFO: stderr: ""
Mar  6 16:23:42.370: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:23:42.370: INFO: validating pod update-demo-nautilus-l2858
Mar  6 16:23:42.373: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:23:42.373: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:23:42.373: INFO: update-demo-nautilus-l2858 is verified up and running
STEP: using delete to clean up resources
Mar  6 16:23:42.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.446: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 16:23:42.446: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 16:23:42.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-xpxnv'
Mar  6 16:23:42.520: INFO: stderr: "No resources found.\n"
Mar  6 16:23:42.520: INFO: stdout: ""
Mar  6 16:23:42.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -l name=update-demo --namespace=e2e-tests-kubectl-xpxnv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 16:23:42.595: INFO: stderr: ""
Mar  6 16:23:42.595: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:23:42.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xpxnv" for this suite.
Mar  6 16:24:04.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:24:04.637: INFO: namespace: e2e-tests-kubectl-xpxnv, resource: bindings, ignored listing per whitelist
Mar  6 16:24:04.675: INFO: namespace e2e-tests-kubectl-xpxnv deletion completed in 22.076515472s

• [SLOW TEST:36.046 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:24:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-4qpbd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar  6 16:24:04.860: INFO: Waiting up to 5m0s for pod "client-containers-426ab9fe-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-containers-4qpbd" to be "success or failure"
Mar  6 16:24:04.861: INFO: Pod "client-containers-426ab9fe-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.414844ms
Mar  6 16:24:06.864: INFO: Pod "client-containers-426ab9fe-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003887142s
Mar  6 16:24:08.867: INFO: Pod "client-containers-426ab9fe-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006648537s
STEP: Saw pod success
Mar  6 16:24:08.867: INFO: Pod "client-containers-426ab9fe-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:24:08.869: INFO: Trying to get logs from node sponde pod client-containers-426ab9fe-402c-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:24:08.889: INFO: Waiting for pod client-containers-426ab9fe-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:24:08.890: INFO: Pod client-containers-426ab9fe-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:24:08.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4qpbd" for this suite.
Mar  6 16:24:14.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:24:14.926: INFO: namespace: e2e-tests-containers-4qpbd, resource: bindings, ignored listing per whitelist
Mar  6 16:24:14.973: INFO: namespace e2e-tests-containers-4qpbd deletion completed in 6.079529972s

• [SLOW TEST:10.299 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:24:14.973: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-7ttph
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar  6 16:24:15.159: INFO: Waiting up to 5m0s for pod "var-expansion-488e0f68-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-var-expansion-7ttph" to be "success or failure"
Mar  6 16:24:15.161: INFO: Pod "var-expansion-488e0f68-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.498909ms
Mar  6 16:24:17.163: INFO: Pod "var-expansion-488e0f68-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004162357s
STEP: Saw pod success
Mar  6 16:24:17.163: INFO: Pod "var-expansion-488e0f68-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:24:17.165: INFO: Trying to get logs from node kronos pod var-expansion-488e0f68-402c-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 16:24:17.184: INFO: Waiting for pod var-expansion-488e0f68-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:24:17.186: INFO: Pod var-expansion-488e0f68-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:24:17.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7ttph" for this suite.
Mar  6 16:24:23.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:24:23.222: INFO: namespace: e2e-tests-var-expansion-7ttph, resource: bindings, ignored listing per whitelist
Mar  6 16:24:23.263: INFO: namespace e2e-tests-var-expansion-7ttph deletion completed in 6.07397762s

• [SLOW TEST:8.290 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:24:23.263: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7x8z4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4d7f4e9b-402c-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:24:23.455: INFO: Waiting up to 5m0s for pod "pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-7x8z4" to be "success or failure"
Mar  6 16:24:23.457: INFO: Pod "pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452964ms
Mar  6 16:24:25.459: INFO: Pod "pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003486152s
STEP: Saw pod success
Mar  6 16:24:25.459: INFO: Pod "pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:24:25.460: INFO: Trying to get logs from node themisto pod pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:24:25.477: INFO: Waiting for pod pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:24:25.478: INFO: Pod pod-secrets-4d8020f6-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:24:25.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7x8z4" for this suite.
Mar  6 16:24:31.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:24:31.526: INFO: namespace: e2e-tests-secrets-7x8z4, resource: bindings, ignored listing per whitelist
Mar  6 16:24:31.556: INFO: namespace e2e-tests-secrets-7x8z4 deletion completed in 6.07492534s

• [SLOW TEST:8.293 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:24:31.557: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gt9sb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 16:24:34.263: INFO: Successfully updated pod "labelsupdate52702ac3-402c-11e9-9071-0a58ac100007"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:24:38.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gt9sb" for this suite.
Mar  6 16:25:00.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:25:00.356: INFO: namespace: e2e-tests-downward-api-gt9sb, resource: bindings, ignored listing per whitelist
Mar  6 16:25:00.356: INFO: namespace e2e-tests-downward-api-gt9sb deletion completed in 22.069696348s

• [SLOW TEST:28.800 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:25:00.356: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-jjk2c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0306 16:25:31.056494      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 16:25:31.056: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:25:31.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jjk2c" for this suite.
Mar  6 16:25:37.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:25:37.112: INFO: namespace: e2e-tests-gc-jjk2c, resource: bindings, ignored listing per whitelist
Mar  6 16:25:37.141: INFO: namespace e2e-tests-gc-jjk2c deletion completed in 6.08131688s

• [SLOW TEST:36.785 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:25:37.141: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4prgm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:25:37.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-4prgm" to be "success or failure"
Mar  6 16:25:37.329: INFO: Pod "downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628061ms
Mar  6 16:25:39.332: INFO: Pod "downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004723525s
STEP: Saw pod success
Mar  6 16:25:39.333: INFO: Pod "downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:25:39.335: INFO: Trying to get logs from node sponde pod downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:25:39.356: INFO: Waiting for pod downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:25:39.358: INFO: Pod downwardapi-volume-79880379-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:25:39.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4prgm" for this suite.
Mar  6 16:25:45.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:25:45.415: INFO: namespace: e2e-tests-projected-4prgm, resource: bindings, ignored listing per whitelist
Mar  6 16:25:45.449: INFO: namespace e2e-tests-projected-4prgm deletion completed in 6.083963415s

• [SLOW TEST:8.308 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:25:45.449: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-qwqbd
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-7e7bd07c-402c-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7e7bd07c-402c-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:25:49.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qwqbd" for this suite.
Mar  6 16:26:11.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:26:11.710: INFO: namespace: e2e-tests-configmap-qwqbd, resource: bindings, ignored listing per whitelist
Mar  6 16:26:11.752: INFO: namespace e2e-tests-configmap-qwqbd deletion completed in 22.075421596s

• [SLOW TEST:26.303 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:26:11.752: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jwp78
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8e28feaa-402c-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 16:26:11.941: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-jwp78" to be "success or failure"
Mar  6 16:26:11.943: INFO: Pod "pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642123ms
Mar  6 16:26:13.945: INFO: Pod "pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003976939s
STEP: Saw pod success
Mar  6 16:26:13.945: INFO: Pod "pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:26:13.948: INFO: Trying to get logs from node themisto pod pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 16:26:13.965: INFO: Waiting for pod pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:26:13.966: INFO: Pod pod-projected-configmaps-8e29b312-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:26:13.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jwp78" for this suite.
Mar  6 16:26:19.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:26:20.047: INFO: namespace: e2e-tests-projected-jwp78, resource: bindings, ignored listing per whitelist
Mar  6 16:26:20.047: INFO: namespace e2e-tests-projected-jwp78 deletion completed in 6.077719455s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:26:20.047: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-r95mg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar  6 16:26:20.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 create -f - --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:20.364: INFO: stderr: ""
Mar  6 16:26:20.364: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 16:26:20.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:20.447: INFO: stderr: ""
Mar  6 16:26:20.447: INFO: stdout: "update-demo-nautilus-bqtg4 update-demo-nautilus-rlzqj "
Mar  6 16:26:20.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-bqtg4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:20.520: INFO: stderr: ""
Mar  6 16:26:20.520: INFO: stdout: ""
Mar  6 16:26:20.520: INFO: update-demo-nautilus-bqtg4 is created but not running
Mar  6 16:26:25.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:25.602: INFO: stderr: ""
Mar  6 16:26:25.602: INFO: stdout: "update-demo-nautilus-bqtg4 update-demo-nautilus-rlzqj "
Mar  6 16:26:25.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-bqtg4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:25.677: INFO: stderr: ""
Mar  6 16:26:25.677: INFO: stdout: "true"
Mar  6 16:26:25.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-bqtg4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:25.749: INFO: stderr: ""
Mar  6 16:26:25.749: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:26:25.749: INFO: validating pod update-demo-nautilus-bqtg4
Mar  6 16:26:25.753: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:26:25.753: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:26:25.753: INFO: update-demo-nautilus-bqtg4 is verified up and running
Mar  6 16:26:25.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-rlzqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:25.822: INFO: stderr: ""
Mar  6 16:26:25.822: INFO: stdout: "true"
Mar  6 16:26:25.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-nautilus-rlzqj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:25.891: INFO: stderr: ""
Mar  6 16:26:25.891: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 16:26:25.891: INFO: validating pod update-demo-nautilus-rlzqj
Mar  6 16:26:25.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 16:26:25.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 16:26:25.894: INFO: update-demo-nautilus-rlzqj is verified up and running
STEP: rolling-update to new replication controller
Mar  6 16:26:25.895: INFO: scanned /root for discovery docs: <nil>
Mar  6 16:26:25.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.220: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 16:26:48.220: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 16:26:48.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.311: INFO: stderr: ""
Mar  6 16:26:48.311: INFO: stdout: "update-demo-kitten-fb9jv update-demo-kitten-qfsj2 "
Mar  6 16:26:48.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-kitten-fb9jv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.380: INFO: stderr: ""
Mar  6 16:26:48.380: INFO: stdout: "true"
Mar  6 16:26:48.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-kitten-fb9jv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.446: INFO: stderr: ""
Mar  6 16:26:48.446: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  6 16:26:48.446: INFO: validating pod update-demo-kitten-fb9jv
Mar  6 16:26:48.450: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 16:26:48.450: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 16:26:48.450: INFO: update-demo-kitten-fb9jv is verified up and running
Mar  6 16:26:48.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-kitten-qfsj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.531: INFO: stderr: ""
Mar  6 16:26:48.531: INFO: stdout: "true"
Mar  6 16:26:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 get pods update-demo-kitten-qfsj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r95mg'
Mar  6 16:26:48.606: INFO: stderr: ""
Mar  6 16:26:48.606: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  6 16:26:48.606: INFO: validating pod update-demo-kitten-qfsj2
Mar  6 16:26:48.609: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 16:26:48.609: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 16:26:48.609: INFO: update-demo-kitten-qfsj2 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:26:48.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r95mg" for this suite.
Mar  6 16:27:10.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:27:10.677: INFO: namespace: e2e-tests-kubectl-r95mg, resource: bindings, ignored listing per whitelist
Mar  6 16:27:10.683: INFO: namespace e2e-tests-kubectl-r95mg deletion completed in 22.070257797s

• [SLOW TEST:50.636 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:27:10.683: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-tkbcr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar  6 16:27:10.865: INFO: Waiting up to 5m0s for pod "client-containers-b148c960-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-containers-tkbcr" to be "success or failure"
Mar  6 16:27:10.867: INFO: Pod "client-containers-b148c960-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555709ms
Mar  6 16:27:12.869: INFO: Pod "client-containers-b148c960-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003783796s
STEP: Saw pod success
Mar  6 16:27:12.869: INFO: Pod "client-containers-b148c960-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:27:12.871: INFO: Trying to get logs from node kronos pod client-containers-b148c960-402c-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:27:12.888: INFO: Waiting for pod client-containers-b148c960-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:27:12.890: INFO: Pod client-containers-b148c960-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:27:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-tkbcr" for this suite.
Mar  6 16:27:18.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:27:18.934: INFO: namespace: e2e-tests-containers-tkbcr, resource: bindings, ignored listing per whitelist
Mar  6 16:27:18.972: INFO: namespace e2e-tests-containers-tkbcr deletion completed in 6.07919435s

• [SLOW TEST:8.289 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:27:18.972: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dvsw6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 16:27:19.157: INFO: Waiting up to 5m0s for pod "pod-b639e687-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-dvsw6" to be "success or failure"
Mar  6 16:27:19.159: INFO: Pod "pod-b639e687-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526253ms
Mar  6 16:27:21.162: INFO: Pod "pod-b639e687-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004281527s
STEP: Saw pod success
Mar  6 16:27:21.162: INFO: Pod "pod-b639e687-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:27:21.164: INFO: Trying to get logs from node kronos pod pod-b639e687-402c-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:27:21.183: INFO: Waiting for pod pod-b639e687-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:27:21.185: INFO: Pod pod-b639e687-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:27:21.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dvsw6" for this suite.
Mar  6 16:27:27.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:27:27.238: INFO: namespace: e2e-tests-emptydir-dvsw6, resource: bindings, ignored listing per whitelist
Mar  6 16:27:27.268: INFO: namespace e2e-tests-emptydir-dvsw6 deletion completed in 6.080093395s

• [SLOW TEST:8.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:27:27.268: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-n5sq6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 16:27:27.449: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:27:30.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-n5sq6" for this suite.
Mar  6 16:27:36.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:27:36.734: INFO: namespace: e2e-tests-init-container-n5sq6, resource: bindings, ignored listing per whitelist
Mar  6 16:27:36.749: INFO: namespace e2e-tests-init-container-n5sq6 deletion completed in 6.069343551s

• [SLOW TEST:9.481 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:27:36.749: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bvtrg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 16:27:39.451: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007"
Mar  6 16:27:39.451: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-pods-bvtrg" to be "terminated due to deadline exceeded"
Mar  6 16:27:39.453: INFO: Pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007": Phase="Running", Reason="", readiness=true. Elapsed: 1.876108ms
Mar  6 16:27:41.455: INFO: Pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007": Phase="Running", Reason="", readiness=true. Elapsed: 2.004742386s
Mar  6 16:27:43.458: INFO: Pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00740271s
Mar  6 16:27:43.458: INFO: Pod "pod-update-activedeadlineseconds-c0d1c293-402c-11e9-9071-0a58ac100007" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:27:43.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bvtrg" for this suite.
Mar  6 16:27:49.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:27:49.519: INFO: namespace: e2e-tests-pods-bvtrg, resource: bindings, ignored listing per whitelist
Mar  6 16:27:49.539: INFO: namespace e2e-tests-pods-bvtrg deletion completed in 6.076534556s

• [SLOW TEST:12.790 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:27:49.539: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-lp6qr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  6 16:27:49.722: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74554,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 16:27:49.722: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74554,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  6 16:27:59.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74592,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 16:27:59.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74592,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  6 16:28:09.739: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74630,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 16:28:09.739: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74630,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  6 16:28:19.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74668,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 16:28:19.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-a,UID:c8724e33-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74668,Generation:0,CreationTimestamp:2019-03-06 16:27:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  6 16:28:29.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-b,UID:e04e94cf-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74706,Generation:0,CreationTimestamp:2019-03-06 16:28:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 16:28:29.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-b,UID:e04e94cf-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74706,Generation:0,CreationTimestamp:2019-03-06 16:28:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  6 16:28:39.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-b,UID:e04e94cf-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74744,Generation:0,CreationTimestamp:2019-03-06 16:28:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 16:28:39.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp6qr,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp6qr/configmaps/e2e-watch-test-configmap-b,UID:e04e94cf-402c-11e9-b3f2-0cc47aaad1b4,ResourceVersion:74744,Generation:0,CreationTimestamp:2019-03-06 16:28:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:28:49.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lp6qr" for this suite.
Mar  6 16:28:55.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:28:55.798: INFO: namespace: e2e-tests-watch-lp6qr, resource: bindings, ignored listing per whitelist
Mar  6 16:28:55.858: INFO: namespace e2e-tests-watch-lp6qr deletion completed in 6.086490612s

• [SLOW TEST:66.319 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:28:55.858: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-f6z4w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 16:28:56.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-f6z4w" to be "success or failure"
Mar  6 16:28:56.043: INFO: Pod "downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.640316ms
Mar  6 16:28:58.046: INFO: Pod "downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004664229s
STEP: Saw pod success
Mar  6 16:28:58.046: INFO: Pod "downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:28:58.048: INFO: Trying to get logs from node themisto pod downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007 container client-container: <nil>
STEP: delete the pod
Mar  6 16:28:58.067: INFO: Waiting for pod downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:28:58.069: INFO: Pod downwardapi-volume-eff94584-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:28:58.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f6z4w" for this suite.
Mar  6 16:29:04.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:29:04.120: INFO: namespace: e2e-tests-projected-f6z4w, resource: bindings, ignored listing per whitelist
Mar  6 16:29:04.151: INFO: namespace e2e-tests-projected-f6z4w deletion completed in 6.077967651s

• [SLOW TEST:8.292 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:29:04.151: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k8stg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-f4eb11c9-402c-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:29:04.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007" in namespace "e2e-tests-projected-k8stg" to be "success or failure"
Mar  6 16:29:04.343: INFO: Pod "pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568801ms
Mar  6 16:29:06.346: INFO: Pod "pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004308336s
STEP: Saw pod success
Mar  6 16:29:06.346: INFO: Pod "pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:29:06.348: INFO: Trying to get logs from node kronos pod pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:29:06.370: INFO: Waiting for pod pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007 to disappear
Mar  6 16:29:06.372: INFO: Pod pod-projected-secrets-f4ebe704-402c-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:29:06.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k8stg" for this suite.
Mar  6 16:29:12.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:29:12.412: INFO: namespace: e2e-tests-projected-k8stg, resource: bindings, ignored listing per whitelist
Mar  6 16:29:12.455: INFO: namespace e2e-tests-projected-k8stg deletion completed in 6.079558878s

• [SLOW TEST:8.304 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:29:12.455: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mpm7w
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-f9df25ba-402c-11e9-9071-0a58ac100007
STEP: Creating secret with name s-test-opt-upd-f9df2604-402c-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f9df25ba-402c-11e9-9071-0a58ac100007
STEP: Updating secret s-test-opt-upd-f9df2604-402c-11e9-9071-0a58ac100007
STEP: Creating secret with name s-test-opt-create-f9df262c-402c-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:29:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mpm7w" for this suite.
Mar  6 16:29:40.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:29:40.791: INFO: namespace: e2e-tests-projected-mpm7w, resource: bindings, ignored listing per whitelist
Mar  6 16:29:40.809: INFO: namespace e2e-tests-projected-mpm7w deletion completed in 22.070403935s

• [SLOW TEST:28.354 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:29:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-92dv9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:29:40.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-92dv9" for this suite.
Mar  6 16:29:47.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:29:47.056: INFO: namespace: e2e-tests-services-92dv9, resource: bindings, ignored listing per whitelist
Mar  6 16:29:47.073: INFO: namespace e2e-tests-services-92dv9 deletion completed in 6.077063463s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.264 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:29:47.073: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lpr7n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar  6 16:29:47.250: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-889989848 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:29:47.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lpr7n" for this suite.
Mar  6 16:29:53.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:29:53.332: INFO: namespace: e2e-tests-kubectl-lpr7n, resource: bindings, ignored listing per whitelist
Mar  6 16:29:53.395: INFO: namespace e2e-tests-kubectl-lpr7n deletion completed in 6.075789963s

• [SLOW TEST:6.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:29:53.395: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-nx877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar  6 16:29:53.574: INFO: Waiting up to 5m0s for pod "var-expansion-1244227d-402d-11e9-9071-0a58ac100007" in namespace "e2e-tests-var-expansion-nx877" to be "success or failure"
Mar  6 16:29:53.576: INFO: Pod "var-expansion-1244227d-402d-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490577ms
Mar  6 16:29:55.578: INFO: Pod "var-expansion-1244227d-402d-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004045794s
STEP: Saw pod success
Mar  6 16:29:55.578: INFO: Pod "var-expansion-1244227d-402d-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:29:55.581: INFO: Trying to get logs from node themisto pod var-expansion-1244227d-402d-11e9-9071-0a58ac100007 container dapi-container: <nil>
STEP: delete the pod
Mar  6 16:29:55.603: INFO: Waiting for pod var-expansion-1244227d-402d-11e9-9071-0a58ac100007 to disappear
Mar  6 16:29:55.605: INFO: Pod var-expansion-1244227d-402d-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:29:55.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-nx877" for this suite.
Mar  6 16:30:01.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:30:01.651: INFO: namespace: e2e-tests-var-expansion-nx877, resource: bindings, ignored listing per whitelist
Mar  6 16:30:01.691: INFO: namespace e2e-tests-var-expansion-nx877 deletion completed in 6.082679161s

• [SLOW TEST:8.296 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:30:01.691: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-fhdgt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:30:03.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-fhdgt" for this suite.
Mar  6 16:30:43.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:30:43.965: INFO: namespace: e2e-tests-kubelet-test-fhdgt, resource: bindings, ignored listing per whitelist
Mar  6 16:30:43.976: INFO: namespace e2e-tests-kubelet-test-fhdgt deletion completed in 40.078547209s

• [SLOW TEST:42.284 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:30:43.976: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vcg9r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 16:30:44.163: INFO: Waiting up to 5m0s for pod "pod-306b70a4-402d-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-vcg9r" to be "success or failure"
Mar  6 16:30:44.165: INFO: Pod "pod-306b70a4-402d-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488179ms
Mar  6 16:30:46.167: INFO: Pod "pod-306b70a4-402d-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00385121s
STEP: Saw pod success
Mar  6 16:30:46.167: INFO: Pod "pod-306b70a4-402d-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:30:46.170: INFO: Trying to get logs from node themisto pod pod-306b70a4-402d-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:30:46.189: INFO: Waiting for pod pod-306b70a4-402d-11e9-9071-0a58ac100007 to disappear
Mar  6 16:30:46.191: INFO: Pod pod-306b70a4-402d-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:30:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vcg9r" for this suite.
Mar  6 16:30:52.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:30:52.223: INFO: namespace: e2e-tests-emptydir-vcg9r, resource: bindings, ignored listing per whitelist
Mar  6 16:30:52.267: INFO: namespace e2e-tests-emptydir-vcg9r deletion completed in 6.072395624s

• [SLOW TEST:8.291 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:30:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-hfm82
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:30:52.456: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  6 16:30:52.463: INFO: Number of nodes with available pods: 0
Mar  6 16:30:52.463: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  6 16:30:52.477: INFO: Number of nodes with available pods: 0
Mar  6 16:30:52.477: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:53.479: INFO: Number of nodes with available pods: 1
Mar  6 16:30:53.479: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  6 16:30:53.491: INFO: Number of nodes with available pods: 1
Mar  6 16:30:53.491: INFO: Number of running nodes: 0, number of available pods: 1
Mar  6 16:30:54.494: INFO: Number of nodes with available pods: 0
Mar  6 16:30:54.494: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  6 16:30:54.502: INFO: Number of nodes with available pods: 0
Mar  6 16:30:54.502: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:55.505: INFO: Number of nodes with available pods: 0
Mar  6 16:30:55.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:56.505: INFO: Number of nodes with available pods: 0
Mar  6 16:30:56.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:57.505: INFO: Number of nodes with available pods: 0
Mar  6 16:30:57.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:58.505: INFO: Number of nodes with available pods: 0
Mar  6 16:30:58.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:30:59.505: INFO: Number of nodes with available pods: 0
Mar  6 16:30:59.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:00.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:00.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:01.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:01.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:02.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:02.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:03.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:03.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:04.506: INFO: Number of nodes with available pods: 0
Mar  6 16:31:04.506: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:05.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:05.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:06.506: INFO: Number of nodes with available pods: 0
Mar  6 16:31:06.506: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:07.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:07.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:08.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:08.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:09.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:09.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:10.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:10.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:11.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:11.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:12.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:12.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:13.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:13.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:14.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:14.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:15.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:15.506: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:16.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:16.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:17.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:17.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:18.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:18.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:19.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:19.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:20.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:20.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:21.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:21.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:22.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:22.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:23.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:23.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:24.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:24.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:25.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:25.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:26.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:26.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:27.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:27.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:28.505: INFO: Number of nodes with available pods: 0
Mar  6 16:31:28.505: INFO: Node kronos is running more than one daemon pod
Mar  6 16:31:29.505: INFO: Number of nodes with available pods: 1
Mar  6 16:31:29.505: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-hfm82, will wait for the garbage collector to delete the pods
Mar  6 16:31:29.569: INFO: Deleting DaemonSet.extensions daemon-set took: 7.383867ms
Mar  6 16:31:29.670: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.222458ms
Mar  6 16:32:13.572: INFO: Number of nodes with available pods: 0
Mar  6 16:32:13.572: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 16:32:13.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-hfm82/daemonsets","resourceVersion":"75845"},"items":null}

Mar  6 16:32:13.576: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-hfm82/pods","resourceVersion":"75845"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:32:13.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-hfm82" for this suite.
Mar  6 16:32:19.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:32:19.653: INFO: namespace: e2e-tests-daemonsets-hfm82, resource: bindings, ignored listing per whitelist
Mar  6 16:32:19.682: INFO: namespace e2e-tests-daemonsets-hfm82 deletion completed in 6.086013587s

• [SLOW TEST:87.415 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:32:19.683: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7n6kl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 16:32:19.866: INFO: Waiting up to 5m0s for pod "pod-69768d1a-402d-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-7n6kl" to be "success or failure"
Mar  6 16:32:19.868: INFO: Pod "pod-69768d1a-402d-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588417ms
Mar  6 16:32:21.871: INFO: Pod "pod-69768d1a-402d-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004136443s
STEP: Saw pod success
Mar  6 16:32:21.871: INFO: Pod "pod-69768d1a-402d-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:32:21.873: INFO: Trying to get logs from node themisto pod pod-69768d1a-402d-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:32:21.890: INFO: Waiting for pod pod-69768d1a-402d-11e9-9071-0a58ac100007 to disappear
Mar  6 16:32:21.892: INFO: Pod pod-69768d1a-402d-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:32:21.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7n6kl" for this suite.
Mar  6 16:32:27.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:32:27.967: INFO: namespace: e2e-tests-emptydir-7n6kl, resource: bindings, ignored listing per whitelist
Mar  6 16:32:27.983: INFO: namespace e2e-tests-emptydir-7n6kl deletion completed in 6.088053217s

• [SLOW TEST:8.301 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:32:27.983: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tqfcz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6e68e06d-402d-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume secrets
Mar  6 16:32:28.170: INFO: Waiting up to 5m0s for pod "pod-secrets-6e699923-402d-11e9-9071-0a58ac100007" in namespace "e2e-tests-secrets-tqfcz" to be "success or failure"
Mar  6 16:32:28.171: INFO: Pod "pod-secrets-6e699923-402d-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.527408ms
Mar  6 16:32:30.174: INFO: Pod "pod-secrets-6e699923-402d-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00439081s
STEP: Saw pod success
Mar  6 16:32:30.174: INFO: Pod "pod-secrets-6e699923-402d-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:32:30.176: INFO: Trying to get logs from node sponde pod pod-secrets-6e699923-402d-11e9-9071-0a58ac100007 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 16:32:30.194: INFO: Waiting for pod pod-secrets-6e699923-402d-11e9-9071-0a58ac100007 to disappear
Mar  6 16:32:30.195: INFO: Pod pod-secrets-6e699923-402d-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:32:30.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tqfcz" for this suite.
Mar  6 16:32:36.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:32:36.220: INFO: namespace: e2e-tests-secrets-tqfcz, resource: bindings, ignored listing per whitelist
Mar  6 16:32:36.266: INFO: namespace e2e-tests-secrets-tqfcz deletion completed in 6.067973288s

• [SLOW TEST:8.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:32:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5c6t9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5c6t9
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-5c6t9
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-5c6t9
Mar  6 16:32:36.450: INFO: Found 0 stateful pods, waiting for 1
Mar  6 16:32:46.453: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  6 16:32:46.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 16:32:46.669: INFO: stderr: ""
Mar  6 16:32:46.669: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 16:32:46.669: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 16:32:46.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 16:32:56.675: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 16:32:56.675: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 16:32:56.689: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar  6 16:32:56.689: INFO: ss-0  sponde  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:36 +0000 UTC  }]
Mar  6 16:32:56.689: INFO: 
Mar  6 16:32:56.689: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  6 16:32:57.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997702878s
Mar  6 16:32:58.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990405641s
Mar  6 16:32:59.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986119647s
Mar  6 16:33:00.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979296666s
Mar  6 16:33:01.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976329743s
Mar  6 16:33:02.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972449362s
Mar  6 16:33:03.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96909342s
Mar  6 16:33:04.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965293401s
Mar  6 16:33:05.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.979394ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-5c6t9
Mar  6 16:33:06.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 16:33:06.935: INFO: stderr: ""
Mar  6 16:33:06.935: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 16:33:06.935: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 16:33:06.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 16:33:07.135: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  6 16:33:07.135: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 16:33:07.135: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 16:33:07.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 16:33:07.353: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  6 16:33:07.353: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 16:33:07.353: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 16:33:07.356: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 16:33:07.356: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 16:33:07.356: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  6 16:33:07.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 16:33:07.547: INFO: stderr: ""
Mar  6 16:33:07.547: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 16:33:07.547: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 16:33:07.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 16:33:07.746: INFO: stderr: ""
Mar  6 16:33:07.746: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 16:33:07.746: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 16:33:07.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 exec --namespace=e2e-tests-statefulset-5c6t9 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 16:33:07.932: INFO: stderr: ""
Mar  6 16:33:07.932: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 16:33:07.932: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 16:33:07.932: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 16:33:07.934: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  6 16:33:17.939: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 16:33:17.939: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 16:33:17.939: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 16:33:17.950: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  6 16:33:17.950: INFO: ss-0  sponde    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:36 +0000 UTC  }]
Mar  6 16:33:17.950: INFO: ss-1  kronos    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:56 +0000 UTC  }]
Mar  6 16:33:17.950: INFO: ss-2  themisto  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:56 +0000 UTC  }]
Mar  6 16:33:17.950: INFO: 
Mar  6 16:33:17.950: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 16:33:18.953: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  6 16:33:18.953: INFO: ss-0  sponde    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:36 +0000 UTC  }]
Mar  6 16:33:18.953: INFO: ss-1  kronos    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:56 +0000 UTC  }]
Mar  6 16:33:18.953: INFO: ss-2  themisto  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 16:32:56 +0000 UTC  }]
Mar  6 16:33:18.953: INFO: 
Mar  6 16:33:18.953: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 16:33:19.956: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993444671s
Mar  6 16:33:20.958: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990905392s
Mar  6 16:33:21.961: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.988764768s
Mar  6 16:33:22.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986062696s
Mar  6 16:33:23.966: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983671741s
Mar  6 16:33:24.969: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.981282143s
Mar  6 16:33:25.971: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.978309537s
Mar  6 16:33:26.974: INFO: Verifying statefulset ss doesn't scale past 0 for another 975.650462ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-5c6t9
Mar  6 16:33:27.977: INFO: Scaling statefulset ss to 0
Mar  6 16:33:27.984: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 16:33:27.986: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5c6t9
Mar  6 16:33:27.988: INFO: Scaling statefulset ss to 0
Mar  6 16:33:27.995: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 16:33:27.997: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:33:28.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5c6t9" for this suite.
Mar  6 16:33:34.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:33:34.050: INFO: namespace: e2e-tests-statefulset-5c6t9, resource: bindings, ignored listing per whitelist
Mar  6 16:33:34.087: INFO: namespace e2e-tests-statefulset-5c6t9 deletion completed in 6.07662052s

• [SLOW TEST:57.821 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:33:34.088: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-xbxvx
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:33:34.266: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:33:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-xbxvx" for this suite.
Mar  6 16:33:41.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:33:41.362: INFO: namespace: e2e-tests-custom-resource-definition-xbxvx, resource: bindings, ignored listing per whitelist
Mar  6 16:33:41.421: INFO: namespace e2e-tests-custom-resource-definition-xbxvx deletion completed in 6.086886266s

• [SLOW TEST:7.334 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:33:41.422: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rcxpk
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-9a2fc30a-402d-11e9-9071-0a58ac100007
STEP: Creating configMap with name cm-test-opt-upd-9a2fc344-402d-11e9-9071-0a58ac100007
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9a2fc30a-402d-11e9-9071-0a58ac100007
STEP: Updating configmap cm-test-opt-upd-9a2fc344-402d-11e9-9071-0a58ac100007
STEP: Creating configMap with name cm-test-opt-create-9a2fc35a-402d-11e9-9071-0a58ac100007
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:33:45.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rcxpk" for this suite.
Mar  6 16:34:07.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:34:07.758: INFO: namespace: e2e-tests-configmap-rcxpk, resource: bindings, ignored listing per whitelist
Mar  6 16:34:07.771: INFO: namespace e2e-tests-configmap-rcxpk deletion completed in 22.077301823s

• [SLOW TEST:26.349 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:34:07.771: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tjzpm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 16:34:07.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-tjzpm'
Mar  6 16:34:08.110: INFO: stderr: ""
Mar  6 16:34:08.110: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Mar  6 16:34:08.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889989848 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tjzpm'
Mar  6 16:34:19.173: INFO: stderr: ""
Mar  6 16:34:19.173: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:34:19.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tjzpm" for this suite.
Mar  6 16:34:25.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:34:25.206: INFO: namespace: e2e-tests-kubectl-tjzpm, resource: bindings, ignored listing per whitelist
Mar  6 16:34:25.255: INFO: namespace e2e-tests-kubectl-tjzpm deletion completed in 6.077751041s

• [SLOW TEST:17.484 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:34:25.255: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-htwtk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  6 16:34:27.451: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-b44f601e-402d-11e9-9071-0a58ac100007", GenerateName:"", Namespace:"e2e-tests-pods-htwtk", SelfLink:"/api/v1/namespaces/e2e-tests-pods-htwtk/pods/pod-submit-remove-b44f601e-402d-11e9-9071-0a58ac100007", UID:"b4502725-402d-11e9-b3f2-0cc47aaad1b4", ResourceVersion:"76709", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687486865, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"431766027"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-c5g7d", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0019ba140), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-c5g7d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002defa58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sponde", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a50d20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002defaa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002defac0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002defac8)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486868, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486868, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486868, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687486860, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.100.96.44", PodIP:"172.16.5.1", StartTime:(*v1.Time)(0xc002de39c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc002de39e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://ad240cdac91881f0a2a3eeb683f2e593ca8e88a0f7db40a16251872c30d1ff12"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  6 16:34:32.464: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:34:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-htwtk" for this suite.
Mar  6 16:34:38.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:34:38.511: INFO: namespace: e2e-tests-pods-htwtk, resource: bindings, ignored listing per whitelist
Mar  6 16:34:38.555: INFO: namespace e2e-tests-pods-htwtk deletion completed in 6.084574819s

• [SLOW TEST:13.299 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:34:38.555: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xh4kk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 16:34:38.735: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:34:40.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xh4kk" for this suite.
Mar  6 16:35:20.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:35:20.927: INFO: namespace: e2e-tests-pods-xh4kk, resource: bindings, ignored listing per whitelist
Mar  6 16:35:20.960: INFO: namespace e2e-tests-pods-xh4kk deletion completed in 40.086070104s

• [SLOW TEST:42.405 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:35:20.960: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-hxxm4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-hxxm4
Mar  6 16:35:23.150: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-hxxm4
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 16:35:23.152: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:39:23.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hxxm4" for this suite.
Mar  6 16:39:29.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:39:29.547: INFO: namespace: e2e-tests-container-probe-hxxm4, resource: bindings, ignored listing per whitelist
Mar  6 16:39:29.584: INFO: namespace e2e-tests-container-probe-hxxm4 deletion completed in 6.078769499s

• [SLOW TEST:248.624 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:39:29.584: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-stbgr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 16:39:29.790: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:29.790: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:29.790: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:29.791: INFO: Number of nodes with available pods: 0
Mar  6 16:39:29.791: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:30.795: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:30.795: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:30.795: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:30.797: INFO: Number of nodes with available pods: 1
Mar  6 16:39:30.797: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:31.796: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.796: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.796: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.799: INFO: Number of nodes with available pods: 3
Mar  6 16:39:31.799: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  6 16:39:31.814: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.814: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.814: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:31.818: INFO: Number of nodes with available pods: 2
Mar  6 16:39:31.818: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:32.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:32.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:32.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:32.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:32.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:33.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:33.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:33.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:33.828: INFO: Number of nodes with available pods: 2
Mar  6 16:39:33.828: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:34.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:34.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:34.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:34.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:34.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:35.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:35.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:35.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:35.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:35.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:36.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:36.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:36.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:36.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:36.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:37.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:37.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:37.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:37.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:37.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:38.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:38.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:38.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:38.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:38.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:39.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:39.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:39.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:39.828: INFO: Number of nodes with available pods: 2
Mar  6 16:39:39.828: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:40.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:40.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:40.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:40.827: INFO: Number of nodes with available pods: 2
Mar  6 16:39:40.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:41.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:41.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:41.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:41.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:41.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:42.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:42.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:42.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:42.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:42.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:43.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:43.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:43.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:43.827: INFO: Number of nodes with available pods: 2
Mar  6 16:39:43.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:44.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:44.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:44.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:44.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:44.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:45.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:45.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:45.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:45.827: INFO: Number of nodes with available pods: 2
Mar  6 16:39:45.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:46.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:46.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:46.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:46.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:46.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:47.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:47.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:47.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:47.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:47.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:48.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:48.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:48.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:48.827: INFO: Number of nodes with available pods: 2
Mar  6 16:39:48.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:49.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:49.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:49.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:49.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:49.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:50.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:50.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:50.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:50.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:50.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:51.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:51.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:51.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:51.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:51.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:52.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:52.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:52.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:52.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:52.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:53.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:53.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:53.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:53.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:53.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:54.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:54.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:54.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:54.827: INFO: Number of nodes with available pods: 2
Mar  6 16:39:54.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:55.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:55.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:55.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:55.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:55.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:56.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:56.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:56.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:56.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:56.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:57.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:57.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:57.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:57.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:57.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:58.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:58.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:58.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:58.825: INFO: Number of nodes with available pods: 2
Mar  6 16:39:58.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:39:59.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:59.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:59.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:39:59.826: INFO: Number of nodes with available pods: 2
Mar  6 16:39:59.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:00.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:00.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:00.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:00.825: INFO: Number of nodes with available pods: 2
Mar  6 16:40:00.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:01.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:01.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:01.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:01.826: INFO: Number of nodes with available pods: 2
Mar  6 16:40:01.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:02.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:02.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:02.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:02.826: INFO: Number of nodes with available pods: 2
Mar  6 16:40:02.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:03.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:03.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:03.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:03.827: INFO: Number of nodes with available pods: 2
Mar  6 16:40:03.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:04.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:04.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:04.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:04.825: INFO: Number of nodes with available pods: 2
Mar  6 16:40:04.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:05.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:05.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:05.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:05.827: INFO: Number of nodes with available pods: 2
Mar  6 16:40:05.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:06.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:06.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:06.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:06.827: INFO: Number of nodes with available pods: 2
Mar  6 16:40:06.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:07.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:07.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:07.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:07.827: INFO: Number of nodes with available pods: 2
Mar  6 16:40:07.827: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:08.824: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:08.824: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:08.824: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:08.827: INFO: Number of nodes with available pods: 2
Mar  6 16:40:08.828: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:09.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:09.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:09.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:09.826: INFO: Number of nodes with available pods: 2
Mar  6 16:40:09.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:10.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:10.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:10.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:10.825: INFO: Number of nodes with available pods: 2
Mar  6 16:40:10.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:11.823: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:11.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:11.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:11.826: INFO: Number of nodes with available pods: 2
Mar  6 16:40:11.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:12.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:12.823: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:12.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:12.826: INFO: Number of nodes with available pods: 2
Mar  6 16:40:12.826: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:13.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:13.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:13.822: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:13.825: INFO: Number of nodes with available pods: 2
Mar  6 16:40:13.825: INFO: Node kronos is running more than one daemon pod
Mar  6 16:40:14.822: INFO: DaemonSet pods can't tolerate node erlara with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:14.822: INFO: DaemonSet pods can't tolerate node hersi with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:14.823: INFO: DaemonSet pods can't tolerate node leto with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 16:40:14.825: INFO: Number of nodes with available pods: 3
Mar  6 16:40:14.825: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-stbgr, will wait for the garbage collector to delete the pods
Mar  6 16:40:14.887: INFO: Deleting DaemonSet.extensions daemon-set took: 8.28811ms
Mar  6 16:40:14.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.186612ms
Mar  6 16:40:53.590: INFO: Number of nodes with available pods: 0
Mar  6 16:40:53.590: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 16:40:53.591: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-stbgr/daemonsets","resourceVersion":"78328"},"items":null}

Mar  6 16:40:53.593: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-stbgr/pods","resourceVersion":"78328"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:40:53.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-stbgr" for this suite.
Mar  6 16:40:59.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:40:59.647: INFO: namespace: e2e-tests-daemonsets-stbgr, resource: bindings, ignored listing per whitelist
Mar  6 16:40:59.679: INFO: namespace e2e-tests-daemonsets-stbgr deletion completed in 6.071804907s

• [SLOW TEST:90.095 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:40:59.679: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-26khl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-9f67a19a-402e-11e9-9071-0a58ac100007
STEP: Creating a pod to test consume configMaps
Mar  6 16:40:59.866: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007" in namespace "e2e-tests-configmap-26khl" to be "success or failure"
Mar  6 16:40:59.868: INFO: Pod "pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489299ms
Mar  6 16:41:01.871: INFO: Pod "pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00445762s
STEP: Saw pod success
Mar  6 16:41:01.871: INFO: Pod "pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:41:01.873: INFO: Trying to get logs from node sponde pod pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 16:41:01.892: INFO: Waiting for pod pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007 to disappear
Mar  6 16:41:01.894: INFO: Pod pod-configmaps-9f68647f-402e-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:41:01.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-26khl" for this suite.
Mar  6 16:41:07.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:41:07.935: INFO: namespace: e2e-tests-configmap-26khl, resource: bindings, ignored listing per whitelist
Mar  6 16:41:07.966: INFO: namespace e2e-tests-configmap-26khl deletion completed in 6.068677537s

• [SLOW TEST:8.287 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:41:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-9xnl4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0306 16:41:14.164681      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 16:41:14.164: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:41:14.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9xnl4" for this suite.
Mar  6 16:41:20.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:41:20.181: INFO: namespace: e2e-tests-gc-9xnl4, resource: bindings, ignored listing per whitelist
Mar  6 16:41:20.238: INFO: namespace e2e-tests-gc-9xnl4 deletion completed in 6.070981866s

• [SLOW TEST:12.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 16:41:20.238: INFO: >>> kubeConfig: /tmp/kubeconfig-889989848
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hx264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 16:41:20.417: INFO: Waiting up to 5m0s for pod "pod-aba819c6-402e-11e9-9071-0a58ac100007" in namespace "e2e-tests-emptydir-hx264" to be "success or failure"
Mar  6 16:41:20.419: INFO: Pod "pod-aba819c6-402e-11e9-9071-0a58ac100007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.516093ms
Mar  6 16:41:22.422: INFO: Pod "pod-aba819c6-402e-11e9-9071-0a58ac100007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004542186s
STEP: Saw pod success
Mar  6 16:41:22.422: INFO: Pod "pod-aba819c6-402e-11e9-9071-0a58ac100007" satisfied condition "success or failure"
Mar  6 16:41:22.424: INFO: Trying to get logs from node kronos pod pod-aba819c6-402e-11e9-9071-0a58ac100007 container test-container: <nil>
STEP: delete the pod
Mar  6 16:41:22.443: INFO: Waiting for pod pod-aba819c6-402e-11e9-9071-0a58ac100007 to disappear
Mar  6 16:41:22.445: INFO: Pod pod-aba819c6-402e-11e9-9071-0a58ac100007 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 16:41:22.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hx264" for this suite.
Mar  6 16:41:28.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 16:41:28.488: INFO: namespace: e2e-tests-emptydir-hx264, resource: bindings, ignored listing per whitelist
Mar  6 16:41:28.517: INFO: namespace e2e-tests-emptydir-hx264 deletion completed in 6.068516863s

• [SLOW TEST:8.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSMar  6 16:41:28.517: INFO: Running AfterSuite actions on all nodes
Mar  6 16:41:28.517: INFO: Running AfterSuite actions on node 1
Mar  6 16:41:28.517: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5366.644 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h29m27.453971483s
Test Suite Passed
