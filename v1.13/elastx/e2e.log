I0411 16:42:04.536019      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-785594743
I0411 16:42:04.536424      15 e2e.go:224] Starting e2e run "bc50fad8-5c78-11e9-bcf1-d217e8a0130c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1555000923 - Will randomize all specs
Will run 201 of 1946 specs

Apr 11 16:42:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 16:42:04.843: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 11 16:42:07.232: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 11 16:42:07.275: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 11 16:42:07.275: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Apr 11 16:42:07.275: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 11 16:42:07.284: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 11 16:42:07.284: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 11 16:42:07.284: INFO: e2e test version: v1.13.0
Apr 11 16:42:07.286: INFO: kube-apiserver version: v1.13.5
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:42:07.287: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
Apr 11 16:42:07.392: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr 11 16:42:07.417: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8cc29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Apr 11 16:42:07.555: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-785594743 proxy --unix-socket=/tmp/kubectl-proxy-unix655523434/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:42:07.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8cc29" for this suite.
Apr 11 16:42:13.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:42:13.797: INFO: namespace: e2e-tests-kubectl-8cc29, resource: bindings, ignored listing per whitelist
Apr 11 16:42:13.866: INFO: namespace e2e-tests-kubectl-8cc29 deletion completed in 6.154316366s

â€¢ [SLOW TEST:6.580 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:42:13.870: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-rc7tj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Apr 11 16:42:20.105: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-c28205d2-5c78-11e9-bcf1-d217e8a0130c", GenerateName:"", Namespace:"e2e-tests-pods-rc7tj", SelfLink:"/api/v1/namespaces/e2e-tests-pods-rc7tj/pods/pod-submit-remove-c28205d2-5c78-11e9-bcf1-d217e8a0130c", UID:"c28314a3-5c78-11e9-8000-fa163e4b6765", ResourceVersion:"5807", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690597734, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"64281945"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f72p4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001254d80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f72p4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014d0ca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cluster1-k8s-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009c9aa0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014d0d10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014d0d30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0014d0d38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0014d0d3c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690597734, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690597738, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690597738, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690597734, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.3", PodIP:"10.233.87.2", StartTime:(*v1.Time)(0xc001a04e20), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001a04e40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7", ContainerID:"docker://729694ba30b445315307a663cc48f23f1821c53b5539cd348ed97ac406c0d112"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:42:33.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rc7tj" for this suite.
Apr 11 16:42:39.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:42:39.675: INFO: namespace: e2e-tests-pods-rc7tj, resource: bindings, ignored listing per whitelist
Apr 11 16:42:39.751: INFO: namespace e2e-tests-pods-rc7tj deletion completed in 6.128086814s

â€¢ [SLOW TEST:25.881 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:42:39.756: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-s2xrs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:42:47.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-s2xrs" for this suite.
Apr 11 16:42:54.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:42:54.031: INFO: namespace: e2e-tests-kubelet-test-s2xrs, resource: bindings, ignored listing per whitelist
Apr 11 16:42:54.129: INFO: namespace e2e-tests-kubelet-test-s2xrs deletion completed in 6.128229874s

â€¢ [SLOW TEST:14.373 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:42:54.133: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-dcmw6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-da854f25-5c78-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 16:42:54.368: INFO: Waiting up to 5m0s for pod "pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-dcmw6" to be "success or failure"
Apr 11 16:42:54.373: INFO: Pod "pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337121ms
Apr 11 16:42:56.379: INFO: Pod "pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010075468s
Apr 11 16:42:58.388: INFO: Pod "pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019700564s
STEP: Saw pod success
Apr 11 16:42:58.388: INFO: Pod "pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:42:58.393: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 16:42:58.428: INFO: Waiting for pod pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:42:58.435: INFO: Pod pod-secrets-da862a93-5c78-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:42:58.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dcmw6" for this suite.
Apr 11 16:43:04.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:43:04.516: INFO: namespace: e2e-tests-secrets-dcmw6, resource: bindings, ignored listing per whitelist
Apr 11 16:43:04.577: INFO: namespace e2e-tests-secrets-dcmw6 deletion completed in 6.13686422s

â€¢ [SLOW TEST:10.445 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:43:04.580: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-l6p67
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 11 16:43:04.804: INFO: Waiting up to 5m0s for pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-l6p67" to be "success or failure"
Apr 11 16:43:04.810: INFO: Pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.669049ms
Apr 11 16:43:06.815: INFO: Pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011226415s
Apr 11 16:43:08.821: INFO: Pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016799036s
Apr 11 16:43:10.826: INFO: Pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022134729s
STEP: Saw pod success
Apr 11 16:43:10.826: INFO: Pod "pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:43:10.830: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 16:43:10.889: INFO: Waiting for pod pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:43:10.898: INFO: Pod pod-e0be93e1-5c78-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:43:10.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-l6p67" for this suite.
Apr 11 16:43:16.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:43:17.022: INFO: namespace: e2e-tests-emptydir-l6p67, resource: bindings, ignored listing per whitelist
Apr 11 16:43:17.036: INFO: namespace e2e-tests-emptydir-l6p67 deletion completed in 6.130444093s

â€¢ [SLOW TEST:12.457 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:43:17.038: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cqmzv
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-e82d1c04-5c78-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e82d1c04-5c78-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:43:21.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cqmzv" for this suite.
Apr 11 16:43:43.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:43:43.424: INFO: namespace: e2e-tests-configmap-cqmzv, resource: bindings, ignored listing per whitelist
Apr 11 16:43:43.478: INFO: namespace e2e-tests-configmap-cqmzv deletion completed in 22.125961987s

â€¢ [SLOW TEST:26.441 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:43:43.481: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-4hpd6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 11 16:43:48.744: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:43:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-4hpd6" for this suite.
Apr 11 16:44:11.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:44:11.833: INFO: namespace: e2e-tests-replicaset-4hpd6, resource: bindings, ignored listing per whitelist
Apr 11 16:44:11.897: INFO: namespace e2e-tests-replicaset-4hpd6 deletion completed in 22.123883875s

â€¢ [SLOW TEST:28.416 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:44:11.900: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-h4zzg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 16:44:12.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-h4zzg" to be "success or failure"
Apr 11 16:44:12.129: INFO: Pod "downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.173575ms
Apr 11 16:44:14.135: INFO: Pod "downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010633176s
STEP: Saw pod success
Apr 11 16:44:14.135: INFO: Pod "downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:44:14.139: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 16:44:14.176: INFO: Waiting for pod downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:44:14.180: INFO: Pod downwardapi-volume-08de5583-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:44:14.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-h4zzg" for this suite.
Apr 11 16:44:20.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:44:20.346: INFO: namespace: e2e-tests-downward-api-h4zzg, resource: bindings, ignored listing per whitelist
Apr 11 16:44:20.366: INFO: namespace e2e-tests-downward-api-h4zzg deletion completed in 6.181266793s

â€¢ [SLOW TEST:8.467 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:44:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c745h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-0de91775-5c79-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 16:44:20.586: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-c745h" to be "success or failure"
Apr 11 16:44:20.594: INFO: Pod "pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025951ms
Apr 11 16:44:22.600: INFO: Pod "pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013327955s
STEP: Saw pod success
Apr 11 16:44:22.600: INFO: Pod "pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:44:22.604: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 16:44:22.638: INFO: Waiting for pod pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:44:22.642: INFO: Pod pod-projected-configmaps-0dea3997-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:44:22.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c745h" for this suite.
Apr 11 16:44:28.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:44:28.718: INFO: namespace: e2e-tests-projected-c745h, resource: bindings, ignored listing per whitelist
Apr 11 16:44:28.778: INFO: namespace e2e-tests-projected-c745h deletion completed in 6.129305696s

â€¢ [SLOW TEST:8.411 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:44:28.781: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tvr8k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-tvr8k/secret-test-12edd7b4-5c79-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 16:44:29.004: INFO: Waiting up to 5m0s for pod "pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-tvr8k" to be "success or failure"
Apr 11 16:44:29.014: INFO: Pod "pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.337414ms
Apr 11 16:44:31.026: INFO: Pod "pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022334923s
STEP: Saw pod success
Apr 11 16:44:31.027: INFO: Pod "pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:44:31.031: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c container env-test: <nil>
STEP: delete the pod
Apr 11 16:44:31.063: INFO: Waiting for pod pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:44:31.067: INFO: Pod pod-configmaps-12eeb341-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:44:31.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tvr8k" for this suite.
Apr 11 16:44:37.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:44:37.159: INFO: namespace: e2e-tests-secrets-tvr8k, resource: bindings, ignored listing per whitelist
Apr 11 16:44:37.206: INFO: namespace e2e-tests-secrets-tvr8k deletion completed in 6.134002278s

â€¢ [SLOW TEST:8.426 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:44:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-2q4xf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-2q4xf
Apr 11 16:44:39.442: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-2q4xf
STEP: checking the pod's current state and verifying that restartCount is present
Apr 11 16:44:39.447: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:48:40.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2q4xf" for this suite.
Apr 11 16:48:46.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:48:46.350: INFO: namespace: e2e-tests-container-probe-2q4xf, resource: bindings, ignored listing per whitelist
Apr 11 16:48:46.445: INFO: namespace e2e-tests-container-probe-2q4xf deletion completed in 6.123100641s

â€¢ [SLOW TEST:249.237 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:48:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-qjv8z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 16:48:46.671: INFO: (0) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.180177ms)
Apr 11 16:48:46.676: INFO: (1) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.012357ms)
Apr 11 16:48:46.680: INFO: (2) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.764934ms)
Apr 11 16:48:46.684: INFO: (3) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.938912ms)
Apr 11 16:48:46.688: INFO: (4) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.775931ms)
Apr 11 16:48:46.692: INFO: (5) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.795874ms)
Apr 11 16:48:46.696: INFO: (6) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.838166ms)
Apr 11 16:48:46.700: INFO: (7) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.806283ms)
Apr 11 16:48:46.704: INFO: (8) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.978071ms)
Apr 11 16:48:46.708: INFO: (9) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.708478ms)
Apr 11 16:48:46.712: INFO: (10) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.687107ms)
Apr 11 16:48:46.716: INFO: (11) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.856513ms)
Apr 11 16:48:46.721: INFO: (12) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.925477ms)
Apr 11 16:48:46.725: INFO: (13) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.86526ms)
Apr 11 16:48:46.729: INFO: (14) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.870325ms)
Apr 11 16:48:46.733: INFO: (15) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.915382ms)
Apr 11 16:48:46.737: INFO: (16) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.088685ms)
Apr 11 16:48:46.741: INFO: (17) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.075285ms)
Apr 11 16:48:46.745: INFO: (18) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.881527ms)
Apr 11 16:48:46.749: INFO: (19) /api/v1/nodes/cluster1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.767521ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:48:46.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-qjv8z" for this suite.
Apr 11 16:48:52.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:48:52.796: INFO: namespace: e2e-tests-proxy-qjv8z, resource: bindings, ignored listing per whitelist
Apr 11 16:48:52.878: INFO: namespace e2e-tests-proxy-qjv8z deletion completed in 6.124527413s

â€¢ [SLOW TEST:6.431 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:48:52.881: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-d6ppf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-b0581eb0-5c79-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 16:48:53.104: INFO: Waiting up to 5m0s for pod "pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-d6ppf" to be "success or failure"
Apr 11 16:48:53.110: INFO: Pod "pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.618819ms
Apr 11 16:48:55.116: INFO: Pod "pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012463558s
Apr 11 16:48:57.123: INFO: Pod "pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018741452s
STEP: Saw pod success
Apr 11 16:48:57.123: INFO: Pod "pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:48:57.127: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 16:48:57.172: INFO: Waiting for pod pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:48:57.179: INFO: Pod pod-secrets-b058e8fe-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:48:57.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-d6ppf" for this suite.
Apr 11 16:49:03.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:49:03.218: INFO: namespace: e2e-tests-secrets-d6ppf, resource: bindings, ignored listing per whitelist
Apr 11 16:49:03.313: INFO: namespace e2e-tests-secrets-d6ppf deletion completed in 6.128406753s

â€¢ [SLOW TEST:10.432 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:49:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-fq4fj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 11 16:49:03.527: INFO: Waiting up to 5m0s for pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-fq4fj" to be "success or failure"
Apr 11 16:49:03.531: INFO: Pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137597ms
Apr 11 16:49:05.537: INFO: Pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00990298s
Apr 11 16:49:07.543: INFO: Pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015767901s
Apr 11 16:49:09.548: INFO: Pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021253155s
STEP: Saw pod success
Apr 11 16:49:09.548: INFO: Pod "pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:49:09.553: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 16:49:09.586: INFO: Waiting for pod pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:49:09.592: INFO: Pod pod-b68f91ee-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:49:09.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fq4fj" for this suite.
Apr 11 16:49:15.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:49:15.659: INFO: namespace: e2e-tests-emptydir-fq4fj, resource: bindings, ignored listing per whitelist
Apr 11 16:49:15.740: INFO: namespace e2e-tests-emptydir-fq4fj deletion completed in 6.142992231s

â€¢ [SLOW TEST:12.423 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:49:15.741: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-qvz72
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Apr 11 16:49:15.959: INFO: PodSpec: initContainers in spec.initContainers
Apr 11 16:50:03.632: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bdfa10cc-5c79-11e9-bcf1-d217e8a0130c", GenerateName:"", Namespace:"e2e-tests-init-container-qvz72", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-qvz72/pods/pod-init-bdfa10cc-5c79-11e9-bcf1-d217e8a0130c", UID:"bdfa5e19-5c79-11e9-8000-fa163e4b6765", ResourceVersion:"7279", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690598155, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"959118557"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wh8dc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001af5500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wh8dc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wh8dc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wh8dc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001af11f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cluster1-k8s-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001ecd200), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001af1270)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001af1290)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001af1298), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001af129c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598155, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598155, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598155, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598155, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.3", PodIP:"10.233.87.10", StartTime:(*v1.Time)(0xc001af6a40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002235b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002235c00)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://4b7ee4434a8853ea6df6086443daf6c3d0a69071ab6bfad3c20c16a8a8a3114b"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001af6a80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001af6a60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:50:03.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-qvz72" for this suite.
Apr 11 16:50:25.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:50:25.712: INFO: namespace: e2e-tests-init-container-qvz72, resource: bindings, ignored listing per whitelist
Apr 11 16:50:25.778: INFO: namespace e2e-tests-init-container-qvz72 deletion completed in 22.133776073s

â€¢ [SLOW TEST:70.037 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:50:25.784: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-pxl9v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 16:50:26.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-pxl9v" to be "success or failure"
Apr 11 16:50:26.008: INFO: Pod "downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671078ms
Apr 11 16:50:28.014: INFO: Pod "downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010277789s
STEP: Saw pod success
Apr 11 16:50:28.014: INFO: Pod "downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:50:28.019: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 16:50:28.061: INFO: Waiting for pod downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:50:28.065: INFO: Pod downwardapi-volume-e7b84441-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:50:28.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pxl9v" for this suite.
Apr 11 16:50:34.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:50:34.121: INFO: namespace: e2e-tests-downward-api-pxl9v, resource: bindings, ignored listing per whitelist
Apr 11 16:50:34.194: INFO: namespace e2e-tests-downward-api-pxl9v deletion completed in 6.124152308s

â€¢ [SLOW TEST:8.411 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:50:34.197: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-gslz6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 11 16:50:34.403: INFO: Waiting up to 5m0s for pod "pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-gslz6" to be "success or failure"
Apr 11 16:50:34.408: INFO: Pod "pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797376ms
Apr 11 16:50:36.420: INFO: Pod "pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016327917s
STEP: Saw pod success
Apr 11 16:50:36.420: INFO: Pod "pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:50:36.424: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 16:50:36.454: INFO: Waiting for pod pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:50:36.459: INFO: Pod pod-ecba419b-5c79-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:50:36.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gslz6" for this suite.
Apr 11 16:50:42.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:50:42.506: INFO: namespace: e2e-tests-emptydir-gslz6, resource: bindings, ignored listing per whitelist
Apr 11 16:50:42.583: INFO: namespace e2e-tests-emptydir-gslz6 deletion completed in 6.12010387s

â€¢ [SLOW TEST:8.387 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:50:42.586: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-7sqj2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 16:50:42.799: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 11 16:50:42.810: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 11 16:50:47.823: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 11 16:50:47.823: INFO: Creating deployment "test-rolling-update-deployment"
Apr 11 16:50:47.830: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 11 16:50:47.837: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 11 16:50:49.845: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 11 16:50:49.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:50:51.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598251, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598247, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:50:53.852: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 11 16:50:53.863: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-7sqj2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7sqj2/deployments/test-rolling-update-deployment,UID:f4bb5e19-5c79-11e9-8000-fa163e4b6765,ResourceVersion:7519,Generation:1,CreationTimestamp:2019-04-11 16:50:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-11 16:50:47 +0000 UTC 2019-04-11 16:50:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-11 16:50:51 +0000 UTC 2019-04-11 16:50:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 11 16:50:53.866: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-7sqj2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7sqj2/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:f4c12f18-5c79-11e9-ab42-fa163e758ab4,ResourceVersion:7510,Generation:1,CreationTimestamp:2019-04-11 16:50:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f4bb5e19-5c79-11e9-8000-fa163e4b6765 0xc0021d4347 0xc0021d4348}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 11 16:50:53.866: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 11 16:50:53.867: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-7sqj2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7sqj2/replicasets/test-rolling-update-controller,UID:f1bcae57-5c79-11e9-8000-fa163e4b6765,ResourceVersion:7518,Generation:2,CreationTimestamp:2019-04-11 16:50:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f4bb5e19-5c79-11e9-8000-fa163e4b6765 0xc0021d427f 0xc0021d4290}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 16:50:53.872: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-4kjc2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-4kjc2,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-7sqj2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-7sqj2/pods/test-rolling-update-deployment-68b55d7bc6-4kjc2,UID:f4c273b5-5c79-11e9-ab42-fa163e758ab4,ResourceVersion:7509,Generation:0,CreationTimestamp:2019-04-11 16:50:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 f4c12f18-5c79-11e9-ab42-fa163e758ab4 0xc0021d4bf7 0xc0021d4bf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rtntl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rtntl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rtntl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d4c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d4c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:50:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:50:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:50:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:50:47 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.10,StartTime:2019-04-11 16:50:47 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-11 16:50:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3d1917df2c86a092bfac7783855fdb4609170056ea0cbbe7f0b34ad361d29402}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:50:53.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-7sqj2" for this suite.
Apr 11 16:50:59.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:50:59.966: INFO: namespace: e2e-tests-deployment-7sqj2, resource: bindings, ignored listing per whitelist
Apr 11 16:51:00.001: INFO: namespace e2e-tests-deployment-7sqj2 deletion completed in 6.124181195s

â€¢ [SLOW TEST:17.415 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:51:00.003: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-dxwnb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 11 16:51:00.238: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:00.238: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:00.238: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:00.244: INFO: Number of nodes with available pods: 0
Apr 11 16:51:00.244: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 16:51:01.251: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:01.251: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:01.251: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:01.256: INFO: Number of nodes with available pods: 0
Apr 11 16:51:01.256: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 16:51:02.250: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:02.251: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:02.251: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:02.256: INFO: Number of nodes with available pods: 0
Apr 11 16:51:02.256: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 16:51:03.250: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:03.250: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:03.250: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:03.255: INFO: Number of nodes with available pods: 0
Apr 11 16:51:03.255: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 16:51:04.249: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:04.249: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:04.249: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:04.255: INFO: Number of nodes with available pods: 1
Apr 11 16:51:04.255: INFO: Node cluster1-k8s-node-2 is running more than one daemon pod
Apr 11 16:51:05.252: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.252: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.252: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.263: INFO: Number of nodes with available pods: 3
Apr 11 16:51:05.263: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 11 16:51:05.291: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.291: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.291: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:05.296: INFO: Number of nodes with available pods: 2
Apr 11 16:51:05.296: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:06.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:06.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:06.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:06.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:06.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:07.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:07.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:07.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:07.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:07.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:08.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:08.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:08.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:08.314: INFO: Number of nodes with available pods: 2
Apr 11 16:51:08.314: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:09.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:09.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:09.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:09.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:09.309: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:10.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:10.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:10.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:10.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:10.309: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:11.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:11.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:11.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:11.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:11.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:12.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:12.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:12.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:12.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:12.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:13.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:13.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:13.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:13.309: INFO: Number of nodes with available pods: 2
Apr 11 16:51:13.309: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:14.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:14.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:14.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:14.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:14.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:15.306: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:15.306: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:15.307: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:15.313: INFO: Number of nodes with available pods: 2
Apr 11 16:51:15.313: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:16.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:16.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:16.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:16.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:16.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:17.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:17.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:17.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:17.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:17.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:18.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:18.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:18.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:18.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:18.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:19.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:19.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:19.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:19.314: INFO: Number of nodes with available pods: 2
Apr 11 16:51:19.314: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:20.327: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:20.327: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:20.328: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:20.334: INFO: Number of nodes with available pods: 2
Apr 11 16:51:20.334: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:21.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:21.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:21.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:21.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:21.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:22.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:22.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:22.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:22.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:22.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:23.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:23.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:23.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:23.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:23.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:24.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:24.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:24.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:24.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:24.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:25.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:25.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:25.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:25.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:25.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:26.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:26.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:26.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:26.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:26.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:27.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:27.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:27.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:27.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:27.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:28.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:28.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:28.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:28.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:28.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:29.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:29.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:29.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:29.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:29.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:30.311: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:30.311: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:30.311: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:30.316: INFO: Number of nodes with available pods: 2
Apr 11 16:51:30.317: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:31.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:31.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:31.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:31.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:31.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:32.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:32.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:32.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:32.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:32.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:33.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:33.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:33.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:33.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:33.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:34.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:34.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:34.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:34.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:34.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:35.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:35.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:35.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:35.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:35.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:36.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:36.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:36.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:36.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:36.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:37.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:37.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:37.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:37.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:37.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:38.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:38.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:38.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:38.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:38.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:39.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:39.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:39.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:39.308: INFO: Number of nodes with available pods: 2
Apr 11 16:51:39.308: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:40.308: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:40.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:40.309: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:40.321: INFO: Number of nodes with available pods: 2
Apr 11 16:51:40.321: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:41.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:41.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:41.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:41.309: INFO: Number of nodes with available pods: 2
Apr 11 16:51:41.309: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:42.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:42.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:42.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:42.309: INFO: Number of nodes with available pods: 2
Apr 11 16:51:42.309: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:43.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:43.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:43.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:43.307: INFO: Number of nodes with available pods: 2
Apr 11 16:51:43.307: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:44.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:44.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:44.302: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:44.309: INFO: Number of nodes with available pods: 2
Apr 11 16:51:44.310: INFO: Node cluster1-k8s-node-3 is running more than one daemon pod
Apr 11 16:51:45.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:45.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:45.303: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 16:51:45.309: INFO: Number of nodes with available pods: 3
Apr 11 16:51:45.309: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-dxwnb, will wait for the garbage collector to delete the pods
Apr 11 16:51:45.375: INFO: Deleting DaemonSet.extensions daemon-set took: 9.181486ms
Apr 11 16:51:45.475: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.194786ms
Apr 11 16:52:23.788: INFO: Number of nodes with available pods: 0
Apr 11 16:52:23.788: INFO: Number of running nodes: 0, number of available pods: 0
Apr 11 16:52:23.793: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dxwnb/daemonsets","resourceVersion":"7830"},"items":null}

Apr 11 16:52:23.797: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dxwnb/pods","resourceVersion":"7830"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:52:23.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dxwnb" for this suite.
Apr 11 16:52:29.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:52:29.885: INFO: namespace: e2e-tests-daemonsets-dxwnb, resource: bindings, ignored listing per whitelist
Apr 11 16:52:29.936: INFO: namespace e2e-tests-daemonsets-dxwnb deletion completed in 6.120872472s

â€¢ [SLOW TEST:89.933 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:52:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-r4jhn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 11 16:52:30.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7887,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 11 16:52:30.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7887,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 11 16:52:40.173: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7907,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 11 16:52:40.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7907,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 11 16:52:50.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7960,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 11 16:52:50.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7960,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 11 16:53:00.213: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7980,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 11 16:53:00.214: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-a,UID:31b863d5-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:7980,Generation:0,CreationTimestamp:2019-04-11 16:52:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 11 16:53:10.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-b,UID:499bb304-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8000,Generation:0,CreationTimestamp:2019-04-11 16:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 11 16:53:10.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-b,UID:499bb304-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8000,Generation:0,CreationTimestamp:2019-04-11 16:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 11 16:53:20.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-b,UID:499bb304-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8020,Generation:0,CreationTimestamp:2019-04-11 16:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 11 16:53:20.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r4jhn,SelfLink:/api/v1/namespaces/e2e-tests-watch-r4jhn/configmaps/e2e-watch-test-configmap-b,UID:499bb304-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8020,Generation:0,CreationTimestamp:2019-04-11 16:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:53:30.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-r4jhn" for this suite.
Apr 11 16:53:36.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:53:36.359: INFO: namespace: e2e-tests-watch-r4jhn, resource: bindings, ignored listing per whitelist
Apr 11 16:53:36.414: INFO: namespace e2e-tests-watch-r4jhn deletion completed in 6.139108345s

â€¢ [SLOW TEST:66.476 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:53:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-rxf25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 16:53:36.629: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 11 16:53:41.641: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 11 16:53:41.641: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 11 16:53:43.645: INFO: Creating deployment "test-rollover-deployment"
Apr 11 16:53:43.655: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 11 16:53:45.662: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 11 16:53:45.669: INFO: Ensure that both replica sets have 1 created replica
Apr 11 16:53:45.675: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 11 16:53:45.685: INFO: Updating deployment test-rollover-deployment
Apr 11 16:53:45.685: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 11 16:53:47.696: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 11 16:53:47.704: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 11 16:53:47.711: INFO: all replica sets need to contain the pod-template-hash label
Apr 11 16:53:47.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598427, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:53:49.723: INFO: all replica sets need to contain the pod-template-hash label
Apr 11 16:53:49.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598427, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:53:51.725: INFO: all replica sets need to contain the pod-template-hash label
Apr 11 16:53:51.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598427, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:53:53.719: INFO: all replica sets need to contain the pod-template-hash label
Apr 11 16:53:53.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598427, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:53:55.719: INFO: all replica sets need to contain the pod-template-hash label
Apr 11 16:53:55.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598427, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690598423, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 11 16:53:57.719: INFO: 
Apr 11 16:53:57.719: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 11 16:53:57.729: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-rxf25,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxf25/deployments/test-rollover-deployment,UID:5d87cd8a-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8185,Generation:2,CreationTimestamp:2019-04-11 16:53:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-11 16:53:43 +0000 UTC 2019-04-11 16:53:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-11 16:53:57 +0000 UTC 2019-04-11 16:53:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 11 16:53:57.733: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-rxf25,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxf25/replicasets/test-rollover-deployment-6b7f9d6597,UID:5ebf3da4-5c7a-11e9-ab42-fa163e758ab4,ResourceVersion:8176,Generation:2,CreationTimestamp:2019-04-11 16:53:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5d87cd8a-5c7a-11e9-8000-fa163e4b6765 0xc001b11407 0xc001b11408}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 11 16:53:57.733: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 11 16:53:57.733: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-rxf25,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxf25/replicasets/test-rollover-controller,UID:5957915e-5c7a-11e9-8000-fa163e4b6765,ResourceVersion:8184,Generation:2,CreationTimestamp:2019-04-11 16:53:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5d87cd8a-5c7a-11e9-8000-fa163e4b6765 0xc001b1120f 0xc001b11220}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 16:53:57.733: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-rxf25,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxf25/replicasets/test-rollover-deployment-6586df867b,UID:5d8bd335-5c7a-11e9-ab42-fa163e758ab4,ResourceVersion:8140,Generation:2,CreationTimestamp:2019-04-11 16:53:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5d87cd8a-5c7a-11e9-8000-fa163e4b6765 0xc001b11337 0xc001b11338}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 16:53:57.738: INFO: Pod "test-rollover-deployment-6b7f9d6597-8mz5d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-8mz5d,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-rxf25,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rxf25/pods/test-rollover-deployment-6b7f9d6597-8mz5d,UID:5ec724a7-5c7a-11e9-ab42-fa163e758ab4,ResourceVersion:8154,Generation:0,CreationTimestamp:2019-04-11 16:53:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 5ebf3da4-5c7a-11e9-ab42-fa163e758ab4 0xc0014893c7 0xc0014893c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tcc2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tcc2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-tcc2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001489430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014894c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:53:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 16:53:45 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.13,StartTime:2019-04-11 16:53:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-11 16:53:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b5c38f16cdde7bdf744c07a2b2a79b343798045785d8772bf8a6d93bfea79176}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:53:57.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-rxf25" for this suite.
Apr 11 16:54:03.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:54:03.839: INFO: namespace: e2e-tests-deployment-rxf25, resource: bindings, ignored listing per whitelist
Apr 11 16:54:03.874: INFO: namespace e2e-tests-deployment-rxf25 deletion completed in 6.130906058s

â€¢ [SLOW TEST:27.459 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:54:03.877: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zp7l5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-69b5eb06-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 16:54:04.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-zp7l5" to be "success or failure"
Apr 11 16:54:04.101: INFO: Pod "pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.558645ms
Apr 11 16:54:06.107: INFO: Pod "pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010353368s
STEP: Saw pod success
Apr 11 16:54:06.107: INFO: Pod "pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:54:06.112: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 16:54:06.150: INFO: Waiting for pod pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:54:06.154: INFO: Pod pod-configmaps-69b6f2e0-5c7a-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:54:06.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zp7l5" for this suite.
Apr 11 16:54:12.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:54:12.218: INFO: namespace: e2e-tests-configmap-zp7l5, resource: bindings, ignored listing per whitelist
Apr 11 16:54:12.283: INFO: namespace e2e-tests-configmap-zp7l5 deletion completed in 6.123883356s

â€¢ [SLOW TEST:8.407 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:54:12.286: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x78gm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-6eba0aab-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 16:54:12.512: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-x78gm" to be "success or failure"
Apr 11 16:54:12.517: INFO: Pod "pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051779ms
Apr 11 16:54:14.522: INFO: Pod "pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009664043s
STEP: Saw pod success
Apr 11 16:54:14.522: INFO: Pod "pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:54:14.526: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 11 16:54:14.558: INFO: Waiting for pod pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:54:14.563: INFO: Pod pod-projected-secrets-6ebad7b4-5c7a-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:54:14.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x78gm" for this suite.
Apr 11 16:54:20.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:54:20.613: INFO: namespace: e2e-tests-projected-x78gm, resource: bindings, ignored listing per whitelist
Apr 11 16:54:20.688: INFO: namespace e2e-tests-projected-x78gm deletion completed in 6.119842639s

â€¢ [SLOW TEST:8.403 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:54:20.693: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-zk45g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c
Apr 11 16:54:20.908: INFO: Pod name my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c: Found 0 pods out of 1
Apr 11 16:54:25.921: INFO: Pod name my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c: Found 1 pods out of 1
Apr 11 16:54:25.921: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c" are running
Apr 11 16:54:25.925: INFO: Pod "my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c-zlbcc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 16:54:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 16:54:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 16:54:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 16:54:20 +0000 UTC Reason: Message:}])
Apr 11 16:54:25.925: INFO: Trying to dial the pod
Apr 11 16:54:30.941: INFO: Controller my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c: Got expected result from replica 1 [my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c-zlbcc]: "my-hostname-basic-73bbd661-5c7a-11e9-bcf1-d217e8a0130c-zlbcc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:54:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-zk45g" for this suite.
Apr 11 16:54:36.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:54:37.013: INFO: namespace: e2e-tests-replication-controller-zk45g, resource: bindings, ignored listing per whitelist
Apr 11 16:54:37.083: INFO: namespace e2e-tests-replication-controller-zk45g deletion completed in 6.13777492s

â€¢ [SLOW TEST:16.391 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:54:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-h7vvc
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-7d884970-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating configMap with name cm-test-opt-upd-7d8849aa-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7d884970-5c7a-11e9-bcf1-d217e8a0130c
STEP: Updating configmap cm-test-opt-upd-7d8849aa-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating configMap with name cm-test-opt-create-7d8849c3-5c7a-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:56:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h7vvc" for this suite.
Apr 11 16:56:28.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:56:28.152: INFO: namespace: e2e-tests-configmap-h7vvc, resource: bindings, ignored listing per whitelist
Apr 11 16:56:28.210: INFO: namespace e2e-tests-configmap-h7vvc deletion completed in 22.124678801s

â€¢ [SLOW TEST:111.123 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:56:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-k2fqq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-bfc0f516-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 16:56:28.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-k2fqq" to be "success or failure"
Apr 11 16:56:28.458: INFO: Pod "pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553798ms
Apr 11 16:56:30.470: INFO: Pod "pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016795794s
STEP: Saw pod success
Apr 11 16:56:30.471: INFO: Pod "pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:56:30.475: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 16:56:30.509: INFO: Waiting for pod pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:56:30.514: INFO: Pod pod-configmaps-bfc1f41a-5c7a-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:56:30.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k2fqq" for this suite.
Apr 11 16:56:36.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:56:36.616: INFO: namespace: e2e-tests-configmap-k2fqq, resource: bindings, ignored listing per whitelist
Apr 11 16:56:36.645: INFO: namespace e2e-tests-configmap-k2fqq deletion completed in 6.125822931s

â€¢ [SLOW TEST:8.435 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:56:36.648: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-q2t9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Apr 11 16:56:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:37.202: INFO: stderr: ""
Apr 11 16:56:37.202: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 16:56:37.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:37.340: INFO: stderr: ""
Apr 11 16:56:37.340: INFO: stdout: "update-demo-nautilus-9khz4 update-demo-nautilus-hqv6w "
Apr 11 16:56:37.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-9khz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:37.448: INFO: stderr: ""
Apr 11 16:56:37.448: INFO: stdout: ""
Apr 11 16:56:37.448: INFO: update-demo-nautilus-9khz4 is created but not running
Apr 11 16:56:42.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:42.559: INFO: stderr: ""
Apr 11 16:56:42.559: INFO: stdout: "update-demo-nautilus-9khz4 update-demo-nautilus-hqv6w "
Apr 11 16:56:42.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-9khz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:42.669: INFO: stderr: ""
Apr 11 16:56:42.669: INFO: stdout: "true"
Apr 11 16:56:42.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-9khz4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:42.771: INFO: stderr: ""
Apr 11 16:56:42.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 16:56:42.772: INFO: validating pod update-demo-nautilus-9khz4
Apr 11 16:56:42.779: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 16:56:42.779: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 16:56:42.779: INFO: update-demo-nautilus-9khz4 is verified up and running
Apr 11 16:56:42.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-hqv6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:42.878: INFO: stderr: ""
Apr 11 16:56:42.878: INFO: stdout: "true"
Apr 11 16:56:42.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-hqv6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:56:42.983: INFO: stderr: ""
Apr 11 16:56:42.983: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 16:56:42.983: INFO: validating pod update-demo-nautilus-hqv6w
Apr 11 16:56:42.991: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 16:56:42.991: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 16:56:42.991: INFO: update-demo-nautilus-hqv6w is verified up and running
STEP: rolling-update to new replication controller
Apr 11 16:56:42.993: INFO: scanned /root for discovery docs: <nil>
Apr 11 16:56:42.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:06.535: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 11 16:57:06.535: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 16:57:06.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:06.650: INFO: stderr: ""
Apr 11 16:57:06.650: INFO: stdout: "update-demo-kitten-j99ml update-demo-kitten-ztc6r "
Apr 11 16:57:06.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-kitten-j99ml -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:06.763: INFO: stderr: ""
Apr 11 16:57:06.763: INFO: stdout: "true"
Apr 11 16:57:06.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-kitten-j99ml -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:06.863: INFO: stderr: ""
Apr 11 16:57:06.863: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 11 16:57:06.863: INFO: validating pod update-demo-kitten-j99ml
Apr 11 16:57:06.871: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 11 16:57:06.871: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 11 16:57:06.871: INFO: update-demo-kitten-j99ml is verified up and running
Apr 11 16:57:06.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-kitten-ztc6r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:06.976: INFO: stderr: ""
Apr 11 16:57:06.976: INFO: stdout: "true"
Apr 11 16:57:06.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-kitten-ztc6r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-q2t9t'
Apr 11 16:57:07.077: INFO: stderr: ""
Apr 11 16:57:07.077: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 11 16:57:07.077: INFO: validating pod update-demo-kitten-ztc6r
Apr 11 16:57:07.084: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 11 16:57:07.084: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 11 16:57:07.084: INFO: update-demo-kitten-ztc6r is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:57:07.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q2t9t" for this suite.
Apr 11 16:57:29.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:57:29.223: INFO: namespace: e2e-tests-kubectl-q2t9t, resource: bindings, ignored listing per whitelist
Apr 11 16:57:29.226: INFO: namespace e2e-tests-kubectl-q2t9t deletion completed in 22.135172021s

â€¢ [SLOW TEST:52.578 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:57:29.230: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tmk6v
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-e41d6853-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating secret with name s-test-opt-upd-e41d6890-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e41d6853-5c7a-11e9-bcf1-d217e8a0130c
STEP: Updating secret s-test-opt-upd-e41d6890-5c7a-11e9-bcf1-d217e8a0130c
STEP: Creating secret with name s-test-opt-create-e41d68a8-5c7a-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:58:44.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tmk6v" for this suite.
Apr 11 16:59:06.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:59:06.098: INFO: namespace: e2e-tests-projected-tmk6v, resource: bindings, ignored listing per whitelist
Apr 11 16:59:06.180: INFO: namespace e2e-tests-projected-tmk6v deletion completed in 22.123972262s

â€¢ [SLOW TEST:96.950 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:59:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-2lhjf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-2lhjf/configmap-test-1de73072-5c7b-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 16:59:06.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-2lhjf" to be "success or failure"
Apr 11 16:59:06.426: INFO: Pod "pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.795309ms
Apr 11 16:59:08.431: INFO: Pod "pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012158487s
STEP: Saw pod success
Apr 11 16:59:08.431: INFO: Pod "pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:59:08.435: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c container env-test: <nil>
STEP: delete the pod
Apr 11 16:59:08.476: INFO: Waiting for pod pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:59:08.481: INFO: Pod pod-configmaps-1de855c2-5c7b-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:59:08.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2lhjf" for this suite.
Apr 11 16:59:14.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:59:14.581: INFO: namespace: e2e-tests-configmap-2lhjf, resource: bindings, ignored listing per whitelist
Apr 11 16:59:14.615: INFO: namespace e2e-tests-configmap-2lhjf deletion completed in 6.129608018s

â€¢ [SLOW TEST:8.431 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:59:14.618: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6ncnk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 11 16:59:14.818: INFO: Waiting up to 5m0s for pod "pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-6ncnk" to be "success or failure"
Apr 11 16:59:14.823: INFO: Pod "pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169893ms
Apr 11 16:59:16.828: INFO: Pod "pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009819599s
STEP: Saw pod success
Apr 11 16:59:16.828: INFO: Pod "pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 16:59:16.832: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 16:59:16.870: INFO: Waiting for pod pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c to disappear
Apr 11 16:59:16.875: INFO: Pod pod-22eb5307-5c7b-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:59:16.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6ncnk" for this suite.
Apr 11 16:59:22.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:59:22.956: INFO: namespace: e2e-tests-emptydir-6ncnk, resource: bindings, ignored listing per whitelist
Apr 11 16:59:23.011: INFO: namespace e2e-tests-emptydir-6ncnk deletion completed in 6.130140848s

â€¢ [SLOW TEST:8.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:59:23.016: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-qrghf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 11 16:59:23.218: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 11 16:59:23.225: INFO: Waiting for terminating namespaces to be deleted...
Apr 11 16:59:23.229: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-1 before test
Apr 11 16:59:23.237: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zzsm9 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 16:59:23.237: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 16:59:23.237: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 16:59:23.237: INFO: nginx-proxy-cluster1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 16:59:23.237: INFO: kube-proxy-ffnl8 from kube-system started at 2019-04-11 16:07:43 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.237: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 16:59:23.237: INFO: calico-node-9mntg from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.238: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 16:59:23.238: INFO: dns-autoscaler-586f58b8bf-s49kj from kube-system started at 2019-04-11 16:08:13 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.238: INFO: 	Container autoscaler ready: true, restart count 0
Apr 11 16:59:23.238: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-11 16:41:26 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.238: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 11 16:59:23.238: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-2 before test
Apr 11 16:59:23.245: INFO: sonobuoy-e2e-job-56bc4b3c94a64376 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 16:59:23.245: INFO: 	Container e2e ready: true, restart count 0
Apr 11 16:59:23.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 16:59:23.245: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-cn2sq from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 16:59:23.246: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 16:59:23.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 16:59:23.246: INFO: nginx-proxy-cluster1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 16:59:23.246: INFO: kube-proxy-8kw6z from kube-system started at 2019-04-11 16:07:37 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.246: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 16:59:23.246: INFO: calico-node-ltdsr from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.246: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 16:59:23.247: INFO: tiller-deploy-dc85f7fbd-zml6j from kube-system started at 2019-04-11 16:13:49 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.247: INFO: 	Container tiller ready: true, restart count 0
Apr 11 16:59:23.247: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-3 before test
Apr 11 16:59:23.254: INFO: nginx-proxy-cluster1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 16:59:23.254: INFO: kube-proxy-zrsdq from kube-system started at 2019-04-11 16:07:54 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.254: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 16:59:23.254: INFO: calico-node-zzxxn from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.254: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 16:59:23.255: INFO: calico-kube-controllers-67f4fffbd-b9hw9 from kube-system started at 2019-04-11 16:07:44 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 11 16:59:23.255: INFO: kubernetes-dashboard-674d5cdc67-vpwkz from kube-system started at 2019-04-11 16:08:17 +0000 UTC (1 container statuses recorded)
Apr 11 16:59:23.255: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 11 16:59:23.255: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zpf9n from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 16:59:23.255: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 16:59:23.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node cluster1-k8s-node-1
STEP: verifying the node has the label node cluster1-k8s-node-2
STEP: verifying the node has the label node cluster1-k8s-node-3
Apr 11 16:59:23.314: INFO: Pod sonobuoy requesting resource cpu=0m on Node cluster1-k8s-node-1
Apr 11 16:59:23.314: INFO: Pod sonobuoy-e2e-job-56bc4b3c94a64376 requesting resource cpu=0m on Node cluster1-k8s-node-2
Apr 11 16:59:23.314: INFO: Pod sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-cn2sq requesting resource cpu=0m on Node cluster1-k8s-node-2
Apr 11 16:59:23.314: INFO: Pod sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zpf9n requesting resource cpu=0m on Node cluster1-k8s-node-3
Apr 11 16:59:23.315: INFO: Pod sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zzsm9 requesting resource cpu=0m on Node cluster1-k8s-node-1
Apr 11 16:59:23.315: INFO: Pod calico-kube-controllers-67f4fffbd-b9hw9 requesting resource cpu=30m on Node cluster1-k8s-node-3
Apr 11 16:59:23.315: INFO: Pod calico-node-9mntg requesting resource cpu=150m on Node cluster1-k8s-node-1
Apr 11 16:59:23.315: INFO: Pod calico-node-ltdsr requesting resource cpu=150m on Node cluster1-k8s-node-2
Apr 11 16:59:23.315: INFO: Pod calico-node-zzxxn requesting resource cpu=150m on Node cluster1-k8s-node-3
Apr 11 16:59:23.315: INFO: Pod dns-autoscaler-586f58b8bf-s49kj requesting resource cpu=20m on Node cluster1-k8s-node-1
Apr 11 16:59:23.315: INFO: Pod kube-proxy-8kw6z requesting resource cpu=0m on Node cluster1-k8s-node-2
Apr 11 16:59:23.316: INFO: Pod kube-proxy-ffnl8 requesting resource cpu=0m on Node cluster1-k8s-node-1
Apr 11 16:59:23.316: INFO: Pod kube-proxy-zrsdq requesting resource cpu=0m on Node cluster1-k8s-node-3
Apr 11 16:59:23.316: INFO: Pod kubernetes-dashboard-674d5cdc67-vpwkz requesting resource cpu=50m on Node cluster1-k8s-node-3
Apr 11 16:59:23.316: INFO: Pod nginx-proxy-cluster1-k8s-node-1 requesting resource cpu=25m on Node cluster1-k8s-node-1
Apr 11 16:59:23.316: INFO: Pod nginx-proxy-cluster1-k8s-node-2 requesting resource cpu=25m on Node cluster1-k8s-node-2
Apr 11 16:59:23.316: INFO: Pod nginx-proxy-cluster1-k8s-node-3 requesting resource cpu=25m on Node cluster1-k8s-node-3
Apr 11 16:59:23.316: INFO: Pod tiller-deploy-dc85f7fbd-zml6j requesting resource cpu=0m on Node cluster1-k8s-node-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c.15947a0c01aaef74], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qrghf/filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c to cluster1-k8s-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c.15947a0c31922704], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c.15947a0c936f1507], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c.15947a0c9566f174], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27fd8b64-5c7b-11e9-bcf1-d217e8a0130c.15947a0c9d1d697a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c.15947a0c02771fc4], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qrghf/filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c to cluster1-k8s-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c.15947a0c3309c130], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c.15947a0c96d80d96], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c.15947a0c98d892f2], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27ff1064-5c7b-11e9-bcf1-d217e8a0130c.15947a0ca1c8be15], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c.15947a0c033f5edc], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qrghf/filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c to cluster1-k8s-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c.15947a0c32240977], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c.15947a0c89bfc0ad], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c.15947a0c8b66e685], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28007b1a-5c7b-11e9-bcf1-d217e8a0130c.15947a0c942032b3], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15947a0cf361ba3b], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node cluster1-k8s-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cluster1-k8s-node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cluster1-k8s-node-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 16:59:28.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-qrghf" for this suite.
Apr 11 16:59:34.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 16:59:34.523: INFO: namespace: e2e-tests-sched-pred-qrghf, resource: bindings, ignored listing per whitelist
Apr 11 16:59:34.578: INFO: namespace e2e-tests-sched-pred-qrghf deletion completed in 6.125923427s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:11.563 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 16:59:34.582: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-vjz4q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-vjz4q
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-vjz4q
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-vjz4q
Apr 11 16:59:34.832: INFO: Found 0 stateful pods, waiting for 1
Apr 11 16:59:44.845: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 11 16:59:44.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 16:59:45.070: INFO: stderr: ""
Apr 11 16:59:45.070: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 16:59:45.070: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 16:59:45.076: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 11 16:59:55.088: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 16:59:55.088: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 16:59:55.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999972s
Apr 11 16:59:56.117: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992527275s
Apr 11 16:59:57.123: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986753083s
Apr 11 16:59:58.128: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980932122s
Apr 11 16:59:59.134: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975864469s
Apr 11 17:00:00.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969985959s
Apr 11 17:00:01.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964494229s
Apr 11 17:00:02.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.955986783s
Apr 11 17:00:03.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.950263599s
Apr 11 17:00:04.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.818551ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-vjz4q
Apr 11 17:00:05.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:00:05.411: INFO: stderr: ""
Apr 11 17:00:05.411: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:00:05.411: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:00:05.430: INFO: Found 1 stateful pods, waiting for 3
Apr 11 17:00:15.444: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:00:15.445: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:00:15.445: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 11 17:00:15.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 17:00:15.691: INFO: stderr: ""
Apr 11 17:00:15.691: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 17:00:15.691: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 17:00:15.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 17:00:15.903: INFO: stderr: ""
Apr 11 17:00:15.903: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 17:00:15.903: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 17:00:15.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 17:00:16.192: INFO: stderr: ""
Apr 11 17:00:16.192: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 17:00:16.192: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 17:00:16.192: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 17:00:16.195: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 11 17:00:26.211: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 17:00:26.211: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 17:00:26.211: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 17:00:26.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999624s
Apr 11 17:00:27.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99518693s
Apr 11 17:00:28.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98950736s
Apr 11 17:00:29.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982802131s
Apr 11 17:00:30.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972580027s
Apr 11 17:00:31.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966318836s
Apr 11 17:00:32.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960293332s
Apr 11 17:00:33.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.953075796s
Apr 11 17:00:34.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.946965578s
Apr 11 17:00:35.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.568442ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-vjz4q
Apr 11 17:00:36.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:00:36.575: INFO: stderr: ""
Apr 11 17:00:36.575: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:00:36.575: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:00:36.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:00:36.868: INFO: stderr: ""
Apr 11 17:00:36.868: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:00:36.868: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:00:36.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vjz4q ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:00:37.090: INFO: stderr: ""
Apr 11 17:00:37.090: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:00:37.090: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:00:37.090: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 11 17:01:07.116: INFO: Deleting all statefulset in ns e2e-tests-statefulset-vjz4q
Apr 11 17:01:07.119: INFO: Scaling statefulset ss to 0
Apr 11 17:01:07.130: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 17:01:07.133: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:01:07.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-vjz4q" for this suite.
Apr 11 17:01:13.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:01:13.197: INFO: namespace: e2e-tests-statefulset-vjz4q, resource: bindings, ignored listing per whitelist
Apr 11 17:01:13.298: INFO: namespace e2e-tests-statefulset-vjz4q deletion completed in 6.144321434s

â€¢ [SLOW TEST:98.717 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:01:13.301: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-4zlsw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:02:13.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4zlsw" for this suite.
Apr 11 17:02:35.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:02:35.667: INFO: namespace: e2e-tests-container-probe-4zlsw, resource: bindings, ignored listing per whitelist
Apr 11 17:02:35.702: INFO: namespace e2e-tests-container-probe-4zlsw deletion completed in 22.172905721s

â€¢ [SLOW TEST:82.401 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:02:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-8ltkw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8ltkw.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8ltkw.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8ltkw.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8ltkw.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8ltkw.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8ltkw.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 11 17:02:52.091: INFO: DNS probes using e2e-tests-dns-8ltkw/dns-test-9aca62d0-5c7b-11e9-bcf1-d217e8a0130c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:02:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8ltkw" for this suite.
Apr 11 17:02:58.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:02:58.173: INFO: namespace: e2e-tests-dns-8ltkw, resource: bindings, ignored listing per whitelist
Apr 11 17:02:58.255: INFO: namespace e2e-tests-dns-8ltkw deletion completed in 6.133422314s

â€¢ [SLOW TEST:22.551 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:02:58.258: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4snpw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 11 17:03:01.004: INFO: Successfully updated pod "pod-update-a839ce4f-5c7b-11e9-bcf1-d217e8a0130c"
STEP: verifying the updated pod is in kubernetes
Apr 11 17:03:01.015: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:03:01.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4snpw" for this suite.
Apr 11 17:03:23.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:03:23.129: INFO: namespace: e2e-tests-pods-4snpw, resource: bindings, ignored listing per whitelist
Apr 11 17:03:23.155: INFO: namespace e2e-tests-pods-4snpw deletion completed in 22.134186773s

â€¢ [SLOW TEST:24.898 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:03:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-dcc8c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 11 17:03:27.406: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-b712bcd7-5c7b-11e9-bcf1-d217e8a0130c,GenerateName:,Namespace:e2e-tests-events-dcc8c,SelfLink:/api/v1/namespaces/e2e-tests-events-dcc8c/pods/send-events-b712bcd7-5c7b-11e9-bcf1-d217e8a0130c,UID:b71383fd-5c7b-11e9-8000-fa163e4b6765,ResourceVersion:10366,Generation:0,CreationTimestamp:2019-04-11 17:03:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 370214971,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8rbhr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8rbhr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-8rbhr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aee580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aee5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 17:03:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 17:03:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 17:03:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 17:03:23 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.22,StartTime:2019-04-11 17:03:23 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-11 17:03:24 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://f6ff9113189f7075e3e08e5ab3800db73da6f515c42c2982685172f4ad99ec2c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr 11 17:03:29.411: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 11 17:03:31.416: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:03:31.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-dcc8c" for this suite.
Apr 11 17:04:09.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:04:09.596: INFO: namespace: e2e-tests-events-dcc8c, resource: bindings, ignored listing per whitelist
Apr 11 17:04:09.605: INFO: namespace e2e-tests-events-dcc8c deletion completed in 38.172427227s

â€¢ [SLOW TEST:46.447 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:04:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8kg9p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-d2c16453-5c7b-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:04:09.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-8kg9p" to be "success or failure"
Apr 11 17:04:09.835: INFO: Pod "pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133145ms
Apr 11 17:04:11.841: INFO: Pod "pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010202661s
STEP: Saw pod success
Apr 11 17:04:11.842: INFO: Pod "pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:04:11.846: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:04:11.891: INFO: Waiting for pod pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:04:11.895: INFO: Pod pod-configmaps-d2c2ad2c-5c7b-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:04:11.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8kg9p" for this suite.
Apr 11 17:04:17.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:04:17.987: INFO: namespace: e2e-tests-configmap-8kg9p, resource: bindings, ignored listing per whitelist
Apr 11 17:04:18.024: INFO: namespace e2e-tests-configmap-8kg9p deletion completed in 6.123501172s

â€¢ [SLOW TEST:8.416 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:04:18.028: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-b9gdz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:04:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:04:20.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-b9gdz" for this suite.
Apr 11 17:05:06.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:05:06.510: INFO: namespace: e2e-tests-pods-b9gdz, resource: bindings, ignored listing per whitelist
Apr 11 17:05:06.556: INFO: namespace e2e-tests-pods-b9gdz deletion completed in 46.132419123s

â€¢ [SLOW TEST:48.529 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:05:06.558: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-h4xmp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f4b263f8-5c7b-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:05:06.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-h4xmp" to be "success or failure"
Apr 11 17:05:06.781: INFO: Pod "pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331666ms
Apr 11 17:05:08.787: INFO: Pod "pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009932195s
STEP: Saw pod success
Apr 11 17:05:08.787: INFO: Pod "pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:05:08.791: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:05:08.828: INFO: Waiting for pod pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:05:08.835: INFO: Pod pod-projected-configmaps-f4b3b851-5c7b-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:05:08.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h4xmp" for this suite.
Apr 11 17:05:14.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:05:14.869: INFO: namespace: e2e-tests-projected-h4xmp, resource: bindings, ignored listing per whitelist
Apr 11 17:05:14.961: INFO: namespace e2e-tests-projected-h4xmp deletion completed in 6.120753225s

â€¢ [SLOW TEST:8.403 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:05:14.965: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-s8k62
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Apr 11 17:05:15.186: INFO: Waiting up to 5m0s for pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-containers-s8k62" to be "success or failure"
Apr 11 17:05:15.191: INFO: Pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.218232ms
Apr 11 17:05:17.203: INFO: Pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016780285s
Apr 11 17:05:19.207: INFO: Pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02132476s
Apr 11 17:05:21.214: INFO: Pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027753255s
STEP: Saw pod success
Apr 11 17:05:21.214: INFO: Pod "client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:05:21.218: INFO: Trying to get logs from node cluster1-k8s-node-1 pod client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:05:21.251: INFO: Waiting for pod client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:05:21.255: INFO: Pod client-containers-f9b68a9e-5c7b-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:05:21.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-s8k62" for this suite.
Apr 11 17:05:27.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:05:27.347: INFO: namespace: e2e-tests-containers-s8k62, resource: bindings, ignored listing per whitelist
Apr 11 17:05:27.398: INFO: namespace e2e-tests-containers-s8k62 deletion completed in 6.136259742s

â€¢ [SLOW TEST:12.433 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:05:27.402: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-h8l9w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0411 17:05:37.722486      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 17:05:37.722: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:05:37.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-h8l9w" for this suite.
Apr 11 17:05:43.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:05:43.820: INFO: namespace: e2e-tests-gc-h8l9w, resource: bindings, ignored listing per whitelist
Apr 11 17:05:43.857: INFO: namespace e2e-tests-gc-h8l9w deletion completed in 6.128596241s

â€¢ [SLOW TEST:16.456 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:05:43.861: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-prdfh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0411 17:05:54.130401      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 17:05:54.130: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:05:54.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-prdfh" for this suite.
Apr 11 17:06:00.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:00.169: INFO: namespace: e2e-tests-gc-prdfh, resource: bindings, ignored listing per whitelist
Apr 11 17:06:00.309: INFO: namespace e2e-tests-gc-prdfh deletion completed in 6.174670643s

â€¢ [SLOW TEST:16.448 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:00.311: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4zkjf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-14bef2e1-5c7c-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:06:00.541: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-4zkjf" to be "success or failure"
Apr 11 17:06:00.545: INFO: Pod "pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310397ms
Apr 11 17:06:02.551: INFO: Pod "pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010035215s
STEP: Saw pod success
Apr 11 17:06:02.551: INFO: Pod "pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:02.555: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:06:02.591: INFO: Waiting for pod pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:02.596: INFO: Pod pod-projected-secrets-14bfaa02-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:02.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4zkjf" for this suite.
Apr 11 17:06:08.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:08.677: INFO: namespace: e2e-tests-projected-4zkjf, resource: bindings, ignored listing per whitelist
Apr 11 17:06:08.728: INFO: namespace e2e-tests-projected-4zkjf deletion completed in 6.12620198s

â€¢ [SLOW TEST:8.418 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-v4gv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Apr 11 17:06:08.947: INFO: Waiting up to 5m0s for pod "client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-containers-v4gv6" to be "success or failure"
Apr 11 17:06:08.951: INFO: Pod "client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370462ms
Apr 11 17:06:10.966: INFO: Pod "client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018507382s
STEP: Saw pod success
Apr 11 17:06:10.966: INFO: Pod "client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:10.971: INFO: Trying to get logs from node cluster1-k8s-node-1 pod client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:06:11.023: INFO: Waiting for pod client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:11.027: INFO: Pod client-containers-19c2390c-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-v4gv6" for this suite.
Apr 11 17:06:17.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:17.090: INFO: namespace: e2e-tests-containers-v4gv6, resource: bindings, ignored listing per whitelist
Apr 11 17:06:17.178: INFO: namespace e2e-tests-containers-v4gv6 deletion completed in 6.145601183s

â€¢ [SLOW TEST:8.446 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nd5db
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 11 17:06:17.410: INFO: Waiting up to 5m0s for pod "pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-nd5db" to be "success or failure"
Apr 11 17:06:17.421: INFO: Pod "pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.462929ms
Apr 11 17:06:19.427: INFO: Pod "pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017028542s
Apr 11 17:06:21.441: INFO: Pod "pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031516648s
STEP: Saw pod success
Apr 11 17:06:21.441: INFO: Pod "pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:21.446: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:06:21.479: INFO: Waiting for pod pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:21.483: INFO: Pod pod-1ecd372d-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nd5db" for this suite.
Apr 11 17:06:27.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:27.567: INFO: namespace: e2e-tests-emptydir-nd5db, resource: bindings, ignored listing per whitelist
Apr 11 17:06:27.612: INFO: namespace e2e-tests-emptydir-nd5db deletion completed in 6.123820422s

â€¢ [SLOW TEST:10.432 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:27.617: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fcjcv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:06:27.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fcjcv'
Apr 11 17:06:27.960: INFO: stderr: ""
Apr 11 17:06:27.960: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Apr 11 17:06:27.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-fcjcv'
Apr 11 17:06:33.688: INFO: stderr: ""
Apr 11 17:06:33.688: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fcjcv" for this suite.
Apr 11 17:06:39.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:39.742: INFO: namespace: e2e-tests-kubectl-fcjcv, resource: bindings, ignored listing per whitelist
Apr 11 17:06:39.841: INFO: namespace e2e-tests-kubectl-fcjcv deletion completed in 6.138310977s

â€¢ [SLOW TEST:12.225 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:39.847: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v6rp8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:06:40.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-v6rp8" to be "success or failure"
Apr 11 17:06:40.071: INFO: Pod "downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.671618ms
Apr 11 17:06:42.077: INFO: Pod "downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016280546s
STEP: Saw pod success
Apr 11 17:06:42.077: INFO: Pod "downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:42.081: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:06:42.115: INFO: Waiting for pod downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:42.120: INFO: Pod downwardapi-volume-2c4d6687-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:42.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v6rp8" for this suite.
Apr 11 17:06:48.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:48.223: INFO: namespace: e2e-tests-projected-v6rp8, resource: bindings, ignored listing per whitelist
Apr 11 17:06:48.252: INFO: namespace e2e-tests-projected-v6rp8 deletion completed in 6.127061597s

â€¢ [SLOW TEST:8.406 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:48.254: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tdtl7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:06:48.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-tdtl7" to be "success or failure"
Apr 11 17:06:48.464: INFO: Pod "downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.528219ms
Apr 11 17:06:50.470: INFO: Pod "downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010648602s
STEP: Saw pod success
Apr 11 17:06:50.471: INFO: Pod "downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:50.475: INFO: Trying to get logs from node cluster1-k8s-node-1 pod downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:06:50.515: INFO: Waiting for pod downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:50.520: INFO: Pod downwardapi-volume-314f535a-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:50.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tdtl7" for this suite.
Apr 11 17:06:56.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:06:56.608: INFO: namespace: e2e-tests-downward-api-tdtl7, resource: bindings, ignored listing per whitelist
Apr 11 17:06:56.645: INFO: namespace e2e-tests-downward-api-tdtl7 deletion completed in 6.120122453s

â€¢ [SLOW TEST:8.391 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:06:56.647: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hdjmm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 11 17:06:56.868: INFO: Waiting up to 5m0s for pod "pod-36521530-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-hdjmm" to be "success or failure"
Apr 11 17:06:56.872: INFO: Pod "pod-36521530-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228239ms
Apr 11 17:06:58.878: INFO: Pod "pod-36521530-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00972098s
STEP: Saw pod success
Apr 11 17:06:58.878: INFO: Pod "pod-36521530-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:06:58.882: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-36521530-5c7c-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:06:58.917: INFO: Waiting for pod pod-36521530-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:06:58.921: INFO: Pod pod-36521530-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:06:58.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hdjmm" for this suite.
Apr 11 17:07:04.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:07:05.046: INFO: namespace: e2e-tests-emptydir-hdjmm, resource: bindings, ignored listing per whitelist
Apr 11 17:07:05.056: INFO: namespace e2e-tests-emptydir-hdjmm deletion completed in 6.128856649s

â€¢ [SLOW TEST:8.409 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:07:05.059: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-npk48
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Apr 11 17:07:05.272: INFO: namespace e2e-tests-kubectl-npk48
Apr 11 17:07:05.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-npk48'
Apr 11 17:07:05.589: INFO: stderr: ""
Apr 11 17:07:05.589: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 11 17:07:06.595: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:07:06.595: INFO: Found 0 / 1
Apr 11 17:07:07.595: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:07:07.595: INFO: Found 1 / 1
Apr 11 17:07:07.595: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 11 17:07:07.599: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:07:07.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 11 17:07:07.600: INFO: wait on redis-master startup in e2e-tests-kubectl-npk48 
Apr 11 17:07:07.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 logs redis-master-vl69d redis-master --namespace=e2e-tests-kubectl-npk48'
Apr 11 17:07:07.759: INFO: stderr: ""
Apr 11 17:07:07.759: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Apr 17:07:06.588 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Apr 17:07:06.588 # Server started, Redis version 3.2.12\n1:M 11 Apr 17:07:06.588 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Apr 17:07:06.588 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr 11 17:07:07.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-npk48'
Apr 11 17:07:07.898: INFO: stderr: ""
Apr 11 17:07:07.898: INFO: stdout: "service/rm2 exposed\n"
Apr 11 17:07:07.902: INFO: Service rm2 in namespace e2e-tests-kubectl-npk48 found.
STEP: exposing service
Apr 11 17:07:09.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-npk48'
Apr 11 17:07:10.040: INFO: stderr: ""
Apr 11 17:07:10.040: INFO: stdout: "service/rm3 exposed\n"
Apr 11 17:07:10.045: INFO: Service rm3 in namespace e2e-tests-kubectl-npk48 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:07:12.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-npk48" for this suite.
Apr 11 17:07:34.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:07:34.127: INFO: namespace: e2e-tests-kubectl-npk48, resource: bindings, ignored listing per whitelist
Apr 11 17:07:34.187: INFO: namespace e2e-tests-kubectl-npk48 deletion completed in 22.128953348s

â€¢ [SLOW TEST:29.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:07:34.193: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-frhk7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-frhk7
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-frhk7
STEP: Deleting pre-stop pod
Apr 11 17:07:47.469: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:07:47.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-frhk7" for this suite.
Apr 11 17:08:25.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:08:25.642: INFO: namespace: e2e-tests-prestop-frhk7, resource: bindings, ignored listing per whitelist
Apr 11 17:08:25.642: INFO: namespace e2e-tests-prestop-frhk7 deletion completed in 38.158048029s

â€¢ [SLOW TEST:51.449 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:08:25.642: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-fqb92
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:08:25.894: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6b611fb0-5c7c-11e9-8000-fa163e4b6765", Controller:(*bool)(0xc00238eec6), BlockOwnerDeletion:(*bool)(0xc00238eec7)}}
Apr 11 17:08:25.906: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6b5ebbbd-5c7c-11e9-8000-fa163e4b6765", Controller:(*bool)(0xc00238f0be), BlockOwnerDeletion:(*bool)(0xc00238f0bf)}}
Apr 11 17:08:25.914: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6b5fec87-5c7c-11e9-8000-fa163e4b6765", Controller:(*bool)(0xc00238f2ae), BlockOwnerDeletion:(*bool)(0xc00238f2af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:08:30.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fqb92" for this suite.
Apr 11 17:08:36.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:08:36.974: INFO: namespace: e2e-tests-gc-fqb92, resource: bindings, ignored listing per whitelist
Apr 11 17:08:37.085: INFO: namespace e2e-tests-gc-fqb92 deletion completed in 6.147144626s

â€¢ [SLOW TEST:11.443 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:08:37.088: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-vp958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:08:39.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-vp958" for this suite.
Apr 11 17:09:17.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:09:17.485: INFO: namespace: e2e-tests-kubelet-test-vp958, resource: bindings, ignored listing per whitelist
Apr 11 17:09:17.489: INFO: namespace e2e-tests-kubelet-test-vp958 deletion completed in 38.139449897s

â€¢ [SLOW TEST:40.401 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:09:17.491: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pj478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:09:17.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-pj478" to be "success or failure"
Apr 11 17:09:17.704: INFO: Pod "downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.974038ms
Apr 11 17:09:19.710: INFO: Pod "downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01042725s
Apr 11 17:09:21.726: INFO: Pod "downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02625786s
STEP: Saw pod success
Apr 11 17:09:21.726: INFO: Pod "downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:09:21.731: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:09:21.766: INFO: Waiting for pod downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:09:21.771: INFO: Pod downwardapi-volume-8a43581c-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:09:21.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pj478" for this suite.
Apr 11 17:09:27.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:09:27.894: INFO: namespace: e2e-tests-projected-pj478, resource: bindings, ignored listing per whitelist
Apr 11 17:09:27.903: INFO: namespace e2e-tests-projected-pj478 deletion completed in 6.126974587s

â€¢ [SLOW TEST:10.413 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:09:27.905: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-l2cvd
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-9079c389-5c7c-11e9-bcf1-d217e8a0130c
STEP: Creating secret with name s-test-opt-upd-9079c3ca-5c7c-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9079c389-5c7c-11e9-bcf1-d217e8a0130c
STEP: Updating secret s-test-opt-upd-9079c3ca-5c7c-11e9-bcf1-d217e8a0130c
STEP: Creating secret with name s-test-opt-create-9079c3e2-5c7c-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:09:34.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-l2cvd" for this suite.
Apr 11 17:09:56.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:09:56.310: INFO: namespace: e2e-tests-secrets-l2cvd, resource: bindings, ignored listing per whitelist
Apr 11 17:09:56.395: INFO: namespace e2e-tests-secrets-l2cvd deletion completed in 22.129766979s

â€¢ [SLOW TEST:28.490 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:09:56.399: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vkrr7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:09:56.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-vkrr7" to be "success or failure"
Apr 11 17:09:56.631: INFO: Pod "downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675413ms
Apr 11 17:09:58.638: INFO: Pod "downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011213387s
STEP: Saw pod success
Apr 11 17:09:58.638: INFO: Pod "downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:09:58.642: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:09:58.677: INFO: Waiting for pod downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:09:58.681: INFO: Pod downwardapi-volume-a17725bc-5c7c-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:09:58.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vkrr7" for this suite.
Apr 11 17:10:04.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:10:04.742: INFO: namespace: e2e-tests-projected-vkrr7, resource: bindings, ignored listing per whitelist
Apr 11 17:10:04.832: INFO: namespace e2e-tests-projected-vkrr7 deletion completed in 6.142939941s

â€¢ [SLOW TEST:8.433 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:10:04.836: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-sq8bp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 11 17:10:13.095: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 11 17:10:13.099: INFO: Pod pod-with-prestop-http-hook still exists
Apr 11 17:10:15.100: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 11 17:10:15.113: INFO: Pod pod-with-prestop-http-hook still exists
Apr 11 17:10:17.100: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 11 17:10:17.105: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:10:17.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-sq8bp" for this suite.
Apr 11 17:10:37.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:10:37.209: INFO: namespace: e2e-tests-container-lifecycle-hook-sq8bp, resource: bindings, ignored listing per whitelist
Apr 11 17:10:37.269: INFO: namespace e2e-tests-container-lifecycle-hook-sq8bp deletion completed in 20.148305224s

â€¢ [SLOW TEST:32.434 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:10:37.273: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-62lqz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 11 17:10:37.855: INFO: Pod name wrapped-volume-race-ba08454e-5c7c-11e9-bcf1-d217e8a0130c: Found 0 pods out of 5
Apr 11 17:10:42.865: INFO: Pod name wrapped-volume-race-ba08454e-5c7c-11e9-bcf1-d217e8a0130c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ba08454e-5c7c-11e9-bcf1-d217e8a0130c in namespace e2e-tests-emptydir-wrapper-62lqz, will wait for the garbage collector to delete the pods
Apr 11 17:12:44.990: INFO: Deleting ReplicationController wrapped-volume-race-ba08454e-5c7c-11e9-bcf1-d217e8a0130c took: 10.040884ms
Apr 11 17:12:45.190: INFO: Terminating ReplicationController wrapped-volume-race-ba08454e-5c7c-11e9-bcf1-d217e8a0130c pods took: 200.508419ms
STEP: Creating RC which spawns configmap-volume pods
Apr 11 17:13:23.819: INFO: Pod name wrapped-volume-race-1cf3af08-5c7d-11e9-bcf1-d217e8a0130c: Found 0 pods out of 5
Apr 11 17:13:28.828: INFO: Pod name wrapped-volume-race-1cf3af08-5c7d-11e9-bcf1-d217e8a0130c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1cf3af08-5c7d-11e9-bcf1-d217e8a0130c in namespace e2e-tests-emptydir-wrapper-62lqz, will wait for the garbage collector to delete the pods
Apr 11 17:15:22.929: INFO: Deleting ReplicationController wrapped-volume-race-1cf3af08-5c7d-11e9-bcf1-d217e8a0130c took: 10.714861ms
Apr 11 17:15:23.029: INFO: Terminating ReplicationController wrapped-volume-race-1cf3af08-5c7d-11e9-bcf1-d217e8a0130c pods took: 100.408877ms
STEP: Creating RC which spawns configmap-volume pods
Apr 11 17:16:04.159: INFO: Pod name wrapped-volume-race-7c858028-5c7d-11e9-bcf1-d217e8a0130c: Found 0 pods out of 5
Apr 11 17:16:09.167: INFO: Pod name wrapped-volume-race-7c858028-5c7d-11e9-bcf1-d217e8a0130c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7c858028-5c7d-11e9-bcf1-d217e8a0130c in namespace e2e-tests-emptydir-wrapper-62lqz, will wait for the garbage collector to delete the pods
Apr 11 17:17:53.263: INFO: Deleting ReplicationController wrapped-volume-race-7c858028-5c7d-11e9-bcf1-d217e8a0130c took: 11.977287ms
Apr 11 17:17:53.463: INFO: Terminating ReplicationController wrapped-volume-race-7c858028-5c7d-11e9-bcf1-d217e8a0130c pods took: 200.255877ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:18:30.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-62lqz" for this suite.
Apr 11 17:18:38.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:18:38.597: INFO: namespace: e2e-tests-emptydir-wrapper-62lqz, resource: bindings, ignored listing per whitelist
Apr 11 17:18:38.674: INFO: namespace e2e-tests-emptydir-wrapper-62lqz deletion completed in 8.126568539s

â€¢ [SLOW TEST:481.401 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:18:38.680: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-z5nfw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0411 17:18:44.931143      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 17:18:44.931: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:18:44.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-z5nfw" for this suite.
Apr 11 17:18:52.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:18:52.989: INFO: namespace: e2e-tests-gc-z5nfw, resource: bindings, ignored listing per whitelist
Apr 11 17:18:53.064: INFO: namespace e2e-tests-gc-z5nfw deletion completed in 8.127922801s

â€¢ [SLOW TEST:14.384 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:18:53.067: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-llkvv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-llkvv
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Apr 11 17:18:53.294: INFO: Found 0 stateful pods, waiting for 3
Apr 11 17:19:03.307: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:19:03.307: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:19:03.307: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:19:03.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-llkvv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 17:19:03.536: INFO: stderr: ""
Apr 11 17:19:03.536: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 17:19:03.536: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 11 17:19:13.584: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 11 17:19:23.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-llkvv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:19:23.898: INFO: stderr: ""
Apr 11 17:19:23.898: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:19:23.898: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:19:43.930: INFO: Waiting for StatefulSet e2e-tests-statefulset-llkvv/ss2 to complete update
Apr 11 17:19:43.930: INFO: Waiting for Pod e2e-tests-statefulset-llkvv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 11 17:19:53.946: INFO: Waiting for StatefulSet e2e-tests-statefulset-llkvv/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 11 17:20:03.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-llkvv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 17:20:04.177: INFO: stderr: ""
Apr 11 17:20:04.177: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 17:20:04.177: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 17:20:14.221: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 11 17:20:24.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-llkvv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 17:20:24.502: INFO: stderr: ""
Apr 11 17:20:24.502: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 17:20:24.502: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 17:20:34.539: INFO: Waiting for StatefulSet e2e-tests-statefulset-llkvv/ss2 to complete update
Apr 11 17:20:34.539: INFO: Waiting for Pod e2e-tests-statefulset-llkvv/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 11 17:20:34.539: INFO: Waiting for Pod e2e-tests-statefulset-llkvv/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 11 17:20:44.561: INFO: Waiting for StatefulSet e2e-tests-statefulset-llkvv/ss2 to complete update
Apr 11 17:20:44.561: INFO: Waiting for Pod e2e-tests-statefulset-llkvv/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 11 17:20:54.548: INFO: Waiting for StatefulSet e2e-tests-statefulset-llkvv/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 11 17:21:04.555: INFO: Deleting all statefulset in ns e2e-tests-statefulset-llkvv
Apr 11 17:21:04.558: INFO: Scaling statefulset ss2 to 0
Apr 11 17:21:14.588: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 17:21:14.592: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:21:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-llkvv" for this suite.
Apr 11 17:21:20.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:21:20.747: INFO: namespace: e2e-tests-statefulset-llkvv, resource: bindings, ignored listing per whitelist
Apr 11 17:21:20.753: INFO: namespace e2e-tests-statefulset-llkvv deletion completed in 6.137629567s

â€¢ [SLOW TEST:147.687 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:21:20.756: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jf7bc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-395dfe2c-5c7e-11e9-bcf1-d217e8a0130c
STEP: Creating secret with name secret-projected-all-test-volume-395dfe1a-5c7e-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 11 17:21:20.989: INFO: Waiting up to 5m0s for pod "projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-jf7bc" to be "success or failure"
Apr 11 17:21:20.996: INFO: Pod "projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.747827ms
Apr 11 17:21:23.002: INFO: Pod "projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012539653s
STEP: Saw pod success
Apr 11 17:21:23.002: INFO: Pod "projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:21:23.007: INFO: Trying to get logs from node cluster1-k8s-node-3 pod projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 11 17:21:23.046: INFO: Waiting for pod projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:21:23.052: INFO: Pod projected-volume-395dfde9-5c7e-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:21:23.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jf7bc" for this suite.
Apr 11 17:21:29.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:21:29.174: INFO: namespace: e2e-tests-projected-jf7bc, resource: bindings, ignored listing per whitelist
Apr 11 17:21:29.186: INFO: namespace e2e-tests-projected-jf7bc deletion completed in 6.128969999s

â€¢ [SLOW TEST:8.430 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:21:29.188: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-mnlxx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 11 17:21:29.431: INFO: Waiting up to 5m0s for pod "pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-mnlxx" to be "success or failure"
Apr 11 17:21:29.438: INFO: Pod "pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.863102ms
Apr 11 17:21:31.444: INFO: Pod "pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012917595s
STEP: Saw pod success
Apr 11 17:21:31.444: INFO: Pod "pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:21:31.449: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:21:31.480: INFO: Waiting for pod pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:21:31.484: INFO: Pod pod-3e6737ab-5c7e-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:21:31.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mnlxx" for this suite.
Apr 11 17:21:37.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:21:37.564: INFO: namespace: e2e-tests-emptydir-mnlxx, resource: bindings, ignored listing per whitelist
Apr 11 17:21:37.628: INFO: namespace e2e-tests-emptydir-mnlxx deletion completed in 6.138183089s

â€¢ [SLOW TEST:8.440 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:21:37.629: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-wf8md
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:21:37.861: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Apr 11 17:21:37.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-wf8md/daemonsets","resourceVersion":"15041"},"items":null}

Apr 11 17:21:37.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-wf8md/pods","resourceVersion":"15041"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:21:37.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-wf8md" for this suite.
Apr 11 17:21:43.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:21:43.917: INFO: namespace: e2e-tests-daemonsets-wf8md, resource: bindings, ignored listing per whitelist
Apr 11 17:21:44.017: INFO: namespace e2e-tests-daemonsets-wf8md deletion completed in 6.12420976s

S [SKIPPING] [6.388 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Apr 11 17:21:37.861: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:21:44.021: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-sk7ll
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:21:44.213: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:21:50.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-sk7ll" for this suite.
Apr 11 17:21:56.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:21:56.420: INFO: namespace: e2e-tests-custom-resource-definition-sk7ll, resource: bindings, ignored listing per whitelist
Apr 11 17:21:56.433: INFO: namespace e2e-tests-custom-resource-definition-sk7ll deletion completed in 6.134257148s

â€¢ [SLOW TEST:12.412 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:21:56.437: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-c65vv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Apr 11 17:21:56.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 --namespace=e2e-tests-kubectl-c65vv run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 11 17:21:58.725: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 11 17:21:58.725: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:22:00.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c65vv" for this suite.
Apr 11 17:22:06.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:22:06.862: INFO: namespace: e2e-tests-kubectl-c65vv, resource: bindings, ignored listing per whitelist
Apr 11 17:22:06.896: INFO: namespace e2e-tests-kubectl-c65vv deletion completed in 6.158529037s

â€¢ [SLOW TEST:10.460 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:22:06.899: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-dl9nh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-dl9nh
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Apr 11 17:22:07.117: INFO: Found 0 stateful pods, waiting for 3
Apr 11 17:22:17.130: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:22:17.130: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:22:17.130: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 11 17:22:17.172: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 11 17:22:27.219: INFO: Updating stateful set ss2
Apr 11 17:22:27.227: INFO: Waiting for Pod e2e-tests-statefulset-dl9nh/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr 11 17:22:37.372: INFO: Found 2 stateful pods, waiting for 3
Apr 11 17:22:47.385: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:22:47.385: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 17:22:47.385: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 11 17:22:47.414: INFO: Updating stateful set ss2
Apr 11 17:22:47.423: INFO: Waiting for Pod e2e-tests-statefulset-dl9nh/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 11 17:22:57.460: INFO: Updating stateful set ss2
Apr 11 17:22:57.468: INFO: Waiting for StatefulSet e2e-tests-statefulset-dl9nh/ss2 to complete update
Apr 11 17:22:57.468: INFO: Waiting for Pod e2e-tests-statefulset-dl9nh/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 11 17:23:07.485: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dl9nh
Apr 11 17:23:07.488: INFO: Scaling statefulset ss2 to 0
Apr 11 17:23:27.509: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 17:23:27.512: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:23:27.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dl9nh" for this suite.
Apr 11 17:23:33.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:23:33.641: INFO: namespace: e2e-tests-statefulset-dl9nh, resource: bindings, ignored listing per whitelist
Apr 11 17:23:33.668: INFO: namespace e2e-tests-statefulset-dl9nh deletion completed in 6.127894321s

â€¢ [SLOW TEST:86.769 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:23:33.670: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sq9p8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-88982b7d-5c7e-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:23:33.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-sq9p8" to be "success or failure"
Apr 11 17:23:33.902: INFO: Pod "pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566094ms
Apr 11 17:23:35.907: INFO: Pod "pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009739298s
Apr 11 17:23:37.920: INFO: Pod "pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022970179s
STEP: Saw pod success
Apr 11 17:23:37.920: INFO: Pod "pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:23:37.924: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:23:37.956: INFO: Waiting for pod pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:23:37.960: INFO: Pod pod-projected-secrets-88990538-5c7e-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:23:37.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sq9p8" for this suite.
Apr 11 17:23:43.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:23:44.059: INFO: namespace: e2e-tests-projected-sq9p8, resource: bindings, ignored listing per whitelist
Apr 11 17:23:44.087: INFO: namespace e2e-tests-projected-sq9p8 deletion completed in 6.12221801s

â€¢ [SLOW TEST:10.418 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:23:44.090: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-x5r56
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 11 17:23:44.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 11 17:23:44.309: INFO: Waiting for terminating namespaces to be deleted...
Apr 11 17:23:44.313: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-1 before test
Apr 11 17:23:44.325: INFO: dns-autoscaler-586f58b8bf-s49kj from kube-system started at 2019-04-11 16:08:13 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.325: INFO: 	Container autoscaler ready: true, restart count 0
Apr 11 17:23:44.326: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-11 16:41:26 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.326: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 11 17:23:44.326: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zzsm9 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:23:44.327: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 17:23:44.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 17:23:44.327: INFO: nginx-proxy-cluster1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:23:44.327: INFO: kube-proxy-ffnl8 from kube-system started at 2019-04-11 16:07:43 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.328: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:23:44.328: INFO: calico-node-9mntg from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.328: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:23:44.329: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-2 before test
Apr 11 17:23:44.347: INFO: calico-node-ltdsr from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.347: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:23:44.347: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-cn2sq from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:23:44.348: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 17:23:44.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 17:23:44.348: INFO: tiller-deploy-dc85f7fbd-zml6j from kube-system started at 2019-04-11 16:13:49 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.348: INFO: 	Container tiller ready: true, restart count 0
Apr 11 17:23:44.349: INFO: sonobuoy-e2e-job-56bc4b3c94a64376 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:23:44.349: INFO: 	Container e2e ready: true, restart count 0
Apr 11 17:23:44.349: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 17:23:44.350: INFO: nginx-proxy-cluster1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:23:44.350: INFO: kube-proxy-8kw6z from kube-system started at 2019-04-11 16:07:37 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.350: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:23:44.351: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-3 before test
Apr 11 17:23:44.365: INFO: kube-proxy-zrsdq from kube-system started at 2019-04-11 16:07:54 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.365: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:23:44.365: INFO: kubernetes-dashboard-674d5cdc67-vpwkz from kube-system started at 2019-04-11 16:08:17 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.365: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 11 17:23:44.366: INFO: nginx-proxy-cluster1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:23:44.366: INFO: calico-node-zzxxn from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.366: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:23:44.366: INFO: calico-kube-controllers-67f4fffbd-b9hw9 from kube-system started at 2019-04-11 16:07:44 +0000 UTC (1 container statuses recorded)
Apr 11 17:23:44.366: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 11 17:23:44.367: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zpf9n from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:23:44.367: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 11 17:23:44.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-90109704-5c7e-11e9-bcf1-d217e8a0130c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-90109704-5c7e-11e9-bcf1-d217e8a0130c off the node cluster1-k8s-node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-90109704-5c7e-11e9-bcf1-d217e8a0130c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:23:50.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-x5r56" for this suite.
Apr 11 17:23:58.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:23:58.524: INFO: namespace: e2e-tests-sched-pred-x5r56, resource: bindings, ignored listing per whitelist
Apr 11 17:23:58.621: INFO: namespace e2e-tests-sched-pred-x5r56 deletion completed in 8.149163926s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:14.532 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:23:58.624: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-k9425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:23:58.850: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 11 17:23:58.860: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:58.860: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:58.860: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:58.865: INFO: Number of nodes with available pods: 0
Apr 11 17:23:58.866: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:23:59.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:59.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:59.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:23:59.877: INFO: Number of nodes with available pods: 0
Apr 11 17:23:59.877: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:24:00.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:00.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:00.873: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:00.878: INFO: Number of nodes with available pods: 2
Apr 11 17:24:00.878: INFO: Node cluster1-k8s-node-2 is running more than one daemon pod
Apr 11 17:24:01.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:01.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:01.872: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:01.877: INFO: Number of nodes with available pods: 3
Apr 11 17:24:01.877: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 11 17:24:01.910: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:01.911: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:01.911: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:01.915: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:01.915: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:01.915: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:02.923: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:02.923: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:02.923: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:02.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:02.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:02.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:03.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:03.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:03.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:03.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:03.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:03.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:04.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:04.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:04.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:05.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:05.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:05.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:06.924: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:06.924: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:06.924: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:06.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:06.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:06.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:07.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:07.923: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:07.923: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:07.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:07.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:07.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:08.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:08.928: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:08.928: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:08.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:08.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:08.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:09.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:09.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:09.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:09.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:09.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:09.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:10.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:10.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:10.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:11.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:11.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:11.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:12.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:12.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:12.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:12.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:12.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:12.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:13.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:13.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:13.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:13.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:13.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:13.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:14.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:14.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:14.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:14.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:14.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:14.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:15.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:15.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:15.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:15.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:15.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:15.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:16.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:16.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:16.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:16.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:16.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:16.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:17.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:17.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:17.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:17.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:17.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:17.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:18.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:18.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:18.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:18.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:18.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:18.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:19.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:19.928: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:19.928: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:19.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:19.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:19.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:20.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:20.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:20.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:21.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:21.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:21.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:21.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:21.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:21.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:22.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:22.923: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:22.923: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:22.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:22.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:22.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:23.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:23.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:23.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:24.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:24.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:24.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:24.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:24.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:24.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:25.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:25.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:25.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:25.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:25.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:25.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:26.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:26.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:26.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:27.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:27.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:27.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:28.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:28.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:28.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:28.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:28.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:28.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:29.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:29.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:29.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:29.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:29.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:29.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:30.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:30.928: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:30.928: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:30.932: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:30.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:30.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:31.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:31.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:31.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:31.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:31.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:31.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:32.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:32.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:32.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:32.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:32.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:32.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:33.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:33.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:33.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:33.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:33.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:33.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:34.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:34.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:34.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:34.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:34.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:34.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:34.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:35.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:35.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:35.921: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:35.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:35.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:35.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:35.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:36.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:36.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:36.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:36.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:36.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:36.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:36.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:37.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:37.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:37.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:37.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:38.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:38.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:38.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:38.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:38.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:38.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:38.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:39.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:39.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:39.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:39.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:39.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:39.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:39.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:40.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:40.922: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:40.922: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:40.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:40.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:40.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:40.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:41.929: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:41.929: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:41.929: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:41.929: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:41.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:41.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:41.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:42.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:42.921: INFO: Wrong image for pod: daemon-set-cxc5v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:42.921: INFO: Pod daemon-set-cxc5v is not available
Apr 11 17:24:42.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:43.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:43.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:43.921: INFO: Pod daemon-set-x9l5w is not available
Apr 11 17:24:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:44.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:44.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:44.921: INFO: Pod daemon-set-x9l5w is not available
Apr 11 17:24:44.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:44.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:44.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:45.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:45.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:45.921: INFO: Pod daemon-set-x9l5w is not available
Apr 11 17:24:45.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:45.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:45.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:46.923: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:46.923: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:46.923: INFO: Pod daemon-set-x9l5w is not available
Apr 11 17:24:46.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:46.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:46.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:47.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:47.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:47.922: INFO: Pod daemon-set-x9l5w is not available
Apr 11 17:24:47.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:47.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:47.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:48.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:48.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:48.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:48.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:48.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:49.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:49.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:50.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:50.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:50.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:50.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:50.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:51.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:51.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:51.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:51.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:51.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:52.927: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:52.927: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:52.932: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:52.932: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:52.932: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:53.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:53.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:53.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:53.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:53.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:54.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:54.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:54.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:54.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:54.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:55.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:55.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:55.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:55.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:55.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:56.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:56.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:56.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:56.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:56.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:57.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:57.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:57.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:57.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:57.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:58.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:58.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:58.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:58.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:58.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:59.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:59.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:24:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:24:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:00.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:00.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:00.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:00.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:00.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:01.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:01.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:01.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:01.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:01.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:02.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:02.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:02.934: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:02.934: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:02.934: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:03.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:03.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:03.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:03.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:03.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:04.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:04.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:04.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:05.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:05.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:05.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:06.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:06.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:06.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:06.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:06.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:07.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:07.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:07.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:07.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:07.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:08.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:08.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:08.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:08.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:08.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:09.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:09.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:09.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:09.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:09.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:10.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:10.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:10.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:11.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:11.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:11.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:12.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:12.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:12.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:12.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:12.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:13.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:13.928: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:13.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:13.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:13.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:14.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:14.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:14.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:14.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:14.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:15.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:15.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:15.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:15.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:15.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:16.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:16.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:16.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:16.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:16.929: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:17.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:17.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:17.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:17.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:17.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:18.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:18.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:18.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:18.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:18.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:19.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:19.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:19.922: INFO: Pod daemon-set-d4zb7 is not available
Apr 11 17:25:19.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:19.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:19.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:20.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:20.921: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:20.921: INFO: Pod daemon-set-d4zb7 is not available
Apr 11 17:25:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:20.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:21.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:21.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:21.922: INFO: Pod daemon-set-d4zb7 is not available
Apr 11 17:25:21.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:21.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:21.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:22.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:22.922: INFO: Wrong image for pod: daemon-set-d4zb7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:22.922: INFO: Pod daemon-set-d4zb7 is not available
Apr 11 17:25:22.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:22.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:22.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:23.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:23.921: INFO: Pod daemon-set-lg6hk is not available
Apr 11 17:25:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:23.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:24.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:24.928: INFO: Pod daemon-set-lg6hk is not available
Apr 11 17:25:24.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:24.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:24.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:25.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:25.921: INFO: Pod daemon-set-lg6hk is not available
Apr 11 17:25:25.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:25.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:25.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:26.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:26.921: INFO: Pod daemon-set-lg6hk is not available
Apr 11 17:25:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:26.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:27.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:27.922: INFO: Pod daemon-set-lg6hk is not available
Apr 11 17:25:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:27.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:28.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:28.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:28.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:28.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:29.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:29.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:29.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:29.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:30.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:30.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:30.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:30.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:31.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:31.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:31.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:31.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:32.924: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:32.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:32.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:32.930: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:33.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:33.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:33.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:33.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:34.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:34.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:34.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:34.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:35.933: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:35.939: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:35.939: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:35.940: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:36.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:36.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:36.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:36.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:37.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:37.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:38.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:38.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:38.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:38.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:39.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:39.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:39.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:39.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:40.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:40.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:40.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:40.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:41.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:41.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:41.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:41.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:42.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:42.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:43.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:43.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:44.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:44.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:44.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:44.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:45.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:45.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:45.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:45.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:46.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:46.932: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:46.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:46.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:47.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:47.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:47.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:47.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:48.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:48.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:48.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:48.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:49.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:49.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:50.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:50.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:50.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:50.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:51.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:51.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:51.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:51.925: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:52.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:52.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:52.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:52.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:53.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:53.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:53.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:53.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:54.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:54.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:54.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:54.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:55.923: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:55.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:55.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:55.928: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:56.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:56.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:56.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:56.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:57.928: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:57.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:57.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:57.933: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:58.921: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:58.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:58.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:58.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:59.922: INFO: Wrong image for pod: daemon-set-74qcb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Apr 11 17:25:59.922: INFO: Pod daemon-set-74qcb is not available
Apr 11 17:25:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:25:59.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.921: INFO: Pod daemon-set-lvvnn is not available
Apr 11 17:26:00.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.926: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.927: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 11 17:26:00.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.931: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:00.936: INFO: Number of nodes with available pods: 2
Apr 11 17:26:00.936: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:26:01.941: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:01.941: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:01.941: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 17:26:01.947: INFO: Number of nodes with available pods: 3
Apr 11 17:26:01.947: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-k9425, will wait for the garbage collector to delete the pods
Apr 11 17:26:02.028: INFO: Deleting DaemonSet.extensions daemon-set took: 9.475088ms
Apr 11 17:26:02.129: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.240896ms
Apr 11 17:26:13.741: INFO: Number of nodes with available pods: 0
Apr 11 17:26:13.741: INFO: Number of running nodes: 0, number of available pods: 0
Apr 11 17:26:13.744: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k9425/daemonsets","resourceVersion":"16247"},"items":null}

Apr 11 17:26:13.751: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k9425/pods","resourceVersion":"16247"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:26:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k9425" for this suite.
Apr 11 17:26:19.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:26:19.882: INFO: namespace: e2e-tests-daemonsets-k9425, resource: bindings, ignored listing per whitelist
Apr 11 17:26:19.905: INFO: namespace e2e-tests-daemonsets-k9425 deletion completed in 6.129241071s

â€¢ [SLOW TEST:141.281 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:26:19.911: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-4wzbl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Apr 11 17:26:20.647: INFO: created pod pod-service-account-defaultsa
Apr 11 17:26:20.647: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 11 17:26:20.657: INFO: created pod pod-service-account-mountsa
Apr 11 17:26:20.657: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 11 17:26:20.666: INFO: created pod pod-service-account-nomountsa
Apr 11 17:26:20.666: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 11 17:26:20.677: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 11 17:26:20.677: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 11 17:26:20.686: INFO: created pod pod-service-account-mountsa-mountspec
Apr 11 17:26:20.686: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 11 17:26:20.697: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 11 17:26:20.697: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 11 17:26:20.707: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 11 17:26:20.707: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 11 17:26:20.716: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 11 17:26:20.716: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 11 17:26:20.723: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 11 17:26:20.723: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:26:20.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-4wzbl" for this suite.
Apr 11 17:26:44.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:26:44.778: INFO: namespace: e2e-tests-svcaccounts-4wzbl, resource: bindings, ignored listing per whitelist
Apr 11 17:26:44.870: INFO: namespace e2e-tests-svcaccounts-4wzbl deletion completed in 24.139535511s

â€¢ [SLOW TEST:24.960 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:26:44.872: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2n7s5
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 11 17:26:45.086: INFO: Waiting up to 5m0s for pod "pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-2n7s5" to be "success or failure"
Apr 11 17:26:45.091: INFO: Pod "pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625814ms
Apr 11 17:26:47.103: INFO: Pod "pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016858768s
STEP: Saw pod success
Apr 11 17:26:47.103: INFO: Pod "pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:26:47.108: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:26:47.141: INFO: Waiting for pod pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:26:47.146: INFO: Pod pod-fa8dca97-5c7e-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:26:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2n7s5" for this suite.
Apr 11 17:26:53.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:26:53.289: INFO: namespace: e2e-tests-emptydir-2n7s5, resource: bindings, ignored listing per whitelist
Apr 11 17:26:53.293: INFO: namespace e2e-tests-emptydir-2n7s5 deletion completed in 6.141686843s

â€¢ [SLOW TEST:8.421 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:26:53.300: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xvxqh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Apr 11 17:26:53.537: INFO: Waiting up to 5m0s for pod "downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-xvxqh" to be "success or failure"
Apr 11 17:26:53.541: INFO: Pod "downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315165ms
Apr 11 17:26:55.547: INFO: Pod "downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009728195s
STEP: Saw pod success
Apr 11 17:26:55.547: INFO: Pod "downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:26:55.552: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 17:26:55.625: INFO: Waiting for pod downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:26:55.630: INFO: Pod downward-api-ff978a57-5c7e-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:26:55.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xvxqh" for this suite.
Apr 11 17:27:01.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:27:01.689: INFO: namespace: e2e-tests-downward-api-xvxqh, resource: bindings, ignored listing per whitelist
Apr 11 17:27:01.770: INFO: namespace e2e-tests-downward-api-xvxqh deletion completed in 6.131134534s

â€¢ [SLOW TEST:8.470 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:27:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4p28g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Apr 11 17:27:01.983: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 11 17:27:01.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:02.207: INFO: stderr: ""
Apr 11 17:27:02.207: INFO: stdout: "service/redis-slave created\n"
Apr 11 17:27:02.207: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 11 17:27:02.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:02.548: INFO: stderr: ""
Apr 11 17:27:02.548: INFO: stdout: "service/redis-master created\n"
Apr 11 17:27:02.548: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 11 17:27:02.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:02.902: INFO: stderr: ""
Apr 11 17:27:02.902: INFO: stdout: "service/frontend created\n"
Apr 11 17:27:02.902: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 11 17:27:02.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:03.243: INFO: stderr: ""
Apr 11 17:27:03.243: INFO: stdout: "deployment.extensions/frontend created\n"
Apr 11 17:27:03.243: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 11 17:27:03.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:03.503: INFO: stderr: ""
Apr 11 17:27:03.503: INFO: stdout: "deployment.extensions/redis-master created\n"
Apr 11 17:27:03.503: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 11 17:27:03.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:03.935: INFO: stderr: ""
Apr 11 17:27:03.935: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Apr 11 17:27:03.935: INFO: Waiting for all frontend pods to be Running.
Apr 11 17:27:23.986: INFO: Waiting for frontend to serve content.
Apr 11 17:27:25.022: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Apr 11 17:27:30.044: INFO: Trying to add a new entry to the guestbook.
Apr 11 17:27:30.063: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 11 17:27:30.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:30.231: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:30.231: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 11 17:27:30.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:30.581: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:30.581: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 11 17:27:30.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:30.846: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:30.846: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 11 17:27:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:31.062: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:31.062: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 11 17:27:31.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:31.217: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:31.218: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 11 17:27:31.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4p28g'
Apr 11 17:27:31.490: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:27:31.490: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:27:31.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4p28g" for this suite.
Apr 11 17:28:15.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:28:15.582: INFO: namespace: e2e-tests-kubectl-4p28g, resource: bindings, ignored listing per whitelist
Apr 11 17:28:15.665: INFO: namespace e2e-tests-kubectl-4p28g deletion completed in 44.169412121s

â€¢ [SLOW TEST:73.894 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:28:15.667: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-rmltz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-rmltz
Apr 11 17:28:17.900: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-rmltz
STEP: checking the pod's current state and verifying that restartCount is present
Apr 11 17:28:17.905: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:32:18.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rmltz" for this suite.
Apr 11 17:32:24.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:32:24.903: INFO: namespace: e2e-tests-container-probe-rmltz, resource: bindings, ignored listing per whitelist
Apr 11 17:32:24.913: INFO: namespace e2e-tests-container-probe-rmltz deletion completed in 6.124747644s

â€¢ [SLOW TEST:249.246 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:32:24.914: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-cdk7w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:32:27.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-cdk7w" for this suite.
Apr 11 17:33:05.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:33:05.215: INFO: namespace: e2e-tests-kubelet-test-cdk7w, resource: bindings, ignored listing per whitelist
Apr 11 17:33:05.285: INFO: namespace e2e-tests-kubelet-test-cdk7w deletion completed in 38.124462955s

â€¢ [SLOW TEST:40.372 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:33:05.287: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-nf78m
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-dd4ba242-5c7f-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:33:09.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nf78m" for this suite.
Apr 11 17:33:31.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:33:31.575: INFO: namespace: e2e-tests-configmap-nf78m, resource: bindings, ignored listing per whitelist
Apr 11 17:33:31.684: INFO: namespace e2e-tests-configmap-nf78m deletion completed in 22.138695321s

â€¢ [SLOW TEST:26.397 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:33:31.685: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p2djj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:33:31.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:32.113: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 11 17:33:32.113: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Apr 11 17:33:32.118: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Apr 11 17:33:32.128: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Apr 11 17:33:32.156: INFO: scanned /root for discovery docs: <nil>
Apr 11 17:33:32.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:48.008: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 11 17:33:48.008: INFO: stdout: "Created e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604\nScaling up e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr 11 17:33:48.008: INFO: stdout: "Created e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604\nScaling up e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr 11 17:33:48.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:48.148: INFO: stderr: ""
Apr 11 17:33:48.148: INFO: stdout: "e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604-jh28s "
Apr 11 17:33:48.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604-jh28s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:48.257: INFO: stderr: ""
Apr 11 17:33:48.257: INFO: stdout: "true"
Apr 11 17:33:48.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604-jh28s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:48.368: INFO: stderr: ""
Apr 11 17:33:48.368: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr 11 17:33:48.368: INFO: e2e-test-nginx-rc-10d00f98f9fdc9b0581b3cee915a4604-jh28s is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Apr 11 17:33:48.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-p2djj'
Apr 11 17:33:48.511: INFO: stderr: ""
Apr 11 17:33:48.511: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:33:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p2djj" for this suite.
Apr 11 17:33:54.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:33:54.630: INFO: namespace: e2e-tests-kubectl-p2djj, resource: bindings, ignored listing per whitelist
Apr 11 17:33:54.640: INFO: namespace e2e-tests-kubectl-p2djj deletion completed in 6.124102742s

â€¢ [SLOW TEST:22.956 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:33:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-lt74g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-lt74g
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 11 17:33:54.841: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 11 17:34:18.982: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.87.68:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lt74g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:34:18.983: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:34:19.092: INFO: Found all expected endpoints: [netserver-0]
Apr 11 17:34:19.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.85.24:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lt74g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:34:19.097: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:34:19.209: INFO: Found all expected endpoints: [netserver-1]
Apr 11 17:34:19.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.86.72:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-lt74g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:34:19.214: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:34:19.330: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:34:19.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-lt74g" for this suite.
Apr 11 17:34:41.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:34:41.402: INFO: namespace: e2e-tests-pod-network-test-lt74g, resource: bindings, ignored listing per whitelist
Apr 11 17:34:41.474: INFO: namespace e2e-tests-pod-network-test-lt74g deletion completed in 22.137585105s

â€¢ [SLOW TEST:46.832 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:34:41.476: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fwmmm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Apr 11 17:34:41.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:41.893: INFO: stderr: ""
Apr 11 17:34:41.893: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 17:34:41.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:41.997: INFO: stderr: ""
Apr 11 17:34:41.997: INFO: stdout: "update-demo-nautilus-bjsxx update-demo-nautilus-fccdd "
Apr 11 17:34:41.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-bjsxx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:42.104: INFO: stderr: ""
Apr 11 17:34:42.104: INFO: stdout: ""
Apr 11 17:34:42.105: INFO: update-demo-nautilus-bjsxx is created but not running
Apr 11 17:34:47.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:47.228: INFO: stderr: ""
Apr 11 17:34:47.228: INFO: stdout: "update-demo-nautilus-bjsxx update-demo-nautilus-fccdd "
Apr 11 17:34:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-bjsxx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:47.348: INFO: stderr: ""
Apr 11 17:34:47.348: INFO: stdout: "true"
Apr 11 17:34:47.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-bjsxx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:47.479: INFO: stderr: ""
Apr 11 17:34:47.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:34:47.479: INFO: validating pod update-demo-nautilus-bjsxx
Apr 11 17:34:47.487: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:34:47.487: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:34:47.487: INFO: update-demo-nautilus-bjsxx is verified up and running
Apr 11 17:34:47.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:47.595: INFO: stderr: ""
Apr 11 17:34:47.595: INFO: stdout: "true"
Apr 11 17:34:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:47.711: INFO: stderr: ""
Apr 11 17:34:47.711: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:34:47.711: INFO: validating pod update-demo-nautilus-fccdd
Apr 11 17:34:47.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:34:47.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:34:47.719: INFO: update-demo-nautilus-fccdd is verified up and running
STEP: scaling down the replication controller
Apr 11 17:34:47.721: INFO: scanned /root for discovery docs: <nil>
Apr 11 17:34:47.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:48.879: INFO: stderr: ""
Apr 11 17:34:48.879: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 17:34:48.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:48.995: INFO: stderr: ""
Apr 11 17:34:48.995: INFO: stdout: "update-demo-nautilus-bjsxx update-demo-nautilus-fccdd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 11 17:34:53.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:54.105: INFO: stderr: ""
Apr 11 17:34:54.105: INFO: stdout: "update-demo-nautilus-fccdd "
Apr 11 17:34:54.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:54.213: INFO: stderr: ""
Apr 11 17:34:54.213: INFO: stdout: "true"
Apr 11 17:34:54.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:54.317: INFO: stderr: ""
Apr 11 17:34:54.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:34:54.317: INFO: validating pod update-demo-nautilus-fccdd
Apr 11 17:34:54.323: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:34:54.323: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:34:54.323: INFO: update-demo-nautilus-fccdd is verified up and running
STEP: scaling up the replication controller
Apr 11 17:34:54.324: INFO: scanned /root for discovery docs: <nil>
Apr 11 17:34:54.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:55.487: INFO: stderr: ""
Apr 11 17:34:55.487: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 17:34:55.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:55.602: INFO: stderr: ""
Apr 11 17:34:55.602: INFO: stdout: "update-demo-nautilus-fccdd update-demo-nautilus-gh55w "
Apr 11 17:34:55.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:55.718: INFO: stderr: ""
Apr 11 17:34:55.718: INFO: stdout: "true"
Apr 11 17:34:55.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-fccdd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:55.822: INFO: stderr: ""
Apr 11 17:34:55.822: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:34:55.822: INFO: validating pod update-demo-nautilus-fccdd
Apr 11 17:34:55.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:34:55.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:34:55.828: INFO: update-demo-nautilus-fccdd is verified up and running
Apr 11 17:34:55.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-gh55w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:55.935: INFO: stderr: ""
Apr 11 17:34:55.935: INFO: stdout: "true"
Apr 11 17:34:55.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-gh55w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:56.046: INFO: stderr: ""
Apr 11 17:34:56.046: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:34:56.046: INFO: validating pod update-demo-nautilus-gh55w
Apr 11 17:34:56.053: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:34:56.053: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:34:56.053: INFO: update-demo-nautilus-gh55w is verified up and running
STEP: using delete to clean up resources
Apr 11 17:34:56.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:56.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:34:56.163: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 11 17:34:56.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-fwmmm'
Apr 11 17:34:56.281: INFO: stderr: "No resources found.\n"
Apr 11 17:34:56.281: INFO: stdout: ""
Apr 11 17:34:56.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -l name=update-demo --namespace=e2e-tests-kubectl-fwmmm -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 11 17:34:56.407: INFO: stderr: ""
Apr 11 17:34:56.407: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:34:56.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fwmmm" for this suite.
Apr 11 17:35:02.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:35:02.495: INFO: namespace: e2e-tests-kubectl-fwmmm, resource: bindings, ignored listing per whitelist
Apr 11 17:35:02.535: INFO: namespace e2e-tests-kubectl-fwmmm deletion completed in 6.123088174s

â€¢ [SLOW TEST:21.059 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:35:02.539: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-bvcwt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 11 17:35:08.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:08.823: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:10.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:10.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:12.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:12.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:14.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:14.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:16.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:16.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:18.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:18.836: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:20.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:20.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:22.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:22.829: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:24.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:24.830: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:26.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:26.830: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 11 17:35:28.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 11 17:35:28.834: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:35:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bvcwt" for this suite.
Apr 11 17:35:50.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:35:50.915: INFO: namespace: e2e-tests-container-lifecycle-hook-bvcwt, resource: bindings, ignored listing per whitelist
Apr 11 17:35:50.999: INFO: namespace e2e-tests-container-lifecycle-hook-bvcwt deletion completed in 22.138064795s

â€¢ [SLOW TEST:48.461 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:35:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d474p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Apr 11 17:35:51.199: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-785594743 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:35:51.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d474p" for this suite.
Apr 11 17:35:57.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:35:57.316: INFO: namespace: e2e-tests-kubectl-d474p, resource: bindings, ignored listing per whitelist
Apr 11 17:35:57.414: INFO: namespace e2e-tests-kubectl-d474p deletion completed in 6.12524738s

â€¢ [SLOW TEST:6.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:35:57.417: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cmhkt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:35:57.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-cmhkt" to be "success or failure"
Apr 11 17:35:57.628: INFO: Pod "downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815987ms
Apr 11 17:35:59.634: INFO: Pod "downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01032586s
Apr 11 17:36:01.645: INFO: Pod "downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021699956s
STEP: Saw pod success
Apr 11 17:36:01.645: INFO: Pod "downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:36:01.649: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:36:01.678: INFO: Waiting for pod downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:36:01.683: INFO: Pod downwardapi-volume-43e46313-5c80-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:36:01.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cmhkt" for this suite.
Apr 11 17:36:07.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:36:07.817: INFO: namespace: e2e-tests-downward-api-cmhkt, resource: bindings, ignored listing per whitelist
Apr 11 17:36:07.823: INFO: namespace e2e-tests-downward-api-cmhkt deletion completed in 6.135094167s

â€¢ [SLOW TEST:10.406 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:36:07.823: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vn6gd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:36:08.044: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-vn6gd" to be "success or failure"
Apr 11 17:36:08.049: INFO: Pod "downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.855032ms
Apr 11 17:36:10.054: INFO: Pod "downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010338935s
STEP: Saw pod success
Apr 11 17:36:10.055: INFO: Pod "downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:36:10.059: INFO: Trying to get logs from node cluster1-k8s-node-1 pod downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:36:10.090: INFO: Waiting for pod downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:36:10.094: INFO: Pod downwardapi-volume-4a1a8e6c-5c80-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:36:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vn6gd" for this suite.
Apr 11 17:36:16.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:36:16.148: INFO: namespace: e2e-tests-projected-vn6gd, resource: bindings, ignored listing per whitelist
Apr 11 17:36:16.243: INFO: namespace e2e-tests-projected-vn6gd deletion completed in 6.143828097s

â€¢ [SLOW TEST:8.420 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:36:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vxb69
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:36:16.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 version --client'
Apr 11 17:36:16.525: INFO: stderr: ""
Apr 11 17:36:16.525: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr 11 17:36:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-vxb69'
Apr 11 17:36:16.743: INFO: stderr: ""
Apr 11 17:36:16.743: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 11 17:36:16.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-vxb69'
Apr 11 17:36:16.963: INFO: stderr: ""
Apr 11 17:36:16.963: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 11 17:36:17.970: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:36:17.971: INFO: Found 1 / 1
Apr 11 17:36:17.971: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 11 17:36:17.978: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:36:17.978: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 11 17:36:17.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 describe pod redis-master-z7j2w --namespace=e2e-tests-kubectl-vxb69'
Apr 11 17:36:18.154: INFO: stderr: ""
Apr 11 17:36:18.154: INFO: stdout: "Name:               redis-master-z7j2w\nNamespace:          e2e-tests-kubectl-vxb69\nPriority:           0\nPriorityClassName:  <none>\nNode:               cluster1-k8s-node-3/10.128.0.3\nStart Time:         Thu, 11 Apr 2019 17:36:16 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 10.233.87.72\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f1d28f309930d8e4d1f6133018932bc40dc1202b6b08d367b7623b4efffbea34\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 11 Apr 2019 17:36:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ctpq7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ctpq7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ctpq7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                          Message\n  ----    ------     ----  ----                          -------\n  Normal  Scheduled  2s    default-scheduler             Successfully assigned e2e-tests-kubectl-vxb69/redis-master-z7j2w to cluster1-k8s-node-3\n  Normal  Pulled     1s    kubelet, cluster1-k8s-node-3  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, cluster1-k8s-node-3  Created container\n  Normal  Started    1s    kubelet, cluster1-k8s-node-3  Started container\n"
Apr 11 17:36:18.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 describe rc redis-master --namespace=e2e-tests-kubectl-vxb69'
Apr 11 17:36:18.335: INFO: stderr: ""
Apr 11 17:36:18.335: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-vxb69\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-z7j2w\n"
Apr 11 17:36:18.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 describe service redis-master --namespace=e2e-tests-kubectl-vxb69'
Apr 11 17:36:18.453: INFO: stderr: ""
Apr 11 17:36:18.453: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-vxb69\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.47.193\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.87.72:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 11 17:36:18.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 describe node cluster1-k8s-master-1'
Apr 11 17:36:18.611: INFO: stderr: ""
Apr 11 17:36:18.611: INFO: stdout: "Name:               cluster1-k8s-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=28717ab2-8746-4a77-969d-04eecb61afcf\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=se-sto\n                    failure-domain.beta.kubernetes.io/zone=sto1\n                    kubernetes.io/hostname=cluster1-k8s-master-1\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 11 Apr 2019 16:05:44 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 11 Apr 2019 17:36:13 +0000   Thu, 11 Apr 2019 16:05:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 11 Apr 2019 17:36:13 +0000   Thu, 11 Apr 2019 16:05:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 11 Apr 2019 17:36:13 +0000   Thu, 11 Apr 2019 16:05:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 11 Apr 2019 17:36:13 +0000   Thu, 11 Apr 2019 16:07:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.0.5\n  ExternalIP:  212.237.149.2\n  Hostname:    cluster1-k8s-master-1\nCapacity:\n cpu:                1\n ephemeral-storage:  40470732Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4039660Ki\n pods:               110\nAllocatable:\n cpu:                800m\n ephemeral-storage:  37297826550\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3437260Ki\n pods:               110\nSystem Info:\n Machine ID:                 552660347cc54fe9afeeb52cfb05eb00\n System UUID:                55266034-7CC5-4FE9-AFEE-B52CFB05EB00\n Boot ID:                    d31a908b-56e7-4bd0-957c-16a88a6b3cc0\n Kernel Version:             4.15.0-34-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.2\n Kubelet Version:            v1.13.5\n Kube-Proxy Version:         v1.13.5\nPodCIDR:                     10.233.65.0/24\nProviderID:                  openstack:///55266034-7cc5-4fe9-afee-b52cfb05eb00\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-ghc2m    0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                calico-node-n9hck                                          150m (18%)    300m (37%)  64M (1%)         500M (14%)     88m\n  kube-system                kube-apiserver-cluster1-k8s-master-1                       250m (31%)    0 (0%)      0 (0%)           0 (0%)         90m\n  kube-system                kube-controller-manager-cluster1-k8s-master-1              200m (25%)    0 (0%)      0 (0%)           0 (0%)         90m\n  kube-system                kube-proxy-nvdrc                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                kube-scheduler-cluster1-k8s-master-1                       100m (12%)    0 (0%)      0 (0%)           0 (0%)         90m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                700m (87%)  300m (37%)\n  memory             64M (1%)    500M (14%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Apr 11 17:36:18.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 describe namespace e2e-tests-kubectl-vxb69'
Apr 11 17:36:18.734: INFO: stderr: ""
Apr 11 17:36:18.735: INFO: stdout: "Name:         e2e-tests-kubectl-vxb69\nLabels:       e2e-framework=kubectl\n              e2e-run=bc50fad8-5c78-11e9-bcf1-d217e8a0130c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:36:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vxb69" for this suite.
Apr 11 17:36:40.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:36:40.770: INFO: namespace: e2e-tests-kubectl-vxb69, resource: bindings, ignored listing per whitelist
Apr 11 17:36:40.870: INFO: namespace e2e-tests-kubectl-vxb69 deletion completed in 22.130161052s

â€¢ [SLOW TEST:24.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:36:40.873: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-kk9rn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:36:41.078: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:36:43.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kk9rn" for this suite.
Apr 11 17:37:21.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:37:21.238: INFO: namespace: e2e-tests-pods-kk9rn, resource: bindings, ignored listing per whitelist
Apr 11 17:37:21.266: INFO: namespace e2e-tests-pods-kk9rn deletion completed in 38.136021156s

â€¢ [SLOW TEST:40.393 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:37:21.270: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zlxrg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Apr 11 17:37:21.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:21.708: INFO: stderr: ""
Apr 11 17:37:21.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 11 17:37:21.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:21.822: INFO: stderr: ""
Apr 11 17:37:21.822: INFO: stdout: "update-demo-nautilus-6kd6z update-demo-nautilus-jpg5m "
Apr 11 17:37:21.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-6kd6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:21.928: INFO: stderr: ""
Apr 11 17:37:21.928: INFO: stdout: ""
Apr 11 17:37:21.928: INFO: update-demo-nautilus-6kd6z is created but not running
Apr 11 17:37:26.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.043: INFO: stderr: ""
Apr 11 17:37:27.043: INFO: stdout: "update-demo-nautilus-6kd6z update-demo-nautilus-jpg5m "
Apr 11 17:37:27.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-6kd6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.141: INFO: stderr: ""
Apr 11 17:37:27.141: INFO: stdout: "true"
Apr 11 17:37:27.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-6kd6z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.245: INFO: stderr: ""
Apr 11 17:37:27.245: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:37:27.245: INFO: validating pod update-demo-nautilus-6kd6z
Apr 11 17:37:27.253: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:37:27.253: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:37:27.253: INFO: update-demo-nautilus-6kd6z is verified up and running
Apr 11 17:37:27.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-jpg5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.359: INFO: stderr: ""
Apr 11 17:37:27.359: INFO: stdout: "true"
Apr 11 17:37:27.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods update-demo-nautilus-jpg5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.463: INFO: stderr: ""
Apr 11 17:37:27.463: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 11 17:37:27.463: INFO: validating pod update-demo-nautilus-jpg5m
Apr 11 17:37:27.470: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 11 17:37:27.470: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 11 17:37:27.470: INFO: update-demo-nautilus-jpg5m is verified up and running
STEP: using delete to clean up resources
Apr 11 17:37:27.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.583: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:37:27.583: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 11 17:37:27.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-zlxrg'
Apr 11 17:37:27.719: INFO: stderr: "No resources found.\n"
Apr 11 17:37:27.719: INFO: stdout: ""
Apr 11 17:37:27.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -l name=update-demo --namespace=e2e-tests-kubectl-zlxrg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 11 17:37:27.842: INFO: stderr: ""
Apr 11 17:37:27.842: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:37:27.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zlxrg" for this suite.
Apr 11 17:37:49.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:37:49.906: INFO: namespace: e2e-tests-kubectl-zlxrg, resource: bindings, ignored listing per whitelist
Apr 11 17:37:49.993: INFO: namespace e2e-tests-kubectl-zlxrg deletion completed in 22.141647295s

â€¢ [SLOW TEST:28.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:37:50.005: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-dwd26
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0411 17:38:30.299383      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 17:38:30.299: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:38:30.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dwd26" for this suite.
Apr 11 17:38:38.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:38:38.391: INFO: namespace: e2e-tests-gc-dwd26, resource: bindings, ignored listing per whitelist
Apr 11 17:38:38.431: INFO: namespace e2e-tests-gc-dwd26 deletion completed in 8.127161548s

â€¢ [SLOW TEST:48.426 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:38:38.436: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-l5gzn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:38:38.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-l5gzn" to be "success or failure"
Apr 11 17:38:38.658: INFO: Pod "downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476607ms
Apr 11 17:38:40.670: INFO: Pod "downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016639431s
STEP: Saw pod success
Apr 11 17:38:40.670: INFO: Pod "downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:38:40.674: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:38:40.707: INFO: Waiting for pod downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:38:40.712: INFO: Pod downwardapi-volume-a3df9f1e-5c80-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:38:40.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l5gzn" for this suite.
Apr 11 17:38:46.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:38:46.760: INFO: namespace: e2e-tests-projected-l5gzn, resource: bindings, ignored listing per whitelist
Apr 11 17:38:46.837: INFO: namespace e2e-tests-projected-l5gzn deletion completed in 6.118959248s

â€¢ [SLOW TEST:8.401 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:38:46.839: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kmx4b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-kmx4b
Apr 11 17:38:53.051: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-kmx4b
STEP: checking the pod's current state and verifying that restartCount is present
Apr 11 17:38:53.055: INFO: Initial restart count of pod liveness-http is 0
Apr 11 17:39:11.123: INFO: Restart count of pod e2e-tests-container-probe-kmx4b/liveness-http is now 1 (18.06732076s elapsed)
Apr 11 17:39:31.197: INFO: Restart count of pod e2e-tests-container-probe-kmx4b/liveness-http is now 2 (38.14147445s elapsed)
Apr 11 17:39:51.275: INFO: Restart count of pod e2e-tests-container-probe-kmx4b/liveness-http is now 3 (58.219894421s elapsed)
Apr 11 17:40:11.348: INFO: Restart count of pod e2e-tests-container-probe-kmx4b/liveness-http is now 4 (1m18.292795749s elapsed)
Apr 11 17:41:21.599: INFO: Restart count of pod e2e-tests-container-probe-kmx4b/liveness-http is now 5 (2m28.543438126s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:41:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kmx4b" for this suite.
Apr 11 17:41:27.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:41:27.649: INFO: namespace: e2e-tests-container-probe-kmx4b, resource: bindings, ignored listing per whitelist
Apr 11 17:41:27.756: INFO: namespace e2e-tests-container-probe-kmx4b deletion completed in 6.132030009s

â€¢ [SLOW TEST:160.918 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:41:27.758: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2dtc4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-08cc3c2c-5c81-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:41:27.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-2dtc4" to be "success or failure"
Apr 11 17:41:27.987: INFO: Pod "pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476503ms
Apr 11 17:41:29.993: INFO: Pod "pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010201945s
STEP: Saw pod success
Apr 11 17:41:29.993: INFO: Pod "pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:41:29.997: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:41:30.039: INFO: Waiting for pod pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:41:30.043: INFO: Pod pod-projected-configmaps-08cd5966-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:41:30.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2dtc4" for this suite.
Apr 11 17:41:36.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:41:36.133: INFO: namespace: e2e-tests-projected-2dtc4, resource: bindings, ignored listing per whitelist
Apr 11 17:41:36.175: INFO: namespace e2e-tests-projected-2dtc4 deletion completed in 6.1260606s

â€¢ [SLOW TEST:8.417 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:41:36.180: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dhsdn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:41:36.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-dhsdn'
Apr 11 17:41:36.494: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 11 17:41:36.494: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Apr 11 17:41:38.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-dhsdn'
Apr 11 17:41:38.696: INFO: stderr: ""
Apr 11 17:41:38.696: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:41:38.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dhsdn" for this suite.
Apr 11 17:41:44.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:41:44.766: INFO: namespace: e2e-tests-kubectl-dhsdn, resource: bindings, ignored listing per whitelist
Apr 11 17:41:44.856: INFO: namespace e2e-tests-kubectl-dhsdn deletion completed in 6.155366471s

â€¢ [SLOW TEST:8.676 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:41:44.859: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kgp6s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 11 17:41:45.091: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kgp6s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kgp6s/configmaps/e2e-watch-test-watch-closed,UID:12fed68a-5c81-11e9-8000-fa163e4b6765,ResourceVersion:19857,Generation:0,CreationTimestamp:2019-04-11 17:41:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 11 17:41:45.091: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kgp6s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kgp6s/configmaps/e2e-watch-test-watch-closed,UID:12fed68a-5c81-11e9-8000-fa163e4b6765,ResourceVersion:19858,Generation:0,CreationTimestamp:2019-04-11 17:41:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 11 17:41:45.121: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kgp6s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kgp6s/configmaps/e2e-watch-test-watch-closed,UID:12fed68a-5c81-11e9-8000-fa163e4b6765,ResourceVersion:19859,Generation:0,CreationTimestamp:2019-04-11 17:41:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 11 17:41:45.121: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kgp6s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kgp6s/configmaps/e2e-watch-test-watch-closed,UID:12fed68a-5c81-11e9-8000-fa163e4b6765,ResourceVersion:19860,Generation:0,CreationTimestamp:2019-04-11 17:41:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:41:45.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kgp6s" for this suite.
Apr 11 17:41:51.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:41:51.225: INFO: namespace: e2e-tests-watch-kgp6s, resource: bindings, ignored listing per whitelist
Apr 11 17:41:51.265: INFO: namespace e2e-tests-watch-kgp6s deletion completed in 6.137537678s

â€¢ [SLOW TEST:6.406 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:41:51.268: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-2dj5c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-rj6r
STEP: Creating a pod to test atomic-volume-subpath
Apr 11 17:41:51.485: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rj6r" in namespace "e2e-tests-subpath-2dj5c" to be "success or failure"
Apr 11 17:41:51.489: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102328ms
Apr 11 17:41:53.503: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017479739s
Apr 11 17:41:55.509: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 4.023621739s
Apr 11 17:41:57.515: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 6.029900565s
Apr 11 17:41:59.521: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 8.036098994s
Apr 11 17:42:01.527: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 10.041848108s
Apr 11 17:42:03.539: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 12.053945042s
Apr 11 17:42:05.545: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 14.059724967s
Apr 11 17:42:07.551: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 16.065799165s
Apr 11 17:42:09.557: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 18.071628961s
Apr 11 17:42:11.565: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 20.07940514s
Apr 11 17:42:13.597: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Running", Reason="", readiness=false. Elapsed: 22.111823781s
Apr 11 17:42:15.603: INFO: Pod "pod-subpath-test-downwardapi-rj6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117320023s
STEP: Saw pod success
Apr 11 17:42:15.603: INFO: Pod "pod-subpath-test-downwardapi-rj6r" satisfied condition "success or failure"
Apr 11 17:42:15.608: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-subpath-test-downwardapi-rj6r container test-container-subpath-downwardapi-rj6r: <nil>
STEP: delete the pod
Apr 11 17:42:15.661: INFO: Waiting for pod pod-subpath-test-downwardapi-rj6r to disappear
Apr 11 17:42:15.666: INFO: Pod pod-subpath-test-downwardapi-rj6r no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rj6r
Apr 11 17:42:15.666: INFO: Deleting pod "pod-subpath-test-downwardapi-rj6r" in namespace "e2e-tests-subpath-2dj5c"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:42:15.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-2dj5c" for this suite.
Apr 11 17:42:21.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:42:21.783: INFO: namespace: e2e-tests-subpath-2dj5c, resource: bindings, ignored listing per whitelist
Apr 11 17:42:21.799: INFO: namespace e2e-tests-subpath-2dj5c deletion completed in 6.123103512s

â€¢ [SLOW TEST:30.531 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:42:21.800: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-52zcn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:42:22.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-52zcn" to be "success or failure"
Apr 11 17:42:22.023: INFO: Pod "downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.495544ms
Apr 11 17:42:24.036: INFO: Pod "downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017366878s
STEP: Saw pod success
Apr 11 17:42:24.036: INFO: Pod "downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:42:24.041: INFO: Trying to get logs from node cluster1-k8s-node-1 pod downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:42:24.075: INFO: Waiting for pod downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:42:24.079: INFO: Pod downwardapi-volume-290282f5-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:42:24.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-52zcn" for this suite.
Apr 11 17:42:30.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:42:30.137: INFO: namespace: e2e-tests-downward-api-52zcn, resource: bindings, ignored listing per whitelist
Apr 11 17:42:30.207: INFO: namespace e2e-tests-downward-api-52zcn deletion completed in 6.123270263s

â€¢ [SLOW TEST:8.408 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:42:30.209: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qwfbr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:42:30.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qwfbr" for this suite.
Apr 11 17:42:52.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:42:52.493: INFO: namespace: e2e-tests-pods-qwfbr, resource: bindings, ignored listing per whitelist
Apr 11 17:42:52.586: INFO: namespace e2e-tests-pods-qwfbr deletion completed in 22.136946396s

â€¢ [SLOW TEST:22.378 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:42:52.589: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-l788b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 11 17:42:56.867: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:56.867: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:56.973: INFO: Exec stderr: ""
Apr 11 17:42:56.973: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:56.974: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.081: INFO: Exec stderr: ""
Apr 11 17:42:57.081: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.081: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.205: INFO: Exec stderr: ""
Apr 11 17:42:57.206: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.206: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.326: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 11 17:42:57.326: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.327: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.448: INFO: Exec stderr: ""
Apr 11 17:42:57.449: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.449: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.551: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 11 17:42:57.552: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.552: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.684: INFO: Exec stderr: ""
Apr 11 17:42:57.684: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.847: INFO: Exec stderr: ""
Apr 11 17:42:57.847: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.847: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:57.995: INFO: Exec stderr: ""
Apr 11 17:42:57.995: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-l788b PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:42:57.995: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:42:58.105: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:42:58.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-l788b" for this suite.
Apr 11 17:43:44.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:43:44.153: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-l788b, resource: bindings, ignored listing per whitelist
Apr 11 17:43:44.241: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-l788b deletion completed in 46.128798668s

â€¢ [SLOW TEST:51.652 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:43:44.243: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xwdnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:43:44.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-xwdnq" to be "success or failure"
Apr 11 17:43:44.469: INFO: Pod "downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.435227ms
Apr 11 17:43:46.474: INFO: Pod "downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009983583s
STEP: Saw pod success
Apr 11 17:43:46.475: INFO: Pod "downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:43:46.480: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:43:46.510: INFO: Waiting for pod downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:43:46.515: INFO: Pod downwardapi-volume-5a26a511-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:43:46.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xwdnq" for this suite.
Apr 11 17:43:52.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:43:52.614: INFO: namespace: e2e-tests-projected-xwdnq, resource: bindings, ignored listing per whitelist
Apr 11 17:43:52.656: INFO: namespace e2e-tests-projected-xwdnq deletion completed in 6.135825876s

â€¢ [SLOW TEST:8.413 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:43:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4vh5r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Apr 11 17:43:54.896: INFO: Pod pod-hostip-5f29566e-5c81-11e9-bcf1-d217e8a0130c has hostIP: 10.128.0.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:43:54.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4vh5r" for this suite.
Apr 11 17:44:16.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:44:17.028: INFO: namespace: e2e-tests-pods-4vh5r, resource: bindings, ignored listing per whitelist
Apr 11 17:44:17.032: INFO: namespace e2e-tests-pods-4vh5r deletion completed in 22.131959422s

â€¢ [SLOW TEST:24.374 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:44:17.036: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vw4bt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:44:17.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-vw4bt'
Apr 11 17:44:17.474: INFO: stderr: ""
Apr 11 17:44:17.474: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr 11 17:44:22.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-vw4bt -o json'
Apr 11 17:44:22.641: INFO: stderr: ""
Apr 11 17:44:22.641: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-04-11T17:44:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-vw4bt\",\n        \"resourceVersion\": \"20552\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-vw4bt/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6dd1f69e-5c81-11e9-a332-fa163ef80534\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-r74j6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cluster1-k8s-node-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-r74j6\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-r74j6\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-11T17:44:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-11T17:44:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-11T17:44:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-11T17:44:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://170b55fd8194cda919fe9e741acdd43985acd8b88e6dfa2553058935da604496\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-11T17:44:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.10\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.86.86\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-11T17:44:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 11 17:44:22.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 replace -f - --namespace=e2e-tests-kubectl-vw4bt'
Apr 11 17:44:22.873: INFO: stderr: ""
Apr 11 17:44:22.873: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Apr 11 17:44:22.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-vw4bt'
Apr 11 17:44:33.700: INFO: stderr: ""
Apr 11 17:44:33.700: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:44:33.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vw4bt" for this suite.
Apr 11 17:44:39.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:44:39.811: INFO: namespace: e2e-tests-kubectl-vw4bt, resource: bindings, ignored listing per whitelist
Apr 11 17:44:39.834: INFO: namespace e2e-tests-kubectl-vw4bt deletion completed in 6.120577223s

â€¢ [SLOW TEST:22.798 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:44:39.837: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6xz4n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:44:42.092: INFO: Waiting up to 5m0s for pod "client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-pods-6xz4n" to be "success or failure"
Apr 11 17:44:42.097: INFO: Pod "client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321892ms
Apr 11 17:44:44.110: INFO: Pod "client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017794636s
STEP: Saw pod success
Apr 11 17:44:44.110: INFO: Pod "client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:44:44.118: INFO: Trying to get logs from node cluster1-k8s-node-1 pod client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c container env3cont: <nil>
STEP: delete the pod
Apr 11 17:44:44.150: INFO: Waiting for pod client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:44:44.158: INFO: Pod client-envvars-7c8052d1-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:44:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6xz4n" for this suite.
Apr 11 17:45:22.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:45:22.261: INFO: namespace: e2e-tests-pods-6xz4n, resource: bindings, ignored listing per whitelist
Apr 11 17:45:22.291: INFO: namespace e2e-tests-pods-6xz4n deletion completed in 38.128324234s

â€¢ [SLOW TEST:42.455 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:45:22.294: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-c9ncf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Apr 11 17:45:22.496: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:45:25.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-c9ncf" for this suite.
Apr 11 17:45:31.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:45:31.858: INFO: namespace: e2e-tests-init-container-c9ncf, resource: bindings, ignored listing per whitelist
Apr 11 17:45:31.932: INFO: namespace e2e-tests-init-container-c9ncf deletion completed in 6.124206383s

â€¢ [SLOW TEST:9.639 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:45:31.934: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5rjht
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-9a5603f5-5c81-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:45:32.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-5rjht" to be "success or failure"
Apr 11 17:45:32.165: INFO: Pod "pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.407465ms
Apr 11 17:45:34.171: INFO: Pod "pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012871829s
Apr 11 17:45:36.176: INFO: Pod "pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018377007s
STEP: Saw pod success
Apr 11 17:45:36.176: INFO: Pod "pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:45:36.180: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:45:36.212: INFO: Waiting for pod pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:45:36.216: INFO: Pod pod-configmaps-9a572425-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:45:36.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5rjht" for this suite.
Apr 11 17:45:42.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:45:42.271: INFO: namespace: e2e-tests-configmap-5rjht, resource: bindings, ignored listing per whitelist
Apr 11 17:45:42.352: INFO: namespace e2e-tests-configmap-5rjht deletion completed in 6.131024801s

â€¢ [SLOW TEST:10.419 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:45:42.355: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-jdpjn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:45:42.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-jdpjn" to be "success or failure"
Apr 11 17:45:42.573: INFO: Pod "downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310028ms
Apr 11 17:45:44.579: INFO: Pod "downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011128765s
STEP: Saw pod success
Apr 11 17:45:44.579: INFO: Pod "downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:45:44.583: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:45:44.615: INFO: Waiting for pod downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:45:44.619: INFO: Pod downwardapi-volume-a08bb72e-5c81-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:45:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jdpjn" for this suite.
Apr 11 17:45:50.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:45:50.676: INFO: namespace: e2e-tests-downward-api-jdpjn, resource: bindings, ignored listing per whitelist
Apr 11 17:45:50.751: INFO: namespace e2e-tests-downward-api-jdpjn deletion completed in 6.126115599s

â€¢ [SLOW TEST:8.397 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:45:50.754: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-spghm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 17:45:51.023: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 11 17:45:51.034: INFO: Number of nodes with available pods: 0
Apr 11 17:45:51.034: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 11 17:45:51.056: INFO: Number of nodes with available pods: 0
Apr 11 17:45:51.056: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:52.062: INFO: Number of nodes with available pods: 0
Apr 11 17:45:52.062: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:53.062: INFO: Number of nodes with available pods: 1
Apr 11 17:45:53.062: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 11 17:45:53.081: INFO: Number of nodes with available pods: 1
Apr 11 17:45:53.081: INFO: Number of running nodes: 0, number of available pods: 1
Apr 11 17:45:54.086: INFO: Number of nodes with available pods: 0
Apr 11 17:45:54.086: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 11 17:45:54.097: INFO: Number of nodes with available pods: 0
Apr 11 17:45:54.097: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:55.103: INFO: Number of nodes with available pods: 0
Apr 11 17:45:55.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:56.102: INFO: Number of nodes with available pods: 0
Apr 11 17:45:56.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:57.102: INFO: Number of nodes with available pods: 0
Apr 11 17:45:57.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:58.103: INFO: Number of nodes with available pods: 0
Apr 11 17:45:58.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:45:59.102: INFO: Number of nodes with available pods: 0
Apr 11 17:45:59.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:00.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:00.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:01.108: INFO: Number of nodes with available pods: 0
Apr 11 17:46:01.109: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:02.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:02.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:03.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:03.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:04.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:04.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:05.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:05.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:06.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:06.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:07.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:07.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:08.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:08.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:09.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:09.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:10.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:10.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:11.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:11.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:12.110: INFO: Number of nodes with available pods: 0
Apr 11 17:46:12.110: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:13.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:13.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:14.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:14.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:15.104: INFO: Number of nodes with available pods: 0
Apr 11 17:46:15.104: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:16.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:16.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:17.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:17.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:18.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:18.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:19.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:19.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:20.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:20.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:21.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:21.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:22.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:22.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:23.110: INFO: Number of nodes with available pods: 0
Apr 11 17:46:23.110: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:24.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:24.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:25.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:25.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:26.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:26.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:27.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:27.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:28.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:28.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:29.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:29.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:30.103: INFO: Number of nodes with available pods: 0
Apr 11 17:46:30.103: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:31.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:31.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:32.104: INFO: Number of nodes with available pods: 0
Apr 11 17:46:32.104: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:33.102: INFO: Number of nodes with available pods: 0
Apr 11 17:46:33.102: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:34.111: INFO: Number of nodes with available pods: 0
Apr 11 17:46:34.111: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 17:46:35.102: INFO: Number of nodes with available pods: 1
Apr 11 17:46:35.102: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-spghm, will wait for the garbage collector to delete the pods
Apr 11 17:46:35.172: INFO: Deleting DaemonSet.extensions daemon-set took: 9.898989ms
Apr 11 17:46:35.272: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.23492ms
Apr 11 17:47:13.785: INFO: Number of nodes with available pods: 0
Apr 11 17:47:13.785: INFO: Number of running nodes: 0, number of available pods: 0
Apr 11 17:47:13.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-spghm/daemonsets","resourceVersion":"21161"},"items":null}

Apr 11 17:47:13.793: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-spghm/pods","resourceVersion":"21161"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:47:13.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-spghm" for this suite.
Apr 11 17:47:19.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:47:19.877: INFO: namespace: e2e-tests-daemonsets-spghm, resource: bindings, ignored listing per whitelist
Apr 11 17:47:20.009: INFO: namespace e2e-tests-daemonsets-spghm deletion completed in 6.189956118s

â€¢ [SLOW TEST:89.256 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:47:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fxlx5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:47:20.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fxlx5'
Apr 11 17:47:20.506: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 11 17:47:20.506: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Apr 11 17:47:20.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-fxlx5'
Apr 11 17:47:20.624: INFO: stderr: ""
Apr 11 17:47:20.624: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:47:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fxlx5" for this suite.
Apr 11 17:47:26.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:47:26.679: INFO: namespace: e2e-tests-kubectl-fxlx5, resource: bindings, ignored listing per whitelist
Apr 11 17:47:26.752: INFO: namespace e2e-tests-kubectl-fxlx5 deletion completed in 6.12304403s

â€¢ [SLOW TEST:6.739 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:47:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-rk592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rk592
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 11 17:47:26.961: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 11 17:47:49.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.88:8080/dial?request=hostName&protocol=http&host=10.233.85.29&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rk592 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:47:49.102: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:47:49.233: INFO: Waiting for endpoints: map[]
Apr 11 17:47:49.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.88:8080/dial?request=hostName&protocol=http&host=10.233.87.87&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rk592 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:47:49.239: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:47:49.352: INFO: Waiting for endpoints: map[]
Apr 11 17:47:49.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.88:8080/dial?request=hostName&protocol=http&host=10.233.86.91&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-rk592 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:47:49.357: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:47:49.471: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:47:49.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rk592" for this suite.
Apr 11 17:48:11.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:48:11.553: INFO: namespace: e2e-tests-pod-network-test-rk592, resource: bindings, ignored listing per whitelist
Apr 11 17:48:11.598: INFO: namespace e2e-tests-pod-network-test-rk592 deletion completed in 22.120451217s

â€¢ [SLOW TEST:44.840 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:48:11.601: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-2hb87
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Apr 11 17:48:11.844: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-2hb87" to be "success or failure"
Apr 11 17:48:11.854: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365589ms
Apr 11 17:48:13.860: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015980064s
Apr 11 17:48:15.866: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021537018s
STEP: Saw pod success
Apr 11 17:48:15.866: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 11 17:48:15.870: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 11 17:48:15.912: INFO: Waiting for pod pod-host-path-test to disappear
Apr 11 17:48:15.918: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:48:15.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-2hb87" for this suite.
Apr 11 17:48:21.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:48:22.005: INFO: namespace: e2e-tests-hostpath-2hb87, resource: bindings, ignored listing per whitelist
Apr 11 17:48:22.050: INFO: namespace e2e-tests-hostpath-2hb87 deletion completed in 6.126756373s

â€¢ [SLOW TEST:10.449 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:48:22.051: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-f4rql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-gw97d
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Apr 11 17:48:24.444: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-pxd5j
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:48:48.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-f4rql" for this suite.
Apr 11 17:48:54.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:48:54.687: INFO: namespace: e2e-tests-namespaces-f4rql, resource: bindings, ignored listing per whitelist
Apr 11 17:48:54.768: INFO: namespace e2e-tests-namespaces-f4rql deletion completed in 6.121639035s
STEP: Destroying namespace "e2e-tests-nsdeletetest-gw97d" for this suite.
Apr 11 17:48:54.771: INFO: Namespace e2e-tests-nsdeletetest-gw97d was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-pxd5j" for this suite.
Apr 11 17:49:00.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:49:00.897: INFO: namespace: e2e-tests-nsdeletetest-pxd5j, resource: bindings, ignored listing per whitelist
Apr 11 17:49:00.900: INFO: namespace e2e-tests-nsdeletetest-pxd5j deletion completed in 6.128052617s

â€¢ [SLOW TEST:38.849 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:49:00.905: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-lxhxj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 11 17:49:05.192: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:05.196: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:07.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:07.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:09.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:09.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:11.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:11.209: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:13.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:13.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:15.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:15.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:17.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:17.204: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:19.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:19.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:21.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:21.202: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:23.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:23.209: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:25.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:25.207: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:27.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:27.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:29.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:29.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:31.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:31.203: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:33.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:33.202: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 11 17:49:35.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 11 17:49:35.209: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:49:35.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-lxhxj" for this suite.
Apr 11 17:49:57.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:49:57.288: INFO: namespace: e2e-tests-container-lifecycle-hook-lxhxj, resource: bindings, ignored listing per whitelist
Apr 11 17:49:57.369: INFO: namespace e2e-tests-container-lifecycle-hook-lxhxj deletion completed in 22.155300962s

â€¢ [SLOW TEST:56.464 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:49:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2zpn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Apr 11 17:49:57.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-2zpn9'
Apr 11 17:49:57.811: INFO: stderr: ""
Apr 11 17:49:57.811: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Apr 11 17:49:58.817: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:49:58.817: INFO: Found 0 / 1
Apr 11 17:49:59.817: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:49:59.817: INFO: Found 1 / 1
Apr 11 17:49:59.817: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 11 17:49:59.821: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:49:59.821: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr 11 17:49:59.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 logs redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9'
Apr 11 17:49:59.940: INFO: stderr: ""
Apr 11 17:49:59.941: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Apr 17:49:58.792 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Apr 17:49:58.792 # Server started, Redis version 3.2.12\n1:M 11 Apr 17:49:58.792 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Apr 17:49:58.792 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr 11 17:49:59.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 log redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9 --tail=1'
Apr 11 17:50:00.087: INFO: stderr: ""
Apr 11 17:50:00.087: INFO: stdout: "1:M 11 Apr 17:49:58.792 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr 11 17:50:00.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 log redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9 --limit-bytes=1'
Apr 11 17:50:00.212: INFO: stderr: ""
Apr 11 17:50:00.212: INFO: stdout: " "
STEP: exposing timestamps
Apr 11 17:50:00.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 log redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9 --tail=1 --timestamps'
Apr 11 17:50:00.473: INFO: stderr: ""
Apr 11 17:50:00.473: INFO: stdout: "2019-04-11T17:49:58.79312091Z 1:M 11 Apr 17:49:58.792 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr 11 17:50:02.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 log redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9 --since=1s'
Apr 11 17:50:03.107: INFO: stderr: ""
Apr 11 17:50:03.107: INFO: stdout: ""
Apr 11 17:50:03.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 log redis-master-xc79d redis-master --namespace=e2e-tests-kubectl-2zpn9 --since=24h'
Apr 11 17:50:03.229: INFO: stderr: ""
Apr 11 17:50:03.229: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Apr 17:49:58.792 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Apr 17:49:58.792 # Server started, Redis version 3.2.12\n1:M 11 Apr 17:49:58.792 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Apr 17:49:58.792 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Apr 11 17:50:03.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-2zpn9'
Apr 11 17:50:03.339: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 17:50:03.339: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr 11 17:50:03.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-2zpn9'
Apr 11 17:50:03.455: INFO: stderr: "No resources found.\n"
Apr 11 17:50:03.455: INFO: stdout: ""
Apr 11 17:50:03.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -l name=nginx --namespace=e2e-tests-kubectl-2zpn9 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 11 17:50:03.560: INFO: stderr: ""
Apr 11 17:50:03.560: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:50:03.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2zpn9" for this suite.
Apr 11 17:50:25.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:50:25.619: INFO: namespace: e2e-tests-kubectl-2zpn9, resource: bindings, ignored listing per whitelist
Apr 11 17:50:25.719: INFO: namespace e2e-tests-kubectl-2zpn9 deletion completed in 22.151096406s

â€¢ [SLOW TEST:28.348 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:50:25.724: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-hgfv5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 11 17:50:28.487: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c"
Apr 11 17:50:28.487: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-pods-hgfv5" to be "terminated due to deadline exceeded"
Apr 11 17:50:28.492: INFO: Pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c": Phase="Running", Reason="", readiness=true. Elapsed: 4.177643ms
Apr 11 17:50:30.504: INFO: Pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017031598s
Apr 11 17:50:32.510: INFO: Pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.022473965s
Apr 11 17:50:32.510: INFO: Pod "pod-update-activedeadlineseconds-4975938d-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:50:32.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hgfv5" for this suite.
Apr 11 17:50:38.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:50:38.538: INFO: namespace: e2e-tests-pods-hgfv5, resource: bindings, ignored listing per whitelist
Apr 11 17:50:38.648: INFO: namespace e2e-tests-pods-hgfv5 deletion completed in 6.13321625s

â€¢ [SLOW TEST:12.925 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:50:38.655: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-slqwb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 11 17:50:38.880: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 11 17:50:38.889: INFO: Waiting for terminating namespaces to be deleted...
Apr 11 17:50:38.892: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-1 before test
Apr 11 17:50:38.898: INFO: dns-autoscaler-586f58b8bf-s49kj from kube-system started at 2019-04-11 16:08:13 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.898: INFO: 	Container autoscaler ready: true, restart count 0
Apr 11 17:50:38.898: INFO: calico-node-9mntg from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.899: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:50:38.899: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-11 16:41:26 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.899: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 11 17:50:38.899: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zzsm9 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:50:38.899: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Apr 11 17:50:38.899: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 11 17:50:38.899: INFO: nginx-proxy-cluster1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:50:38.900: INFO: kube-proxy-ffnl8 from kube-system started at 2019-04-11 16:07:43 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.900: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:50:38.900: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-2 before test
Apr 11 17:50:38.908: INFO: nginx-proxy-cluster1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:50:38.909: INFO: kube-proxy-8kw6z from kube-system started at 2019-04-11 16:07:37 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.909: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:50:38.909: INFO: tiller-deploy-dc85f7fbd-zml6j from kube-system started at 2019-04-11 16:13:49 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.909: INFO: 	Container tiller ready: true, restart count 0
Apr 11 17:50:38.909: INFO: sonobuoy-e2e-job-56bc4b3c94a64376 from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:50:38.909: INFO: 	Container e2e ready: true, restart count 0
Apr 11 17:50:38.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 11 17:50:38.909: INFO: calico-node-ltdsr from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.910: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:50:38.910: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-cn2sq from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:50:38.910: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Apr 11 17:50:38.910: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 11 17:50:38.910: INFO: 
Logging pods the kubelet thinks is on node cluster1-k8s-node-3 before test
Apr 11 17:50:38.918: INFO: nginx-proxy-cluster1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Apr 11 17:50:38.918: INFO: calico-node-zzxxn from kube-system started at 2019-04-11 16:07:32 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.918: INFO: 	Container calico-node ready: true, restart count 0
Apr 11 17:50:38.919: INFO: calico-kube-controllers-67f4fffbd-b9hw9 from kube-system started at 2019-04-11 16:07:44 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.919: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 11 17:50:38.919: INFO: sonobuoy-systemd-logs-daemon-set-570dd31494414fdb-zpf9n from heptio-sonobuoy started at 2019-04-11 16:41:31 +0000 UTC (2 container statuses recorded)
Apr 11 17:50:38.919: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Apr 11 17:50:38.919: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 11 17:50:38.919: INFO: kube-proxy-zrsdq from kube-system started at 2019-04-11 16:07:54 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.919: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 11 17:50:38.920: INFO: kubernetes-dashboard-674d5cdc67-vpwkz from kube-system started at 2019-04-11 16:08:17 +0000 UTC (1 container statuses recorded)
Apr 11 17:50:38.920: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15947cd81a5b8988], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:50:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-slqwb" for this suite.
Apr 11 17:50:45.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:50:46.050: INFO: namespace: e2e-tests-sched-pred-slqwb, resource: bindings, ignored listing per whitelist
Apr 11 17:50:46.080: INFO: namespace e2e-tests-sched-pred-slqwb deletion completed in 6.11939222s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.426 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:50:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gv8ld
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Apr 11 17:50:46.310: INFO: Waiting up to 5m0s for pod "downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-gv8ld" to be "success or failure"
Apr 11 17:50:46.315: INFO: Pod "downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.674216ms
Apr 11 17:50:48.321: INFO: Pod "downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01059231s
Apr 11 17:50:50.326: INFO: Pod "downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016034431s
STEP: Saw pod success
Apr 11 17:50:50.326: INFO: Pod "downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:50:50.331: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 17:50:50.366: INFO: Waiting for pod downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:50:50.371: INFO: Pod downward-api-5597313c-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:50:50.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gv8ld" for this suite.
Apr 11 17:50:56.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:50:56.432: INFO: namespace: e2e-tests-downward-api-gv8ld, resource: bindings, ignored listing per whitelist
Apr 11 17:50:56.516: INFO: namespace e2e-tests-downward-api-gv8ld deletion completed in 6.14047302s

â€¢ [SLOW TEST:10.431 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:50:56.519: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gdjx7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:50:56.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-gdjx7'
Apr 11 17:50:56.844: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 11 17:50:56.844: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr 11 17:50:56.859: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-s5xzk]
Apr 11 17:50:56.859: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-s5xzk" in namespace "e2e-tests-kubectl-gdjx7" to be "running and ready"
Apr 11 17:50:56.868: INFO: Pod "e2e-test-nginx-rc-s5xzk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.765448ms
Apr 11 17:50:58.876: INFO: Pod "e2e-test-nginx-rc-s5xzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016589477s
Apr 11 17:50:58.876: INFO: Pod "e2e-test-nginx-rc-s5xzk" satisfied condition "running and ready"
Apr 11 17:50:58.876: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-s5xzk]
Apr 11 17:50:58.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gdjx7'
Apr 11 17:50:59.038: INFO: stderr: ""
Apr 11 17:50:59.038: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Apr 11 17:50:59.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gdjx7'
Apr 11 17:50:59.165: INFO: stderr: ""
Apr 11 17:50:59.165: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:50:59.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gdjx7" for this suite.
Apr 11 17:51:21.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:51:21.200: INFO: namespace: e2e-tests-kubectl-gdjx7, resource: bindings, ignored listing per whitelist
Apr 11 17:51:21.310: INFO: namespace e2e-tests-kubectl-gdjx7 deletion completed in 22.139319236s

â€¢ [SLOW TEST:24.792 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:51:21.314: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-g54pc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:51:21.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-g54pc" for this suite.
Apr 11 17:51:27.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:51:27.635: INFO: namespace: e2e-tests-services-g54pc, resource: bindings, ignored listing per whitelist
Apr 11 17:51:27.660: INFO: namespace e2e-tests-services-g54pc deletion completed in 6.134222658s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.346 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:51:27.660: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kx5kc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:51:27.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-kx5kc" to be "success or failure"
Apr 11 17:51:27.877: INFO: Pod "downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014749ms
Apr 11 17:51:29.883: INFO: Pod "downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010843166s
STEP: Saw pod success
Apr 11 17:51:29.883: INFO: Pod "downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:51:29.888: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:51:29.928: INFO: Waiting for pod downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:51:29.933: INFO: Pod downwardapi-volume-6e5cc89c-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:51:29.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kx5kc" for this suite.
Apr 11 17:51:35.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:51:36.039: INFO: namespace: e2e-tests-projected-kx5kc, resource: bindings, ignored listing per whitelist
Apr 11 17:51:36.079: INFO: namespace e2e-tests-projected-kx5kc deletion completed in 6.139250531s

â€¢ [SLOW TEST:8.419 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:51:36.083: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-qj2xc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:51:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-qj2xc" for this suite.
Apr 11 17:51:42.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:51:42.443: INFO: namespace: e2e-tests-kubelet-test-qj2xc, resource: bindings, ignored listing per whitelist
Apr 11 17:51:42.456: INFO: namespace e2e-tests-kubelet-test-qj2xc deletion completed in 6.126611962s

â€¢ [SLOW TEST:6.373 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:51:42.457: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-btgjr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-btgjr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-btgjr to expose endpoints map[]
Apr 11 17:51:42.682: INFO: Get endpoints failed (4.263999ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Apr 11 17:51:43.686: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-btgjr exposes endpoints map[] (1.008138566s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-btgjr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-btgjr to expose endpoints map[pod1:[80]]
Apr 11 17:51:46.756: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-btgjr exposes endpoints map[pod1:[80]] (3.043058133s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-btgjr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-btgjr to expose endpoints map[pod1:[80] pod2:[80]]
Apr 11 17:51:48.803: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-btgjr exposes endpoints map[pod1:[80] pod2:[80]] (2.038392173s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-btgjr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-btgjr to expose endpoints map[pod2:[80]]
Apr 11 17:51:48.831: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-btgjr exposes endpoints map[pod2:[80]] (12.951128ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-btgjr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-btgjr to expose endpoints map[]
Apr 11 17:51:49.852: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-btgjr exposes endpoints map[] (1.008460081s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:51:49.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-btgjr" for this suite.
Apr 11 17:52:11.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:52:11.944: INFO: namespace: e2e-tests-services-btgjr, resource: bindings, ignored listing per whitelist
Apr 11 17:52:12.026: INFO: namespace e2e-tests-services-btgjr deletion completed in 22.136647287s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:29.569 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:52:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-wzqh5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-wzqh5
I0411 17:52:12.246484      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-wzqh5, replica count: 1
I0411 17:52:13.296919      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0411 17:52:14.297137      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 11 17:52:14.410: INFO: Created: latency-svc-t4vxz
Apr 11 17:52:14.423: INFO: Got endpoints: latency-svc-t4vxz [26.196938ms]
Apr 11 17:52:14.438: INFO: Created: latency-svc-5p9sc
Apr 11 17:52:14.445: INFO: Created: latency-svc-7j7gt
Apr 11 17:52:14.450: INFO: Got endpoints: latency-svc-5p9sc [26.236705ms]
Apr 11 17:52:14.453: INFO: Created: latency-svc-77d8x
Apr 11 17:52:14.457: INFO: Got endpoints: latency-svc-7j7gt [33.076383ms]
Apr 11 17:52:14.461: INFO: Created: latency-svc-xwsjj
Apr 11 17:52:14.463: INFO: Got endpoints: latency-svc-77d8x [38.479042ms]
Apr 11 17:52:14.468: INFO: Created: latency-svc-vmrwb
Apr 11 17:52:14.469: INFO: Got endpoints: latency-svc-xwsjj [45.056587ms]
Apr 11 17:52:14.474: INFO: Got endpoints: latency-svc-vmrwb [49.969353ms]
Apr 11 17:52:14.480: INFO: Created: latency-svc-z88t6
Apr 11 17:52:14.484: INFO: Created: latency-svc-lt5xk
Apr 11 17:52:14.489: INFO: Got endpoints: latency-svc-lt5xk [64.181278ms]
Apr 11 17:52:14.489: INFO: Got endpoints: latency-svc-z88t6 [31.91061ms]
Apr 11 17:52:14.494: INFO: Created: latency-svc-pg77g
Apr 11 17:52:14.502: INFO: Got endpoints: latency-svc-pg77g [77.441053ms]
Apr 11 17:52:14.503: INFO: Created: latency-svc-7nlq8
Apr 11 17:52:14.510: INFO: Created: latency-svc-9mg2c
Apr 11 17:52:14.518: INFO: Created: latency-svc-v2t6x
Apr 11 17:52:14.519: INFO: Got endpoints: latency-svc-7nlq8 [94.736277ms]
Apr 11 17:52:14.522: INFO: Got endpoints: latency-svc-9mg2c [96.789221ms]
Apr 11 17:52:14.529: INFO: Got endpoints: latency-svc-v2t6x [101.783321ms]
Apr 11 17:52:14.530: INFO: Created: latency-svc-sv6m7
Apr 11 17:52:14.537: INFO: Created: latency-svc-ql454
Apr 11 17:52:14.540: INFO: Got endpoints: latency-svc-sv6m7 [113.261475ms]
Apr 11 17:52:14.546: INFO: Created: latency-svc-qd854
Apr 11 17:52:14.548: INFO: Got endpoints: latency-svc-ql454 [121.235842ms]
Apr 11 17:52:14.552: INFO: Created: latency-svc-hfdjn
Apr 11 17:52:14.556: INFO: Got endpoints: latency-svc-qd854 [129.143182ms]
Apr 11 17:52:14.561: INFO: Created: latency-svc-p9xsw
Apr 11 17:52:14.562: INFO: Got endpoints: latency-svc-hfdjn [135.016032ms]
Apr 11 17:52:14.570: INFO: Created: latency-svc-92zg2
Apr 11 17:52:14.571: INFO: Got endpoints: latency-svc-p9xsw [141.256929ms]
Apr 11 17:52:14.577: INFO: Created: latency-svc-mtdrf
Apr 11 17:52:14.578: INFO: Got endpoints: latency-svc-92zg2 [128.183528ms]
Apr 11 17:52:14.587: INFO: Got endpoints: latency-svc-mtdrf [122.263347ms]
Apr 11 17:52:14.587: INFO: Created: latency-svc-979df
Apr 11 17:52:14.596: INFO: Created: latency-svc-9htfz
Apr 11 17:52:14.599: INFO: Got endpoints: latency-svc-979df [129.820574ms]
Apr 11 17:52:14.602: INFO: Created: latency-svc-gdvkr
Apr 11 17:52:14.607: INFO: Got endpoints: latency-svc-9htfz [132.835022ms]
Apr 11 17:52:14.611: INFO: Got endpoints: latency-svc-gdvkr [122.235474ms]
Apr 11 17:52:14.611: INFO: Created: latency-svc-5z6jq
Apr 11 17:52:14.617: INFO: Created: latency-svc-fd654
Apr 11 17:52:14.622: INFO: Got endpoints: latency-svc-5z6jq [133.235643ms]
Apr 11 17:52:14.626: INFO: Got endpoints: latency-svc-fd654 [124.0666ms]
Apr 11 17:52:14.630: INFO: Created: latency-svc-nnvft
Apr 11 17:52:14.639: INFO: Created: latency-svc-tr5c7
Apr 11 17:52:14.641: INFO: Got endpoints: latency-svc-nnvft [121.640623ms]
Apr 11 17:52:14.643: INFO: Created: latency-svc-xzl5x
Apr 11 17:52:14.648: INFO: Got endpoints: latency-svc-tr5c7 [125.863885ms]
Apr 11 17:52:14.650: INFO: Created: latency-svc-vcllq
Apr 11 17:52:14.650: INFO: Got endpoints: latency-svc-xzl5x [121.003931ms]
Apr 11 17:52:14.659: INFO: Got endpoints: latency-svc-vcllq [118.777723ms]
Apr 11 17:52:14.662: INFO: Created: latency-svc-p7qq6
Apr 11 17:52:14.671: INFO: Got endpoints: latency-svc-p7qq6 [122.254119ms]
Apr 11 17:52:14.671: INFO: Created: latency-svc-xqj2k
Apr 11 17:52:14.681: INFO: Got endpoints: latency-svc-xqj2k [124.434857ms]
Apr 11 17:52:14.681: INFO: Created: latency-svc-qdbdk
Apr 11 17:52:14.682: INFO: Created: latency-svc-9mb2x
Apr 11 17:52:14.685: INFO: Got endpoints: latency-svc-qdbdk [122.066034ms]
Apr 11 17:52:14.692: INFO: Created: latency-svc-t65mb
Apr 11 17:52:14.693: INFO: Got endpoints: latency-svc-9mb2x [121.550155ms]
Apr 11 17:52:14.702: INFO: Got endpoints: latency-svc-t65mb [123.64341ms]
Apr 11 17:52:14.703: INFO: Created: latency-svc-nmf25
Apr 11 17:52:14.706: INFO: Created: latency-svc-vzhvm
Apr 11 17:52:14.713: INFO: Got endpoints: latency-svc-nmf25 [126.511682ms]
Apr 11 17:52:14.715: INFO: Created: latency-svc-jgntf
Apr 11 17:52:14.721: INFO: Got endpoints: latency-svc-vzhvm [121.896794ms]
Apr 11 17:52:14.725: INFO: Got endpoints: latency-svc-jgntf [117.227885ms]
Apr 11 17:52:14.725: INFO: Created: latency-svc-44hzk
Apr 11 17:52:14.732: INFO: Created: latency-svc-b4scv
Apr 11 17:52:14.737: INFO: Created: latency-svc-qsbbx
Apr 11 17:52:14.747: INFO: Created: latency-svc-bxj6q
Apr 11 17:52:14.760: INFO: Created: latency-svc-rpkfd
Apr 11 17:52:14.769: INFO: Created: latency-svc-5hrsw
Apr 11 17:52:14.769: INFO: Got endpoints: latency-svc-44hzk [157.933386ms]
Apr 11 17:52:14.784: INFO: Created: latency-svc-dzbsf
Apr 11 17:52:14.800: INFO: Created: latency-svc-7dzdl
Apr 11 17:52:14.808: INFO: Created: latency-svc-qmzcp
Apr 11 17:52:14.816: INFO: Created: latency-svc-nds8r
Apr 11 17:52:14.819: INFO: Got endpoints: latency-svc-b4scv [196.557719ms]
Apr 11 17:52:14.828: INFO: Created: latency-svc-xnwtk
Apr 11 17:52:14.834: INFO: Created: latency-svc-758rf
Apr 11 17:52:14.840: INFO: Created: latency-svc-ml96k
Apr 11 17:52:14.847: INFO: Created: latency-svc-fn6c5
Apr 11 17:52:14.854: INFO: Created: latency-svc-s8td9
Apr 11 17:52:14.860: INFO: Created: latency-svc-mq966
Apr 11 17:52:14.866: INFO: Got endpoints: latency-svc-qsbbx [239.723253ms]
Apr 11 17:52:14.867: INFO: Created: latency-svc-6tp9t
Apr 11 17:52:14.879: INFO: Created: latency-svc-77trz
Apr 11 17:52:14.917: INFO: Got endpoints: latency-svc-bxj6q [275.420702ms]
Apr 11 17:52:14.931: INFO: Created: latency-svc-q8xvf
Apr 11 17:52:14.966: INFO: Got endpoints: latency-svc-rpkfd [318.285817ms]
Apr 11 17:52:14.987: INFO: Created: latency-svc-k7n4n
Apr 11 17:52:15.016: INFO: Got endpoints: latency-svc-5hrsw [365.921147ms]
Apr 11 17:52:15.030: INFO: Created: latency-svc-jv8vg
Apr 11 17:52:15.066: INFO: Got endpoints: latency-svc-dzbsf [406.835516ms]
Apr 11 17:52:15.077: INFO: Created: latency-svc-6l5t7
Apr 11 17:52:15.121: INFO: Got endpoints: latency-svc-7dzdl [450.346274ms]
Apr 11 17:52:15.135: INFO: Created: latency-svc-pfxzv
Apr 11 17:52:15.165: INFO: Got endpoints: latency-svc-qmzcp [484.255006ms]
Apr 11 17:52:15.177: INFO: Created: latency-svc-5zxlk
Apr 11 17:52:15.217: INFO: Got endpoints: latency-svc-nds8r [531.225552ms]
Apr 11 17:52:15.231: INFO: Created: latency-svc-drzw7
Apr 11 17:52:15.267: INFO: Got endpoints: latency-svc-xnwtk [573.772425ms]
Apr 11 17:52:15.280: INFO: Created: latency-svc-m78cc
Apr 11 17:52:15.320: INFO: Got endpoints: latency-svc-758rf [617.463014ms]
Apr 11 17:52:15.334: INFO: Created: latency-svc-ss6tj
Apr 11 17:52:15.368: INFO: Got endpoints: latency-svc-ml96k [654.006091ms]
Apr 11 17:52:15.381: INFO: Created: latency-svc-xk285
Apr 11 17:52:15.416: INFO: Got endpoints: latency-svc-fn6c5 [694.398589ms]
Apr 11 17:52:15.428: INFO: Created: latency-svc-lggjk
Apr 11 17:52:15.468: INFO: Got endpoints: latency-svc-s8td9 [743.194829ms]
Apr 11 17:52:15.483: INFO: Created: latency-svc-bzp7g
Apr 11 17:52:15.517: INFO: Got endpoints: latency-svc-mq966 [747.861745ms]
Apr 11 17:52:15.537: INFO: Created: latency-svc-d9x7z
Apr 11 17:52:15.568: INFO: Got endpoints: latency-svc-6tp9t [748.613307ms]
Apr 11 17:52:15.588: INFO: Created: latency-svc-62znh
Apr 11 17:52:15.618: INFO: Got endpoints: latency-svc-77trz [751.440633ms]
Apr 11 17:52:15.636: INFO: Created: latency-svc-llf67
Apr 11 17:52:15.670: INFO: Got endpoints: latency-svc-q8xvf [753.369547ms]
Apr 11 17:52:15.690: INFO: Created: latency-svc-4fwlt
Apr 11 17:52:15.716: INFO: Got endpoints: latency-svc-k7n4n [749.425943ms]
Apr 11 17:52:15.732: INFO: Created: latency-svc-fhw8p
Apr 11 17:52:15.773: INFO: Got endpoints: latency-svc-jv8vg [756.344869ms]
Apr 11 17:52:15.789: INFO: Created: latency-svc-st8s5
Apr 11 17:52:15.817: INFO: Got endpoints: latency-svc-6l5t7 [750.576714ms]
Apr 11 17:52:15.830: INFO: Created: latency-svc-4k668
Apr 11 17:52:15.867: INFO: Got endpoints: latency-svc-pfxzv [745.496751ms]
Apr 11 17:52:15.883: INFO: Created: latency-svc-7fzjc
Apr 11 17:52:15.915: INFO: Got endpoints: latency-svc-5zxlk [749.936068ms]
Apr 11 17:52:15.929: INFO: Created: latency-svc-fv896
Apr 11 17:52:15.966: INFO: Got endpoints: latency-svc-drzw7 [749.493933ms]
Apr 11 17:52:15.981: INFO: Created: latency-svc-25ln6
Apr 11 17:52:16.020: INFO: Got endpoints: latency-svc-m78cc [753.422716ms]
Apr 11 17:52:16.034: INFO: Created: latency-svc-t7dnz
Apr 11 17:52:16.072: INFO: Got endpoints: latency-svc-ss6tj [752.021862ms]
Apr 11 17:52:16.085: INFO: Created: latency-svc-8k5hg
Apr 11 17:52:16.116: INFO: Got endpoints: latency-svc-xk285 [747.813616ms]
Apr 11 17:52:16.129: INFO: Created: latency-svc-x5lh9
Apr 11 17:52:16.174: INFO: Got endpoints: latency-svc-lggjk [758.163744ms]
Apr 11 17:52:16.189: INFO: Created: latency-svc-5lnvx
Apr 11 17:52:16.215: INFO: Got endpoints: latency-svc-bzp7g [746.98431ms]
Apr 11 17:52:16.229: INFO: Created: latency-svc-cdwhv
Apr 11 17:52:16.266: INFO: Got endpoints: latency-svc-d9x7z [748.657502ms]
Apr 11 17:52:16.282: INFO: Created: latency-svc-mcr7x
Apr 11 17:52:16.315: INFO: Got endpoints: latency-svc-62znh [747.223279ms]
Apr 11 17:52:16.334: INFO: Created: latency-svc-5rjj2
Apr 11 17:52:16.366: INFO: Got endpoints: latency-svc-llf67 [748.707455ms]
Apr 11 17:52:16.381: INFO: Created: latency-svc-v2m27
Apr 11 17:52:16.416: INFO: Got endpoints: latency-svc-4fwlt [745.741909ms]
Apr 11 17:52:16.431: INFO: Created: latency-svc-8qbtz
Apr 11 17:52:16.469: INFO: Got endpoints: latency-svc-fhw8p [752.188658ms]
Apr 11 17:52:16.485: INFO: Created: latency-svc-crjvr
Apr 11 17:52:16.516: INFO: Got endpoints: latency-svc-st8s5 [742.946802ms]
Apr 11 17:52:16.529: INFO: Created: latency-svc-drcs5
Apr 11 17:52:16.570: INFO: Got endpoints: latency-svc-4k668 [752.87193ms]
Apr 11 17:52:16.584: INFO: Created: latency-svc-dlc6h
Apr 11 17:52:16.617: INFO: Got endpoints: latency-svc-7fzjc [749.733582ms]
Apr 11 17:52:16.629: INFO: Created: latency-svc-tzhsb
Apr 11 17:52:16.667: INFO: Got endpoints: latency-svc-fv896 [751.027064ms]
Apr 11 17:52:16.681: INFO: Created: latency-svc-s8dsm
Apr 11 17:52:16.717: INFO: Got endpoints: latency-svc-25ln6 [750.297495ms]
Apr 11 17:52:16.731: INFO: Created: latency-svc-ljjwj
Apr 11 17:52:16.766: INFO: Got endpoints: latency-svc-t7dnz [745.050564ms]
Apr 11 17:52:16.781: INFO: Created: latency-svc-2mrdj
Apr 11 17:52:16.816: INFO: Got endpoints: latency-svc-8k5hg [744.166022ms]
Apr 11 17:52:16.830: INFO: Created: latency-svc-w29qg
Apr 11 17:52:16.866: INFO: Got endpoints: latency-svc-x5lh9 [750.249317ms]
Apr 11 17:52:16.882: INFO: Created: latency-svc-ndgpr
Apr 11 17:52:16.917: INFO: Got endpoints: latency-svc-5lnvx [742.229346ms]
Apr 11 17:52:16.930: INFO: Created: latency-svc-rw8zc
Apr 11 17:52:16.968: INFO: Got endpoints: latency-svc-cdwhv [752.388184ms]
Apr 11 17:52:16.984: INFO: Created: latency-svc-kd9x6
Apr 11 17:52:17.018: INFO: Got endpoints: latency-svc-mcr7x [751.554544ms]
Apr 11 17:52:17.038: INFO: Created: latency-svc-x7knl
Apr 11 17:52:17.068: INFO: Got endpoints: latency-svc-5rjj2 [752.675318ms]
Apr 11 17:52:17.083: INFO: Created: latency-svc-spv65
Apr 11 17:52:17.115: INFO: Got endpoints: latency-svc-v2m27 [748.925159ms]
Apr 11 17:52:17.129: INFO: Created: latency-svc-s7ktm
Apr 11 17:52:17.167: INFO: Got endpoints: latency-svc-8qbtz [750.045452ms]
Apr 11 17:52:17.178: INFO: Created: latency-svc-mdcdh
Apr 11 17:52:17.219: INFO: Got endpoints: latency-svc-crjvr [749.853358ms]
Apr 11 17:52:17.229: INFO: Created: latency-svc-dp9br
Apr 11 17:52:17.273: INFO: Got endpoints: latency-svc-drcs5 [756.583452ms]
Apr 11 17:52:17.286: INFO: Created: latency-svc-sgblw
Apr 11 17:52:17.319: INFO: Got endpoints: latency-svc-dlc6h [748.063513ms]
Apr 11 17:52:17.333: INFO: Created: latency-svc-tplbz
Apr 11 17:52:17.366: INFO: Got endpoints: latency-svc-tzhsb [749.325347ms]
Apr 11 17:52:17.381: INFO: Created: latency-svc-zg8gg
Apr 11 17:52:17.417: INFO: Got endpoints: latency-svc-s8dsm [749.638072ms]
Apr 11 17:52:17.448: INFO: Created: latency-svc-r8wvh
Apr 11 17:52:17.466: INFO: Got endpoints: latency-svc-ljjwj [748.669968ms]
Apr 11 17:52:17.479: INFO: Created: latency-svc-zcdv7
Apr 11 17:52:17.515: INFO: Got endpoints: latency-svc-2mrdj [749.37387ms]
Apr 11 17:52:17.528: INFO: Created: latency-svc-pkp2s
Apr 11 17:52:17.567: INFO: Got endpoints: latency-svc-w29qg [750.795506ms]
Apr 11 17:52:17.580: INFO: Created: latency-svc-vz8z8
Apr 11 17:52:17.624: INFO: Got endpoints: latency-svc-ndgpr [757.910094ms]
Apr 11 17:52:17.637: INFO: Created: latency-svc-vzdx9
Apr 11 17:52:17.666: INFO: Got endpoints: latency-svc-rw8zc [748.84531ms]
Apr 11 17:52:17.676: INFO: Created: latency-svc-fctq4
Apr 11 17:52:17.716: INFO: Got endpoints: latency-svc-kd9x6 [747.877728ms]
Apr 11 17:52:17.731: INFO: Created: latency-svc-b7qwn
Apr 11 17:52:17.765: INFO: Got endpoints: latency-svc-x7knl [747.451758ms]
Apr 11 17:52:17.780: INFO: Created: latency-svc-47xsx
Apr 11 17:52:17.816: INFO: Got endpoints: latency-svc-spv65 [748.24809ms]
Apr 11 17:52:17.831: INFO: Created: latency-svc-jrc87
Apr 11 17:52:17.866: INFO: Got endpoints: latency-svc-s7ktm [750.34578ms]
Apr 11 17:52:17.881: INFO: Created: latency-svc-7frzs
Apr 11 17:52:17.916: INFO: Got endpoints: latency-svc-mdcdh [749.647158ms]
Apr 11 17:52:17.931: INFO: Created: latency-svc-vnww9
Apr 11 17:52:17.966: INFO: Got endpoints: latency-svc-dp9br [747.049159ms]
Apr 11 17:52:17.979: INFO: Created: latency-svc-tggj6
Apr 11 17:52:18.019: INFO: Got endpoints: latency-svc-sgblw [745.159647ms]
Apr 11 17:52:18.031: INFO: Created: latency-svc-8kdll
Apr 11 17:52:18.066: INFO: Got endpoints: latency-svc-tplbz [747.171936ms]
Apr 11 17:52:18.082: INFO: Created: latency-svc-sfkl9
Apr 11 17:52:18.118: INFO: Got endpoints: latency-svc-zg8gg [751.06753ms]
Apr 11 17:52:18.135: INFO: Created: latency-svc-82js7
Apr 11 17:52:18.166: INFO: Got endpoints: latency-svc-r8wvh [748.849147ms]
Apr 11 17:52:18.182: INFO: Created: latency-svc-w6jdq
Apr 11 17:52:18.216: INFO: Got endpoints: latency-svc-zcdv7 [750.082937ms]
Apr 11 17:52:18.231: INFO: Created: latency-svc-jnrb7
Apr 11 17:52:18.267: INFO: Got endpoints: latency-svc-pkp2s [750.693273ms]
Apr 11 17:52:18.279: INFO: Created: latency-svc-jxhbr
Apr 11 17:52:18.318: INFO: Got endpoints: latency-svc-vz8z8 [750.255521ms]
Apr 11 17:52:18.332: INFO: Created: latency-svc-kpgdk
Apr 11 17:52:18.367: INFO: Got endpoints: latency-svc-vzdx9 [742.883887ms]
Apr 11 17:52:18.380: INFO: Created: latency-svc-j6s8z
Apr 11 17:52:18.416: INFO: Got endpoints: latency-svc-fctq4 [750.681799ms]
Apr 11 17:52:18.431: INFO: Created: latency-svc-w9b2l
Apr 11 17:52:18.466: INFO: Got endpoints: latency-svc-b7qwn [749.811887ms]
Apr 11 17:52:18.480: INFO: Created: latency-svc-nnvkw
Apr 11 17:52:18.516: INFO: Got endpoints: latency-svc-47xsx [750.949334ms]
Apr 11 17:52:18.530: INFO: Created: latency-svc-2mghc
Apr 11 17:52:18.566: INFO: Got endpoints: latency-svc-jrc87 [749.543805ms]
Apr 11 17:52:18.581: INFO: Created: latency-svc-tzmbh
Apr 11 17:52:18.616: INFO: Got endpoints: latency-svc-7frzs [750.04017ms]
Apr 11 17:52:18.629: INFO: Created: latency-svc-sr9kl
Apr 11 17:52:18.668: INFO: Got endpoints: latency-svc-vnww9 [751.686492ms]
Apr 11 17:52:18.681: INFO: Created: latency-svc-zfkd5
Apr 11 17:52:18.719: INFO: Got endpoints: latency-svc-tggj6 [752.574382ms]
Apr 11 17:52:18.736: INFO: Created: latency-svc-8v2tp
Apr 11 17:52:18.766: INFO: Got endpoints: latency-svc-8kdll [747.645292ms]
Apr 11 17:52:18.782: INFO: Created: latency-svc-2bbdw
Apr 11 17:52:18.817: INFO: Got endpoints: latency-svc-sfkl9 [750.177288ms]
Apr 11 17:52:18.832: INFO: Created: latency-svc-z4xh4
Apr 11 17:52:18.866: INFO: Got endpoints: latency-svc-82js7 [748.549275ms]
Apr 11 17:52:18.881: INFO: Created: latency-svc-d24nx
Apr 11 17:52:18.920: INFO: Got endpoints: latency-svc-w6jdq [753.554558ms]
Apr 11 17:52:18.933: INFO: Created: latency-svc-twssx
Apr 11 17:52:18.966: INFO: Got endpoints: latency-svc-jnrb7 [749.914605ms]
Apr 11 17:52:18.984: INFO: Created: latency-svc-vjlhp
Apr 11 17:52:19.017: INFO: Got endpoints: latency-svc-jxhbr [750.181096ms]
Apr 11 17:52:19.030: INFO: Created: latency-svc-nnkw8
Apr 11 17:52:19.069: INFO: Got endpoints: latency-svc-kpgdk [750.877579ms]
Apr 11 17:52:19.083: INFO: Created: latency-svc-2wxhp
Apr 11 17:52:19.117: INFO: Got endpoints: latency-svc-j6s8z [749.194133ms]
Apr 11 17:52:19.135: INFO: Created: latency-svc-lmfst
Apr 11 17:52:19.168: INFO: Got endpoints: latency-svc-w9b2l [750.965316ms]
Apr 11 17:52:19.181: INFO: Created: latency-svc-wtf7m
Apr 11 17:52:19.217: INFO: Got endpoints: latency-svc-nnvkw [750.025403ms]
Apr 11 17:52:19.230: INFO: Created: latency-svc-s2tbn
Apr 11 17:52:19.267: INFO: Got endpoints: latency-svc-2mghc [749.832832ms]
Apr 11 17:52:19.279: INFO: Created: latency-svc-bcz9j
Apr 11 17:52:19.316: INFO: Got endpoints: latency-svc-tzmbh [749.859163ms]
Apr 11 17:52:19.327: INFO: Created: latency-svc-jgcsl
Apr 11 17:52:19.369: INFO: Got endpoints: latency-svc-sr9kl [751.93091ms]
Apr 11 17:52:19.383: INFO: Created: latency-svc-dlg8t
Apr 11 17:52:19.416: INFO: Got endpoints: latency-svc-zfkd5 [747.6473ms]
Apr 11 17:52:19.430: INFO: Created: latency-svc-v7jlx
Apr 11 17:52:19.466: INFO: Got endpoints: latency-svc-8v2tp [746.968236ms]
Apr 11 17:52:19.479: INFO: Created: latency-svc-tg6fx
Apr 11 17:52:19.517: INFO: Got endpoints: latency-svc-2bbdw [750.780291ms]
Apr 11 17:52:19.532: INFO: Created: latency-svc-98kvr
Apr 11 17:52:19.566: INFO: Got endpoints: latency-svc-z4xh4 [749.857633ms]
Apr 11 17:52:19.580: INFO: Created: latency-svc-2dtjg
Apr 11 17:52:19.623: INFO: Got endpoints: latency-svc-d24nx [756.639256ms]
Apr 11 17:52:19.640: INFO: Created: latency-svc-b4nk7
Apr 11 17:52:19.666: INFO: Got endpoints: latency-svc-twssx [746.712328ms]
Apr 11 17:52:19.679: INFO: Created: latency-svc-ht2c4
Apr 11 17:52:19.716: INFO: Got endpoints: latency-svc-vjlhp [749.642806ms]
Apr 11 17:52:19.728: INFO: Created: latency-svc-c5jfs
Apr 11 17:52:19.766: INFO: Got endpoints: latency-svc-nnkw8 [748.719494ms]
Apr 11 17:52:19.778: INFO: Created: latency-svc-jmmlb
Apr 11 17:52:19.820: INFO: Got endpoints: latency-svc-2wxhp [750.923376ms]
Apr 11 17:52:19.833: INFO: Created: latency-svc-2k54k
Apr 11 17:52:19.867: INFO: Got endpoints: latency-svc-lmfst [749.501546ms]
Apr 11 17:52:19.883: INFO: Created: latency-svc-82skn
Apr 11 17:52:19.916: INFO: Got endpoints: latency-svc-wtf7m [748.505982ms]
Apr 11 17:52:19.940: INFO: Created: latency-svc-crqzh
Apr 11 17:52:19.970: INFO: Got endpoints: latency-svc-s2tbn [752.896789ms]
Apr 11 17:52:19.987: INFO: Created: latency-svc-xlhzc
Apr 11 17:52:20.016: INFO: Got endpoints: latency-svc-bcz9j [748.945204ms]
Apr 11 17:52:20.029: INFO: Created: latency-svc-2w6pc
Apr 11 17:52:20.066: INFO: Got endpoints: latency-svc-jgcsl [750.011117ms]
Apr 11 17:52:20.083: INFO: Created: latency-svc-k558h
Apr 11 17:52:20.117: INFO: Got endpoints: latency-svc-dlg8t [748.573799ms]
Apr 11 17:52:20.132: INFO: Created: latency-svc-r8btr
Apr 11 17:52:20.167: INFO: Got endpoints: latency-svc-v7jlx [750.256026ms]
Apr 11 17:52:20.182: INFO: Created: latency-svc-xqf77
Apr 11 17:52:20.217: INFO: Got endpoints: latency-svc-tg6fx [751.262927ms]
Apr 11 17:52:20.235: INFO: Created: latency-svc-d465k
Apr 11 17:52:20.270: INFO: Got endpoints: latency-svc-98kvr [752.755906ms]
Apr 11 17:52:20.288: INFO: Created: latency-svc-b7lkt
Apr 11 17:52:20.334: INFO: Got endpoints: latency-svc-2dtjg [767.087261ms]
Apr 11 17:52:20.348: INFO: Created: latency-svc-jz9v2
Apr 11 17:52:20.365: INFO: Got endpoints: latency-svc-b4nk7 [741.517234ms]
Apr 11 17:52:20.379: INFO: Created: latency-svc-jc89v
Apr 11 17:52:20.416: INFO: Got endpoints: latency-svc-ht2c4 [748.900468ms]
Apr 11 17:52:20.435: INFO: Created: latency-svc-2l6v9
Apr 11 17:52:20.466: INFO: Got endpoints: latency-svc-c5jfs [749.137074ms]
Apr 11 17:52:20.476: INFO: Created: latency-svc-ptjhb
Apr 11 17:52:20.516: INFO: Got endpoints: latency-svc-jmmlb [749.8593ms]
Apr 11 17:52:20.530: INFO: Created: latency-svc-t4ffl
Apr 11 17:52:20.568: INFO: Got endpoints: latency-svc-2k54k [748.095766ms]
Apr 11 17:52:20.582: INFO: Created: latency-svc-n4phg
Apr 11 17:52:20.616: INFO: Got endpoints: latency-svc-82skn [749.636449ms]
Apr 11 17:52:20.638: INFO: Created: latency-svc-8nd5v
Apr 11 17:52:20.666: INFO: Got endpoints: latency-svc-crqzh [749.058813ms]
Apr 11 17:52:20.682: INFO: Created: latency-svc-lpwfw
Apr 11 17:52:20.723: INFO: Got endpoints: latency-svc-xlhzc [753.098215ms]
Apr 11 17:52:20.735: INFO: Created: latency-svc-k2bjm
Apr 11 17:52:20.767: INFO: Got endpoints: latency-svc-2w6pc [751.408727ms]
Apr 11 17:52:20.779: INFO: Created: latency-svc-9sllm
Apr 11 17:52:20.817: INFO: Got endpoints: latency-svc-k558h [750.125268ms]
Apr 11 17:52:20.832: INFO: Created: latency-svc-hn4d2
Apr 11 17:52:20.868: INFO: Got endpoints: latency-svc-r8btr [750.467155ms]
Apr 11 17:52:20.883: INFO: Created: latency-svc-q42sf
Apr 11 17:52:20.917: INFO: Got endpoints: latency-svc-xqf77 [749.723868ms]
Apr 11 17:52:20.933: INFO: Created: latency-svc-9mxnd
Apr 11 17:52:20.967: INFO: Got endpoints: latency-svc-d465k [749.59729ms]
Apr 11 17:52:20.982: INFO: Created: latency-svc-9v2kl
Apr 11 17:52:21.016: INFO: Got endpoints: latency-svc-b7lkt [745.688739ms]
Apr 11 17:52:21.031: INFO: Created: latency-svc-vlnlf
Apr 11 17:52:21.071: INFO: Got endpoints: latency-svc-jz9v2 [737.446532ms]
Apr 11 17:52:21.088: INFO: Created: latency-svc-htzbt
Apr 11 17:52:21.117: INFO: Got endpoints: latency-svc-jc89v [751.925558ms]
Apr 11 17:52:21.131: INFO: Created: latency-svc-8khpj
Apr 11 17:52:21.167: INFO: Got endpoints: latency-svc-2l6v9 [751.054135ms]
Apr 11 17:52:21.182: INFO: Created: latency-svc-g2xvp
Apr 11 17:52:21.216: INFO: Got endpoints: latency-svc-ptjhb [750.234383ms]
Apr 11 17:52:21.228: INFO: Created: latency-svc-w2qld
Apr 11 17:52:21.270: INFO: Got endpoints: latency-svc-t4ffl [753.239766ms]
Apr 11 17:52:21.284: INFO: Created: latency-svc-6cvft
Apr 11 17:52:21.316: INFO: Got endpoints: latency-svc-n4phg [747.798497ms]
Apr 11 17:52:21.330: INFO: Created: latency-svc-bd9xd
Apr 11 17:52:21.374: INFO: Got endpoints: latency-svc-8nd5v [756.974796ms]
Apr 11 17:52:21.389: INFO: Created: latency-svc-hvf7l
Apr 11 17:52:21.427: INFO: Got endpoints: latency-svc-lpwfw [760.474416ms]
Apr 11 17:52:21.449: INFO: Created: latency-svc-9prl2
Apr 11 17:52:21.466: INFO: Got endpoints: latency-svc-k2bjm [743.181933ms]
Apr 11 17:52:21.483: INFO: Created: latency-svc-qxpmz
Apr 11 17:52:21.517: INFO: Got endpoints: latency-svc-9sllm [749.152196ms]
Apr 11 17:52:21.529: INFO: Created: latency-svc-gpqfd
Apr 11 17:52:21.566: INFO: Got endpoints: latency-svc-hn4d2 [749.253201ms]
Apr 11 17:52:21.581: INFO: Created: latency-svc-hpcct
Apr 11 17:52:21.618: INFO: Got endpoints: latency-svc-q42sf [750.057844ms]
Apr 11 17:52:21.636: INFO: Created: latency-svc-phznf
Apr 11 17:52:21.665: INFO: Got endpoints: latency-svc-9mxnd [748.25162ms]
Apr 11 17:52:21.677: INFO: Created: latency-svc-2gqvk
Apr 11 17:52:21.716: INFO: Got endpoints: latency-svc-9v2kl [749.081292ms]
Apr 11 17:52:21.729: INFO: Created: latency-svc-8qb7f
Apr 11 17:52:21.765: INFO: Got endpoints: latency-svc-vlnlf [748.78678ms]
Apr 11 17:52:21.777: INFO: Created: latency-svc-s5qvv
Apr 11 17:52:21.819: INFO: Got endpoints: latency-svc-htzbt [747.243011ms]
Apr 11 17:52:21.831: INFO: Created: latency-svc-lnmqc
Apr 11 17:52:21.867: INFO: Got endpoints: latency-svc-8khpj [749.912846ms]
Apr 11 17:52:21.879: INFO: Created: latency-svc-g976f
Apr 11 17:52:21.917: INFO: Got endpoints: latency-svc-g2xvp [749.627539ms]
Apr 11 17:52:21.931: INFO: Created: latency-svc-snxs8
Apr 11 17:52:21.967: INFO: Got endpoints: latency-svc-w2qld [751.215449ms]
Apr 11 17:52:21.983: INFO: Created: latency-svc-tdtg9
Apr 11 17:52:22.017: INFO: Got endpoints: latency-svc-6cvft [746.611093ms]
Apr 11 17:52:22.030: INFO: Created: latency-svc-jjkmr
Apr 11 17:52:22.067: INFO: Got endpoints: latency-svc-bd9xd [750.170488ms]
Apr 11 17:52:22.080: INFO: Created: latency-svc-pl6lp
Apr 11 17:52:22.117: INFO: Got endpoints: latency-svc-hvf7l [742.637172ms]
Apr 11 17:52:22.128: INFO: Created: latency-svc-k8ms7
Apr 11 17:52:22.170: INFO: Got endpoints: latency-svc-9prl2 [743.231692ms]
Apr 11 17:52:22.183: INFO: Created: latency-svc-c8f5c
Apr 11 17:52:22.218: INFO: Got endpoints: latency-svc-qxpmz [751.817405ms]
Apr 11 17:52:22.239: INFO: Created: latency-svc-pn7hw
Apr 11 17:52:22.270: INFO: Got endpoints: latency-svc-gpqfd [752.793062ms]
Apr 11 17:52:22.317: INFO: Got endpoints: latency-svc-hpcct [750.104685ms]
Apr 11 17:52:22.367: INFO: Got endpoints: latency-svc-phznf [748.408867ms]
Apr 11 17:52:22.417: INFO: Got endpoints: latency-svc-2gqvk [751.748146ms]
Apr 11 17:52:22.466: INFO: Got endpoints: latency-svc-8qb7f [749.768007ms]
Apr 11 17:52:22.516: INFO: Got endpoints: latency-svc-s5qvv [750.347878ms]
Apr 11 17:52:22.568: INFO: Got endpoints: latency-svc-lnmqc [748.872424ms]
Apr 11 17:52:22.617: INFO: Got endpoints: latency-svc-g976f [749.489633ms]
Apr 11 17:52:22.669: INFO: Got endpoints: latency-svc-snxs8 [752.140301ms]
Apr 11 17:52:22.719: INFO: Got endpoints: latency-svc-tdtg9 [751.371377ms]
Apr 11 17:52:22.766: INFO: Got endpoints: latency-svc-jjkmr [749.415869ms]
Apr 11 17:52:22.819: INFO: Got endpoints: latency-svc-pl6lp [751.766514ms]
Apr 11 17:52:22.868: INFO: Got endpoints: latency-svc-k8ms7 [751.21977ms]
Apr 11 17:52:22.918: INFO: Got endpoints: latency-svc-c8f5c [747.302891ms]
Apr 11 17:52:22.966: INFO: Got endpoints: latency-svc-pn7hw [747.072302ms]
Apr 11 17:52:22.966: INFO: Latencies: [26.236705ms 31.91061ms 33.076383ms 38.479042ms 45.056587ms 49.969353ms 64.181278ms 77.441053ms 94.736277ms 96.789221ms 101.783321ms 113.261475ms 117.227885ms 118.777723ms 121.003931ms 121.235842ms 121.550155ms 121.640623ms 121.896794ms 122.066034ms 122.235474ms 122.254119ms 122.263347ms 123.64341ms 124.0666ms 124.434857ms 125.863885ms 126.511682ms 128.183528ms 129.143182ms 129.820574ms 132.835022ms 133.235643ms 135.016032ms 141.256929ms 157.933386ms 196.557719ms 239.723253ms 275.420702ms 318.285817ms 365.921147ms 406.835516ms 450.346274ms 484.255006ms 531.225552ms 573.772425ms 617.463014ms 654.006091ms 694.398589ms 737.446532ms 741.517234ms 742.229346ms 742.637172ms 742.883887ms 742.946802ms 743.181933ms 743.194829ms 743.231692ms 744.166022ms 745.050564ms 745.159647ms 745.496751ms 745.688739ms 745.741909ms 746.611093ms 746.712328ms 746.968236ms 746.98431ms 747.049159ms 747.072302ms 747.171936ms 747.223279ms 747.243011ms 747.302891ms 747.451758ms 747.645292ms 747.6473ms 747.798497ms 747.813616ms 747.861745ms 747.877728ms 748.063513ms 748.095766ms 748.24809ms 748.25162ms 748.408867ms 748.505982ms 748.549275ms 748.573799ms 748.613307ms 748.657502ms 748.669968ms 748.707455ms 748.719494ms 748.78678ms 748.84531ms 748.849147ms 748.872424ms 748.900468ms 748.925159ms 748.945204ms 749.058813ms 749.081292ms 749.137074ms 749.152196ms 749.194133ms 749.253201ms 749.325347ms 749.37387ms 749.415869ms 749.425943ms 749.489633ms 749.493933ms 749.501546ms 749.543805ms 749.59729ms 749.627539ms 749.636449ms 749.638072ms 749.642806ms 749.647158ms 749.723868ms 749.733582ms 749.768007ms 749.811887ms 749.832832ms 749.853358ms 749.857633ms 749.859163ms 749.8593ms 749.912846ms 749.914605ms 749.936068ms 750.011117ms 750.025403ms 750.04017ms 750.045452ms 750.057844ms 750.082937ms 750.104685ms 750.125268ms 750.170488ms 750.177288ms 750.181096ms 750.234383ms 750.249317ms 750.255521ms 750.256026ms 750.297495ms 750.34578ms 750.347878ms 750.467155ms 750.576714ms 750.681799ms 750.693273ms 750.780291ms 750.795506ms 750.877579ms 750.923376ms 750.949334ms 750.965316ms 751.027064ms 751.054135ms 751.06753ms 751.215449ms 751.21977ms 751.262927ms 751.371377ms 751.408727ms 751.440633ms 751.554544ms 751.686492ms 751.748146ms 751.766514ms 751.817405ms 751.925558ms 751.93091ms 752.021862ms 752.140301ms 752.188658ms 752.388184ms 752.574382ms 752.675318ms 752.755906ms 752.793062ms 752.87193ms 752.896789ms 753.098215ms 753.239766ms 753.369547ms 753.422716ms 753.554558ms 756.344869ms 756.583452ms 756.639256ms 756.974796ms 757.910094ms 758.163744ms 760.474416ms 767.087261ms]
Apr 11 17:52:22.966: INFO: 50 %ile: 748.945204ms
Apr 11 17:52:22.967: INFO: 90 %ile: 752.388184ms
Apr 11 17:52:22.967: INFO: 99 %ile: 760.474416ms
Apr 11 17:52:22.967: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:52:22.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-wzqh5" for this suite.
Apr 11 17:52:36.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:52:37.021: INFO: namespace: e2e-tests-svc-latency-wzqh5, resource: bindings, ignored listing per whitelist
Apr 11 17:52:37.107: INFO: namespace e2e-tests-svc-latency-wzqh5 deletion completed in 14.134194493s

â€¢ [SLOW TEST:25.079 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:52:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-dv4bj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-7tpx
STEP: Creating a pod to test atomic-volume-subpath
Apr 11 17:52:37.346: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7tpx" in namespace "e2e-tests-subpath-dv4bj" to be "success or failure"
Apr 11 17:52:37.350: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318697ms
Apr 11 17:52:39.356: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009704365s
Apr 11 17:52:41.369: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 4.022597881s
Apr 11 17:52:43.374: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 6.028079937s
Apr 11 17:52:45.380: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 8.033547894s
Apr 11 17:52:47.385: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 10.039296498s
Apr 11 17:52:49.391: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 12.044429628s
Apr 11 17:52:51.403: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 14.056512926s
Apr 11 17:52:53.408: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 16.0618677s
Apr 11 17:52:55.413: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 18.066908642s
Apr 11 17:52:57.428: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 20.082032417s
Apr 11 17:52:59.435: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Running", Reason="", readiness=false. Elapsed: 22.089062914s
Apr 11 17:53:01.447: INFO: Pod "pod-subpath-test-projected-7tpx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.101282003s
STEP: Saw pod success
Apr 11 17:53:01.448: INFO: Pod "pod-subpath-test-projected-7tpx" satisfied condition "success or failure"
Apr 11 17:53:01.452: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-subpath-test-projected-7tpx container test-container-subpath-projected-7tpx: <nil>
STEP: delete the pod
Apr 11 17:53:01.491: INFO: Waiting for pod pod-subpath-test-projected-7tpx to disappear
Apr 11 17:53:01.497: INFO: Pod pod-subpath-test-projected-7tpx no longer exists
STEP: Deleting pod pod-subpath-test-projected-7tpx
Apr 11 17:53:01.497: INFO: Deleting pod "pod-subpath-test-projected-7tpx" in namespace "e2e-tests-subpath-dv4bj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:53:01.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-dv4bj" for this suite.
Apr 11 17:53:07.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:53:07.602: INFO: namespace: e2e-tests-subpath-dv4bj, resource: bindings, ignored listing per whitelist
Apr 11 17:53:07.629: INFO: namespace e2e-tests-subpath-dv4bj deletion completed in 6.123495764s

â€¢ [SLOW TEST:30.520 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:53:07.630: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-vzq7f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 11 17:53:07.852: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 11 17:53:12.864: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:53:13.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-vzq7f" for this suite.
Apr 11 17:53:19.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:53:19.931: INFO: namespace: e2e-tests-replication-controller-vzq7f, resource: bindings, ignored listing per whitelist
Apr 11 17:53:20.025: INFO: namespace e2e-tests-replication-controller-vzq7f deletion completed in 6.129093809s

â€¢ [SLOW TEST:12.396 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:53:20.028: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-wnp86
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b15756fd-5c82-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:53:20.245: INFO: Waiting up to 5m0s for pod "pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-wnp86" to be "success or failure"
Apr 11 17:53:20.250: INFO: Pod "pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492301ms
Apr 11 17:53:22.256: INFO: Pod "pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010743912s
STEP: Saw pod success
Apr 11 17:53:22.256: INFO: Pod "pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:53:22.261: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c container secret-env-test: <nil>
STEP: delete the pod
Apr 11 17:53:22.307: INFO: Waiting for pod pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:53:22.312: INFO: Pod pod-secrets-b1582446-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:53:22.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wnp86" for this suite.
Apr 11 17:53:28.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:53:28.343: INFO: namespace: e2e-tests-secrets-wnp86, resource: bindings, ignored listing per whitelist
Apr 11 17:53:28.440: INFO: namespace e2e-tests-secrets-wnp86 deletion completed in 6.122411855s

â€¢ [SLOW TEST:8.413 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:53:28.444: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-2qk8p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Apr 11 17:53:28.655: INFO: Waiting up to 5m0s for pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-containers-2qk8p" to be "success or failure"
Apr 11 17:53:28.659: INFO: Pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.506656ms
Apr 11 17:53:30.665: INFO: Pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009945089s
Apr 11 17:53:32.670: INFO: Pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015571646s
Apr 11 17:53:34.682: INFO: Pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02707241s
STEP: Saw pod success
Apr 11 17:53:34.682: INFO: Pod "client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:53:34.686: INFO: Trying to get logs from node cluster1-k8s-node-3 pod client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:53:34.721: INFO: Waiting for pod client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:53:34.725: INFO: Pod client-containers-b65b3dc0-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:53:34.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2qk8p" for this suite.
Apr 11 17:53:40.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:53:40.802: INFO: namespace: e2e-tests-containers-2qk8p, resource: bindings, ignored listing per whitelist
Apr 11 17:53:40.856: INFO: namespace e2e-tests-containers-2qk8p deletion completed in 6.125975519s

â€¢ [SLOW TEST:12.413 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:53:40.860: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gxjjn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Apr 11 17:53:43.620: INFO: Successfully updated pod "labelsupdatebdc29821-5c82-11e9-bcf1-d217e8a0130c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:53:47.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gxjjn" for this suite.
Apr 11 17:54:09.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:54:09.713: INFO: namespace: e2e-tests-downward-api-gxjjn, resource: bindings, ignored listing per whitelist
Apr 11 17:54:09.804: INFO: namespace e2e-tests-downward-api-gxjjn deletion completed in 22.133715604s

â€¢ [SLOW TEST:28.945 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:54:09.809: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-k6tm6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 11 17:54:10.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-k6tm6'
Apr 11 17:54:10.146: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 11 17:54:10.146: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Apr 11 17:54:14.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-k6tm6'
Apr 11 17:54:14.294: INFO: stderr: ""
Apr 11 17:54:14.294: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:54:14.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k6tm6" for this suite.
Apr 11 17:54:20.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:54:20.412: INFO: namespace: e2e-tests-kubectl-k6tm6, resource: bindings, ignored listing per whitelist
Apr 11 17:54:20.433: INFO: namespace e2e-tests-kubectl-k6tm6 deletion completed in 6.133592513s

â€¢ [SLOW TEST:10.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:54:20.434: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ws9jv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 17:54:20.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-ws9jv" to be "success or failure"
Apr 11 17:54:20.657: INFO: Pod "downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.216619ms
Apr 11 17:54:22.663: INFO: Pod "downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009620422s
Apr 11 17:54:24.670: INFO: Pod "downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016662008s
STEP: Saw pod success
Apr 11 17:54:24.670: INFO: Pod "downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:54:24.675: INFO: Trying to get logs from node cluster1-k8s-node-1 pod downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 17:54:24.708: INFO: Waiting for pod downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:54:24.711: INFO: Pod downwardapi-volume-d5597b8d-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:54:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ws9jv" for this suite.
Apr 11 17:54:30.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:54:30.764: INFO: namespace: e2e-tests-downward-api-ws9jv, resource: bindings, ignored listing per whitelist
Apr 11 17:54:30.835: INFO: namespace e2e-tests-downward-api-ws9jv deletion completed in 6.118665538s

â€¢ [SLOW TEST:10.402 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:54:30.837: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-pbrq9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Apr 11 17:54:31.067: INFO: Waiting up to 5m0s for pod "var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-var-expansion-pbrq9" to be "success or failure"
Apr 11 17:54:31.071: INFO: Pod "var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291178ms
Apr 11 17:54:33.077: INFO: Pod "var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009935408s
STEP: Saw pod success
Apr 11 17:54:33.077: INFO: Pod "var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:54:33.081: INFO: Trying to get logs from node cluster1-k8s-node-3 pod var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 17:54:33.125: INFO: Waiting for pod var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:54:33.130: INFO: Pod var-expansion-db8e340f-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:54:33.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-pbrq9" for this suite.
Apr 11 17:54:39.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:54:39.244: INFO: namespace: e2e-tests-var-expansion-pbrq9, resource: bindings, ignored listing per whitelist
Apr 11 17:54:39.289: INFO: namespace e2e-tests-var-expansion-pbrq9 deletion completed in 6.140847681s

â€¢ [SLOW TEST:8.453 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:54:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cmj45
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Apr 11 17:54:42.077: INFO: Successfully updated pod "annotationupdatee098a4d4-5c82-11e9-bcf1-d217e8a0130c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:54:44.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cmj45" for this suite.
Apr 11 17:55:06.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:55:06.168: INFO: namespace: e2e-tests-downward-api-cmj45, resource: bindings, ignored listing per whitelist
Apr 11 17:55:06.223: INFO: namespace e2e-tests-downward-api-cmj45 deletion completed in 22.118046504s

â€¢ [SLOW TEST:26.923 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:55:06.228: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-h5h8s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f0a35b91-5c82-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:55:06.442: INFO: Waiting up to 5m0s for pod "pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-h5h8s" to be "success or failure"
Apr 11 17:55:06.448: INFO: Pod "pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.495399ms
Apr 11 17:55:08.453: INFO: Pod "pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010852068s
STEP: Saw pod success
Apr 11 17:55:08.454: INFO: Pod "pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:55:08.458: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:55:08.499: INFO: Waiting for pod pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:55:08.506: INFO: Pod pod-secrets-f0a420d4-5c82-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:55:08.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-h5h8s" for this suite.
Apr 11 17:55:14.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:55:14.628: INFO: namespace: e2e-tests-secrets-h5h8s, resource: bindings, ignored listing per whitelist
Apr 11 17:55:14.641: INFO: namespace e2e-tests-secrets-h5h8s deletion completed in 6.128852346s

â€¢ [SLOW TEST:8.414 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:55:14.644: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-q9l9k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Apr 11 17:55:14.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-q9l9k'
Apr 11 17:55:15.147: INFO: stderr: ""
Apr 11 17:55:15.147: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 11 17:55:16.152: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:55:16.152: INFO: Found 0 / 1
Apr 11 17:55:17.153: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:55:17.153: INFO: Found 1 / 1
Apr 11 17:55:17.153: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 11 17:55:17.158: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:55:17.158: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 11 17:55:17.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 patch pod redis-master-n6qjj --namespace=e2e-tests-kubectl-q9l9k -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 11 17:55:17.285: INFO: stderr: ""
Apr 11 17:55:17.285: INFO: stdout: "pod/redis-master-n6qjj patched\n"
STEP: checking annotations
Apr 11 17:55:17.290: INFO: Selector matched 1 pods for map[app:redis]
Apr 11 17:55:17.290: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:55:17.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q9l9k" for this suite.
Apr 11 17:55:37.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:55:37.332: INFO: namespace: e2e-tests-kubectl-q9l9k, resource: bindings, ignored listing per whitelist
Apr 11 17:55:37.429: INFO: namespace e2e-tests-kubectl-q9l9k deletion completed in 20.134548022s

â€¢ [SLOW TEST:22.785 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:55:37.432: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-wngkr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-033f5f28-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:55:37.668: INFO: Waiting up to 5m0s for pod "pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-wngkr" to be "success or failure"
Apr 11 17:55:37.672: INFO: Pod "pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703968ms
Apr 11 17:55:39.678: INFO: Pod "pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009953517s
Apr 11 17:55:41.683: INFO: Pod "pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015619772s
STEP: Saw pod success
Apr 11 17:55:41.683: INFO: Pod "pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:55:41.688: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:55:41.718: INFO: Waiting for pod pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:55:41.723: INFO: Pod pod-configmaps-0340acec-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:55:41.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wngkr" for this suite.
Apr 11 17:55:47.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:55:47.794: INFO: namespace: e2e-tests-configmap-wngkr, resource: bindings, ignored listing per whitelist
Apr 11 17:55:47.849: INFO: namespace e2e-tests-configmap-wngkr deletion completed in 6.122195027s

â€¢ [SLOW TEST:10.418 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:55:47.855: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-r9ff7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Apr 11 17:55:48.060: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:55:51.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-r9ff7" for this suite.
Apr 11 17:55:57.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:55:57.888: INFO: namespace: e2e-tests-init-container-r9ff7, resource: bindings, ignored listing per whitelist
Apr 11 17:55:57.907: INFO: namespace e2e-tests-init-container-r9ff7 deletion completed in 6.122770064s

â€¢ [SLOW TEST:10.053 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:55:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-km8lz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-km8lz
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 11 17:55:58.110: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 11 17:56:22.244: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.233.87.104 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-km8lz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:56:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:56:23.367: INFO: Found all expected endpoints: [netserver-0]
Apr 11 17:56:23.373: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.233.85.30 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-km8lz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:56:23.373: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:56:24.495: INFO: Found all expected endpoints: [netserver-1]
Apr 11 17:56:24.500: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.233.86.104 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-km8lz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 17:56:24.500: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 17:56:25.630: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:56:25.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-km8lz" for this suite.
Apr 11 17:56:47.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:56:47.710: INFO: namespace: e2e-tests-pod-network-test-km8lz, resource: bindings, ignored listing per whitelist
Apr 11 17:56:47.767: INFO: namespace e2e-tests-pod-network-test-km8lz deletion completed in 22.131832713s

â€¢ [SLOW TEST:49.858 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:56:47.770: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-txftk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Apr 11 17:56:47.995: INFO: Waiting up to 5m0s for pod "downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-txftk" to be "success or failure"
Apr 11 17:56:47.999: INFO: Pod "downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489056ms
Apr 11 17:56:50.005: INFO: Pod "downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010585861s
STEP: Saw pod success
Apr 11 17:56:50.005: INFO: Pod "downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:56:50.010: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 17:56:50.043: INFO: Waiting for pod downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:56:50.047: INFO: Pod downward-api-2d2c069b-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:56:50.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-txftk" for this suite.
Apr 11 17:56:56.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:56:56.143: INFO: namespace: e2e-tests-downward-api-txftk, resource: bindings, ignored listing per whitelist
Apr 11 17:56:56.176: INFO: namespace e2e-tests-downward-api-txftk deletion completed in 6.123812753s

â€¢ [SLOW TEST:8.406 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:56:56.182: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-9pgmj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-9pgmj
Apr 11 17:56:58.417: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-9pgmj
STEP: checking the pod's current state and verifying that restartCount is present
Apr 11 17:56:58.421: INFO: Initial restart count of pod liveness-http is 0
Apr 11 17:57:22.516: INFO: Restart count of pod e2e-tests-container-probe-9pgmj/liveness-http is now 1 (24.094762782s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:57:22.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9pgmj" for this suite.
Apr 11 17:57:28.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:57:28.636: INFO: namespace: e2e-tests-container-probe-9pgmj, resource: bindings, ignored listing per whitelist
Apr 11 17:57:28.671: INFO: namespace e2e-tests-container-probe-9pgmj deletion completed in 6.127982317s

â€¢ [SLOW TEST:32.490 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:57:28.674: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-4dzhw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-458c2a3d-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:57:28.896: INFO: Waiting up to 5m0s for pod "pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-4dzhw" to be "success or failure"
Apr 11 17:57:28.901: INFO: Pod "pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.70436ms
Apr 11 17:57:30.907: INFO: Pod "pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01070105s
STEP: Saw pod success
Apr 11 17:57:30.907: INFO: Pod "pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:57:30.913: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:57:30.944: INFO: Waiting for pod pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:57:30.948: INFO: Pod pod-secrets-458d443b-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:57:30.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4dzhw" for this suite.
Apr 11 17:57:36.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:57:37.078: INFO: namespace: e2e-tests-secrets-4dzhw, resource: bindings, ignored listing per whitelist
Apr 11 17:57:37.078: INFO: namespace e2e-tests-secrets-4dzhw deletion completed in 6.125351528s

â€¢ [SLOW TEST:8.404 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:57:37.081: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xt6d2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4a945a0b-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 17:57:37.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-xt6d2" to be "success or failure"
Apr 11 17:57:37.354: INFO: Pod "pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.265759ms
Apr 11 17:57:39.359: INFO: Pod "pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012822869s
STEP: Saw pod success
Apr 11 17:57:39.359: INFO: Pod "pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:57:39.364: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 17:57:39.396: INFO: Waiting for pod pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:57:39.400: INFO: Pod pod-projected-configmaps-4a957e70-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:57:39.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xt6d2" for this suite.
Apr 11 17:57:45.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:57:45.463: INFO: namespace: e2e-tests-projected-xt6d2, resource: bindings, ignored listing per whitelist
Apr 11 17:57:45.549: INFO: namespace e2e-tests-projected-xt6d2 deletion completed in 6.143854395s

â€¢ [SLOW TEST:8.469 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:57:45.553: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zr792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Apr 11 17:57:48.318: INFO: Successfully updated pod "labelsupdate4f9c089f-5c83-11e9-bcf1-d217e8a0130c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:57:52.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zr792" for this suite.
Apr 11 17:58:14.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:58:14.449: INFO: namespace: e2e-tests-projected-zr792, resource: bindings, ignored listing per whitelist
Apr 11 17:58:14.493: INFO: namespace e2e-tests-projected-zr792 deletion completed in 22.12644804s

â€¢ [SLOW TEST:28.941 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:58:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mn5xv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-c8d4p
STEP: Creating secret with name secret-test-60db3ed3-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:58:14.880: INFO: Waiting up to 5m0s for pod "pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-mn5xv" to be "success or failure"
Apr 11 17:58:14.888: INFO: Pod "pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012032ms
Apr 11 17:58:16.900: INFO: Pod "pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019772946s
STEP: Saw pod success
Apr 11 17:58:16.900: INFO: Pod "pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:58:16.905: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:58:16.938: INFO: Waiting for pod pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:58:16.942: INFO: Pod pod-secrets-60f5c8dd-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:58:16.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mn5xv" for this suite.
Apr 11 17:58:22.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:58:22.994: INFO: namespace: e2e-tests-secrets-mn5xv, resource: bindings, ignored listing per whitelist
Apr 11 17:58:23.073: INFO: namespace e2e-tests-secrets-mn5xv deletion completed in 6.125690824s
STEP: Destroying namespace "e2e-tests-secret-namespace-c8d4p" for this suite.
Apr 11 17:58:29.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:58:29.208: INFO: namespace: e2e-tests-secret-namespace-c8d4p, resource: bindings, ignored listing per whitelist
Apr 11 17:58:29.221: INFO: namespace e2e-tests-secret-namespace-c8d4p deletion completed in 6.147883517s

â€¢ [SLOW TEST:14.721 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:58:29.221: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-lqnnq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-69a31861-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 17:58:29.446: INFO: Waiting up to 5m0s for pod "pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-lqnnq" to be "success or failure"
Apr 11 17:58:29.454: INFO: Pod "pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.235995ms
Apr 11 17:58:31.460: INFO: Pod "pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013617921s
STEP: Saw pod success
Apr 11 17:58:31.460: INFO: Pod "pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:58:31.464: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 17:58:31.498: INFO: Waiting for pod pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:58:31.502: INFO: Pod pod-secrets-69a3eb58-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:58:31.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lqnnq" for this suite.
Apr 11 17:58:37.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:58:37.613: INFO: namespace: e2e-tests-secrets-lqnnq, resource: bindings, ignored listing per whitelist
Apr 11 17:58:37.639: INFO: namespace e2e-tests-secrets-lqnnq deletion completed in 6.131316954s

â€¢ [SLOW TEST:8.417 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:58:37.639: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zfx26
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Apr 11 17:58:37.852: INFO: Waiting up to 5m0s for pod "downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-zfx26" to be "success or failure"
Apr 11 17:58:37.856: INFO: Pod "downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441708ms
Apr 11 17:58:39.862: INFO: Pod "downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009779961s
STEP: Saw pod success
Apr 11 17:58:39.862: INFO: Pod "downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:58:39.866: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 17:58:39.907: INFO: Waiting for pod downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:58:39.911: INFO: Pod downward-api-6ea6e167-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:58:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zfx26" for this suite.
Apr 11 17:58:45.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:58:45.976: INFO: namespace: e2e-tests-downward-api-zfx26, resource: bindings, ignored listing per whitelist
Apr 11 17:58:46.046: INFO: namespace e2e-tests-downward-api-zfx26 deletion completed in 6.128886363s

â€¢ [SLOW TEST:8.407 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:58:46.048: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-75lx9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-73aad140-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating configMap with name cm-test-opt-upd-73aad177-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-73aad140-5c83-11e9-bcf1-d217e8a0130c
STEP: Updating configmap cm-test-opt-upd-73aad177-5c83-11e9-bcf1-d217e8a0130c
STEP: Creating configMap with name cm-test-opt-create-73aad18f-5c83-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:58:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-75lx9" for this suite.
Apr 11 17:59:12.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:59:12.516: INFO: namespace: e2e-tests-projected-75lx9, resource: bindings, ignored listing per whitelist
Apr 11 17:59:12.539: INFO: namespace e2e-tests-projected-75lx9 deletion completed in 22.124986023s

â€¢ [SLOW TEST:26.492 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:59:12.543: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rmcdg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 11 17:59:12.756: INFO: Waiting up to 5m0s for pod "pod-837517e4-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-rmcdg" to be "success or failure"
Apr 11 17:59:12.761: INFO: Pod "pod-837517e4-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356271ms
Apr 11 17:59:14.772: INFO: Pod "pod-837517e4-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015414903s
STEP: Saw pod success
Apr 11 17:59:14.772: INFO: Pod "pod-837517e4-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 17:59:14.780: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-837517e4-5c83-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 17:59:14.817: INFO: Waiting for pod pod-837517e4-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 17:59:14.822: INFO: Pod pod-837517e4-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:59:14.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rmcdg" for this suite.
Apr 11 17:59:20.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:59:20.862: INFO: namespace: e2e-tests-emptydir-rmcdg, resource: bindings, ignored listing per whitelist
Apr 11 17:59:20.961: INFO: namespace e2e-tests-emptydir-rmcdg deletion completed in 6.13400345s

â€¢ [SLOW TEST:8.419 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:59:20.966: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-n6mdn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-n6mdn
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-n6mdn
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-n6mdn
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-n6mdn
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-n6mdn
Apr 11 17:59:23.232: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n6mdn, name: ss-0, uid: 889da82a-5c83-11e9-ab42-fa163e758ab4, status phase: Pending. Waiting for statefulset controller to delete.
Apr 11 17:59:23.670: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n6mdn, name: ss-0, uid: 889da82a-5c83-11e9-ab42-fa163e758ab4, status phase: Failed. Waiting for statefulset controller to delete.
Apr 11 17:59:23.683: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n6mdn, name: ss-0, uid: 889da82a-5c83-11e9-ab42-fa163e758ab4, status phase: Failed. Waiting for statefulset controller to delete.
Apr 11 17:59:23.692: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-n6mdn
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-n6mdn
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-n6mdn and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 11 17:59:27.728: INFO: Deleting all statefulset in ns e2e-tests-statefulset-n6mdn
Apr 11 17:59:27.732: INFO: Scaling statefulset ss to 0
Apr 11 17:59:37.758: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 17:59:37.762: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 17:59:37.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-n6mdn" for this suite.
Apr 11 17:59:43.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 17:59:43.859: INFO: namespace: e2e-tests-statefulset-n6mdn, resource: bindings, ignored listing per whitelist
Apr 11 17:59:43.908: INFO: namespace e2e-tests-statefulset-n6mdn deletion completed in 6.123526998s

â€¢ [SLOW TEST:22.942 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 17:59:43.912: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-c8zpk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-dnpp
STEP: Creating a pod to test atomic-volume-subpath
Apr 11 17:59:44.140: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dnpp" in namespace "e2e-tests-subpath-c8zpk" to be "success or failure"
Apr 11 17:59:44.147: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005663ms
Apr 11 17:59:46.153: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013166268s
Apr 11 17:59:48.166: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 4.025734047s
Apr 11 17:59:50.171: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 6.031216237s
Apr 11 17:59:52.177: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 8.036765584s
Apr 11 17:59:54.183: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 10.042824493s
Apr 11 17:59:56.189: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 12.048488209s
Apr 11 17:59:58.201: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 14.061059641s
Apr 11 18:00:00.207: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 16.066825143s
Apr 11 18:00:02.213: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 18.073056154s
Apr 11 18:00:04.219: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 20.078919974s
Apr 11 18:00:06.225: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Running", Reason="", readiness=false. Elapsed: 22.084398236s
Apr 11 18:00:08.238: INFO: Pod "pod-subpath-test-configmap-dnpp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.097282425s
STEP: Saw pod success
Apr 11 18:00:08.238: INFO: Pod "pod-subpath-test-configmap-dnpp" satisfied condition "success or failure"
Apr 11 18:00:08.242: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-subpath-test-configmap-dnpp container test-container-subpath-configmap-dnpp: <nil>
STEP: delete the pod
Apr 11 18:00:08.278: INFO: Waiting for pod pod-subpath-test-configmap-dnpp to disappear
Apr 11 18:00:08.282: INFO: Pod pod-subpath-test-configmap-dnpp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dnpp
Apr 11 18:00:08.282: INFO: Deleting pod "pod-subpath-test-configmap-dnpp" in namespace "e2e-tests-subpath-c8zpk"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:00:08.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-c8zpk" for this suite.
Apr 11 18:00:14.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:00:14.350: INFO: namespace: e2e-tests-subpath-c8zpk, resource: bindings, ignored listing per whitelist
Apr 11 18:00:14.415: INFO: namespace e2e-tests-subpath-c8zpk deletion completed in 6.122785067s

â€¢ [SLOW TEST:30.504 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:00:14.418: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mj589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Apr 11 18:00:19.203: INFO: Successfully updated pod "annotationupdatea8589b30-5c83-11e9-bcf1-d217e8a0130c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:00:21.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mj589" for this suite.
Apr 11 18:00:37.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:00:37.377: INFO: namespace: e2e-tests-projected-mj589, resource: bindings, ignored listing per whitelist
Apr 11 18:00:37.380: INFO: namespace e2e-tests-projected-mj589 deletion completed in 16.148467307s

â€¢ [SLOW TEST:22.962 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:00:37.380: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5tm2p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 11 18:00:37.603: INFO: Waiting up to 5m0s for pod "pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-5tm2p" to be "success or failure"
Apr 11 18:00:37.607: INFO: Pod "pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544659ms
Apr 11 18:00:39.622: INFO: Pod "pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018716439s
Apr 11 18:00:41.627: INFO: Pod "pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024086607s
STEP: Saw pod success
Apr 11 18:00:41.627: INFO: Pod "pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:00:41.631: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:00:41.662: INFO: Waiting for pod pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:00:41.666: INFO: Pod pod-b6072e1d-5c83-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:00:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5tm2p" for this suite.
Apr 11 18:00:47.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:00:47.792: INFO: namespace: e2e-tests-emptydir-5tm2p, resource: bindings, ignored listing per whitelist
Apr 11 18:00:47.798: INFO: namespace e2e-tests-emptydir-5tm2p deletion completed in 6.126026729s

â€¢ [SLOW TEST:10.418 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:00:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-qzk6k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:01:13.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-qzk6k" for this suite.
Apr 11 18:01:19.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:01:19.507: INFO: namespace: e2e-tests-container-runtime-qzk6k, resource: bindings, ignored listing per whitelist
Apr 11 18:01:19.564: INFO: namespace e2e-tests-container-runtime-qzk6k deletion completed in 6.124962137s

â€¢ [SLOW TEST:31.764 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:01:19.568: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kcsgs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 11 18:01:19.819: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-kcsgs,SelfLink:/api/v1/namespaces/e2e-tests-watch-kcsgs/configmaps/e2e-watch-test-resource-version,UID:cf2c234a-5c83-11e9-8000-fa163e4b6765,ResourceVersion:26266,Generation:0,CreationTimestamp:2019-04-11 18:01:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 11 18:01:19.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-kcsgs,SelfLink:/api/v1/namespaces/e2e-tests-watch-kcsgs/configmaps/e2e-watch-test-resource-version,UID:cf2c234a-5c83-11e9-8000-fa163e4b6765,ResourceVersion:26267,Generation:0,CreationTimestamp:2019-04-11 18:01:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:01:19.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kcsgs" for this suite.
Apr 11 18:01:25.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:01:25.916: INFO: namespace: e2e-tests-watch-kcsgs, resource: bindings, ignored listing per whitelist
Apr 11 18:01:25.961: INFO: namespace e2e-tests-watch-kcsgs deletion completed in 6.137290198s

â€¢ [SLOW TEST:6.394 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:01:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-z69kf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-r2vl9
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-vhkg9
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:01:32.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-z69kf" for this suite.
Apr 11 18:01:38.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:01:38.586: INFO: namespace: e2e-tests-namespaces-z69kf, resource: bindings, ignored listing per whitelist
Apr 11 18:01:38.651: INFO: namespace e2e-tests-namespaces-z69kf deletion completed in 6.12337613s
STEP: Destroying namespace "e2e-tests-nsdeletetest-r2vl9" for this suite.
Apr 11 18:01:38.654: INFO: Namespace e2e-tests-nsdeletetest-r2vl9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-vhkg9" for this suite.
Apr 11 18:01:44.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:01:44.746: INFO: namespace: e2e-tests-nsdeletetest-vhkg9, resource: bindings, ignored listing per whitelist
Apr 11 18:01:44.807: INFO: namespace e2e-tests-nsdeletetest-vhkg9 deletion completed in 6.153124674s

â€¢ [SLOW TEST:18.844 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:01:44.810: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-vcw2v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-vcw2v
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-vcw2v
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-vcw2v
Apr 11 18:01:45.039: INFO: Found 0 stateful pods, waiting for 1
Apr 11 18:01:55.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 11 18:01:55.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 18:01:55.296: INFO: stderr: ""
Apr 11 18:01:55.297: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 18:01:55.297: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 18:01:55.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 11 18:02:05.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 18:02:05.315: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 18:02:05.333: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:05.334: INFO: ss-0  cluster1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:05.334: INFO: 
Apr 11 18:02:05.334: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 11 18:02:06.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993375117s
Apr 11 18:02:07.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986283399s
Apr 11 18:02:08.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980123378s
Apr 11 18:02:09.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973858961s
Apr 11 18:02:10.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960900694s
Apr 11 18:02:11.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954644553s
Apr 11 18:02:12.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948452649s
Apr 11 18:02:13.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.942486992s
Apr 11 18:02:14.396: INFO: Verifying statefulset ss doesn't scale past 3 for another 936.863438ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-vcw2v
Apr 11 18:02:15.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 18:02:15.652: INFO: stderr: ""
Apr 11 18:02:15.652: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 18:02:15.652: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 18:02:15.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 18:02:15.894: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Apr 11 18:02:15.894: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 18:02:15.894: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 18:02:15.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 11 18:02:16.316: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Apr 11 18:02:16.316: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 11 18:02:16.316: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 11 18:02:16.322: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 11 18:02:26.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 18:02:26.335: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 11 18:02:26.335: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 11 18:02:26.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 18:02:26.557: INFO: stderr: ""
Apr 11 18:02:26.557: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 18:02:26.557: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 18:02:26.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 18:02:26.934: INFO: stderr: ""
Apr 11 18:02:26.934: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 18:02:26.934: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 18:02:26.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 exec --namespace=e2e-tests-statefulset-vcw2v ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 11 18:02:27.153: INFO: stderr: ""
Apr 11 18:02:27.153: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 11 18:02:27.153: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 11 18:02:27.153: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 18:02:27.157: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 11 18:02:37.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 18:02:37.173: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 18:02:37.173: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 11 18:02:37.188: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:37.188: INFO: ss-0  cluster1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:37.188: INFO: ss-1  cluster1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:37.188: INFO: ss-2  cluster1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:37.188: INFO: 
Apr 11 18:02:37.188: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:38.194: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:38.194: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:38.194: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:38.194: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:38.194: INFO: 
Apr 11 18:02:38.194: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:39.201: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:39.201: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:39.201: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:39.201: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:39.201: INFO: 
Apr 11 18:02:39.201: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:40.207: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:40.207: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:40.207: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:40.207: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:40.207: INFO: 
Apr 11 18:02:40.207: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:41.213: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:41.213: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:41.213: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:41.213: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:41.213: INFO: 
Apr 11 18:02:41.213: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:42.219: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:42.219: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:42.219: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:42.219: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:42.219: INFO: 
Apr 11 18:02:42.219: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:43.225: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Apr 11 18:02:43.225: INFO: ss-0  cluster1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:01:45 +0000 UTC  }]
Apr 11 18:02:43.225: INFO: ss-1  cluster1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:43.225: INFO: ss-2  cluster1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:02:05 +0000 UTC  }]
Apr 11 18:02:43.225: INFO: 
Apr 11 18:02:43.225: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 11 18:02:44.231: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.956406457s
Apr 11 18:02:45.237: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.950285925s
Apr 11 18:02:46.243: INFO: Verifying statefulset ss doesn't scale past 0 for another 944.71035ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-vcw2v
Apr 11 18:02:47.254: INFO: Scaling statefulset ss to 0
Apr 11 18:02:47.265: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 11 18:02:47.268: INFO: Deleting all statefulset in ns e2e-tests-statefulset-vcw2v
Apr 11 18:02:47.271: INFO: Scaling statefulset ss to 0
Apr 11 18:02:47.283: INFO: Waiting for statefulset status.replicas updated to 0
Apr 11 18:02:47.285: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:02:47.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-vcw2v" for this suite.
Apr 11 18:02:53.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:02:53.375: INFO: namespace: e2e-tests-statefulset-vcw2v, resource: bindings, ignored listing per whitelist
Apr 11 18:02:53.455: INFO: namespace e2e-tests-statefulset-vcw2v deletion completed in 6.148934121s

â€¢ [SLOW TEST:68.646 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:02:53.459: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5fwvt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Apr 11 18:02:53.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 cluster-info'
Apr 11 18:02:53.774: INFO: stderr: ""
Apr 11 18:02:53.774: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:02:53.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5fwvt" for this suite.
Apr 11 18:02:59.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:02:59.892: INFO: namespace: e2e-tests-kubectl-5fwvt, resource: bindings, ignored listing per whitelist
Apr 11 18:02:59.900: INFO: namespace e2e-tests-kubectl-5fwvt deletion completed in 6.121666546s

â€¢ [SLOW TEST:6.442 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:02:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-mtjt5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:03:00.162: INFO: (0) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.071592ms)
Apr 11 18:03:00.167: INFO: (1) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.152707ms)
Apr 11 18:03:00.171: INFO: (2) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.806144ms)
Apr 11 18:03:00.175: INFO: (3) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.702582ms)
Apr 11 18:03:00.182: INFO: (4) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.126925ms)
Apr 11 18:03:00.186: INFO: (5) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.760533ms)
Apr 11 18:03:00.190: INFO: (6) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.958874ms)
Apr 11 18:03:00.194: INFO: (7) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.030624ms)
Apr 11 18:03:00.198: INFO: (8) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.950127ms)
Apr 11 18:03:00.202: INFO: (9) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.677551ms)
Apr 11 18:03:00.206: INFO: (10) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.80287ms)
Apr 11 18:03:00.209: INFO: (11) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.780321ms)
Apr 11 18:03:00.213: INFO: (12) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.71603ms)
Apr 11 18:03:00.217: INFO: (13) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.677109ms)
Apr 11 18:03:00.221: INFO: (14) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.549007ms)
Apr 11 18:03:00.224: INFO: (15) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.792632ms)
Apr 11 18:03:00.228: INFO: (16) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.88832ms)
Apr 11 18:03:00.232: INFO: (17) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.747155ms)
Apr 11 18:03:00.236: INFO: (18) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.991015ms)
Apr 11 18:03:00.240: INFO: (19) /api/v1/nodes/cluster1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.713832ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:03:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-mtjt5" for this suite.
Apr 11 18:03:06.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:03:06.329: INFO: namespace: e2e-tests-proxy-mtjt5, resource: bindings, ignored listing per whitelist
Apr 11 18:03:06.370: INFO: namespace e2e-tests-proxy-mtjt5 deletion completed in 6.126420889s

â€¢ [SLOW TEST:6.467 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:03:06.374: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-w7sf6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 11 18:03:06.610: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:06.611: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:06.611: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:06.615: INFO: Number of nodes with available pods: 0
Apr 11 18:03:06.615: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 18:03:07.621: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:07.622: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:07.622: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:07.630: INFO: Number of nodes with available pods: 0
Apr 11 18:03:07.630: INFO: Node cluster1-k8s-node-1 is running more than one daemon pod
Apr 11 18:03:08.628: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.629: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.629: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.633: INFO: Number of nodes with available pods: 3
Apr 11 18:03:08.634: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 11 18:03:08.662: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.663: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.663: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:08.671: INFO: Number of nodes with available pods: 2
Apr 11 18:03:08.671: INFO: Node cluster1-k8s-node-2 is running more than one daemon pod
Apr 11 18:03:09.678: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:09.678: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:09.678: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:09.683: INFO: Number of nodes with available pods: 2
Apr 11 18:03:09.683: INFO: Node cluster1-k8s-node-2 is running more than one daemon pod
Apr 11 18:03:10.676: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:10.677: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:10.677: INFO: DaemonSet pods can't tolerate node cluster1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 11 18:03:10.682: INFO: Number of nodes with available pods: 3
Apr 11 18:03:10.682: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-w7sf6, will wait for the garbage collector to delete the pods
Apr 11 18:03:10.755: INFO: Deleting DaemonSet.extensions daemon-set took: 9.362215ms
Apr 11 18:03:10.855: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.315933ms
Apr 11 18:03:53.766: INFO: Number of nodes with available pods: 0
Apr 11 18:03:53.767: INFO: Number of running nodes: 0, number of available pods: 0
Apr 11 18:03:53.769: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w7sf6/daemonsets","resourceVersion":"26969"},"items":null}

Apr 11 18:03:53.775: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w7sf6/pods","resourceVersion":"26969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:03:53.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w7sf6" for this suite.
Apr 11 18:03:59.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:03:59.819: INFO: namespace: e2e-tests-daemonsets-w7sf6, resource: bindings, ignored listing per whitelist
Apr 11 18:03:59.917: INFO: namespace e2e-tests-daemonsets-w7sf6 deletion completed in 6.123230436s

â€¢ [SLOW TEST:53.543 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:03:59.921: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-58z4p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-58z4p;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-58z4p.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-58z4p.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.0.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.0.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.0.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.0.109_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-58z4p;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-58z4p;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-58z4p.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-58z4p.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-58z4p.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-58z4p.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-58z4p.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-58z4p.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.0.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.0.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.0.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.0.109_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 11 18:04:02.170: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.177: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.182: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-58z4p from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.188: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.195: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-58z4p.svc from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.200: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p.svc from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.246: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.251: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.257: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-58z4p from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.262: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-58z4p from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.266: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-58z4p.svc from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.274: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-58z4p.svc from pod e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c: the server could not find the requested resource (get pods dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c)
Apr 11 18:04:02.320: INFO: Lookups using e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-58z4p wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p wheezy_udp@dns-test-service.e2e-tests-dns-58z4p.svc wheezy_tcp@dns-test-service.e2e-tests-dns-58z4p.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-58z4p jessie_tcp@dns-test-service.e2e-tests-dns-58z4p jessie_udp@dns-test-service.e2e-tests-dns-58z4p.svc jessie_tcp@dns-test-service.e2e-tests-dns-58z4p.svc]

Apr 11 18:04:07.485: INFO: DNS probes using e2e-tests-dns-58z4p/dns-test-2ec0bc46-5c84-11e9-bcf1-d217e8a0130c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:04:07.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-58z4p" for this suite.
Apr 11 18:04:13.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:04:13.620: INFO: namespace: e2e-tests-dns-58z4p, resource: bindings, ignored listing per whitelist
Apr 11 18:04:13.696: INFO: namespace e2e-tests-dns-58z4p deletion completed in 6.134993324s

â€¢ [SLOW TEST:13.775 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:04:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8dzqk
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-36f5dd22-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-36f5dd22-5c84-11e9-bcf1-d217e8a0130c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:04:17.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8dzqk" for this suite.
Apr 11 18:04:40.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:04:40.033: INFO: namespace: e2e-tests-projected-8dzqk, resource: bindings, ignored listing per whitelist
Apr 11 18:04:40.127: INFO: namespace e2e-tests-projected-8dzqk deletion completed in 22.130316776s

â€¢ [SLOW TEST:26.430 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:04:40.129: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sbcjd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-46b623e4-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:04:40.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-sbcjd" to be "success or failure"
Apr 11 18:04:40.365: INFO: Pod "pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.72601ms
Apr 11 18:04:42.370: INFO: Pod "pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017763894s
STEP: Saw pod success
Apr 11 18:04:42.371: INFO: Pod "pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:04:42.375: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 18:04:42.409: INFO: Waiting for pod pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:04:42.413: INFO: Pod pod-projected-configmaps-46b7b713-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:04:42.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sbcjd" for this suite.
Apr 11 18:04:48.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:04:48.531: INFO: namespace: e2e-tests-projected-sbcjd, resource: bindings, ignored listing per whitelist
Apr 11 18:04:48.541: INFO: namespace e2e-tests-projected-sbcjd deletion completed in 6.12283862s

â€¢ [SLOW TEST:8.413 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:04:48.547: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-mqmjk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-mqmjk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 11 18:04:48.738: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 11 18:05:10.852: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.120:8080/dial?request=hostName&protocol=udp&host=10.233.86.117&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mqmjk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 18:05:10.852: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 18:05:10.998: INFO: Waiting for endpoints: map[]
Apr 11 18:05:11.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.120:8080/dial?request=hostName&protocol=udp&host=10.233.87.119&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mqmjk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 18:05:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 18:05:11.115: INFO: Waiting for endpoints: map[]
Apr 11 18:05:11.122: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.120:8080/dial?request=hostName&protocol=udp&host=10.233.85.34&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mqmjk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 11 18:05:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
Apr 11 18:05:11.244: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:05:11.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-mqmjk" for this suite.
Apr 11 18:05:33.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:05:33.304: INFO: namespace: e2e-tests-pod-network-test-mqmjk, resource: bindings, ignored listing per whitelist
Apr 11 18:05:33.372: INFO: namespace e2e-tests-pod-network-test-mqmjk deletion completed in 22.122039588s

â€¢ [SLOW TEST:44.826 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:05:33.376: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4bnl5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-6675451d-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:05:33.611: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-4bnl5" to be "success or failure"
Apr 11 18:05:33.616: INFO: Pod "pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061669ms
Apr 11 18:05:35.633: INFO: Pod "pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021387781s
Apr 11 18:05:37.638: INFO: Pod "pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026631204s
STEP: Saw pod success
Apr 11 18:05:37.638: INFO: Pod "pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:05:37.642: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 18:05:37.694: INFO: Waiting for pod pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:05:37.702: INFO: Pod pod-projected-configmaps-66765f97-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:05:37.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4bnl5" for this suite.
Apr 11 18:05:43.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:05:43.757: INFO: namespace: e2e-tests-projected-4bnl5, resource: bindings, ignored listing per whitelist
Apr 11 18:05:43.859: INFO: namespace e2e-tests-projected-4bnl5 deletion completed in 6.151496866s

â€¢ [SLOW TEST:10.484 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:05:43.860: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-dgvsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Apr 11 18:05:44.059: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:05:47.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-dgvsr" for this suite.
Apr 11 18:06:09.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:06:09.983: INFO: namespace: e2e-tests-init-container-dgvsr, resource: bindings, ignored listing per whitelist
Apr 11 18:06:09.986: INFO: namespace e2e-tests-init-container-dgvsr deletion completed in 22.135604898s

â€¢ [SLOW TEST:26.127 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:06:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8swn2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 11 18:06:10.209: INFO: Waiting up to 5m0s for pod "pod-7c472217-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-8swn2" to be "success or failure"
Apr 11 18:06:10.217: INFO: Pod "pod-7c472217-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.380158ms
Apr 11 18:06:12.223: INFO: Pod "pod-7c472217-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013974025s
STEP: Saw pod success
Apr 11 18:06:12.224: INFO: Pod "pod-7c472217-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:06:12.228: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-7c472217-5c84-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:06:12.271: INFO: Waiting for pod pod-7c472217-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:06:12.275: INFO: Pod pod-7c472217-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:06:12.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8swn2" for this suite.
Apr 11 18:06:18.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:06:18.379: INFO: namespace: e2e-tests-emptydir-8swn2, resource: bindings, ignored listing per whitelist
Apr 11 18:06:18.426: INFO: namespace e2e-tests-emptydir-8swn2 deletion completed in 6.140795271s

â€¢ [SLOW TEST:8.437 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:06:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-6x5pr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0411 18:06:19.699162      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 18:06:19.699: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:06:19.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6x5pr" for this suite.
Apr 11 18:06:25.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:06:25.814: INFO: namespace: e2e-tests-gc-6x5pr, resource: bindings, ignored listing per whitelist
Apr 11 18:06:25.826: INFO: namespace e2e-tests-gc-6x5pr deletion completed in 6.121919493s

â€¢ [SLOW TEST:7.395 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:06:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-p62xb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-p62xb
Apr 11 18:06:28.064: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-p62xb
STEP: checking the pod's current state and verifying that restartCount is present
Apr 11 18:06:28.069: INFO: Initial restart count of pod liveness-exec is 0
Apr 11 18:07:14.245: INFO: Restart count of pod e2e-tests-container-probe-p62xb/liveness-exec is now 1 (46.176505625s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:14.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-p62xb" for this suite.
Apr 11 18:07:20.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:07:20.391: INFO: namespace: e2e-tests-container-probe-p62xb, resource: bindings, ignored listing per whitelist
Apr 11 18:07:20.436: INFO: namespace e2e-tests-container-probe-p62xb deletion completed in 6.140274216s

â€¢ [SLOW TEST:54.608 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:07:20.437: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-brwqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 11 18:07:20.656: INFO: Waiting up to 5m0s for pod "pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-brwqp" to be "success or failure"
Apr 11 18:07:20.662: INFO: Pod "pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.21648ms
Apr 11 18:07:22.674: INFO: Pod "pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018076174s
STEP: Saw pod success
Apr 11 18:07:22.675: INFO: Pod "pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:07:22.679: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:07:22.712: INFO: Waiting for pod pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:07:22.716: INFO: Pod pod-a64473fd-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-brwqp" for this suite.
Apr 11 18:07:28.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:07:28.750: INFO: namespace: e2e-tests-emptydir-brwqp, resource: bindings, ignored listing per whitelist
Apr 11 18:07:28.842: INFO: namespace e2e-tests-emptydir-brwqp deletion completed in 6.121363865s

â€¢ [SLOW TEST:8.406 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:07:28.846: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jfmdm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-jfmdm/configmap-test-ab4525cb-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:07:29.056: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-jfmdm" to be "success or failure"
Apr 11 18:07:29.062: INFO: Pod "pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.791973ms
Apr 11 18:07:31.068: INFO: Pod "pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011346187s
STEP: Saw pod success
Apr 11 18:07:31.068: INFO: Pod "pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:07:31.072: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c container env-test: <nil>
STEP: delete the pod
Apr 11 18:07:31.103: INFO: Waiting for pod pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:07:31.107: INFO: Pod pod-configmaps-ab463e53-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:31.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jfmdm" for this suite.
Apr 11 18:07:37.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:07:37.233: INFO: namespace: e2e-tests-configmap-jfmdm, resource: bindings, ignored listing per whitelist
Apr 11 18:07:37.239: INFO: namespace e2e-tests-configmap-jfmdm deletion completed in 6.1260714s

â€¢ [SLOW TEST:8.393 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:07:37.241: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-prk6f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 18:07:37.458: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-prk6f" to be "success or failure"
Apr 11 18:07:37.467: INFO: Pod "downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.504078ms
Apr 11 18:07:39.473: INFO: Pod "downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014214287s
STEP: Saw pod success
Apr 11 18:07:39.473: INFO: Pod "downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:07:39.477: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 18:07:39.513: INFO: Waiting for pod downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:07:39.518: INFO: Pod downwardapi-volume-b047d62a-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:39.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-prk6f" for this suite.
Apr 11 18:07:45.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:07:45.638: INFO: namespace: e2e-tests-downward-api-prk6f, resource: bindings, ignored listing per whitelist
Apr 11 18:07:45.653: INFO: namespace e2e-tests-downward-api-prk6f deletion completed in 6.129622392s

â€¢ [SLOW TEST:8.413 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:07:45.654: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wjpqm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 18:07:45.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-wjpqm" to be "success or failure"
Apr 11 18:07:45.868: INFO: Pod "downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.445581ms
Apr 11 18:07:47.874: INFO: Pod "downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010197394s
STEP: Saw pod success
Apr 11 18:07:47.874: INFO: Pod "downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:07:47.879: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 18:07:47.910: INFO: Waiting for pod downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:07:47.914: INFO: Pod downwardapi-volume-b54ad058-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:47.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wjpqm" for this suite.
Apr 11 18:07:53.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:07:53.974: INFO: namespace: e2e-tests-downward-api-wjpqm, resource: bindings, ignored listing per whitelist
Apr 11 18:07:54.045: INFO: namespace e2e-tests-downward-api-wjpqm deletion completed in 6.1253624s

â€¢ [SLOW TEST:8.391 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:07:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qjgql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Apr 11 18:07:54.253: INFO: Waiting up to 5m0s for pod "downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-downward-api-qjgql" to be "success or failure"
Apr 11 18:07:54.258: INFO: Pod "downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.039766ms
Apr 11 18:07:56.264: INFO: Pod "downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010498723s
STEP: Saw pod success
Apr 11 18:07:56.264: INFO: Pod "downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:07:56.268: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 18:07:56.306: INFO: Waiting for pod downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:07:56.312: INFO: Pod downward-api-ba4aafc9-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:07:56.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qjgql" for this suite.
Apr 11 18:08:02.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:08:02.408: INFO: namespace: e2e-tests-downward-api-qjgql, resource: bindings, ignored listing per whitelist
Apr 11 18:08:02.449: INFO: namespace e2e-tests-downward-api-qjgql deletion completed in 6.131627568s

â€¢ [SLOW TEST:8.403 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:08:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-ggtbk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Apr 11 18:08:02.663: INFO: Waiting up to 5m0s for pod "var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-var-expansion-ggtbk" to be "success or failure"
Apr 11 18:08:02.668: INFO: Pod "var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.605401ms
Apr 11 18:08:04.680: INFO: Pod "var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016705231s
STEP: Saw pod success
Apr 11 18:08:04.680: INFO: Pod "var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:08:04.684: INFO: Trying to get logs from node cluster1-k8s-node-1 pod var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 18:08:04.718: INFO: Waiting for pod var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:08:04.723: INFO: Pod var-expansion-bf4e4de2-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:08:04.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-ggtbk" for this suite.
Apr 11 18:08:10.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:08:10.787: INFO: namespace: e2e-tests-var-expansion-ggtbk, resource: bindings, ignored listing per whitelist
Apr 11 18:08:10.861: INFO: namespace e2e-tests-var-expansion-ggtbk deletion completed in 6.132917453s

â€¢ [SLOW TEST:8.409 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:08:10.865: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-s2x68
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Apr 11 18:08:11.593: INFO: Waiting up to 5m0s for pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8" in namespace "e2e-tests-svcaccounts-s2x68" to be "success or failure"
Apr 11 18:08:11.597: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124344ms
Apr 11 18:08:13.603: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009776659s
STEP: Saw pod success
Apr 11 18:08:13.603: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8" satisfied condition "success or failure"
Apr 11 18:08:13.607: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8 container token-test: <nil>
STEP: delete the pod
Apr 11 18:08:13.650: INFO: Waiting for pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8 to disappear
Apr 11 18:08:13.654: INFO: Pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-jzmp8 no longer exists
STEP: Creating a pod to test consume service account root CA
Apr 11 18:08:13.683: INFO: Waiting up to 5m0s for pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc" in namespace "e2e-tests-svcaccounts-s2x68" to be "success or failure"
Apr 11 18:08:13.694: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.273836ms
Apr 11 18:08:15.708: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025621206s
STEP: Saw pod success
Apr 11 18:08:15.708: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc" satisfied condition "success or failure"
Apr 11 18:08:15.713: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc container root-ca-test: <nil>
STEP: delete the pod
Apr 11 18:08:15.753: INFO: Waiting for pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc to disappear
Apr 11 18:08:15.759: INFO: Pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-kkbwc no longer exists
STEP: Creating a pod to test consume service account namespace
Apr 11 18:08:15.769: INFO: Waiting up to 5m0s for pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c" in namespace "e2e-tests-svcaccounts-s2x68" to be "success or failure"
Apr 11 18:08:15.776: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.341684ms
Apr 11 18:08:17.781: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011676886s
Apr 11 18:08:19.786: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0170283s
STEP: Saw pod success
Apr 11 18:08:19.786: INFO: Pod "pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c" satisfied condition "success or failure"
Apr 11 18:08:19.790: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c container namespace-test: <nil>
STEP: delete the pod
Apr 11 18:08:19.824: INFO: Waiting for pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c to disappear
Apr 11 18:08:19.831: INFO: Pod pod-service-account-c4a0ff55-5c84-11e9-bcf1-d217e8a0130c-vft2c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:08:19.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-s2x68" for this suite.
Apr 11 18:08:25.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:08:25.867: INFO: namespace: e2e-tests-svcaccounts-s2x68, resource: bindings, ignored listing per whitelist
Apr 11 18:08:25.973: INFO: namespace e2e-tests-svcaccounts-s2x68 deletion completed in 6.13720155s

â€¢ [SLOW TEST:15.109 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:08:25.975: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-xtg47
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-ks6w
STEP: Creating a pod to test atomic-volume-subpath
Apr 11 18:08:26.210: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ks6w" in namespace "e2e-tests-subpath-xtg47" to be "success or failure"
Apr 11 18:08:26.215: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243587ms
Apr 11 18:08:28.221: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01123282s
Apr 11 18:08:30.227: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 4.017201317s
Apr 11 18:08:32.233: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 6.0229212s
Apr 11 18:08:34.238: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 8.028560992s
Apr 11 18:08:36.251: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 10.041756796s
Apr 11 18:08:38.257: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 12.04733266s
Apr 11 18:08:40.314: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 14.10464769s
Apr 11 18:08:42.320: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 16.110177002s
Apr 11 18:08:44.325: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 18.115327881s
Apr 11 18:08:46.337: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 20.127526061s
Apr 11 18:08:48.343: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Running", Reason="", readiness=false. Elapsed: 22.133457421s
Apr 11 18:08:50.349: INFO: Pod "pod-subpath-test-secret-ks6w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.139203933s
STEP: Saw pod success
Apr 11 18:08:50.349: INFO: Pod "pod-subpath-test-secret-ks6w" satisfied condition "success or failure"
Apr 11 18:08:50.354: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-subpath-test-secret-ks6w container test-container-subpath-secret-ks6w: <nil>
STEP: delete the pod
Apr 11 18:08:50.429: INFO: Waiting for pod pod-subpath-test-secret-ks6w to disappear
Apr 11 18:08:50.433: INFO: Pod pod-subpath-test-secret-ks6w no longer exists
STEP: Deleting pod pod-subpath-test-secret-ks6w
Apr 11 18:08:50.433: INFO: Deleting pod "pod-subpath-test-secret-ks6w" in namespace "e2e-tests-subpath-xtg47"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:08:50.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-xtg47" for this suite.
Apr 11 18:08:56.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:08:56.477: INFO: namespace: e2e-tests-subpath-xtg47, resource: bindings, ignored listing per whitelist
Apr 11 18:08:56.581: INFO: namespace e2e-tests-subpath-xtg47 deletion completed in 6.136829459s

â€¢ [SLOW TEST:30.606 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:08:56.585: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8z7jl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-df929eee-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:08:56.806: INFO: Waiting up to 5m0s for pod "pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-8z7jl" to be "success or failure"
Apr 11 18:08:56.811: INFO: Pod "pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.357592ms
Apr 11 18:08:58.817: INFO: Pod "pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010424274s
STEP: Saw pod success
Apr 11 18:08:58.817: INFO: Pod "pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:08:58.822: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 18:08:58.864: INFO: Waiting for pod pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:08:58.870: INFO: Pod pod-configmaps-df93b3c7-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:08:58.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8z7jl" for this suite.
Apr 11 18:09:04.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:09:04.922: INFO: namespace: e2e-tests-configmap-8z7jl, resource: bindings, ignored listing per whitelist
Apr 11 18:09:05.001: INFO: namespace e2e-tests-configmap-8z7jl deletion completed in 6.1245738s

â€¢ [SLOW TEST:8.416 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:09:05.004: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7vhwv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Apr 11 18:09:05.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 create -f - --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:05.515: INFO: stderr: ""
Apr 11 18:09:05.515: INFO: stdout: "pod/pause created\n"
Apr 11 18:09:05.515: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 11 18:09:05.515: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-7vhwv" to be "running and ready"
Apr 11 18:09:05.523: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.622614ms
Apr 11 18:09:07.535: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019818582s
Apr 11 18:09:07.535: INFO: Pod "pause" satisfied condition "running and ready"
Apr 11 18:09:07.535: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 11 18:09:07.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:07.680: INFO: stderr: ""
Apr 11 18:09:07.680: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 11 18:09:07.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pod pause -L testing-label --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:07.814: INFO: stderr: ""
Apr 11 18:09:07.814: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 11 18:09:07.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 label pods pause testing-label- --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:07.932: INFO: stderr: ""
Apr 11 18:09:07.932: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 11 18:09:07.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pod pause -L testing-label --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:08.044: INFO: stderr: ""
Apr 11 18:09:08.044: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Apr 11 18:09:08.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:08.168: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 11 18:09:08.168: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 11 18:09:08.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-7vhwv'
Apr 11 18:09:08.358: INFO: stderr: "No resources found.\n"
Apr 11 18:09:08.358: INFO: stdout: ""
Apr 11 18:09:08.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 get pods -l name=pause --namespace=e2e-tests-kubectl-7vhwv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 11 18:09:08.468: INFO: stderr: ""
Apr 11 18:09:08.468: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:09:08.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7vhwv" for this suite.
Apr 11 18:09:14.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:09:14.536: INFO: namespace: e2e-tests-kubectl-7vhwv, resource: bindings, ignored listing per whitelist
Apr 11 18:09:14.595: INFO: namespace e2e-tests-kubectl-7vhwv deletion completed in 6.121683343s

â€¢ [SLOW TEST:9.591 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:09:14.600: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-g6d4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:09:14.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 version'
Apr 11 18:09:14.914: INFO: stderr: ""
Apr 11 18:09:14.914: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.5\", GitCommit:\"2166946f41b36dea2c4626f90a77706f426cdea2\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:19:22Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:09:14.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g6d4x" for this suite.
Apr 11 18:09:20.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:09:20.998: INFO: namespace: e2e-tests-kubectl-g6d4x, resource: bindings, ignored listing per whitelist
Apr 11 18:09:21.049: INFO: namespace e2e-tests-kubectl-g6d4x deletion completed in 6.129678769s

â€¢ [SLOW TEST:6.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:09:21.053: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-knq96
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-ee2abb4b-5c84-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 18:09:21.291: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-knq96" to be "success or failure"
Apr 11 18:09:21.298: INFO: Pod "pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064479ms
Apr 11 18:09:23.304: INFO: Pod "pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012756007s
STEP: Saw pod success
Apr 11 18:09:23.304: INFO: Pod "pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:09:23.308: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 11 18:09:23.342: INFO: Waiting for pod pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:09:23.345: INFO: Pod pod-projected-secrets-ee2b8aed-5c84-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:09:23.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-knq96" for this suite.
Apr 11 18:09:29.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:09:29.505: INFO: namespace: e2e-tests-projected-knq96, resource: bindings, ignored listing per whitelist
Apr 11 18:09:29.510: INFO: namespace e2e-tests-projected-knq96 deletion completed in 6.160463253s

â€¢ [SLOW TEST:8.458 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:09:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-65x8h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:09:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-65x8h" for this suite.
Apr 11 18:09:41.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:09:41.881: INFO: namespace: e2e-tests-emptydir-wrapper-65x8h, resource: bindings, ignored listing per whitelist
Apr 11 18:09:41.911: INFO: namespace e2e-tests-emptydir-wrapper-65x8h deletion completed in 6.126129035s

â€¢ [SLOW TEST:12.400 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:09:41.911: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-s8kqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:09:47.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-s8kqp" for this suite.
Apr 11 18:10:09.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:10:09.282: INFO: namespace: e2e-tests-replication-controller-s8kqp, resource: bindings, ignored listing per whitelist
Apr 11 18:10:09.305: INFO: namespace e2e-tests-replication-controller-s8kqp deletion completed in 22.142834495s

â€¢ [SLOW TEST:27.394 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:10:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-gmsjz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:10:09.508: INFO: Creating deployment "test-recreate-deployment"
Apr 11 18:10:09.514: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 11 18:10:09.521: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Apr 11 18:10:11.528: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 11 18:10:11.531: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 11 18:10:11.540: INFO: Updating deployment test-recreate-deployment
Apr 11 18:10:11.540: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 11 18:10:11.660: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-gmsjz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gmsjz/deployments/test-recreate-deployment,UID:0aeaf321-5c85-11e9-8000-fa163e4b6765,ResourceVersion:28874,Generation:2,CreationTimestamp:2019-04-11 18:10:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-11 18:10:11 +0000 UTC 2019-04-11 18:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-11 18:10:11 +0000 UTC 2019-04-11 18:10:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr 11 18:10:11.664: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-gmsjz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gmsjz/replicasets/test-recreate-deployment-697fbf54bf,UID:0c29667c-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:28871,Generation:1,CreationTimestamp:2019-04-11 18:10:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0aeaf321-5c85-11e9-8000-fa163e4b6765 0xc000a304b7 0xc000a304b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 18:10:11.664: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 11 18:10:11.664: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-gmsjz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gmsjz/replicasets/test-recreate-deployment-5dfdcc846d,UID:0aed4dfb-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:28862,Generation:2,CreationTimestamp:2019-04-11 18:10:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0aeaf321-5c85-11e9-8000-fa163e4b6765 0xc000a303f7 0xc000a303f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 18:10:11.669: INFO: Pod "test-recreate-deployment-697fbf54bf-frnls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-frnls,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-gmsjz,SelfLink:/api/v1/namespaces/e2e-tests-deployment-gmsjz/pods/test-recreate-deployment-697fbf54bf-frnls,UID:0c2aab3d-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:28873,Generation:0,CreationTimestamp:2019-04-11 18:10:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 0c29667c-5c85-11e9-ab42-fa163e758ab4 0xc000a30d37 0xc000a30d38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-w7p2s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w7p2s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w7p2s true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a30da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a30dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:10:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:10:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:10:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:10:11 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:10:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:10:11.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-gmsjz" for this suite.
Apr 11 18:10:17.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:10:17.755: INFO: namespace: e2e-tests-deployment-gmsjz, resource: bindings, ignored listing per whitelist
Apr 11 18:10:17.799: INFO: namespace e2e-tests-deployment-gmsjz deletion completed in 6.124852846s

â€¢ [SLOW TEST:8.492 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:10:17.801: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-wqtkx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Apr 11 18:10:18.007: INFO: Waiting up to 5m0s for pod "client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-containers-wqtkx" to be "success or failure"
Apr 11 18:10:18.015: INFO: Pod "client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.283243ms
Apr 11 18:10:20.020: INFO: Pod "client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012623948s
STEP: Saw pod success
Apr 11 18:10:20.020: INFO: Pod "client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:10:20.024: INFO: Trying to get logs from node cluster1-k8s-node-1 pod client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:10:20.055: INFO: Waiting for pod client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:10:20.060: INFO: Pod client-containers-0ff9e487-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:10:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wqtkx" for this suite.
Apr 11 18:10:26.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:10:26.121: INFO: namespace: e2e-tests-containers-wqtkx, resource: bindings, ignored listing per whitelist
Apr 11 18:10:26.186: INFO: namespace e2e-tests-containers-wqtkx deletion completed in 6.120878651s

â€¢ [SLOW TEST:8.385 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:10:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-znmq2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-14fadecb-5c85-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 18:10:26.411: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-znmq2" to be "success or failure"
Apr 11 18:10:26.418: INFO: Pod "pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.927755ms
Apr 11 18:10:28.424: INFO: Pod "pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012409069s
STEP: Saw pod success
Apr 11 18:10:28.424: INFO: Pod "pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:10:28.428: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 11 18:10:28.463: INFO: Waiting for pod pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:10:28.467: INFO: Pod pod-projected-secrets-14fbbf01-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:10:28.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-znmq2" for this suite.
Apr 11 18:10:34.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:10:34.573: INFO: namespace: e2e-tests-projected-znmq2, resource: bindings, ignored listing per whitelist
Apr 11 18:10:34.608: INFO: namespace e2e-tests-projected-znmq2 deletion completed in 6.135676927s

â€¢ [SLOW TEST:8.419 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:10:34.613: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-pl68s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:10:34.810: INFO: Creating ReplicaSet my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c
Apr 11 18:10:34.820: INFO: Pod name my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c: Found 0 pods out of 1
Apr 11 18:10:39.827: INFO: Pod name my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c: Found 1 pods out of 1
Apr 11 18:10:39.827: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c" is running
Apr 11 18:10:39.831: INFO: Pod "my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c-mkhxc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 18:10:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 18:10:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 18:10:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-11 18:10:34 +0000 UTC Reason: Message:}])
Apr 11 18:10:39.831: INFO: Trying to dial the pod
Apr 11 18:10:44.855: INFO: Controller my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c: Got expected result from replica 1 [my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c-mkhxc]: "my-hostname-basic-19ffa6b7-5c85-11e9-bcf1-d217e8a0130c-mkhxc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:10:44.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-pl68s" for this suite.
Apr 11 18:10:50.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:10:50.897: INFO: namespace: e2e-tests-replicaset-pl68s, resource: bindings, ignored listing per whitelist
Apr 11 18:10:50.989: INFO: namespace e2e-tests-replicaset-pl68s deletion completed in 6.128377135s

â€¢ [SLOW TEST:16.377 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:10:50.993: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-t54hs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-j6xw
STEP: Creating a pod to test atomic-volume-subpath
Apr 11 18:10:51.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j6xw" in namespace "e2e-tests-subpath-t54hs" to be "success or failure"
Apr 11 18:10:51.227: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.647095ms
Apr 11 18:10:53.232: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012202906s
Apr 11 18:10:55.245: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 4.024590893s
Apr 11 18:10:57.251: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 6.030505316s
Apr 11 18:10:59.256: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 8.035693772s
Apr 11 18:11:01.267: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 10.046798615s
Apr 11 18:11:03.273: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 12.052674832s
Apr 11 18:11:05.286: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 14.065736014s
Apr 11 18:11:07.292: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 16.07162463s
Apr 11 18:11:09.297: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 18.077232623s
Apr 11 18:11:11.303: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 20.082643342s
Apr 11 18:11:13.324: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Running", Reason="", readiness=false. Elapsed: 22.103548072s
Apr 11 18:11:15.336: INFO: Pod "pod-subpath-test-configmap-j6xw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.115977905s
STEP: Saw pod success
Apr 11 18:11:15.336: INFO: Pod "pod-subpath-test-configmap-j6xw" satisfied condition "success or failure"
Apr 11 18:11:15.341: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-subpath-test-configmap-j6xw container test-container-subpath-configmap-j6xw: <nil>
STEP: delete the pod
Apr 11 18:11:15.378: INFO: Waiting for pod pod-subpath-test-configmap-j6xw to disappear
Apr 11 18:11:15.383: INFO: Pod pod-subpath-test-configmap-j6xw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j6xw
Apr 11 18:11:15.383: INFO: Deleting pod "pod-subpath-test-configmap-j6xw" in namespace "e2e-tests-subpath-t54hs"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:11:15.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-t54hs" for this suite.
Apr 11 18:11:21.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:11:21.526: INFO: namespace: e2e-tests-subpath-t54hs, resource: bindings, ignored listing per whitelist
Apr 11 18:11:21.537: INFO: namespace e2e-tests-subpath-t54hs deletion completed in 6.143842295s

â€¢ [SLOW TEST:30.544 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:11:21.539: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-nxvsl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-35f9451d-5c85-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 18:11:21.760: INFO: Waiting up to 5m0s for pod "pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-secrets-nxvsl" to be "success or failure"
Apr 11 18:11:21.765: INFO: Pod "pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980835ms
Apr 11 18:11:23.771: INFO: Pod "pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010857737s
STEP: Saw pod success
Apr 11 18:11:23.771: INFO: Pod "pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:11:23.776: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c container secret-volume-test: <nil>
STEP: delete the pod
Apr 11 18:11:23.807: INFO: Waiting for pod pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:11:23.812: INFO: Pod pod-secrets-35fa19bd-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:11:23.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nxvsl" for this suite.
Apr 11 18:11:29.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:11:29.892: INFO: namespace: e2e-tests-secrets-nxvsl, resource: bindings, ignored listing per whitelist
Apr 11 18:11:29.937: INFO: namespace e2e-tests-secrets-nxvsl deletion completed in 6.120271466s

â€¢ [SLOW TEST:8.398 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:11:29.940: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5fdp5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Apr 11 18:11:30.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-785594743 api-versions'
Apr 11 18:11:30.235: INFO: stderr: ""
Apr 11 18:11:30.235: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:11:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5fdp5" for this suite.
Apr 11 18:11:36.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:11:36.340: INFO: namespace: e2e-tests-kubectl-5fdp5, resource: bindings, ignored listing per whitelist
Apr 11 18:11:36.367: INFO: namespace e2e-tests-kubectl-5fdp5 deletion completed in 6.127283036s

â€¢ [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:11:36.370: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7wjgv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-3ed03c04-5c85-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:11:36.593: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-7wjgv" to be "success or failure"
Apr 11 18:11:36.599: INFO: Pod "pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713443ms
Apr 11 18:11:38.606: INFO: Pod "pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012738193s
STEP: Saw pod success
Apr 11 18:11:38.606: INFO: Pod "pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:11:38.612: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 18:11:38.653: INFO: Waiting for pod pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:11:38.662: INFO: Pod pod-projected-configmaps-3ed14537-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:11:38.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7wjgv" for this suite.
Apr 11 18:11:44.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:11:44.712: INFO: namespace: e2e-tests-projected-7wjgv, resource: bindings, ignored listing per whitelist
Apr 11 18:11:44.824: INFO: namespace e2e-tests-projected-7wjgv deletion completed in 6.156923089s

â€¢ [SLOW TEST:8.455 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:11:44.827: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-whqmf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 11 18:11:45.045: INFO: Waiting up to 5m0s for pod "pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-whqmf" to be "success or failure"
Apr 11 18:11:45.051: INFO: Pod "pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02442ms
Apr 11 18:11:47.064: INFO: Pod "pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018230487s
Apr 11 18:11:49.068: INFO: Pod "pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023037008s
STEP: Saw pod success
Apr 11 18:11:49.069: INFO: Pod "pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:11:49.073: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:11:49.105: INFO: Waiting for pod pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:11:49.109: INFO: Pod pod-43da87c4-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:11:49.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-whqmf" for this suite.
Apr 11 18:11:55.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:11:55.165: INFO: namespace: e2e-tests-emptydir-whqmf, resource: bindings, ignored listing per whitelist
Apr 11 18:11:55.241: INFO: namespace e2e-tests-emptydir-whqmf deletion completed in 6.126554863s

â€¢ [SLOW TEST:10.414 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:11:55.243: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-9bv67
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:11:55.456: INFO: Creating deployment "nginx-deployment"
Apr 11 18:11:55.464: INFO: Waiting for observed generation 1
Apr 11 18:11:57.493: INFO: Waiting for all required pods to come up
Apr 11 18:11:57.501: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 11 18:12:01.527: INFO: Waiting for deployment "nginx-deployment" to complete
Apr 11 18:12:01.533: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr 11 18:12:01.542: INFO: Updating deployment nginx-deployment
Apr 11 18:12:01.542: INFO: Waiting for observed generation 2
Apr 11 18:12:03.550: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 11 18:12:03.553: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 11 18:12:03.556: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 11 18:12:03.565: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 11 18:12:03.566: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 11 18:12:03.568: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 11 18:12:03.574: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr 11 18:12:03.574: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr 11 18:12:03.583: INFO: Updating deployment nginx-deployment
Apr 11 18:12:03.583: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr 11 18:12:03.591: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 11 18:12:03.593: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 11 18:12:05.618: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-9bv67,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9bv67/deployments/nginx-deployment,UID:4a117213-5c85-11e9-8000-fa163e4b6765,ResourceVersion:29732,Generation:3,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-04-11 18:12:03 +0000 UTC 2019-04-11 18:12:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-11 18:12:03 +0000 UTC 2019-04-11 18:11:55 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr 11 18:12:05.641: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-9bv67,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9bv67/replicasets/nginx-deployment-65bbdb5f8,UID:4db24a2d-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29727,Generation:3,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4a117213-5c85-11e9-8000-fa163e4b6765 0xc0022f0e67 0xc0022f0e68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 11 18:12:05.641: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr 11 18:12:05.642: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-9bv67,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9bv67/replicasets/nginx-deployment-555b55d965,UID:4a140bf1-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29716,Generation:3,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4a117213-5c85-11e9-8000-fa163e4b6765 0xc0022f0da7 0xc0022f0da8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr 11 18:12:05.660: INFO: Pod "nginx-deployment-555b55d965-2p5dh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2p5dh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-2p5dh,UID:4ef045e9-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29699,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1817 0xc0022f1818}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f1880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f18a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.661: INFO: Pod "nginx-deployment-555b55d965-2pnmv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2pnmv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-2pnmv,UID:4a1f236c-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29562,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1950 0xc0022f1951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f19b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f19d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.131,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4abd5a9c8b04b681bceb434e2a45bafb6b06ee8abcc0d2a2136b9df154ec51d0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.661: INFO: Pod "nginx-deployment-555b55d965-5rg24" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5rg24,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-5rg24,UID:4a237f66-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29559,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1ad0 0xc0022f1ad1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f1b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f1b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:10.233.85.36,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:58 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://518b1d72233e498a4e80930eac798781fd956435ae0f7833fc732efca4f4ceb2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.661: INFO: Pod "nginx-deployment-555b55d965-89kpr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-89kpr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-89kpr,UID:4a1ac2bf-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29532,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1c10 0xc0022f1c11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f1c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f1c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.129,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c77988a9682242c8834bd68d4c71aac4be7f0770c1b2c44da3e22fa50bd17154}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.665: INFO: Pod "nginx-deployment-555b55d965-8d5b2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8d5b2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-8d5b2,UID:4eeaf5f3-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29684,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1d90 0xc0022f1d91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f1df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f1e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.665: INFO: Pod "nginx-deployment-555b55d965-8q4x4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8q4x4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-8q4x4,UID:4ef83c86-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29748,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1ec0 0xc0022f1ec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022f1f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022f1f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.666: INFO: Pod "nginx-deployment-555b55d965-9cxrs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9cxrs,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-9cxrs,UID:4a185257-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29522,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc0022f1ff0 0xc0022f1ff1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:10.233.87.138,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f18892ec027572fc6571ed692e2988ef96a5c3dcdcf0b43b1c1fe98e7facb9ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.666: INFO: Pod "nginx-deployment-555b55d965-csn8s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-csn8s,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-csn8s,UID:4a1ef9d6-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29524,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0140 0xc001eb0141}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb01a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb01c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:10.233.87.140,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://14dc66790f5b14db10748611b6ea7cfb2ecf12366b8dca313b2a22b2c2842a49}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.666: INFO: Pod "nginx-deployment-555b55d965-f7xwg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f7xwg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-f7xwg,UID:4a1ecdd1-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29518,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0280 0xc001eb0281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb02f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:10.233.87.139,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e7aa43c43846a33e6c3c5c47047693a5608acaf0c7192678460d4bfce6d3eff0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.666: INFO: Pod "nginx-deployment-555b55d965-g2ldt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-g2ldt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-g2ldt,UID:4ef13def-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29703,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0570 0xc001eb0571}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb05d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb05f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.667: INFO: Pod "nginx-deployment-555b55d965-g2lw6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-g2lw6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-g2lw6,UID:4ef152af-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29713,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb06a0 0xc001eb06a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.667: INFO: Pod "nginx-deployment-555b55d965-gzp5f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gzp5f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-gzp5f,UID:4ef80624-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29789,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb07d0 0xc001eb07d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb08d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb08f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.667: INFO: Pod "nginx-deployment-555b55d965-hnr4h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hnr4h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-hnr4h,UID:4ef890a4-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29718,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0a20 0xc001eb0a21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.667: INFO: Pod "nginx-deployment-555b55d965-lc9m5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lc9m5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-lc9m5,UID:4a223c53-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29538,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0b50 0xc001eb0b51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.130,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://07d9f9f6ed02ea17ceacd7a58f621e6fb541f6bc004e846ea48247ac2fb25a79}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.669: INFO: Pod "nginx-deployment-555b55d965-lm5st" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lm5st,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-lm5st,UID:4eed5889-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29690,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0c90 0xc001eb0c91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.670: INFO: Pod "nginx-deployment-555b55d965-lr544" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lr544,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-lr544,UID:4eed0eba-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29711,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0dc0 0xc001eb0dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.670: INFO: Pod "nginx-deployment-555b55d965-pxj5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-pxj5x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-pxj5x,UID:4ef0ff7c-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29731,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb0ef0 0xc001eb0ef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb0f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb0f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.670: INFO: Pod "nginx-deployment-555b55d965-s264s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-s264s,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-s264s,UID:4ef87b0f-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29762,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb1020 0xc001eb1021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb10a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.670: INFO: Pod "nginx-deployment-555b55d965-tqrht" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tqrht,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-tqrht,UID:4a1f0fb6-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29571,Generation:0,CreationTimestamp:2019-04-11 18:11:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb1150 0xc001eb1151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb11b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb11d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:11:55 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:10.233.85.38,StartTime:2019-04-11 18:11:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-11 18:11:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dbb1d588cebbb0565e7fe49530cc944b7683225473989e3101254443db2bfe65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.670: INFO: Pod "nginx-deployment-555b55d965-zxnb8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zxnb8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-555b55d965-zxnb8,UID:4ef79a83-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29738,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 4a140bf1-5c85-11e9-ab42-fa163e758ab4 0xc001eb1290 0xc001eb1291}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb12f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb1310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-4gcvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4gcvc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-4gcvc,UID:4db5b2b5-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29606,Generation:0,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb13c0 0xc001eb13c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb1450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-69s7r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-69s7r,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-69s7r,UID:4f002e4d-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29733,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1510 0xc001eb1511}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb15a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-6vfbx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6vfbx,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-6vfbx,UID:4eff8fae-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29780,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1660 0xc001eb1661}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb16d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb16f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-78w99" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-78w99,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-78w99,UID:4f000365-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29734,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb17b0 0xc001eb17b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb18e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-d7m7k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-d7m7k,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-d7m7k,UID:4db54576-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29792,Generation:0,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1960 0xc001eb1961}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb19d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb1b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.86.132,StartTime:2019-04-11 18:12:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.671: INFO: Pod "nginx-deployment-65bbdb5f8-dfgxl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dfgxl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-dfgxl,UID:4ef51b02-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29744,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1ce0 0xc001eb1ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1d50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb1d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.674: INFO: Pod "nginx-deployment-65bbdb5f8-dnsr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dnsr5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-dnsr5,UID:4f07a940-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29799,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1e30 0xc001eb1e31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb1ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-fhpjd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-fhpjd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-fhpjd,UID:4db37592-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29600,Generation:0,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc001eb1f80 0xc001eb1f81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb1ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019ae010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-lp8ch" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lp8ch,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-lp8ch,UID:4efe95f0-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29717,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc0019ae0f0 0xc0019ae0f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019ae160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019ae180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-nvh9m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nvh9m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-nvh9m,UID:4eee5592-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29693,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc0019ae250 0xc0019ae251}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019ae330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019ae380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.11,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-thbx2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-thbx2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-thbx2,UID:4ef45b87-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29701,Generation:0,CreationTimestamp:2019-04-11 18:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc0019ae550 0xc0019ae551}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019ae600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019ae620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:03 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-xj2mw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xj2mw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-xj2mw,UID:4dc82e17-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29624,Generation:0,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc0019ae7e0 0xc0019ae7e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019ae890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019ae8e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-04-11 18:12:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 11 18:12:05.675: INFO: Pod "nginx-deployment-65bbdb5f8-zt47n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zt47n,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9bv67,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9bv67/pods/nginx-deployment-65bbdb5f8-zt47n,UID:4dc486bb-5c85-11e9-ab42-fa163e758ab4,ResourceVersion:29626,Generation:0,CreationTimestamp:2019-04-11 18:12:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4db24a2d-5c85-11e9-ab42-fa163e758ab4 0xc0019aebd0 0xc0019aebd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gv742 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gv742,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gv742 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019aecc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019aed20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-11 18:12:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-04-11 18:12:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:12:05.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9bv67" for this suite.
Apr 11 18:12:15.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:12:15.768: INFO: namespace: e2e-tests-deployment-9bv67, resource: bindings, ignored listing per whitelist
Apr 11 18:12:15.943: INFO: namespace e2e-tests-deployment-9bv67 deletion completed in 10.262676586s

â€¢ [SLOW TEST:20.701 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:12:15.947: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rkwdt
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 11 18:12:16.194: INFO: Waiting up to 5m0s for pod "pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-emptydir-rkwdt" to be "success or failure"
Apr 11 18:12:16.203: INFO: Pod "pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.412811ms
Apr 11 18:12:18.214: INFO: Pod "pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019942347s
Apr 11 18:12:20.220: INFO: Pod "pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025568032s
STEP: Saw pod success
Apr 11 18:12:20.220: INFO: Pod "pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:12:20.224: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c container test-container: <nil>
STEP: delete the pod
Apr 11 18:12:20.255: INFO: Waiting for pod pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:12:20.298: INFO: Pod pod-566bc53b-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:12:20.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rkwdt" for this suite.
Apr 11 18:12:26.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:12:26.362: INFO: namespace: e2e-tests-emptydir-rkwdt, resource: bindings, ignored listing per whitelist
Apr 11 18:12:26.444: INFO: namespace e2e-tests-emptydir-rkwdt deletion completed in 6.139462055s

â€¢ [SLOW TEST:10.498 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:12:26.447: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-4b8nw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-4b8nw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4b8nw to expose endpoints map[]
Apr 11 18:12:26.666: INFO: Get endpoints failed (5.459603ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Apr 11 18:12:27.670: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4b8nw exposes endpoints map[] (1.009464025s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-4b8nw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4b8nw to expose endpoints map[pod1:[100]]
Apr 11 18:12:29.713: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4b8nw exposes endpoints map[pod1:[100]] (2.032517954s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-4b8nw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4b8nw to expose endpoints map[pod1:[100] pod2:[101]]
Apr 11 18:12:31.760: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4b8nw exposes endpoints map[pod1:[100] pod2:[101]] (2.03782321s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-4b8nw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4b8nw to expose endpoints map[pod2:[101]]
Apr 11 18:12:31.784: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4b8nw exposes endpoints map[pod2:[101]] (10.456583ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-4b8nw
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4b8nw to expose endpoints map[]
Apr 11 18:12:32.805: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4b8nw exposes endpoints map[] (1.008476184s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:12:32.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4b8nw" for this suite.
Apr 11 18:12:54.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:12:54.927: INFO: namespace: e2e-tests-services-4b8nw, resource: bindings, ignored listing per whitelist
Apr 11 18:12:54.970: INFO: namespace e2e-tests-services-4b8nw deletion completed in 22.137140279s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:28.523 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:12:54.974: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-w9q4t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0411 18:13:25.750576      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 11 18:13:25.750: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:13:25.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-w9q4t" for this suite.
Apr 11 18:13:31.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:13:31.827: INFO: namespace: e2e-tests-gc-w9q4t, resource: bindings, ignored listing per whitelist
Apr 11 18:13:31.883: INFO: namespace e2e-tests-gc-w9q4t deletion completed in 6.128664069s

â€¢ [SLOW TEST:36.910 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:13:31.886: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8z8s6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-83aa6ba8-5c85-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume secrets
Apr 11 18:13:32.108: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-8z8s6" to be "success or failure"
Apr 11 18:13:32.112: INFO: Pod "pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193785ms
Apr 11 18:13:34.118: INFO: Pod "pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010416612s
STEP: Saw pod success
Apr 11 18:13:34.118: INFO: Pod "pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:13:34.123: INFO: Trying to get logs from node cluster1-k8s-node-3 pod pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 11 18:13:34.168: INFO: Waiting for pod pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:13:34.172: INFO: Pod pod-projected-secrets-83ab4686-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:13:34.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8z8s6" for this suite.
Apr 11 18:13:40.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:13:40.330: INFO: namespace: e2e-tests-projected-8z8s6, resource: bindings, ignored listing per whitelist
Apr 11 18:13:40.345: INFO: namespace e2e-tests-projected-8z8s6 deletion completed in 6.166998947s

â€¢ [SLOW TEST:8.459 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:13:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-5qrh8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:14:06.583: INFO: Container started at 2019-04-11 18:13:41 +0000 UTC, pod became ready at 2019-04-11 18:14:04 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:14:06.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5qrh8" for this suite.
Apr 11 18:14:28.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:14:28.658: INFO: namespace: e2e-tests-container-probe-5qrh8, resource: bindings, ignored listing per whitelist
Apr 11 18:14:28.716: INFO: namespace e2e-tests-container-probe-5qrh8 deletion completed in 22.126375745s

â€¢ [SLOW TEST:48.370 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:14:28.720: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vj4bh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 18:14:28.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-vj4bh" to be "success or failure"
Apr 11 18:14:28.940: INFO: Pod "downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281529ms
Apr 11 18:14:30.952: INFO: Pod "downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016228637s
STEP: Saw pod success
Apr 11 18:14:30.952: INFO: Pod "downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:14:30.956: INFO: Trying to get logs from node cluster1-k8s-node-3 pod downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 18:14:30.991: INFO: Waiting for pod downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:14:30.995: INFO: Pod downwardapi-volume-a58ad525-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:14:30.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vj4bh" for this suite.
Apr 11 18:14:37.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:14:37.090: INFO: namespace: e2e-tests-projected-vj4bh, resource: bindings, ignored listing per whitelist
Apr 11 18:14:37.151: INFO: namespace e2e-tests-projected-vj4bh deletion completed in 6.149854776s

â€¢ [SLOW TEST:8.432 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:14:37.153: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-dfksn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-aa942c46-5c85-11e9-bcf1-d217e8a0130c
STEP: Creating a pod to test consume configMaps
Apr 11 18:14:37.398: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-configmap-dfksn" to be "success or failure"
Apr 11 18:14:37.405: INFO: Pod "pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457513ms
Apr 11 18:14:39.410: INFO: Pod "pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011582681s
STEP: Saw pod success
Apr 11 18:14:39.410: INFO: Pod "pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:14:39.414: INFO: Trying to get logs from node cluster1-k8s-node-1 pod pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 11 18:14:39.445: INFO: Waiting for pod pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:14:39.449: INFO: Pod pod-configmaps-aa95415e-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:14:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dfksn" for this suite.
Apr 11 18:14:45.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:14:45.547: INFO: namespace: e2e-tests-configmap-dfksn, resource: bindings, ignored listing per whitelist
Apr 11 18:14:45.580: INFO: namespace e2e-tests-configmap-dfksn deletion completed in 6.124847164s

â€¢ [SLOW TEST:8.427 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:14:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-r7wlw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Apr 11 18:14:45.826: INFO: Waiting up to 5m0s for pod "var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-var-expansion-r7wlw" to be "success or failure"
Apr 11 18:14:45.830: INFO: Pod "var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368393ms
Apr 11 18:14:47.836: INFO: Pod "var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009850122s
STEP: Saw pod success
Apr 11 18:14:47.836: INFO: Pod "var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:14:47.841: INFO: Trying to get logs from node cluster1-k8s-node-3 pod var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c container dapi-container: <nil>
STEP: delete the pod
Apr 11 18:14:47.881: INFO: Waiting for pod var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:14:47.887: INFO: Pod var-expansion-af9bbb66-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:14:47.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-r7wlw" for this suite.
Apr 11 18:14:53.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:14:53.930: INFO: namespace: e2e-tests-var-expansion-r7wlw, resource: bindings, ignored listing per whitelist
Apr 11 18:14:54.017: INFO: namespace e2e-tests-var-expansion-r7wlw deletion completed in 6.123363398s

â€¢ [SLOW TEST:8.435 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:14:54.019: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-5vh7m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 11 18:14:54.299: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30959,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 11 18:14:54.299: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30960,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 11 18:14:54.299: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30962,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 11 18:15:04.356: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30983,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 11 18:15:04.356: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30984,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 11 18:15:04.356: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-5vh7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-5vh7m/configmaps/e2e-watch-test-label-changed,UID:b4a59e9f-5c85-11e9-8000-fa163e4b6765,ResourceVersion:30986,Generation:0,CreationTimestamp:2019-04-11 18:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:15:04.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-5vh7m" for this suite.
Apr 11 18:15:10.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:15:10.482: INFO: namespace: e2e-tests-watch-5vh7m, resource: bindings, ignored listing per whitelist
Apr 11 18:15:10.483: INFO: namespace e2e-tests-watch-5vh7m deletion completed in 6.121845367s

â€¢ [SLOW TEST:16.464 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:15:10.489: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-jcm2b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 11 18:15:14.774: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 11 18:15:14.785: INFO: Pod pod-with-poststart-http-hook still exists
Apr 11 18:15:16.785: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 11 18:15:16.791: INFO: Pod pod-with-poststart-http-hook still exists
Apr 11 18:15:18.785: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 11 18:15:18.791: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:15:18.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jcm2b" for this suite.
Apr 11 18:15:36.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:15:36.882: INFO: namespace: e2e-tests-container-lifecycle-hook-jcm2b, resource: bindings, ignored listing per whitelist
Apr 11 18:15:36.926: INFO: namespace e2e-tests-container-lifecycle-hook-jcm2b deletion completed in 18.130399905s

â€¢ [SLOW TEST:26.438 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:15:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-zjl7w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:15:39.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-zjl7w" for this suite.
Apr 11 18:16:17.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:16:17.228: INFO: namespace: e2e-tests-kubelet-test-zjl7w, resource: bindings, ignored listing per whitelist
Apr 11 18:16:17.328: INFO: namespace e2e-tests-kubelet-test-zjl7w deletion completed in 38.146122757s

â€¢ [SLOW TEST:40.398 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:16:17.331: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-m7mv7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vfwn6 in namespace e2e-tests-proxy-m7mv7
I0411 18:16:17.574103      15 runners.go:184] Created replication controller with name: proxy-service-vfwn6, namespace: e2e-tests-proxy-m7mv7, replica count: 1
I0411 18:16:18.637248      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0411 18:16:19.637494      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0411 18:16:20.637704      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0411 18:16:21.637911      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0411 18:16:22.638085      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0411 18:16:23.638283      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0411 18:16:24.638590      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0411 18:16:25.638801      15 runners.go:184] proxy-service-vfwn6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 11 18:16:25.649: INFO: setup took 8.101100637s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 11 18:16:25.679: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 29.268621ms)
Apr 11 18:16:25.679: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 29.995006ms)
Apr 11 18:16:25.679: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 29.353162ms)
Apr 11 18:16:25.679: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 29.591263ms)
Apr 11 18:16:25.684: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 33.869642ms)
Apr 11 18:16:25.684: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 34.139058ms)
Apr 11 18:16:25.687: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 36.819379ms)
Apr 11 18:16:25.688: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 38.262627ms)
Apr 11 18:16:25.688: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 38.965366ms)
Apr 11 18:16:25.689: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 39.434059ms)
Apr 11 18:16:25.689: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 39.773705ms)
Apr 11 18:16:25.689: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 38.622183ms)
Apr 11 18:16:25.689: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 40.501458ms)
Apr 11 18:16:25.690: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 39.565922ms)
Apr 11 18:16:25.690: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 40.693969ms)
Apr 11 18:16:25.690: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 39.504814ms)
Apr 11 18:16:25.702: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 11.88049ms)
Apr 11 18:16:25.702: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.131232ms)
Apr 11 18:16:25.703: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 12.651062ms)
Apr 11 18:16:25.703: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 12.595945ms)
Apr 11 18:16:25.703: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 12.575087ms)
Apr 11 18:16:25.705: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.454637ms)
Apr 11 18:16:25.705: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.553623ms)
Apr 11 18:16:25.705: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 13.800795ms)
Apr 11 18:16:25.705: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 14.479112ms)
Apr 11 18:16:25.706: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 14.061894ms)
Apr 11 18:16:25.707: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 16.191886ms)
Apr 11 18:16:25.707: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 16.079715ms)
Apr 11 18:16:25.707: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 16.318769ms)
Apr 11 18:16:25.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.668935ms)
Apr 11 18:16:25.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.807428ms)
Apr 11 18:16:25.709: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 17.300019ms)
Apr 11 18:16:25.719: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 9.827825ms)
Apr 11 18:16:25.721: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 11.954882ms)
Apr 11 18:16:25.722: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 12.431513ms)
Apr 11 18:16:25.722: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 13.529494ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.909182ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.013138ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 13.980845ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.521761ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.084637ms)
Apr 11 18:16:25.723: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 14.225913ms)
Apr 11 18:16:25.724: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 14.668673ms)
Apr 11 18:16:25.724: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 14.250733ms)
Apr 11 18:16:25.724: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 14.978399ms)
Apr 11 18:16:25.724: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 14.300269ms)
Apr 11 18:16:25.725: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 14.992596ms)
Apr 11 18:16:25.725: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 15.199005ms)
Apr 11 18:16:25.735: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 9.541219ms)
Apr 11 18:16:25.736: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 10.627094ms)
Apr 11 18:16:25.736: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 10.450729ms)
Apr 11 18:16:25.737: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 11.056064ms)
Apr 11 18:16:25.739: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 11.56024ms)
Apr 11 18:16:25.739: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 12.0848ms)
Apr 11 18:16:25.740: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 12.901203ms)
Apr 11 18:16:25.740: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 13.652973ms)
Apr 11 18:16:25.740: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.59702ms)
Apr 11 18:16:25.740: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 13.961547ms)
Apr 11 18:16:25.741: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 14.858736ms)
Apr 11 18:16:25.741: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 15.807385ms)
Apr 11 18:16:25.743: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 16.07396ms)
Apr 11 18:16:25.743: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 16.987503ms)
Apr 11 18:16:25.744: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 16.770523ms)
Apr 11 18:16:25.744: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.444659ms)
Apr 11 18:16:25.755: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 10.336824ms)
Apr 11 18:16:25.756: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 9.975348ms)
Apr 11 18:16:25.758: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 13.442217ms)
Apr 11 18:16:25.761: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 12.580978ms)
Apr 11 18:16:25.761: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 13.676424ms)
Apr 11 18:16:25.762: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 15.146472ms)
Apr 11 18:16:25.762: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.509112ms)
Apr 11 18:16:25.762: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 16.176471ms)
Apr 11 18:16:25.763: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 15.523812ms)
Apr 11 18:16:25.763: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 15.457168ms)
Apr 11 18:16:25.763: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 15.27506ms)
Apr 11 18:16:25.764: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 18.799417ms)
Apr 11 18:16:25.764: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 18.599088ms)
Apr 11 18:16:25.765: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 17.818296ms)
Apr 11 18:16:25.765: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 16.719034ms)
Apr 11 18:16:25.765: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 16.521281ms)
Apr 11 18:16:25.777: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 11.868114ms)
Apr 11 18:16:25.777: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.26792ms)
Apr 11 18:16:25.777: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 12.686379ms)
Apr 11 18:16:25.778: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 12.624804ms)
Apr 11 18:16:25.778: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 12.616133ms)
Apr 11 18:16:25.778: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 13.120983ms)
Apr 11 18:16:25.779: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.234107ms)
Apr 11 18:16:25.779: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.453099ms)
Apr 11 18:16:25.780: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 13.977993ms)
Apr 11 18:16:25.780: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.672977ms)
Apr 11 18:16:25.781: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 15.432939ms)
Apr 11 18:16:25.783: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 18.136299ms)
Apr 11 18:16:25.783: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.784331ms)
Apr 11 18:16:25.783: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 17.718677ms)
Apr 11 18:16:25.783: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.848153ms)
Apr 11 18:16:25.783: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 18.266952ms)
Apr 11 18:16:25.796: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 11.590658ms)
Apr 11 18:16:25.797: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 12.591211ms)
Apr 11 18:16:25.798: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.589531ms)
Apr 11 18:16:25.799: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 13.37603ms)
Apr 11 18:16:25.799: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 13.588736ms)
Apr 11 18:16:25.800: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.611728ms)
Apr 11 18:16:25.801: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 13.524795ms)
Apr 11 18:16:25.802: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 16.378421ms)
Apr 11 18:16:25.802: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 17.419269ms)
Apr 11 18:16:25.803: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.794974ms)
Apr 11 18:16:25.803: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 15.268752ms)
Apr 11 18:16:25.803: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 15.54974ms)
Apr 11 18:16:25.803: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 15.046316ms)
Apr 11 18:16:25.804: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.539029ms)
Apr 11 18:16:25.805: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 18.074779ms)
Apr 11 18:16:25.805: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 18.041518ms)
Apr 11 18:16:25.815: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 10.224315ms)
Apr 11 18:16:25.818: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 11.645127ms)
Apr 11 18:16:25.824: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 16.997216ms)
Apr 11 18:16:25.825: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 16.728957ms)
Apr 11 18:16:25.825: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 16.9159ms)
Apr 11 18:16:25.825: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 19.634425ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 20.173766ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 19.872187ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 19.0856ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 18.475879ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 18.265852ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 19.493242ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 19.208592ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 19.716226ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 19.019139ms)
Apr 11 18:16:25.826: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 19.443548ms)
Apr 11 18:16:25.837: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 9.580309ms)
Apr 11 18:16:25.840: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 12.31057ms)
Apr 11 18:16:25.843: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 15.556646ms)
Apr 11 18:16:25.845: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 16.242428ms)
Apr 11 18:16:25.846: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 17.119257ms)
Apr 11 18:16:25.846: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 17.303759ms)
Apr 11 18:16:25.846: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 17.08101ms)
Apr 11 18:16:25.846: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 18.2445ms)
Apr 11 18:16:25.846: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 18.50507ms)
Apr 11 18:16:25.847: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 18.538506ms)
Apr 11 18:16:25.847: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 18.495076ms)
Apr 11 18:16:25.847: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 19.02771ms)
Apr 11 18:16:25.847: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 18.698436ms)
Apr 11 18:16:25.848: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 19.753955ms)
Apr 11 18:16:25.848: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 19.670201ms)
Apr 11 18:16:25.848: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 19.895183ms)
Apr 11 18:16:25.859: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 10.267724ms)
Apr 11 18:16:25.862: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.545568ms)
Apr 11 18:16:25.862: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 13.17076ms)
Apr 11 18:16:25.863: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.473288ms)
Apr 11 18:16:25.863: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 14.036952ms)
Apr 11 18:16:25.864: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.310782ms)
Apr 11 18:16:25.864: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.143705ms)
Apr 11 18:16:25.864: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 14.251879ms)
Apr 11 18:16:25.864: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.993473ms)
Apr 11 18:16:25.865: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 15.093351ms)
Apr 11 18:16:25.865: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 16.665492ms)
Apr 11 18:16:25.867: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 18.63891ms)
Apr 11 18:16:25.868: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 18.175759ms)
Apr 11 18:16:25.868: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 19.490418ms)
Apr 11 18:16:25.868: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 18.597937ms)
Apr 11 18:16:25.868: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 19.18258ms)
Apr 11 18:16:25.879: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 9.646367ms)
Apr 11 18:16:25.879: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 10.394377ms)
Apr 11 18:16:25.880: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 10.882535ms)
Apr 11 18:16:25.880: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 10.958744ms)
Apr 11 18:16:25.880: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 11.581238ms)
Apr 11 18:16:25.881: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 11.223985ms)
Apr 11 18:16:25.882: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 13.124928ms)
Apr 11 18:16:25.882: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 13.231416ms)
Apr 11 18:16:25.883: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 12.879841ms)
Apr 11 18:16:25.883: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.609356ms)
Apr 11 18:16:25.883: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 13.372773ms)
Apr 11 18:16:25.883: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 13.476968ms)
Apr 11 18:16:25.884: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 14.795702ms)
Apr 11 18:16:25.886: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 16.363938ms)
Apr 11 18:16:25.886: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 16.228807ms)
Apr 11 18:16:25.887: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.098653ms)
Apr 11 18:16:25.896: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 9.41651ms)
Apr 11 18:16:25.899: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 11.962934ms)
Apr 11 18:16:25.899: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.146372ms)
Apr 11 18:16:25.904: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 16.496681ms)
Apr 11 18:16:25.904: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 16.490826ms)
Apr 11 18:16:25.907: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 18.769541ms)
Apr 11 18:16:25.907: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 19.43222ms)
Apr 11 18:16:25.908: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 20.166863ms)
Apr 11 18:16:25.908: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 20.506517ms)
Apr 11 18:16:25.908: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 20.628816ms)
Apr 11 18:16:25.910: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 22.970492ms)
Apr 11 18:16:25.911: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 23.105225ms)
Apr 11 18:16:25.911: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 23.229824ms)
Apr 11 18:16:25.911: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 24.018101ms)
Apr 11 18:16:25.911: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 23.2838ms)
Apr 11 18:16:25.912: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 24.499405ms)
Apr 11 18:16:25.925: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 11.697198ms)
Apr 11 18:16:25.925: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 13.210966ms)
Apr 11 18:16:25.926: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 13.03746ms)
Apr 11 18:16:25.927: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.115673ms)
Apr 11 18:16:25.927: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.432441ms)
Apr 11 18:16:25.927: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 14.910163ms)
Apr 11 18:16:25.927: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.470462ms)
Apr 11 18:16:25.928: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 15.290309ms)
Apr 11 18:16:25.928: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 14.097944ms)
Apr 11 18:16:25.928: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.884836ms)
Apr 11 18:16:25.929: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 14.962143ms)
Apr 11 18:16:25.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 18.79249ms)
Apr 11 18:16:25.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 17.973398ms)
Apr 11 18:16:25.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.15504ms)
Apr 11 18:16:25.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.36797ms)
Apr 11 18:16:25.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 17.326154ms)
Apr 11 18:16:25.945: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 12.213857ms)
Apr 11 18:16:25.945: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.421341ms)
Apr 11 18:16:25.945: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 13.165967ms)
Apr 11 18:16:25.945: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 12.462676ms)
Apr 11 18:16:25.946: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 12.076016ms)
Apr 11 18:16:25.947: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 12.633512ms)
Apr 11 18:16:25.947: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.805491ms)
Apr 11 18:16:25.948: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.22019ms)
Apr 11 18:16:25.948: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.474746ms)
Apr 11 18:16:25.954: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 19.230203ms)
Apr 11 18:16:25.954: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 21.329183ms)
Apr 11 18:16:25.954: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 21.241458ms)
Apr 11 18:16:25.955: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 17.113531ms)
Apr 11 18:16:25.955: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 20.91014ms)
Apr 11 18:16:25.955: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 22.616709ms)
Apr 11 18:16:25.955: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 21.633979ms)
Apr 11 18:16:25.967: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 11.92005ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 14.08756ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.359318ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.520439ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 14.408444ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.830728ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.240403ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 14.575027ms)
Apr 11 18:16:25.970: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.548265ms)
Apr 11 18:16:25.971: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 14.628205ms)
Apr 11 18:16:25.973: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 17.380728ms)
Apr 11 18:16:25.973: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 17.237369ms)
Apr 11 18:16:25.973: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 17.37136ms)
Apr 11 18:16:25.975: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 18.406288ms)
Apr 11 18:16:25.975: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 18.873661ms)
Apr 11 18:16:25.975: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 18.984857ms)
Apr 11 18:16:25.985: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 9.851891ms)
Apr 11 18:16:25.986: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 10.903524ms)
Apr 11 18:16:25.987: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 12.084715ms)
Apr 11 18:16:25.988: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 12.771618ms)
Apr 11 18:16:25.989: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.004422ms)
Apr 11 18:16:25.989: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 13.140611ms)
Apr 11 18:16:25.990: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 13.704577ms)
Apr 11 18:16:25.990: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.343404ms)
Apr 11 18:16:25.990: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.552412ms)
Apr 11 18:16:25.990: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 14.653279ms)
Apr 11 18:16:25.991: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 15.002202ms)
Apr 11 18:16:25.993: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 16.686776ms)
Apr 11 18:16:25.993: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 16.950676ms)
Apr 11 18:16:25.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 18.164809ms)
Apr 11 18:16:25.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 17.949075ms)
Apr 11 18:16:25.994: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.792091ms)
Apr 11 18:16:26.008: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 13.674384ms)
Apr 11 18:16:26.008: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.184772ms)
Apr 11 18:16:26.008: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 14.400087ms)
Apr 11 18:16:26.008: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.028684ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 14.185027ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 14.045709ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 13.699686ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 13.558252ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 13.240782ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.736462ms)
Apr 11 18:16:26.009: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 14.655658ms)
Apr 11 18:16:26.012: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 17.120241ms)
Apr 11 18:16:26.012: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 17.627742ms)
Apr 11 18:16:26.013: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.198088ms)
Apr 11 18:16:26.013: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 18.158785ms)
Apr 11 18:16:26.013: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 17.524107ms)
Apr 11 18:16:26.022: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 9.388395ms)
Apr 11 18:16:26.024: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 10.872887ms)
Apr 11 18:16:26.027: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.370913ms)
Apr 11 18:16:26.027: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 12.612202ms)
Apr 11 18:16:26.028: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.675568ms)
Apr 11 18:16:26.028: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 14.33887ms)
Apr 11 18:16:26.028: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.034175ms)
Apr 11 18:16:26.028: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 14.166321ms)
Apr 11 18:16:26.028: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 14.570229ms)
Apr 11 18:16:26.031: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 17.090993ms)
Apr 11 18:16:26.037: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 23.169523ms)
Apr 11 18:16:26.037: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 23.166752ms)
Apr 11 18:16:26.037: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 22.837404ms)
Apr 11 18:16:26.037: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 23.480851ms)
Apr 11 18:16:26.038: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 24.189158ms)
Apr 11 18:16:26.038: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 24.163667ms)
Apr 11 18:16:26.049: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 11.154418ms)
Apr 11 18:16:26.053: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 14.287725ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 16.067524ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 16.689535ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 17.002013ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 16.145491ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 17.264324ms)
Apr 11 18:16:26.055: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 16.143673ms)
Apr 11 18:16:26.056: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 17.003335ms)
Apr 11 18:16:26.057: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 18.160224ms)
Apr 11 18:16:26.057: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 18.928918ms)
Apr 11 18:16:26.058: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 19.216854ms)
Apr 11 18:16:26.058: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 19.485539ms)
Apr 11 18:16:26.062: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 23.040792ms)
Apr 11 18:16:26.062: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 22.832439ms)
Apr 11 18:16:26.062: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 23.581702ms)
Apr 11 18:16:26.075: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 12.838056ms)
Apr 11 18:16:26.076: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:462/proxy/: tls qux (200; 12.898993ms)
Apr 11 18:16:26.076: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:460/proxy/: tls baz (200; 13.088142ms)
Apr 11 18:16:26.076: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 13.846924ms)
Apr 11 18:16:26.077: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:162/proxy/: bar (200; 13.984382ms)
Apr 11 18:16:26.077: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:1080/proxy/rewri... (200; 13.567923ms)
Apr 11 18:16:26.078: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/https:proxy-service-vfwn6-9z2qs:443/proxy/... (200; 14.757005ms)
Apr 11 18:16:26.079: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs:160/proxy/: foo (200; 15.153557ms)
Apr 11 18:16:26.079: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/http:proxy-service-vfwn6-9z2qs:1080/proxy/... (200; 14.727579ms)
Apr 11 18:16:26.080: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m7mv7/pods/proxy-service-vfwn6-9z2qs/proxy/rewriteme"... (200; 15.724401ms)
Apr 11 18:16:26.080: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname2/proxy/: bar (200; 16.565584ms)
Apr 11 18:16:26.081: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname2/proxy/: bar (200; 17.043226ms)
Apr 11 18:16:26.081: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname1/proxy/: tls baz (200; 17.567562ms)
Apr 11 18:16:26.081: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/http:proxy-service-vfwn6:portname1/proxy/: foo (200; 17.254153ms)
Apr 11 18:16:26.081: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/proxy-service-vfwn6:portname1/proxy/: foo (200; 18.600091ms)
Apr 11 18:16:26.081: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m7mv7/services/https:proxy-service-vfwn6:tlsportname2/proxy/: tls qux (200; 17.953573ms)
STEP: deleting ReplicationController proxy-service-vfwn6 in namespace e2e-tests-proxy-m7mv7, will wait for the garbage collector to delete the pods
Apr 11 18:16:26.145: INFO: Deleting ReplicationController proxy-service-vfwn6 took: 10.382406ms
Apr 11 18:16:26.245: INFO: Terminating ReplicationController proxy-service-vfwn6 pods took: 100.181592ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:16:33.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-m7mv7" for this suite.
Apr 11 18:16:39.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:16:39.702: INFO: namespace: e2e-tests-proxy-m7mv7, resource: bindings, ignored listing per whitelist
Apr 11 18:16:39.779: INFO: namespace e2e-tests-proxy-m7mv7 deletion completed in 6.127259215s

â€¢ [SLOW TEST:22.448 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:16:39.781: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v5dtg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Apr 11 18:16:39.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c" in namespace "e2e-tests-projected-v5dtg" to be "success or failure"
Apr 11 18:16:40.004: INFO: Pod "downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310219ms
Apr 11 18:16:42.010: INFO: Pod "downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010438984s
STEP: Saw pod success
Apr 11 18:16:42.010: INFO: Pod "downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c" satisfied condition "success or failure"
Apr 11 18:16:42.015: INFO: Trying to get logs from node cluster1-k8s-node-1 pod downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c container client-container: <nil>
STEP: delete the pod
Apr 11 18:16:42.053: INFO: Waiting for pod downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c to disappear
Apr 11 18:16:42.058: INFO: Pod downwardapi-volume-f3a93ce0-5c85-11e9-bcf1-d217e8a0130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:16:42.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v5dtg" for this suite.
Apr 11 18:16:48.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:16:48.109: INFO: namespace: e2e-tests-projected-v5dtg, resource: bindings, ignored listing per whitelist
Apr 11 18:16:48.194: INFO: namespace e2e-tests-projected-v5dtg deletion completed in 6.13136567s

â€¢ [SLOW TEST:8.413 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Apr 11 18:16:48.195: INFO: >>> kubeConfig: /tmp/kubeconfig-785594743
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-fzcxf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Apr 11 18:16:48.426: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 11 18:16:53.438: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 11 18:16:53.439: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 11 18:16:53.462: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-fzcxf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fzcxf/deployments/test-cleanup-deployment,UID:fbaed195-5c85-11e9-8000-fa163e4b6765,ResourceVersion:31422,Generation:1,CreationTimestamp:2019-04-11 18:16:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Apr 11 18:16:53.465: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Apr 11 18:16:53.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-fzcxf" for this suite.
Apr 11 18:16:59.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 11 18:16:59.582: INFO: namespace: e2e-tests-deployment-fzcxf, resource: bindings, ignored listing per whitelist
Apr 11 18:16:59.597: INFO: namespace e2e-tests-deployment-fzcxf deletion completed in 6.120638182s

â€¢ [SLOW TEST:11.402 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSApr 11 18:16:59.599: INFO: Running AfterSuite actions on all nodes
Apr 11 18:16:59.599: INFO: Running AfterSuite actions on node 1
Apr 11 18:16:59.599: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5694.761 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h34m56.061431756s
Test Suite Passed
