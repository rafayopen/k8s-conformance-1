I0306 03:55:56.026835      18 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-610812775
I0306 03:55:56.027574      18 e2e.go:224] Starting e2e run "be3acd62-3fc3-11e9-8de6-d63fb0ed442e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551844555 - Will randomize all specs
Will run 201 of 1946 specs

Mar  6 03:55:56.179: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 03:55:56.181: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  6 03:55:56.198: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  6 03:55:56.238: INFO: 43 / 43 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  6 03:55:56.238: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Mar  6 03:55:56.238: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  6 03:55:56.246: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  6 03:55:56.246: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Mar  6 03:55:56.246: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  6 03:55:56.246: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-feature-discovery' (0 seconds elapsed)
Mar  6 03:55:56.246: INFO: e2e test version: v1.13.0
Mar  6 03:55:56.247: INFO: kube-apiserver version: v1.13.3
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:55:56.247: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
Mar  6 03:55:56.315: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-bec3af68-3fc3-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 03:55:56.330: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-bhww9" to be "success or failure"
Mar  6 03:55:56.333: INFO: Pod "pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.619989ms
Mar  6 03:55:58.336: INFO: Pod "pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006006611s
Mar  6 03:56:00.346: INFO: Pod "pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015926646s
STEP: Saw pod success
Mar  6 03:56:00.347: INFO: Pod "pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 03:56:00.349: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 03:56:00.381: INFO: Waiting for pod pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e to disappear
Mar  6 03:56:00.384: INFO: Pod pod-projected-secrets-bec48d24-3fc3-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:56:00.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bhww9" for this suite.
Mar  6 03:56:06.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:56:06.614: INFO: namespace: e2e-tests-projected-bhww9, resource: bindings, ignored listing per whitelist
Mar  6 03:56:06.614: INFO: namespace e2e-tests-projected-bhww9 deletion completed in 6.227014495s

• [SLOW TEST:10.367 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:56:06.614: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 03:56:06.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-w4ml8" to be "success or failure"
Mar  6 03:56:06.686: INFO: Pod "downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233707ms
Mar  6 03:56:08.689: INFO: Pod "downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005715651s
Mar  6 03:56:10.693: INFO: Pod "downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009131328s
STEP: Saw pod success
Mar  6 03:56:10.693: INFO: Pod "downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 03:56:10.695: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 03:56:10.721: INFO: Waiting for pod downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e to disappear
Mar  6 03:56:10.724: INFO: Pod downwardapi-volume-c4f0314f-3fc3-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:56:10.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w4ml8" for this suite.
Mar  6 03:56:16.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:56:16.855: INFO: namespace: e2e-tests-projected-w4ml8, resource: bindings, ignored listing per whitelist
Mar  6 03:56:16.863: INFO: namespace e2e-tests-projected-w4ml8 deletion completed in 6.135853185s

• [SLOW TEST:10.249 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:56:16.863: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-fc6jd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 03:56:16.928: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 03:56:39.015: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.11:8080/dial?request=hostName&protocol=http&host=192.168.68.138&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fc6jd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 03:56:39.015: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 03:56:39.114: INFO: Waiting for endpoints: map[]
Mar  6 03:56:39.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.11:8080/dial?request=hostName&protocol=http&host=192.168.160.10&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fc6jd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 03:56:39.117: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 03:56:39.205: INFO: Waiting for endpoints: map[]
Mar  6 03:56:39.208: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.11:8080/dial?request=hostName&protocol=http&host=192.168.232.74&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fc6jd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 03:56:39.208: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 03:56:39.297: INFO: Waiting for endpoints: map[]
Mar  6 03:56:39.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.11:8080/dial?request=hostName&protocol=http&host=192.168.40.138&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fc6jd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 03:56:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 03:56:39.388: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:56:39.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-fc6jd" for this suite.
Mar  6 03:57:01.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:57:01.479: INFO: namespace: e2e-tests-pod-network-test-fc6jd, resource: bindings, ignored listing per whitelist
Mar  6 03:57:01.553: INFO: namespace e2e-tests-pod-network-test-fc6jd deletion completed in 22.160257459s

• [SLOW TEST:44.690 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:57:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 03:57:01.634: INFO: Waiting up to 5m0s for pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-l7kqt" to be "success or failure"
Mar  6 03:57:01.637: INFO: Pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76781ms
Mar  6 03:57:03.641: INFO: Pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006664927s
Mar  6 03:57:05.644: INFO: Pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010409022s
Mar  6 03:57:07.649: INFO: Pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015098327s
STEP: Saw pod success
Mar  6 03:57:07.649: INFO: Pod "downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 03:57:07.652: INFO: Trying to get logs from node vmw3-k8s-05.local.dev pod downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 03:57:07.678: INFO: Waiting for pod downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e to disappear
Mar  6 03:57:07.680: INFO: Pod downward-api-e5b0dbbb-3fc3-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:57:07.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-l7kqt" for this suite.
Mar  6 03:57:13.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:57:13.777: INFO: namespace: e2e-tests-downward-api-l7kqt, resource: bindings, ignored listing per whitelist
Mar  6 03:57:13.840: INFO: namespace e2e-tests-downward-api-l7kqt deletion completed in 6.156673788s

• [SLOW TEST:12.287 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:57:13.841: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 03:57:13.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-6cxv5" to be "success or failure"
Mar  6 03:57:13.921: INFO: Pod "downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663406ms
Mar  6 03:57:15.925: INFO: Pod "downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00718246s
STEP: Saw pod success
Mar  6 03:57:15.925: INFO: Pod "downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 03:57:15.927: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 03:57:15.945: INFO: Waiting for pod downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e to disappear
Mar  6 03:57:15.948: INFO: Pod downwardapi-volume-ed02df0a-3fc3-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:57:15.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6cxv5" for this suite.
Mar  6 03:57:21.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:57:22.063: INFO: namespace: e2e-tests-downward-api-6cxv5, resource: bindings, ignored listing per whitelist
Mar  6 03:57:22.088: INFO: namespace e2e-tests-downward-api-6cxv5 deletion completed in 6.135464463s

• [SLOW TEST:8.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:57:22.088: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Mar  6 03:57:33.431: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:57:50.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-bts7z" for this suite.
Mar  6 03:57:56.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:57:56.368: INFO: namespace: e2e-tests-namespaces-bts7z, resource: bindings, ignored listing per whitelist
Mar  6 03:57:56.381: INFO: namespace e2e-tests-namespaces-bts7z deletion completed in 6.145229126s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mdgpk" for this suite.
Mar  6 03:57:56.383: INFO: Namespace e2e-tests-nsdeletetest-mdgpk was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-dkm72" for this suite.
Mar  6 03:58:02.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:58:02.419: INFO: namespace: e2e-tests-nsdeletetest-dkm72, resource: bindings, ignored listing per whitelist
Mar  6 03:58:02.527: INFO: namespace e2e-tests-nsdeletetest-dkm72 deletion completed in 6.14413635s

• [SLOW TEST:40.439 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:58:02.527: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 03:58:24.617: INFO: Container started at 2019-03-06 03:58:05 +0000 UTC, pod became ready at 2019-03-06 03:58:23 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:58:24.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-58ch7" for this suite.
Mar  6 03:58:46.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:58:46.684: INFO: namespace: e2e-tests-container-probe-58ch7, resource: bindings, ignored listing per whitelist
Mar  6 03:58:46.775: INFO: namespace e2e-tests-container-probe-58ch7 deletion completed in 22.152895052s

• [SLOW TEST:44.248 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:58:46.775: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar  6 03:58:46.868: INFO: Waiting up to 5m0s for pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-var-expansion-c7n4n" to be "success or failure"
Mar  6 03:58:46.872: INFO: Pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128716ms
Mar  6 03:58:48.876: INFO: Pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007573269s
Mar  6 03:58:50.879: INFO: Pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010663951s
Mar  6 03:58:52.882: INFO: Pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014453707s
STEP: Saw pod success
Mar  6 03:58:52.882: INFO: Pod "var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 03:58:52.885: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 03:58:52.907: INFO: Waiting for pod var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e to disappear
Mar  6 03:58:52.910: INFO: Pod var-expansion-246a5fcd-3fc4-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:58:52.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-c7n4n" for this suite.
Mar  6 03:58:58.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:58:58.970: INFO: namespace: e2e-tests-var-expansion-c7n4n, resource: bindings, ignored listing per whitelist
Mar  6 03:58:59.064: INFO: namespace e2e-tests-var-expansion-c7n4n deletion completed in 6.149952388s

• [SLOW TEST:12.289 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:58:59.065: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 03:59:01.675: INFO: Successfully updated pod "annotationupdate2bbb6221-3fc4-11e9-8de6-d63fb0ed442e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 03:59:05.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pplkz" for this suite.
Mar  6 03:59:27.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 03:59:27.760: INFO: namespace: e2e-tests-downward-api-pplkz, resource: bindings, ignored listing per whitelist
Mar  6 03:59:27.853: INFO: namespace e2e-tests-downward-api-pplkz deletion completed in 22.139639993s

• [SLOW TEST:28.788 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 03:59:27.853: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:00:27.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-f64t6" for this suite.
Mar  6 04:00:49.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:00:50.051: INFO: namespace: e2e-tests-container-probe-f64t6, resource: bindings, ignored listing per whitelist
Mar  6 04:00:50.091: INFO: namespace e2e-tests-container-probe-f64t6 deletion completed in 22.154364369s

• [SLOW TEST:82.238 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:00:50.091: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar  6 04:00:50.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 cluster-info'
Mar  6 04:00:50.322: INFO: stderr: ""
Mar  6 04:00:50.322: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:00:50.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f6cls" for this suite.
Mar  6 04:00:56.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:00:56.470: INFO: namespace: e2e-tests-kubectl-f6cls, resource: bindings, ignored listing per whitelist
Mar  6 04:00:56.475: INFO: namespace e2e-tests-kubectl-f6cls deletion completed in 6.148301067s

• [SLOW TEST:6.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:00:56.475: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:01:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-4sdfg" for this suite.
Mar  6 04:01:27.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:01:27.704: INFO: namespace: e2e-tests-replication-controller-4sdfg, resource: bindings, ignored listing per whitelist
Mar  6 04:01:27.716: INFO: namespace e2e-tests-replication-controller-4sdfg deletion completed in 22.135516933s

• [SLOW TEST:31.241 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:01:27.716: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-84548b5e-3fc4-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:01:27.791: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-cfmbf" to be "success or failure"
Mar  6 04:01:27.793: INFO: Pod "pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749775ms
Mar  6 04:01:29.797: INFO: Pod "pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006416438s
STEP: Saw pod success
Mar  6 04:01:29.797: INFO: Pod "pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:01:29.800: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:01:29.821: INFO: Waiting for pod pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:01:29.824: INFO: Pod pod-projected-configmaps-84556fb8-3fc4-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:01:29.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cfmbf" for this suite.
Mar  6 04:01:35.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:01:35.965: INFO: namespace: e2e-tests-projected-cfmbf, resource: bindings, ignored listing per whitelist
Mar  6 04:01:35.982: INFO: namespace e2e-tests-projected-cfmbf deletion completed in 6.152907935s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:01:35.982: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-8942c590-3fc4-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:01:42.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gnldw" for this suite.
Mar  6 04:02:04.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:02:04.188: INFO: namespace: e2e-tests-configmap-gnldw, resource: bindings, ignored listing per whitelist
Mar  6 04:02:04.253: INFO: namespace e2e-tests-configmap-gnldw deletion completed in 22.144234022s

• [SLOW TEST:28.271 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:02:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:02:04.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-9mh9w" to be "success or failure"
Mar  6 04:02:04.333: INFO: Pod "downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863403ms
Mar  6 04:02:06.336: INFO: Pod "downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006117194s
STEP: Saw pod success
Mar  6 04:02:06.336: INFO: Pod "downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:02:06.339: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:02:06.358: INFO: Waiting for pod downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:02:06.361: INFO: Pod downwardapi-volume-9a1cb140-3fc4-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:02:06.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9mh9w" for this suite.
Mar  6 04:02:12.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:02:12.474: INFO: namespace: e2e-tests-projected-9mh9w, resource: bindings, ignored listing per whitelist
Mar  6 04:02:12.489: INFO: namespace e2e-tests-projected-9mh9w deletion completed in 6.125244206s

• [SLOW TEST:8.236 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:02:12.489: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  6 04:02:16.568: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9f038160-3fc4-11e9-8de6-d63fb0ed442e,GenerateName:,Namespace:e2e-tests-events-7vmsg,SelfLink:/api/v1/namespaces/e2e-tests-events-7vmsg/pods/send-events-9f038160-3fc4-11e9-8de6-d63fb0ed442e,UID:9f03edde-3fc4-11e9-91c9-005056979acf,ResourceVersion:5361,Generation:0,CreationTimestamp:2019-03-06 04:02:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 545783247,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.232.79"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-m2hcp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m2hcp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-m2hcp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001da2ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001da2bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:02:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:02:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:02:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:02:12 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.25,PodIP:192.168.232.79,StartTime:2019-03-06 04:02:12 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-06 04:02:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://d485d2ff94fec032558aae69e1e82537badc0db8d503f06f642a1e9a89c2f107}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  6 04:02:18.572: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  6 04:02:20.575: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:02:20.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-7vmsg" for this suite.
Mar  6 04:02:58.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:02:58.666: INFO: namespace: e2e-tests-events-7vmsg, resource: bindings, ignored listing per whitelist
Mar  6 04:02:58.738: INFO: namespace e2e-tests-events-7vmsg deletion completed in 38.152490995s

• [SLOW TEST:46.248 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:02:58.738: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qb8lt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 04:02:58.816: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 04:03:20.952: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.17:8080/dial?request=hostName&protocol=udp&host=192.168.68.140&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qb8lt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 04:03:20.952: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 04:03:21.084: INFO: Waiting for endpoints: map[]
Mar  6 04:03:21.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.17:8080/dial?request=hostName&protocol=udp&host=192.168.160.16&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qb8lt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 04:03:21.088: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 04:03:21.178: INFO: Waiting for endpoints: map[]
Mar  6 04:03:21.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.17:8080/dial?request=hostName&protocol=udp&host=192.168.232.80&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qb8lt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 04:03:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 04:03:21.277: INFO: Waiting for endpoints: map[]
Mar  6 04:03:21.280: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.160.17:8080/dial?request=hostName&protocol=udp&host=192.168.40.141&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qb8lt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 04:03:21.280: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 04:03:21.387: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:03:21.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qb8lt" for this suite.
Mar  6 04:03:43.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:03:43.425: INFO: namespace: e2e-tests-pod-network-test-qb8lt, resource: bindings, ignored listing per whitelist
Mar  6 04:03:43.538: INFO: namespace e2e-tests-pod-network-test-qb8lt deletion completed in 22.146058247s

• [SLOW TEST:44.800 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:03:43.538: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 04:03:43.611: INFO: Waiting up to 5m0s for pod "pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-c5jfl" to be "success or failure"
Mar  6 04:03:43.614: INFO: Pod "pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386071ms
Mar  6 04:03:45.618: INFO: Pod "pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006118811s
STEP: Saw pod success
Mar  6 04:03:45.618: INFO: Pod "pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:03:45.620: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:03:45.639: INFO: Waiting for pod pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:03:45.641: INFO: Pod pod-d549d476-3fc4-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:03:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-c5jfl" for this suite.
Mar  6 04:03:51.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:03:51.685: INFO: namespace: e2e-tests-emptydir-c5jfl, resource: bindings, ignored listing per whitelist
Mar  6 04:03:51.777: INFO: namespace e2e-tests-emptydir-c5jfl deletion completed in 6.131917059s

• [SLOW TEST:8.239 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:03:51.777: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:03:51.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-hm2h5" to be "success or failure"
Mar  6 04:03:51.849: INFO: Pod "downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110428ms
Mar  6 04:03:53.852: INFO: Pod "downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006832403s
STEP: Saw pod success
Mar  6 04:03:53.852: INFO: Pod "downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:03:53.855: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:03:53.873: INFO: Waiting for pod downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:03:53.876: INFO: Pod downwardapi-volume-da322b75-3fc4-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:03:53.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hm2h5" for this suite.
Mar  6 04:03:59.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:03:59.936: INFO: namespace: e2e-tests-downward-api-hm2h5, resource: bindings, ignored listing per whitelist
Mar  6 04:04:00.019: INFO: namespace e2e-tests-downward-api-hm2h5 deletion completed in 6.140157609s

• [SLOW TEST:8.242 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:04:00.020: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:04:00.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mxw6t" for this suite.
Mar  6 04:04:22.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:04:22.157: INFO: namespace: e2e-tests-pods-mxw6t, resource: bindings, ignored listing per whitelist
Mar  6 04:04:22.267: INFO: namespace e2e-tests-pods-mxw6t deletion completed in 22.154111611s

• [SLOW TEST:22.247 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:04:22.267: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-c7pm4
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-c7pm4
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-c7pm4
Mar  6 04:04:22.346: INFO: Found 0 stateful pods, waiting for 1
Mar  6 04:04:32.349: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  6 04:04:32.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 04:04:32.556: INFO: stderr: ""
Mar  6 04:04:32.556: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 04:04:32.556: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 04:04:32.559: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 04:04:42.564: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 04:04:42.564: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 04:04:42.575: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:04:42.575: INFO: ss-0  vmw3-k8s-07.local.dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:04:42.575: INFO: 
Mar  6 04:04:42.575: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  6 04:04:43.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99724136s
Mar  6 04:04:44.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992386634s
Mar  6 04:04:45.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98856456s
Mar  6 04:04:46.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984638699s
Mar  6 04:04:47.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980605347s
Mar  6 04:04:48.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97658593s
Mar  6 04:04:49.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972020334s
Mar  6 04:04:50.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968123713s
Mar  6 04:04:51.613: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.887651ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-c7pm4
Mar  6 04:04:52.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:04:52.844: INFO: stderr: ""
Mar  6 04:04:52.844: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 04:04:52.844: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 04:04:52.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:04:53.041: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  6 04:04:53.041: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 04:04:53.041: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 04:04:53.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:04:53.230: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  6 04:04:53.230: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 04:04:53.230: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 04:04:53.234: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 04:04:53.234: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 04:04:53.234: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  6 04:04:53.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 04:04:53.442: INFO: stderr: ""
Mar  6 04:04:53.442: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 04:04:53.442: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 04:04:53.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 04:04:53.610: INFO: stderr: ""
Mar  6 04:04:53.610: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 04:04:53.610: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 04:04:53.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 04:04:53.812: INFO: stderr: ""
Mar  6 04:04:53.812: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 04:04:53.812: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 04:04:53.812: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 04:04:53.816: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  6 04:05:03.823: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 04:05:03.823: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 04:05:03.823: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 04:05:03.833: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:03.833: INFO: ss-0  vmw3-k8s-07.local.dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:03.833: INFO: ss-1  vmw3-k8s-06.local.dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:03.833: INFO: ss-2  vmw3-k8s-04.local.dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:03.833: INFO: 
Mar  6 04:05:03.833: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 04:05:04.838: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:04.838: INFO: ss-0  vmw3-k8s-07.local.dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:04.838: INFO: ss-1  vmw3-k8s-06.local.dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:04.838: INFO: ss-2  vmw3-k8s-04.local.dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:04.838: INFO: 
Mar  6 04:05:04.838: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 04:05:05.842: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:05.843: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:05.843: INFO: ss-1  vmw3-k8s-06.local.dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:05.843: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:05.843: INFO: 
Mar  6 04:05:05.843: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 04:05:06.847: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:06.847: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:06.847: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:06.847: INFO: 
Mar  6 04:05:06.847: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:07.852: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:07.852: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:07.852: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:07.852: INFO: 
Mar  6 04:05:07.852: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:08.856: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:08.856: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:08.856: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:08.856: INFO: 
Mar  6 04:05:08.856: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:09.860: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:09.860: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:09.860: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:09.860: INFO: 
Mar  6 04:05:09.860: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:10.864: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:10.864: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:10.864: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:10.864: INFO: 
Mar  6 04:05:10.864: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:11.869: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:11.869: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:11.869: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:11.869: INFO: 
Mar  6 04:05:11.869: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  6 04:05:12.873: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Mar  6 04:05:12.873: INFO: ss-0  vmw3-k8s-07.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:22 +0000 UTC  }]
Mar  6 04:05:12.873: INFO: ss-2  vmw3-k8s-04.local.dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:04:42 +0000 UTC  }]
Mar  6 04:05:12.873: INFO: 
Mar  6 04:05:12.873: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-c7pm4
Mar  6 04:05:13.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:05:14.016: INFO: rc: 1
Mar  6 04:05:14.016: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00217dcb0 exit status 1 <nil> <nil> true [0xc001a729c8 0xc001a729e0 0xc001a729f8] [0xc001a729c8 0xc001a729e0 0xc001a729f8] [0xc001a729d8 0xc001a729f0] [0x92f8e0 0x92f8e0] 0xc001d78180 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Mar  6 04:05:24.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:05:24.108: INFO: rc: 1
Mar  6 04:05:24.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001cd70e0 exit status 1 <nil> <nil> true [0xc001a72a00 0xc001a72a18 0xc001a72a30] [0xc001a72a00 0xc001a72a18 0xc001a72a30] [0xc001a72a10 0xc001a72a28] [0x92f8e0 0x92f8e0] 0xc002423740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:05:34.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:05:34.194: INFO: rc: 1
Mar  6 04:05:34.194: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001966390 exit status 1 <nil> <nil> true [0xc001e7e028 0xc001e7e048 0xc001e7e060] [0xc001e7e028 0xc001e7e048 0xc001e7e060] [0xc001e7e040 0xc001e7e058] [0x92f8e0 0x92f8e0] 0xc001be4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:05:44.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:05:44.280: INFO: rc: 1
Mar  6 04:05:44.281: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024083c0 exit status 1 <nil> <nil> true [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8028 0xc0015f8060] [0x92f8e0 0x92f8e0] 0xc0015ee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:05:54.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:05:54.359: INFO: rc: 1
Mar  6 04:05:54.359: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f884b0 exit status 1 <nil> <nil> true [0xc000320050 0xc0003202a0 0xc0003206c0] [0xc000320050 0xc0003202a0 0xc0003206c0] [0xc000320220 0xc000320498] [0x92f8e0 0x92f8e0] 0xc002136240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:04.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:04.445: INFO: rc: 1
Mar  6 04:06:04.445: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f88840 exit status 1 <nil> <nil> true [0xc000320710 0xc0003209c8 0xc000320ca8] [0xc000320710 0xc0003209c8 0xc000320ca8] [0xc000320850 0xc000320c48] [0x92f8e0 0x92f8e0] 0xc002136540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:14.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:14.534: INFO: rc: 1
Mar  6 04:06:14.534: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f88bd0 exit status 1 <nil> <nil> true [0xc000320d78 0xc000320e98 0xc0003210a8] [0xc000320d78 0xc000320e98 0xc0003210a8] [0xc000320e40 0xc000320fd0] [0x92f8e0 0x92f8e0] 0xc002136840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:24.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:24.616: INFO: rc: 1
Mar  6 04:06:24.616: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019667b0 exit status 1 <nil> <nil> true [0xc001e7e080 0xc001e7e0f0 0xc001e7e110] [0xc001e7e080 0xc001e7e0f0 0xc001e7e110] [0xc001e7e0c8 0xc001e7e108] [0x92f8e0 0x92f8e0] 0xc001be4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:34.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:34.709: INFO: rc: 1
Mar  6 04:06:34.709: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001966b40 exit status 1 <nil> <nil> true [0xc001e7e120 0xc001e7e138 0xc001e7e158] [0xc001e7e120 0xc001e7e138 0xc001e7e158] [0xc001e7e130 0xc001e7e148] [0x92f8e0 0x92f8e0] 0xc001be4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:44.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:44.840: INFO: rc: 1
Mar  6 04:06:44.840: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024087e0 exit status 1 <nil> <nil> true [0xc0015f8080 0xc0015f80a0 0xc0015f80b8] [0xc0015f8080 0xc0015f80a0 0xc0015f80b8] [0xc0015f8098 0xc0015f80b0] [0x92f8e0 0x92f8e0] 0xc0015eea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:06:54.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:06:54.919: INFO: rc: 1
Mar  6 04:06:54.919: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002408b70 exit status 1 <nil> <nil> true [0xc0015f80c0 0xc0015f80d8 0xc0015f80f0] [0xc0015f80c0 0xc0015f80d8 0xc0015f80f0] [0xc0015f80d0 0xc0015f80e8] [0x92f8e0 0x92f8e0] 0xc0015eede0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:04.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:04.992: INFO: rc: 1
Mar  6 04:07:04.992: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f88ff0 exit status 1 <nil> <nil> true [0xc000321100 0xc0003211b0 0xc0003212b8] [0xc000321100 0xc0003211b0 0xc0003212b8] [0xc000321168 0xc000321238] [0x92f8e0 0x92f8e0] 0xc002136c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:14.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:15.077: INFO: rc: 1
Mar  6 04:07:15.078: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002408f00 exit status 1 <nil> <nil> true [0xc0015f80f8 0xc0015f8110 0xc0015f8128] [0xc0015f80f8 0xc0015f8110 0xc0015f8128] [0xc0015f8108 0xc0015f8120] [0x92f8e0 0x92f8e0] 0xc0015ef0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:25.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:25.182: INFO: rc: 1
Mar  6 04:07:25.182: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024092f0 exit status 1 <nil> <nil> true [0xc0015f8138 0xc0015f8150 0xc0015f8168] [0xc0015f8138 0xc0015f8150 0xc0015f8168] [0xc0015f8148 0xc0015f8160] [0x92f8e0 0x92f8e0] 0xc0015ef3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:35.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:35.264: INFO: rc: 1
Mar  6 04:07:35.264: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f884e0 exit status 1 <nil> <nil> true [0xc001e7e000 0xc001e7e040 0xc001e7e058] [0xc001e7e000 0xc001e7e040 0xc001e7e058] [0xc001e7e038 0xc001e7e050] [0x92f8e0 0x92f8e0] 0xc001be4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:45.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:45.360: INFO: rc: 1
Mar  6 04:07:45.360: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f888a0 exit status 1 <nil> <nil> true [0xc001e7e060 0xc001e7e0c8 0xc001e7e108] [0xc001e7e060 0xc001e7e0c8 0xc001e7e108] [0xc001e7e0c0 0xc001e7e0f8] [0x92f8e0 0x92f8e0] 0xc001be4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:07:55.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:07:55.444: INFO: rc: 1
Mar  6 04:07:55.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001510390 exit status 1 <nil> <nil> true [0xc000320050 0xc0003202a0 0xc0003206c0] [0xc000320050 0xc0003202a0 0xc0003206c0] [0xc000320220 0xc000320498] [0x92f8e0 0x92f8e0] 0xc002136240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:05.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:05.534: INFO: rc: 1
Mar  6 04:08:05.534: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001510990 exit status 1 <nil> <nil> true [0xc000320710 0xc0003209c8 0xc000320ca8] [0xc000320710 0xc0003209c8 0xc000320ca8] [0xc000320850 0xc000320c48] [0x92f8e0 0x92f8e0] 0xc002136540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:15.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:15.625: INFO: rc: 1
Mar  6 04:08:15.625: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f88c90 exit status 1 <nil> <nil> true [0xc001e7e110 0xc001e7e130 0xc001e7e148] [0xc001e7e110 0xc001e7e130 0xc001e7e148] [0xc001e7e128 0xc001e7e140] [0x92f8e0 0x92f8e0] 0xc001be4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:25.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:25.709: INFO: rc: 1
Mar  6 04:08:25.709: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f89050 exit status 1 <nil> <nil> true [0xc001e7e158 0xc001e7e178 0xc001e7e1a0] [0xc001e7e158 0xc001e7e178 0xc001e7e1a0] [0xc001e7e170 0xc001e7e190] [0x92f8e0 0x92f8e0] 0xc001be4b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:35.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:35.785: INFO: rc: 1
Mar  6 04:08:35.785: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024083f0 exit status 1 <nil> <nil> true [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8028 0xc0015f8060] [0x92f8e0 0x92f8e0] 0xc0015ee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:45.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:45.868: INFO: rc: 1
Mar  6 04:08:45.868: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024087b0 exit status 1 <nil> <nil> true [0xc0015f8080 0xc0015f80a0 0xc0015f80b8] [0xc0015f8080 0xc0015f80a0 0xc0015f80b8] [0xc0015f8098 0xc0015f80b0] [0x92f8e0 0x92f8e0] 0xc0015eea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:08:55.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:08:55.970: INFO: rc: 1
Mar  6 04:08:55.970: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f89470 exit status 1 <nil> <nil> true [0xc001e7e1a8 0xc001e7e1c8 0xc001e7e1e0] [0xc001e7e1a8 0xc001e7e1c8 0xc001e7e1e0] [0xc001e7e1c0 0xc001e7e1d8] [0x92f8e0 0x92f8e0] 0xc001be4e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:05.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:06.048: INFO: rc: 1
Mar  6 04:09:06.049: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002408bd0 exit status 1 <nil> <nil> true [0xc0015f80c0 0xc0015f80d8 0xc0015f80f0] [0xc0015f80c0 0xc0015f80d8 0xc0015f80f0] [0xc0015f80d0 0xc0015f80e8] [0x92f8e0 0x92f8e0] 0xc0015eede0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:16.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:16.130: INFO: rc: 1
Mar  6 04:09:16.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001510e40 exit status 1 <nil> <nil> true [0xc000320d78 0xc000320e98 0xc0003210a8] [0xc000320d78 0xc000320e98 0xc0003210a8] [0xc000320e40 0xc000320fd0] [0x92f8e0 0x92f8e0] 0xc002136840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:26.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:26.228: INFO: rc: 1
Mar  6 04:09:26.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015103c0 exit status 1 <nil> <nil> true [0xc0003200e8 0xc0003203a0 0xc000320710] [0xc0003200e8 0xc0003203a0 0xc000320710] [0xc0003202a0 0xc0003206c0] [0x92f8e0 0x92f8e0] 0xc002136240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:36.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:36.310: INFO: rc: 1
Mar  6 04:09:36.310: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015109c0 exit status 1 <nil> <nil> true [0xc000320750 0xc000320c20 0xc000320d78] [0xc000320750 0xc000320c20 0xc000320d78] [0xc0003209c8 0xc000320ca8] [0x92f8e0 0x92f8e0] 0xc002136540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:46.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:46.394: INFO: rc: 1
Mar  6 04:09:46.394: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f88510 exit status 1 <nil> <nil> true [0xc001e7e000 0xc001e7e040 0xc001e7e058] [0xc001e7e000 0xc001e7e040 0xc001e7e058] [0xc001e7e038 0xc001e7e050] [0x92f8e0 0x92f8e0] 0xc001be4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:09:56.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:09:56.471: INFO: rc: 1
Mar  6 04:09:56.472: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001966390 exit status 1 <nil> <nil> true [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8000 0xc0015f8030 0xc0015f8078] [0xc0015f8028 0xc0015f8060] [0x92f8e0 0x92f8e0] 0xc0015ee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:10:06.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:10:06.544: INFO: rc: 1
Mar  6 04:10:06.544: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001510ed0 exit status 1 <nil> <nil> true [0xc000320da0 0xc000320f80 0xc000321100] [0xc000320da0 0xc000320f80 0xc000321100] [0xc000320e98 0xc0003210a8] [0x92f8e0 0x92f8e0] 0xc002136840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar  6 04:10:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-c7pm4 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 04:10:16.639: INFO: rc: 1
Mar  6 04:10:16.639: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Mar  6 04:10:16.639: INFO: Scaling statefulset ss to 0
Mar  6 04:10:16.650: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 04:10:16.653: INFO: Deleting all statefulset in ns e2e-tests-statefulset-c7pm4
Mar  6 04:10:16.656: INFO: Scaling statefulset ss to 0
Mar  6 04:10:16.668: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 04:10:16.671: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:16.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-c7pm4" for this suite.
Mar  6 04:10:22.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:10:22.792: INFO: namespace: e2e-tests-statefulset-c7pm4, resource: bindings, ignored listing per whitelist
Mar  6 04:10:22.821: INFO: namespace e2e-tests-statefulset-c7pm4 deletion completed in 6.131474918s

• [SLOW TEST:360.554 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:10:22.821: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:10:22.899: INFO: (0) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.339946ms)
Mar  6 04:10:22.902: INFO: (1) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.2782ms)
Mar  6 04:10:22.905: INFO: (2) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.378116ms)
Mar  6 04:10:22.909: INFO: (3) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.517338ms)
Mar  6 04:10:22.913: INFO: (4) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.637212ms)
Mar  6 04:10:22.916: INFO: (5) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.958293ms)
Mar  6 04:10:22.919: INFO: (6) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.670337ms)
Mar  6 04:10:22.923: INFO: (7) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.848055ms)
Mar  6 04:10:22.927: INFO: (8) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.775921ms)
Mar  6 04:10:22.930: INFO: (9) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.272481ms)
Mar  6 04:10:22.934: INFO: (10) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.289329ms)
Mar  6 04:10:22.937: INFO: (11) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.047667ms)
Mar  6 04:10:22.940: INFO: (12) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.667227ms)
Mar  6 04:10:22.944: INFO: (13) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.951137ms)
Mar  6 04:10:22.948: INFO: (14) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.412272ms)
Mar  6 04:10:22.951: INFO: (15) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.586713ms)
Mar  6 04:10:22.955: INFO: (16) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.866132ms)
Mar  6 04:10:22.959: INFO: (17) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.306465ms)
Mar  6 04:10:22.963: INFO: (18) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.012157ms)
Mar  6 04:10:22.966: INFO: (19) /api/v1/nodes/vmw3-k8s-04.local.dev:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.209221ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:22.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-sh4gd" for this suite.
Mar  6 04:10:28.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:10:29.133: INFO: namespace: e2e-tests-proxy-sh4gd, resource: bindings, ignored listing per whitelist
Mar  6 04:10:29.133: INFO: namespace e2e-tests-proxy-sh4gd deletion completed in 6.163567602s

• [SLOW TEST:6.312 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:10:29.134: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c70ac97b-3fc5-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:10:29.211: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-bv284" to be "success or failure"
Mar  6 04:10:29.214: INFO: Pod "pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.516314ms
Mar  6 04:10:31.218: INFO: Pod "pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007559973s
STEP: Saw pod success
Mar  6 04:10:31.218: INFO: Pod "pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:10:31.221: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:10:31.248: INFO: Waiting for pod pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:10:31.250: INFO: Pod pod-projected-configmaps-c70ba2ea-3fc5-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bv284" for this suite.
Mar  6 04:10:37.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:10:37.309: INFO: namespace: e2e-tests-projected-bv284, resource: bindings, ignored listing per whitelist
Mar  6 04:10:37.399: INFO: namespace e2e-tests-projected-bv284 deletion completed in 6.144346134s

• [SLOW TEST:8.266 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:10:37.399: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-cbf7f8ad-3fc5-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:10:37.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-lcrkv" to be "success or failure"
Mar  6 04:10:37.478: INFO: Pod "pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355238ms
Mar  6 04:10:39.482: INFO: Pod "pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194451s
Mar  6 04:10:41.485: INFO: Pod "pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009372474s
STEP: Saw pod success
Mar  6 04:10:41.485: INFO: Pod "pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:10:41.488: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:10:41.510: INFO: Waiting for pod pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:10:41.512: INFO: Pod pod-configmaps-cbf8eb98-3fc5-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:41.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lcrkv" for this suite.
Mar  6 04:10:47.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:10:47.548: INFO: namespace: e2e-tests-configmap-lcrkv, resource: bindings, ignored listing per whitelist
Mar  6 04:10:47.662: INFO: namespace e2e-tests-configmap-lcrkv deletion completed in 6.145802807s

• [SLOW TEST:10.262 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:10:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:47.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-9j4fm" for this suite.
Mar  6 04:10:53.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:10:53.800: INFO: namespace: e2e-tests-kubelet-test-9j4fm, resource: bindings, ignored listing per whitelist
Mar  6 04:10:53.893: INFO: namespace e2e-tests-kubelet-test-9j4fm deletion completed in 6.142424411s

• [SLOW TEST:6.232 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:10:53.894: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:10:55.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-446rr" for this suite.
Mar  6 04:11:40.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:11:40.062: INFO: namespace: e2e-tests-kubelet-test-446rr, resource: bindings, ignored listing per whitelist
Mar  6 04:11:40.127: INFO: namespace e2e-tests-kubelet-test-446rr deletion completed in 44.135708471s

• [SLOW TEST:46.234 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:11:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  6 04:11:40.201: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2llkw,SelfLink:/api/v1/namespaces/e2e-tests-watch-2llkw/configmaps/e2e-watch-test-watch-closed,UID:f15b38f3-3fc5-11e9-91c9-005056979acf,ResourceVersion:7544,Generation:0,CreationTimestamp:2019-03-06 04:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 04:11:40.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2llkw,SelfLink:/api/v1/namespaces/e2e-tests-watch-2llkw/configmaps/e2e-watch-test-watch-closed,UID:f15b38f3-3fc5-11e9-91c9-005056979acf,ResourceVersion:7545,Generation:0,CreationTimestamp:2019-03-06 04:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  6 04:11:40.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2llkw,SelfLink:/api/v1/namespaces/e2e-tests-watch-2llkw/configmaps/e2e-watch-test-watch-closed,UID:f15b38f3-3fc5-11e9-91c9-005056979acf,ResourceVersion:7546,Generation:0,CreationTimestamp:2019-03-06 04:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 04:11:40.215: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2llkw,SelfLink:/api/v1/namespaces/e2e-tests-watch-2llkw/configmaps/e2e-watch-test-watch-closed,UID:f15b38f3-3fc5-11e9-91c9-005056979acf,ResourceVersion:7547,Generation:0,CreationTimestamp:2019-03-06 04:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:11:40.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-2llkw" for this suite.
Mar  6 04:11:46.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:11:46.370: INFO: namespace: e2e-tests-watch-2llkw, resource: bindings, ignored listing per whitelist
Mar  6 04:11:46.370: INFO: namespace e2e-tests-watch-2llkw deletion completed in 6.15152483s

• [SLOW TEST:6.243 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:11:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-f5198f97-3fc5-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f5198f97-3fc5-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:11:50.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vklhn" for this suite.
Mar  6 04:12:12.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:12:12.552: INFO: namespace: e2e-tests-configmap-vklhn, resource: bindings, ignored listing per whitelist
Mar  6 04:12:12.675: INFO: namespace e2e-tests-configmap-vklhn deletion completed in 22.141245813s

• [SLOW TEST:26.305 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:12:12.675: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:12:12.762: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 04:12:12.772: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:12.772: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:12.772: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:12.774: INFO: Number of nodes with available pods: 0
Mar  6 04:12:12.774: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:12:13.779: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:13.780: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:13.780: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:13.783: INFO: Number of nodes with available pods: 0
Mar  6 04:12:13.783: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:12:14.778: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:14.778: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:14.778: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:14.782: INFO: Number of nodes with available pods: 1
Mar  6 04:12:14.782: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:12:15.779: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:15.779: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:15.779: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:15.782: INFO: Number of nodes with available pods: 1
Mar  6 04:12:15.782: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:12:16.780: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:16.780: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:16.780: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:16.783: INFO: Number of nodes with available pods: 4
Mar  6 04:12:16.783: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  6 04:12:16.809: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:16.809: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:16.809: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:16.809: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:16.814: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:16.814: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:16.814: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:17.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:17.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:17.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:17.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:17.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:17.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:17.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:18.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:18.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:18.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:18.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:19.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:19.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:19.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:19.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:19.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:19.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:19.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:20.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:20.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:20.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:20.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:20.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:20.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:20.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:21.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:21.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:21.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:21.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:21.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:21.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:21.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:22.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:22.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:22.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:22.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:23.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:23.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:23.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:23.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:24.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:24.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:24.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:24.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:24.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:24.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:25.820: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:25.820: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:25.820: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:25.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:25.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:25.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:25.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:26.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:26.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:26.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:26.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:26.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:26.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:26.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:27.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:27.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:27.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:27.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:27.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:27.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:27.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:28.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:28.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:28.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:28.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:28.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:28.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:28.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:29.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:29.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:29.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:29.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:29.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:29.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:29.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:30.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:30.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:30.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:30.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:31.820: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:31.820: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:31.820: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:31.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:31.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:31.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:31.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:32.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:32.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:32.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:32.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:33.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:33.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:33.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:33.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:33.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:33.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:33.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:34.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:34.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:34.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:34.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:34.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:34.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:34.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:35.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:35.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:35.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:35.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:35.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:35.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:35.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:36.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:36.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:36.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:36.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:37.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:37.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:37.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:37.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:37.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:37.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:37.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:38.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:38.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:38.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:38.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:38.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:38.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:38.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:39.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:39.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:39.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:39.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:40.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:40.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:40.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:40.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:41.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:41.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:41.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:41.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:42.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:42.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:42.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:42.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:42.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:42.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:42.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:43.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:43.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:43.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:43.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:43.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:43.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:43.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:44.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:44.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:44.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:44.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:45.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:45.819: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:45.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:45.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:45.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:45.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:45.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:46.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:46.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:46.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:46.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:47.820: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:47.820: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:47.820: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:47.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:47.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:47.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:47.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:48.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:48.818: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:48.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:48.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:48.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:48.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:48.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:49.823: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:49.823: INFO: Wrong image for pod: daemon-set-gzxc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:49.823: INFO: Pod daemon-set-gzxc2 is not available
Mar  6 04:12:49.823: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:49.823: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:49.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:49.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:49.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:50.818: INFO: Pod daemon-set-8crlj is not available
Mar  6 04:12:50.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:50.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:50.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:50.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:50.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:50.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:51.819: INFO: Pod daemon-set-8crlj is not available
Mar  6 04:12:51.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:51.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:51.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:52.819: INFO: Pod daemon-set-8crlj is not available
Mar  6 04:12:52.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:52.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:52.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:52.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:52.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:52.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:53.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:53.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:53.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:53.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:53.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:53.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:54.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:54.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:54.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:54.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:54.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:54.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:55.820: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:55.820: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:55.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:55.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:55.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:55.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:56.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:56.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:56.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:56.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:56.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:56.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:57.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:57.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:57.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:58.821: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:58.821: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:58.821: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:58.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:58.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:58.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:59.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:59.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:59.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:12:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:12:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:00.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:00.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:00.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:00.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:00.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:00.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:01.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:01.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:01.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:02.820: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:02.820: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:02.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:02.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:02.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:02.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:03.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:03.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:03.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:03.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:03.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:03.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:04.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:04.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:04.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:04.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:04.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:04.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:05.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:05.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:05.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:06.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:06.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:06.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:06.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:06.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:06.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:07.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:07.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:07.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:07.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:07.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:07.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:08.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:08.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:08.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:08.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:08.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:08.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:09.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:09.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:09.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:09.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:09.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:09.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:10.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:10.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:10.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:10.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:10.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:10.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:11.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:11.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:11.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:11.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:11.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:11.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:12.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:12.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:12.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:13.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:13.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:13.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:14.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:14.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:14.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:14.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:14.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:14.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:15.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:15.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:15.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:16.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:16.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:16.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:16.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:16.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:16.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:17.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:17.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:17.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:18.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:18.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:18.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:19.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:19.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:19.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:19.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:19.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:19.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:20.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:20.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:20.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:21.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:21.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:21.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:22.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:22.819: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:22.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:22.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:22.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:22.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:23.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:23.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:23.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:23.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:23.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:23.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:24.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:24.818: INFO: Wrong image for pod: daemon-set-j7dxh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:24.818: INFO: Pod daemon-set-j7dxh is not available
Mar  6 04:13:24.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:25.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:25.818: INFO: Pod daemon-set-hxxtm is not available
Mar  6 04:13:25.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:25.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:25.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:25.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:26.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:26.819: INFO: Pod daemon-set-hxxtm is not available
Mar  6 04:13:26.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:26.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:26.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:26.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:27.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:27.818: INFO: Pod daemon-set-hxxtm is not available
Mar  6 04:13:27.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:28.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:28.819: INFO: Pod daemon-set-hxxtm is not available
Mar  6 04:13:28.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:28.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:28.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:28.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:29.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:29.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:29.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:29.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:29.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:30.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:30.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:30.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:31.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:31.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:31.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:31.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:31.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:32.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:32.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:32.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:32.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:32.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:33.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:33.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:34.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:34.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:34.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:34.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:34.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:35.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:35.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:35.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:35.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:35.828: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:36.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:36.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:37.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:37.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:38.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:38.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:39.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:39.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:39.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:39.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:40.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:40.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:40.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:41.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:41.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:41.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:41.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:41.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:42.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:42.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:42.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:42.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:42.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:43.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:43.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:43.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:43.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:43.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:44.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:44.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:44.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:45.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:45.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:45.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:45.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:45.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:46.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:46.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:46.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:47.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:47.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:47.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:47.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:47.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:48.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:48.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:48.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:48.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:48.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:49.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:49.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:49.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:49.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:49.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:50.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:50.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:50.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:50.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:50.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:51.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:51.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:51.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:52.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:52.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:52.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:52.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:52.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:53.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:53.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:53.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:53.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:53.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:54.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:54.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:54.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:54.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:54.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:55.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:55.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:55.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:55.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:55.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:56.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:56.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:56.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:56.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:56.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:57.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:57.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:57.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:58.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:58.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:58.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:58.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:58.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:59.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:59.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:13:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:13:59.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:00.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:00.818: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:00.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:00.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:00.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:00.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:01.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:01.819: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:01.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:01.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:02.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:02.818: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:02.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:02.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:02.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:02.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:03.818: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:03.818: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:03.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:03.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:03.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:03.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:04.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:04.819: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:04.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:04.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:04.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:04.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:05.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:05.819: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:05.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:05.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:06.819: INFO: Wrong image for pod: daemon-set-8kdnf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:06.819: INFO: Pod daemon-set-8kdnf is not available
Mar  6 04:14:06.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:06.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:06.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:06.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:07.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:07.820: INFO: Pod daemon-set-z5qv4 is not available
Mar  6 04:14:07.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:07.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:07.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:08.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:08.819: INFO: Pod daemon-set-z5qv4 is not available
Mar  6 04:14:08.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:08.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:08.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:09.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:09.819: INFO: Pod daemon-set-z5qv4 is not available
Mar  6 04:14:09.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:09.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:09.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:10.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:10.819: INFO: Pod daemon-set-z5qv4 is not available
Mar  6 04:14:10.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:10.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:10.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:11.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:11.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:11.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:11.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:12.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:12.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:13.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:13.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:14.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:14.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:14.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:14.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:15.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:15.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:16.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:16.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:16.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:16.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:17.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:17.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:18.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:18.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:19.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:19.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:19.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:19.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:20.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:20.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:21.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:21.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:22.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:22.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:23.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:23.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:24.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:24.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:25.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:25.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:25.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:25.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:26.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:26.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:26.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:26.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:27.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:27.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:28.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:28.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:28.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:28.826: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:29.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:29.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:29.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:29.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:30.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:30.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:30.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:30.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:31.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:31.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:31.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:31.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:32.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:32.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:33.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:33.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:34.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:34.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:34.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:34.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:35.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:35.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:35.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:35.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:36.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:36.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:36.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:36.824: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:37.820: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:37.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:38.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:38.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:39.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:39.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:40.819: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:40.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:40.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:40.825: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:41.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:41.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:42.818: INFO: Wrong image for pod: daemon-set-xmxh6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  6 04:14:42.818: INFO: Pod daemon-set-xmxh6 is not available
Mar  6 04:14:42.821: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:42.821: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:42.822: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.819: INFO: Pod daemon-set-fv49g is not available
Mar  6 04:14:43.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.823: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  6 04:14:43.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.827: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:43.830: INFO: Number of nodes with available pods: 3
Mar  6 04:14:43.830: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:14:44.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:44.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:44.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:44.839: INFO: Number of nodes with available pods: 3
Mar  6 04:14:44.839: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:14:45.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:45.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:45.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:45.839: INFO: Number of nodes with available pods: 3
Mar  6 04:14:45.839: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:14:46.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:46.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:46.836: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:46.840: INFO: Number of nodes with available pods: 3
Mar  6 04:14:46.840: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:14:47.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:47.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:47.835: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:14:47.838: INFO: Number of nodes with available pods: 4
Mar  6 04:14:47.838: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-xsq8l, will wait for the garbage collector to delete the pods
Mar  6 04:14:47.914: INFO: Deleting DaemonSet.extensions daemon-set took: 7.01598ms
Mar  6 04:14:48.014: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.684083ms
Mar  6 04:14:51.518: INFO: Number of nodes with available pods: 0
Mar  6 04:14:51.518: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 04:14:51.521: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-xsq8l/daemonsets","resourceVersion":"8368"},"items":null}

Mar  6 04:14:51.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-xsq8l/pods","resourceVersion":"8368"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:14:51.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-xsq8l" for this suite.
Mar  6 04:14:57.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:14:57.647: INFO: namespace: e2e-tests-daemonsets-xsq8l, resource: bindings, ignored listing per whitelist
Mar  6 04:14:57.676: INFO: namespace e2e-tests-daemonsets-xsq8l deletion completed in 6.133584868s

• [SLOW TEST:165.001 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:14:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0306 04:15:37.778259      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 04:15:37.778: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:15:37.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-sjxvc" for this suite.
Mar  6 04:15:43.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:15:43.885: INFO: namespace: e2e-tests-gc-sjxvc, resource: bindings, ignored listing per whitelist
Mar  6 04:15:43.911: INFO: namespace e2e-tests-gc-sjxvc deletion completed in 6.127541643s

• [SLOW TEST:46.234 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:15:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-pkmh
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 04:15:44.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pkmh" in namespace "e2e-tests-subpath-dxm4c" to be "success or failure"
Mar  6 04:15:44.066: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Pending", Reason="", readiness=false. Elapsed: 16.596429ms
Mar  6 04:15:46.069: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020170741s
Mar  6 04:15:48.073: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 4.024095294s
Mar  6 04:15:50.077: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 6.027709505s
Mar  6 04:15:52.081: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 8.031944498s
Mar  6 04:15:54.085: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 10.036304372s
Mar  6 04:15:56.090: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 12.04073414s
Mar  6 04:15:58.093: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 14.044297685s
Mar  6 04:16:00.096: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 16.047541114s
Mar  6 04:16:02.102: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 18.052814295s
Mar  6 04:16:04.106: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Running", Reason="", readiness=false. Elapsed: 20.057110469s
Mar  6 04:16:06.111: INFO: Pod "pod-subpath-test-configmap-pkmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061644619s
STEP: Saw pod success
Mar  6 04:16:06.111: INFO: Pod "pod-subpath-test-configmap-pkmh" satisfied condition "success or failure"
Mar  6 04:16:06.114: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-subpath-test-configmap-pkmh container test-container-subpath-configmap-pkmh: <nil>
STEP: delete the pod
Mar  6 04:16:06.138: INFO: Waiting for pod pod-subpath-test-configmap-pkmh to disappear
Mar  6 04:16:06.141: INFO: Pod pod-subpath-test-configmap-pkmh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pkmh
Mar  6 04:16:06.141: INFO: Deleting pod "pod-subpath-test-configmap-pkmh" in namespace "e2e-tests-subpath-dxm4c"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:06.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-dxm4c" for this suite.
Mar  6 04:16:12.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:16:12.221: INFO: namespace: e2e-tests-subpath-dxm4c, resource: bindings, ignored listing per whitelist
Mar  6 04:16:12.287: INFO: namespace e2e-tests-subpath-dxm4c deletion completed in 6.139163173s

• [SLOW TEST:28.376 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:16:12.287: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9393f03f-3fc6-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:16:12.364: INFO: Waiting up to 5m0s for pod "pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-xdl2s" to be "success or failure"
Mar  6 04:16:12.370: INFO: Pod "pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.966559ms
Mar  6 04:16:14.374: INFO: Pod "pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009741297s
STEP: Saw pod success
Mar  6 04:16:14.374: INFO: Pod "pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:16:14.377: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:16:14.399: INFO: Waiting for pod pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:16:14.401: INFO: Pod pod-secrets-9394c3a5-3fc6-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:14.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xdl2s" for this suite.
Mar  6 04:16:20.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:16:20.530: INFO: namespace: e2e-tests-secrets-xdl2s, resource: bindings, ignored listing per whitelist
Mar  6 04:16:20.550: INFO: namespace e2e-tests-secrets-xdl2s deletion completed in 6.144203574s

• [SLOW TEST:8.263 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:16:20.550: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:20.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-zr544" for this suite.
Mar  6 04:16:26.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:16:26.683: INFO: namespace: e2e-tests-services-zr544, resource: bindings, ignored listing per whitelist
Mar  6 04:16:26.763: INFO: namespace e2e-tests-services-zr544 deletion completed in 6.141176795s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.213 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:16:26.764: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 04:16:26.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-x72c9'
Mar  6 04:16:27.080: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 04:16:27.080: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar  6 04:16:27.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-x72c9'
Mar  6 04:16:27.188: INFO: stderr: ""
Mar  6 04:16:27.188: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:27.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x72c9" for this suite.
Mar  6 04:16:49.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:16:49.324: INFO: namespace: e2e-tests-kubectl-x72c9, resource: bindings, ignored listing per whitelist
Mar  6 04:16:49.339: INFO: namespace e2e-tests-kubectl-x72c9 deletion completed in 22.145832925s

• [SLOW TEST:22.576 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:16:49.339: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  6 04:16:49.427: INFO: Waiting up to 5m0s for pod "pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-tknpc" to be "success or failure"
Mar  6 04:16:49.433: INFO: Pod "pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083348ms
Mar  6 04:16:51.437: INFO: Pod "pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009516153s
STEP: Saw pod success
Mar  6 04:16:51.437: INFO: Pod "pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:16:51.442: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:16:51.459: INFO: Waiting for pod pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:16:51.462: INFO: Pod pod-a9abbfa6-3fc6-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:51.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tknpc" for this suite.
Mar  6 04:16:57.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:16:57.520: INFO: namespace: e2e-tests-emptydir-tknpc, resource: bindings, ignored listing per whitelist
Mar  6 04:16:57.608: INFO: namespace e2e-tests-emptydir-tknpc deletion completed in 6.142784425s

• [SLOW TEST:8.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:16:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-ae97445e-3fc6-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:16:57.686: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-wwn98" to be "success or failure"
Mar  6 04:16:57.689: INFO: Pod "pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936218ms
Mar  6 04:16:59.693: INFO: Pod "pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006787263s
STEP: Saw pod success
Mar  6 04:16:59.693: INFO: Pod "pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:16:59.695: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:16:59.729: INFO: Waiting for pod pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:16:59.732: INFO: Pod pod-projected-secrets-ae983414-3fc6-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:16:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wwn98" for this suite.
Mar  6 04:17:05.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:17:05.817: INFO: namespace: e2e-tests-projected-wwn98, resource: bindings, ignored listing per whitelist
Mar  6 04:17:05.919: INFO: namespace e2e-tests-projected-wwn98 deletion completed in 6.181324157s

• [SLOW TEST:8.311 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:17:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 04:17:05.997: INFO: Waiting up to 5m0s for pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-bqgv7" to be "success or failure"
Mar  6 04:17:06.000: INFO: Pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503366ms
Mar  6 04:17:08.005: INFO: Pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007214732s
Mar  6 04:17:10.008: INFO: Pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01059998s
Mar  6 04:17:12.012: INFO: Pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014372993s
STEP: Saw pod success
Mar  6 04:17:12.012: INFO: Pod "pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:17:12.015: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:17:12.042: INFO: Waiting for pod pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:17:12.045: INFO: Pod pod-b38c2f72-3fc6-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:17:12.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bqgv7" for this suite.
Mar  6 04:17:18.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:17:18.183: INFO: namespace: e2e-tests-emptydir-bqgv7, resource: bindings, ignored listing per whitelist
Mar  6 04:17:18.194: INFO: namespace e2e-tests-emptydir-bqgv7 deletion completed in 6.144167039s

• [SLOW TEST:12.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:17:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-84qx
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 04:17:18.275: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-84qx" in namespace "e2e-tests-subpath-lqdlf" to be "success or failure"
Mar  6 04:17:18.279: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351797ms
Mar  6 04:17:20.282: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0069563s
Mar  6 04:17:22.287: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 4.012104445s
Mar  6 04:17:24.292: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 6.016254676s
Mar  6 04:17:26.295: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 8.020064992s
Mar  6 04:17:28.300: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 10.02444052s
Mar  6 04:17:30.304: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 12.028228862s
Mar  6 04:17:32.308: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 14.032860051s
Mar  6 04:17:34.312: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 16.036658694s
Mar  6 04:17:36.315: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 18.040007456s
Mar  6 04:17:38.320: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 20.044686978s
Mar  6 04:17:40.324: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Running", Reason="", readiness=false. Elapsed: 22.04914432s
Mar  6 04:17:42.329: INFO: Pod "pod-subpath-test-configmap-84qx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.053393506s
STEP: Saw pod success
Mar  6 04:17:42.329: INFO: Pod "pod-subpath-test-configmap-84qx" satisfied condition "success or failure"
Mar  6 04:17:42.331: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-subpath-test-configmap-84qx container test-container-subpath-configmap-84qx: <nil>
STEP: delete the pod
Mar  6 04:17:42.353: INFO: Waiting for pod pod-subpath-test-configmap-84qx to disappear
Mar  6 04:17:42.356: INFO: Pod pod-subpath-test-configmap-84qx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-84qx
Mar  6 04:17:42.356: INFO: Deleting pod "pod-subpath-test-configmap-84qx" in namespace "e2e-tests-subpath-lqdlf"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:17:42.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-lqdlf" for this suite.
Mar  6 04:17:48.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:17:48.488: INFO: namespace: e2e-tests-subpath-lqdlf, resource: bindings, ignored listing per whitelist
Mar  6 04:17:48.503: INFO: namespace e2e-tests-subpath-lqdlf deletion completed in 6.140755403s

• [SLOW TEST:30.309 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:17:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 04:17:52.606: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 04:17:52.609: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 04:17:54.609: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 04:17:54.613: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 04:17:56.609: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 04:17:56.613: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:17:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-fqsm7" for this suite.
Mar  6 04:18:18.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:18:18.732: INFO: namespace: e2e-tests-container-lifecycle-hook-fqsm7, resource: bindings, ignored listing per whitelist
Mar  6 04:18:18.767: INFO: namespace e2e-tests-container-lifecycle-hook-fqsm7 deletion completed in 22.136969851s

• [SLOW TEST:30.263 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:18:18.767: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 04:18:24.871: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:24.874: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:26.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:26.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:28.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:28.879: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:30.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:30.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:32.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:32.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:34.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:34.879: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:36.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:36.879: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:38.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:38.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:40.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:40.879: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:42.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:42.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:44.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:44.878: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:46.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:46.879: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 04:18:48.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 04:18:48.878: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:18:48.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-k97jw" for this suite.
Mar  6 04:19:10.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:19:10.924: INFO: namespace: e2e-tests-container-lifecycle-hook-k97jw, resource: bindings, ignored listing per whitelist
Mar  6 04:19:11.039: INFO: namespace e2e-tests-container-lifecycle-hook-k97jw deletion completed in 22.147409123s

• [SLOW TEST:52.272 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:19:11.039: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-fe21a402-3fc6-11e9-8de6-d63fb0ed442e
STEP: Creating secret with name s-test-opt-upd-fe21a5ac-3fc6-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-fe21a402-3fc6-11e9-8de6-d63fb0ed442e
STEP: Updating secret s-test-opt-upd-fe21a5ac-3fc6-11e9-8de6-d63fb0ed442e
STEP: Creating secret with name s-test-opt-create-fe21a658-3fc6-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:19:15.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4f7k2" for this suite.
Mar  6 04:19:37.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:19:37.352: INFO: namespace: e2e-tests-secrets-4f7k2, resource: bindings, ignored listing per whitelist
Mar  6 04:19:37.371: INFO: namespace e2e-tests-secrets-4f7k2 deletion completed in 22.137701753s

• [SLOW TEST:26.332 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:19:37.372: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:19:37.437: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:19:39.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-9gltb" for this suite.
Mar  6 04:20:31.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:20:31.534: INFO: namespace: e2e-tests-pods-9gltb, resource: bindings, ignored listing per whitelist
Mar  6 04:20:31.642: INFO: namespace e2e-tests-pods-9gltb deletion completed in 52.152169231s

• [SLOW TEST:54.270 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:20:31.642: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  6 04:20:33.738: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-2e2b2876-3fc7-11e9-8de6-d63fb0ed442e", GenerateName:"", Namespace:"e2e-tests-pods-hhgvt", SelfLink:"/api/v1/namespaces/e2e-tests-pods-hhgvt/pods/pod-submit-remove-2e2b2876-3fc7-11e9-8de6-d63fb0ed442e", UID:"2e2c3a35-3fc7-11e9-91c9-005056979acf", ResourceVersion:"10168", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687442831, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"713297378"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.40.155\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-nb8sc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000f7f740), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nb8sc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00133c008), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"vmw3-k8s-04.local.dev", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002459f20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00133c040)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00133c060)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00133c068), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00133c06c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687442831, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687442832, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687442832, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687442831, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.5.1.20", PodIP:"192.168.40.155", StartTime:(*v1.Time)(0xc001a7ae60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001a7ae80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/nginx:1.14-alpine", ImageID:"docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://b024b7903bccb5276cbd827825d5a70134236e930a01d2328cbb9729c340da21"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:20:47.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hhgvt" for this suite.
Mar  6 04:20:53.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:20:53.429: INFO: namespace: e2e-tests-pods-hhgvt, resource: bindings, ignored listing per whitelist
Mar  6 04:20:53.460: INFO: namespace e2e-tests-pods-hhgvt deletion completed in 6.156039389s

• [SLOW TEST:21.818 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:20:53.460: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 04:20:53.549: INFO: Waiting up to 5m0s for pod "pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-m5pz4" to be "success or failure"
Mar  6 04:20:53.552: INFO: Pod "pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596675ms
Mar  6 04:20:55.555: INFO: Pod "pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006469372s
STEP: Saw pod success
Mar  6 04:20:55.555: INFO: Pod "pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:20:55.558: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:20:55.577: INFO: Waiting for pod pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:20:55.580: INFO: Pod pod-3b2dca2c-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:20:55.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m5pz4" for this suite.
Mar  6 04:21:01.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:21:01.633: INFO: namespace: e2e-tests-emptydir-m5pz4, resource: bindings, ignored listing per whitelist
Mar  6 04:21:01.735: INFO: namespace e2e-tests-emptydir-m5pz4 deletion completed in 6.151391152s

• [SLOW TEST:8.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:21:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:21:01.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-xb58g" to be "success or failure"
Mar  6 04:21:01.805: INFO: Pod "downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280552ms
Mar  6 04:21:03.809: INFO: Pod "downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005780862s
STEP: Saw pod success
Mar  6 04:21:03.809: INFO: Pod "downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:21:03.811: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:21:03.828: INFO: Waiting for pod downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:21:03.831: INFO: Pod downwardapi-volume-40196872-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:21:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xb58g" for this suite.
Mar  6 04:21:09.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:21:09.936: INFO: namespace: e2e-tests-downward-api-xb58g, resource: bindings, ignored listing per whitelist
Mar  6 04:21:10.006: INFO: namespace e2e-tests-downward-api-xb58g deletion completed in 6.171729511s

• [SLOW TEST:8.272 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:21:10.007: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-4508fe41-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:21:10.088: INFO: Waiting up to 5m0s for pod "pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-5f26k" to be "success or failure"
Mar  6 04:21:10.092: INFO: Pod "pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746472ms
Mar  6 04:21:12.096: INFO: Pod "pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007632965s
STEP: Saw pod success
Mar  6 04:21:12.096: INFO: Pod "pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:21:12.098: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:21:12.118: INFO: Waiting for pod pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:21:12.121: INFO: Pod pod-secrets-4509da28-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:21:12.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5f26k" for this suite.
Mar  6 04:21:18.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:21:18.262: INFO: namespace: e2e-tests-secrets-5f26k, resource: bindings, ignored listing per whitelist
Mar  6 04:21:18.265: INFO: namespace e2e-tests-secrets-5f26k deletion completed in 6.140297928s

• [SLOW TEST:8.259 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:21:18.266: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:21:18.331: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:21:19.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-6q65k" for this suite.
Mar  6 04:21:25.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:21:25.436: INFO: namespace: e2e-tests-custom-resource-definition-6q65k, resource: bindings, ignored listing per whitelist
Mar  6 04:21:25.534: INFO: namespace e2e-tests-custom-resource-definition-6q65k deletion completed in 6.153240472s

• [SLOW TEST:7.269 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:21:25.535: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-lgqq9
Mar  6 04:21:29.624: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-lgqq9
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 04:21:29.626: INFO: Initial restart count of pod liveness-http is 0
Mar  6 04:21:53.674: INFO: Restart count of pod e2e-tests-container-probe-lgqq9/liveness-http is now 1 (24.048006269s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:21:53.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-lgqq9" for this suite.
Mar  6 04:21:59.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:21:59.808: INFO: namespace: e2e-tests-container-probe-lgqq9, resource: bindings, ignored listing per whitelist
Mar  6 04:21:59.831: INFO: namespace e2e-tests-container-probe-lgqq9 deletion completed in 6.141488372s

• [SLOW TEST:34.296 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:21:59.831: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:21:59.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-qb6t6" to be "success or failure"
Mar  6 04:21:59.909: INFO: Pod "downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512842ms
Mar  6 04:22:01.913: INFO: Pod "downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007336731s
Mar  6 04:22:03.917: INFO: Pod "downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011186737s
STEP: Saw pod success
Mar  6 04:22:03.917: INFO: Pod "downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:22:03.919: INFO: Trying to get logs from node vmw3-k8s-05.local.dev pod downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:22:03.941: INFO: Waiting for pod downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:22:03.943: INFO: Pod downwardapi-volume-62baf490-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:22:03.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qb6t6" for this suite.
Mar  6 04:22:09.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:22:10.053: INFO: namespace: e2e-tests-projected-qb6t6, resource: bindings, ignored listing per whitelist
Mar  6 04:22:10.087: INFO: namespace e2e-tests-projected-qb6t6 deletion completed in 6.140763019s

• [SLOW TEST:10.257 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:22:10.087: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Mar  6 04:22:10.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-c9s4r'
Mar  6 04:22:10.351: INFO: stderr: ""
Mar  6 04:22:10.351: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar  6 04:22:11.355: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 04:22:11.355: INFO: Found 0 / 1
Mar  6 04:22:12.355: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 04:22:12.355: INFO: Found 1 / 1
Mar  6 04:22:12.355: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 04:22:12.357: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 04:22:12.357: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  6 04:22:12.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 logs redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r'
Mar  6 04:22:12.461: INFO: stderr: ""
Mar  6 04:22:12.461: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 04:22:11.318 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 04:22:11.318 # Server started, Redis version 3.2.12\n1:M 06 Mar 04:22:11.318 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 04:22:11.318 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  6 04:22:12.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 log redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r --tail=1'
Mar  6 04:22:12.551: INFO: stderr: ""
Mar  6 04:22:12.551: INFO: stdout: "1:M 06 Mar 04:22:11.318 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  6 04:22:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 log redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r --limit-bytes=1'
Mar  6 04:22:12.651: INFO: stderr: ""
Mar  6 04:22:12.651: INFO: stdout: " "
STEP: exposing timestamps
Mar  6 04:22:12.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 log redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r --tail=1 --timestamps'
Mar  6 04:22:12.740: INFO: stderr: ""
Mar  6 04:22:12.740: INFO: stdout: "2019-03-06T04:22:11.319174606Z 1:M 06 Mar 04:22:11.318 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  6 04:22:15.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 log redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r --since=1s'
Mar  6 04:22:15.349: INFO: stderr: ""
Mar  6 04:22:15.349: INFO: stdout: ""
Mar  6 04:22:15.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 log redis-master-w2bxl redis-master --namespace=e2e-tests-kubectl-c9s4r --since=24h'
Mar  6 04:22:15.448: INFO: stderr: ""
Mar  6 04:22:15.448: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 04:22:11.318 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 04:22:11.318 # Server started, Redis version 3.2.12\n1:M 06 Mar 04:22:11.318 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 04:22:11.318 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Mar  6 04:22:15.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-c9s4r'
Mar  6 04:22:15.534: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 04:22:15.534: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  6 04:22:15.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-c9s4r'
Mar  6 04:22:15.613: INFO: stderr: "No resources found.\n"
Mar  6 04:22:15.613: INFO: stdout: ""
Mar  6 04:22:15.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -l name=nginx --namespace=e2e-tests-kubectl-c9s4r -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 04:22:15.688: INFO: stderr: ""
Mar  6 04:22:15.688: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:22:15.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c9s4r" for this suite.
Mar  6 04:22:37.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:22:37.784: INFO: namespace: e2e-tests-kubectl-c9s4r, resource: bindings, ignored listing per whitelist
Mar  6 04:22:37.847: INFO: namespace e2e-tests-kubectl-c9s4r deletion completed in 22.155021108s

• [SLOW TEST:27.759 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:22:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar  6 04:22:37.926: INFO: Waiting up to 5m0s for pod "client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-containers-ktprd" to be "success or failure"
Mar  6 04:22:37.928: INFO: Pod "client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335092ms
Mar  6 04:22:39.932: INFO: Pod "client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005689737s
Mar  6 04:22:41.935: INFO: Pod "client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009460323s
STEP: Saw pod success
Mar  6 04:22:41.936: INFO: Pod "client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:22:41.938: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:22:41.957: INFO: Waiting for pod client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:22:41.959: INFO: Pod client-containers-79649435-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:22:41.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ktprd" for this suite.
Mar  6 04:22:47.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:22:48.102: INFO: namespace: e2e-tests-containers-ktprd, resource: bindings, ignored listing per whitelist
Mar  6 04:22:48.156: INFO: namespace e2e-tests-containers-ktprd deletion completed in 6.192070675s

• [SLOW TEST:10.309 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:22:48.156: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 04:22:48.218: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:22:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-v5tjp" for this suite.
Mar  6 04:22:57.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:22:57.585: INFO: namespace: e2e-tests-init-container-v5tjp, resource: bindings, ignored listing per whitelist
Mar  6 04:22:57.628: INFO: namespace e2e-tests-init-container-v5tjp deletion completed in 6.137049184s

• [SLOW TEST:9.472 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:22:57.628: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:22:57.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-qqdhd" to be "success or failure"
Mar  6 04:22:57.694: INFO: Pod "downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08111ms
Mar  6 04:22:59.698: INFO: Pod "downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005750975s
STEP: Saw pod success
Mar  6 04:22:59.698: INFO: Pod "downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:22:59.700: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:22:59.721: INFO: Waiting for pod downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:22:59.723: INFO: Pod downwardapi-volume-852ca115-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:22:59.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qqdhd" for this suite.
Mar  6 04:23:05.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:05.864: INFO: namespace: e2e-tests-projected-qqdhd, resource: bindings, ignored listing per whitelist
Mar  6 04:23:05.884: INFO: namespace e2e-tests-projected-qqdhd deletion completed in 6.156186829s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 04:23:05.958: INFO: Waiting up to 5m0s for pod "pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-s8kq7" to be "success or failure"
Mar  6 04:23:05.960: INFO: Pod "pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214904ms
Mar  6 04:23:07.964: INFO: Pod "pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005659863s
STEP: Saw pod success
Mar  6 04:23:07.964: INFO: Pod "pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:23:07.967: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:23:07.989: INFO: Waiting for pod pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:23:07.992: INFO: Pod pod-8a19dd0f-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:07.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s8kq7" for this suite.
Mar  6 04:23:14.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:14.076: INFO: namespace: e2e-tests-emptydir-s8kq7, resource: bindings, ignored listing per whitelist
Mar  6 04:23:14.138: INFO: namespace e2e-tests-emptydir-s8kq7 deletion completed in 6.135518693s

• [SLOW TEST:8.255 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:14.139: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-8f048ca6-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:23:14.211: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-lrj8p" to be "success or failure"
Mar  6 04:23:14.214: INFO: Pod "pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596911ms
Mar  6 04:23:16.218: INFO: Pod "pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006621s
STEP: Saw pod success
Mar  6 04:23:16.218: INFO: Pod "pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:23:16.221: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:23:16.242: INFO: Waiting for pod pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:23:16.245: INFO: Pod pod-projected-secrets-8f0562cd-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lrj8p" for this suite.
Mar  6 04:23:22.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:22.312: INFO: namespace: e2e-tests-projected-lrj8p, resource: bindings, ignored listing per whitelist
Mar  6 04:23:22.390: INFO: namespace e2e-tests-projected-lrj8p deletion completed in 6.139273615s

• [SLOW TEST:8.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:22.390: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:23:22.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-2fcwx" to be "success or failure"
Mar  6 04:23:22.475: INFO: Pod "downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775764ms
Mar  6 04:23:24.478: INFO: Pod "downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006516008s
STEP: Saw pod success
Mar  6 04:23:24.479: INFO: Pod "downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:23:24.481: INFO: Trying to get logs from node vmw3-k8s-05.local.dev pod downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:23:24.501: INFO: Waiting for pod downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:23:24.503: INFO: Pod downwardapi-volume-93f1598b-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:24.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2fcwx" for this suite.
Mar  6 04:23:30.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:30.561: INFO: namespace: e2e-tests-downward-api-2fcwx, resource: bindings, ignored listing per whitelist
Mar  6 04:23:30.636: INFO: namespace e2e-tests-downward-api-2fcwx deletion completed in 6.12984402s

• [SLOW TEST:8.246 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:30.637: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 04:23:30.706: INFO: Waiting up to 5m0s for pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-bscml" to be "success or failure"
Mar  6 04:23:30.708: INFO: Pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358365ms
Mar  6 04:23:32.712: INFO: Pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006615583s
Mar  6 04:23:34.716: INFO: Pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010455236s
Mar  6 04:23:36.720: INFO: Pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013875506s
STEP: Saw pod success
Mar  6 04:23:36.720: INFO: Pod "downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:23:36.722: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 04:23:36.746: INFO: Waiting for pod downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:23:36.748: INFO: Pod downward-api-98da1092-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bscml" for this suite.
Mar  6 04:23:42.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:42.802: INFO: namespace: e2e-tests-downward-api-bscml, resource: bindings, ignored listing per whitelist
Mar  6 04:23:42.887: INFO: namespace e2e-tests-downward-api-bscml deletion completed in 6.134778956s

• [SLOW TEST:12.250 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:42.887: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:23:42.954: INFO: Creating deployment "test-recreate-deployment"
Mar  6 04:23:42.960: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  6 04:23:42.965: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  6 04:23:44.971: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  6 04:23:44.973: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  6 04:23:44.982: INFO: Updating deployment test-recreate-deployment
Mar  6 04:23:44.982: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 04:23:45.055: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-s5fd9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-s5fd9/deployments/test-recreate-deployment,UID:a0285ac7-3fc7-11e9-91c9-005056979acf,ResourceVersion:11333,Generation:2,CreationTimestamp:2019-03-06 04:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-06 04:23:45 +0000 UTC 2019-03-06 04:23:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-06 04:23:45 +0000 UTC 2019-03-06 04:23:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 04:23:45.058: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-s5fd9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-s5fd9/replicasets/test-recreate-deployment-697fbf54bf,UID:a162aa46-3fc7-11e9-a5d3-00505697ee14,ResourceVersion:11331,Generation:1,CreationTimestamp:2019-03-06 04:23:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment a0285ac7-3fc7-11e9-91c9-005056979acf 0xc001e549f7 0xc001e549f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 04:23:45.058: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  6 04:23:45.058: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-s5fd9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-s5fd9/replicasets/test-recreate-deployment-5dfdcc846d,UID:a02a1798-3fc7-11e9-a5d3-00505697ee14,ResourceVersion:11321,Generation:2,CreationTimestamp:2019-03-06 04:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment a0285ac7-3fc7-11e9-91c9-005056979acf 0xc001e54937 0xc001e54938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 04:23:45.061: INFO: Pod "test-recreate-deployment-697fbf54bf-77zfm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-77zfm,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-s5fd9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-s5fd9/pods/test-recreate-deployment-697fbf54bf-77zfm,UID:a1636114-3fc7-11e9-a5d3-00505697ee14,ResourceVersion:11332,Generation:0,CreationTimestamp:2019-03-06 04:23:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf a162aa46-3fc7-11e9-a5d3-00505697ee14 0xc001e55277 0xc001e55278}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-2r2h7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2r2h7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-2r2h7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e552e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e55300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:23:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:23:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:23:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:23:45 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.20,PodIP:,StartTime:2019-03-06 04:23:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:45.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-s5fd9" for this suite.
Mar  6 04:23:51.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:51.181: INFO: namespace: e2e-tests-deployment-s5fd9, resource: bindings, ignored listing per whitelist
Mar  6 04:23:51.205: INFO: namespace e2e-tests-deployment-s5fd9 deletion completed in 6.139859948s

• [SLOW TEST:8.318 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:51.206: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar  6 04:23:51.279: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-vgnnf" to be "success or failure"
Mar  6 04:23:51.283: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90131ms
Mar  6 04:23:53.287: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008040485s
STEP: Saw pod success
Mar  6 04:23:53.287: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  6 04:23:53.290: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  6 04:23:53.308: INFO: Waiting for pod pod-host-path-test to disappear
Mar  6 04:23:53.310: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:23:53.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-vgnnf" for this suite.
Mar  6 04:23:59.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:23:59.410: INFO: namespace: e2e-tests-hostpath-vgnnf, resource: bindings, ignored listing per whitelist
Mar  6 04:23:59.454: INFO: namespace e2e-tests-hostpath-vgnnf deletion completed in 6.13996557s

• [SLOW TEST:8.248 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:23:59.454: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 04:23:59.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:23:59.642: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 04:23:59.642: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  6 04:23:59.646: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar  6 04:23:59.650: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  6 04:23:59.658: INFO: scanned /root for discovery docs: <nil>
Mar  6 04:23:59.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:24:15.446: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 04:24:15.446: INFO: stdout: "Created e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b\nScaling up e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  6 04:24:15.446: INFO: stdout: "Created e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b\nScaling up e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  6 04:24:15.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:24:15.534: INFO: stderr: ""
Mar  6 04:24:15.535: INFO: stdout: "e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b-xpkmc "
Mar  6 04:24:15.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b-xpkmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:24:15.615: INFO: stderr: ""
Mar  6 04:24:15.615: INFO: stdout: "true"
Mar  6 04:24:15.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b-xpkmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:24:15.690: INFO: stderr: ""
Mar  6 04:24:15.690: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  6 04:24:15.690: INFO: e2e-test-nginx-rc-4703e0bc922ec5a1e99d4eca6660d60b-xpkmc is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Mar  6 04:24:15.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vnqdc'
Mar  6 04:24:15.776: INFO: stderr: ""
Mar  6 04:24:15.776: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:24:15.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vnqdc" for this suite.
Mar  6 04:24:21.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:24:21.836: INFO: namespace: e2e-tests-kubectl-vnqdc, resource: bindings, ignored listing per whitelist
Mar  6 04:24:21.914: INFO: namespace e2e-tests-kubectl-vnqdc deletion completed in 6.13344932s

• [SLOW TEST:22.460 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:24:21.914: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b76b88a7-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b76b88a7-3fc7-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:24:26.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k2gfp" for this suite.
Mar  6 04:24:48.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:24:48.063: INFO: namespace: e2e-tests-projected-k2gfp, resource: bindings, ignored listing per whitelist
Mar  6 04:24:48.196: INFO: namespace e2e-tests-projected-k2gfp deletion completed in 22.157787553s

• [SLOW TEST:26.282 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:24:48.196: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:24:48.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-w4chm" to be "success or failure"
Mar  6 04:24:48.280: INFO: Pod "downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874088ms
Mar  6 04:24:50.283: INFO: Pod "downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006315759s
STEP: Saw pod success
Mar  6 04:24:50.283: INFO: Pod "downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:24:50.286: INFO: Trying to get logs from node vmw3-k8s-05.local.dev pod downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:24:50.312: INFO: Waiting for pod downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:24:50.314: INFO: Pod downwardapi-volume-c71675d6-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:24:50.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w4chm" for this suite.
Mar  6 04:24:56.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:24:56.382: INFO: namespace: e2e-tests-projected-w4chm, resource: bindings, ignored listing per whitelist
Mar  6 04:24:56.468: INFO: namespace e2e-tests-projected-w4chm deletion completed in 6.149791011s

• [SLOW TEST:8.272 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:24:56.469: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 04:24:56.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-svw7c'
Mar  6 04:24:56.649: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 04:24:56.649: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Mar  6 04:25:00.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-svw7c'
Mar  6 04:25:00.745: INFO: stderr: ""
Mar  6 04:25:00.745: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:25:00.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-svw7c" for this suite.
Mar  6 04:25:22.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:25:22.791: INFO: namespace: e2e-tests-kubectl-svw7c, resource: bindings, ignored listing per whitelist
Mar  6 04:25:22.905: INFO: namespace e2e-tests-kubectl-svw7c deletion completed in 22.156907521s

• [SLOW TEST:26.437 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:25:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-dbc747af-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating secret with name s-test-opt-upd-dbc747f4-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dbc747af-3fc7-11e9-8de6-d63fb0ed442e
STEP: Updating secret s-test-opt-upd-dbc747f4-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating secret with name s-test-opt-create-dbc7480c-3fc7-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:25:27.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mfd6p" for this suite.
Mar  6 04:25:49.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:25:49.170: INFO: namespace: e2e-tests-projected-mfd6p, resource: bindings, ignored listing per whitelist
Mar  6 04:25:49.233: INFO: namespace e2e-tests-projected-mfd6p deletion completed in 22.132376584s

• [SLOW TEST:26.328 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:25:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-eb7975d2-3fc7-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:25:49.330: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-7j5sb" to be "success or failure"
Mar  6 04:25:49.334: INFO: Pod "pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.692284ms
Mar  6 04:25:51.338: INFO: Pod "pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007937111s
Mar  6 04:25:53.341: INFO: Pod "pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011237619s
STEP: Saw pod success
Mar  6 04:25:53.341: INFO: Pod "pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:25:53.343: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:25:53.364: INFO: Waiting for pod pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:25:53.366: INFO: Pod pod-configmaps-eb7a7234-3fc7-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:25:53.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7j5sb" for this suite.
Mar  6 04:25:59.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:25:59.475: INFO: namespace: e2e-tests-configmap-7j5sb, resource: bindings, ignored listing per whitelist
Mar  6 04:25:59.520: INFO: namespace e2e-tests-configmap-7j5sb deletion completed in 6.149454111s

• [SLOW TEST:10.287 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:25:59.520: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-lwmqf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lwmqf to expose endpoints map[]
Mar  6 04:25:59.604: INFO: Get endpoints failed (3.126778ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  6 04:26:00.607: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lwmqf exposes endpoints map[] (1.006507566s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-lwmqf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lwmqf to expose endpoints map[pod1:[80]]
Mar  6 04:26:04.645: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lwmqf exposes endpoints map[pod1:[80]] (4.029845508s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-lwmqf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lwmqf to expose endpoints map[pod2:[80] pod1:[80]]
Mar  6 04:26:05.667: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lwmqf exposes endpoints map[pod1:[80] pod2:[80]] (1.018095087s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-lwmqf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lwmqf to expose endpoints map[pod2:[80]]
Mar  6 04:26:06.685: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lwmqf exposes endpoints map[pod2:[80]] (1.01163227s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-lwmqf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lwmqf to expose endpoints map[]
Mar  6 04:26:07.695: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lwmqf exposes endpoints map[] (1.005791523s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:26:07.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-lwmqf" for this suite.
Mar  6 04:26:13.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:26:13.851: INFO: namespace: e2e-tests-services-lwmqf, resource: bindings, ignored listing per whitelist
Mar  6 04:26:13.899: INFO: namespace e2e-tests-services-lwmqf deletion completed in 6.172292665s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:14.379 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:26:13.899: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0306 04:26:19.998344      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 04:26:19.998: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:26:19.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-q9tvq" for this suite.
Mar  6 04:26:26.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:26:26.035: INFO: namespace: e2e-tests-gc-q9tvq, resource: bindings, ignored listing per whitelist
Mar  6 04:26:26.135: INFO: namespace e2e-tests-gc-q9tvq deletion completed in 6.133146759s

• [SLOW TEST:12.235 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:26:26.135: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-0176a7f6-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:26:26.218: INFO: Waiting up to 5m0s for pod "pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-8m5sm" to be "success or failure"
Mar  6 04:26:26.221: INFO: Pod "pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255737ms
Mar  6 04:26:28.224: INFO: Pod "pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005405965s
STEP: Saw pod success
Mar  6 04:26:28.224: INFO: Pod "pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:26:28.226: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:26:28.246: INFO: Waiting for pod pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:26:28.248: INFO: Pod pod-secrets-0177806e-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:26:28.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8m5sm" for this suite.
Mar  6 04:26:34.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:26:34.379: INFO: namespace: e2e-tests-secrets-8m5sm, resource: bindings, ignored listing per whitelist
Mar  6 04:26:34.401: INFO: namespace e2e-tests-secrets-8m5sm deletion completed in 6.149216223s

• [SLOW TEST:8.266 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:26:34.401: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-06620bb0-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating secret with name secret-projected-all-test-volume-06620b9a-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  6 04:26:34.475: INFO: Waiting up to 5m0s for pod "projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-jbvrl" to be "success or failure"
Mar  6 04:26:34.478: INFO: Pod "projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.729649ms
Mar  6 04:26:36.482: INFO: Pod "projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006363835s
STEP: Saw pod success
Mar  6 04:26:36.482: INFO: Pod "projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:26:36.484: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  6 04:26:36.502: INFO: Waiting for pod projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:26:36.505: INFO: Pod projected-volume-06620b64-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:26:36.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jbvrl" for this suite.
Mar  6 04:26:42.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:26:42.529: INFO: namespace: e2e-tests-projected-jbvrl, resource: bindings, ignored listing per whitelist
Mar  6 04:26:42.738: INFO: namespace e2e-tests-projected-jbvrl deletion completed in 6.22951453s

• [SLOW TEST:8.337 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:26:42.739: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:26:42.823: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar  6 04:26:42.829: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-zlbz6/daemonsets","resourceVersion":"12636"},"items":null}

Mar  6 04:26:42.831: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-zlbz6/pods","resourceVersion":"12636"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:26:42.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-zlbz6" for this suite.
Mar  6 04:26:48.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:26:48.900: INFO: namespace: e2e-tests-daemonsets-zlbz6, resource: bindings, ignored listing per whitelist
Mar  6 04:26:48.993: INFO: namespace e2e-tests-daemonsets-zlbz6 deletion completed in 6.144946779s

S [SKIPPING] [6.254 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar  6 04:26:42.823: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:26:48.993: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-j222f.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-j222f.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-j222f.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-j222f.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-j222f.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-j222f.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 04:27:05.151: INFO: DNS probes using e2e-tests-dns-j222f/dns-test-0f14ec2a-3fc8-11e9-8de6-d63fb0ed442e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:27:05.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-j222f" for this suite.
Mar  6 04:27:11.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:27:11.231: INFO: namespace: e2e-tests-dns-j222f, resource: bindings, ignored listing per whitelist
Mar  6 04:27:11.312: INFO: namespace e2e-tests-dns-j222f deletion completed in 6.143599356s

• [SLOW TEST:22.319 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:27:11.312: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar  6 04:27:11.409: INFO: Waiting up to 5m0s for pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-containers-2j2nd" to be "success or failure"
Mar  6 04:27:11.412: INFO: Pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649445ms
Mar  6 04:27:13.415: INFO: Pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005721245s
Mar  6 04:27:15.419: INFO: Pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 4.009247144s
Mar  6 04:27:17.423: INFO: Pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013352424s
STEP: Saw pod success
Mar  6 04:27:17.423: INFO: Pod "client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:27:17.425: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:27:17.444: INFO: Waiting for pod client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:27:17.447: INFO: Pod client-containers-1c66c5d7-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:27:17.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2j2nd" for this suite.
Mar  6 04:27:23.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:27:23.552: INFO: namespace: e2e-tests-containers-2j2nd, resource: bindings, ignored listing per whitelist
Mar  6 04:27:23.590: INFO: namespace e2e-tests-containers-2j2nd deletion completed in 6.135253552s

• [SLOW TEST:12.278 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:27:23.590: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  6 04:27:23.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-kvn4n,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvn4n/configmaps/e2e-watch-test-resource-version,UID:23b451ec-3fc8-11e9-91c9-005056979acf,ResourceVersion:12849,Generation:0,CreationTimestamp:2019-03-06 04:27:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 04:27:23.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-kvn4n,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvn4n/configmaps/e2e-watch-test-resource-version,UID:23b451ec-3fc8-11e9-91c9-005056979acf,ResourceVersion:12850,Generation:0,CreationTimestamp:2019-03-06 04:27:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:27:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kvn4n" for this suite.
Mar  6 04:27:29.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:27:29.793: INFO: namespace: e2e-tests-watch-kvn4n, resource: bindings, ignored listing per whitelist
Mar  6 04:27:29.828: INFO: namespace e2e-tests-watch-kvn4n deletion completed in 6.147164637s

• [SLOW TEST:6.237 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:27:29.828: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 04:27:29.898: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 04:27:29.907: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 04:27:29.909: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-04.local.dev before test
Mar  6 04:27:29.919: INFO: rook-ceph-mon0-wgbcm from rook started at 2019-03-06 03:52:44 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:27:29.919: INFO: rook-ceph-osd-6cjzd from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:27:29.919: INFO: calico-node-grzxg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:27:29.919: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:27:29.919: INFO: kube-multus-ds-amd64-zjcth from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:27:29.919: INFO: rook-ceph-mgr0-c69f5fd99-snnnj from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Mar  6 04:27:29.919: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 03:55:26 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 04:27:29.919: INFO: node-feature-discovery-chnnf from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:27:29.919: INFO: prometheus-prometheus-node-exporter-dwxpj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:27:29.919: INFO: kube-proxy-vcgjg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:27:29.919: INFO: nfs-provisioner-54db5878bf-7rzqj from default started at 2019-03-06 03:50:45 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container nfs-provisioner ready: true, restart count 0
Mar  6 04:27:29.919: INFO: local-volume-provisioner-ltngn from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:27:29.919: INFO: rook-agent-pqt8r from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:27:29.919: INFO: dex-547c49d486-94z28 from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container dex ready: true, restart count 0
Mar  6 04:27:29.919: INFO: prometheus-prometheus-kube-state-metrics-5b55c99749-mczll from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Mar  6 04:27:29.919: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-vcqv2 from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.919: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  6 04:27:29.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 04:27:29.919: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-05.local.dev before test
Mar  6 04:27:29.930: INFO: kube-proxy-pzh8g from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:27:29.930: INFO: calico-node-dlgtm from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:27:29.930: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:27:29.930: INFO: rook-ceph-mon1-d2vgn from rook started at 2019-03-06 03:52:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:27:29.930: INFO: rook-ceph-osd-czks7 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:27:29.930: INFO: prometheus-prometheus-server-86566f7789-44rqb from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  6 04:27:29.930: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  6 04:27:29.930: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-2dkkl from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  6 04:27:29.930: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 04:27:29.930: INFO: prometheus-prometheus-node-exporter-8px6c from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:27:29.930: INFO: node-feature-discovery-jf2bq from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:27:29.930: INFO: rook-api-8596f5cffd-h55tg from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container rook-api ready: true, restart count 0
Mar  6 04:27:29.930: INFO: rook-ceph-rgw-erikube-rook-rgw-568fb7598d-psm2f from rook started at 2019-03-06 03:53:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container rook-ceph-rgw-erikube-rook-rgw ready: true, restart count 0
Mar  6 04:27:29.930: INFO: kube-multus-ds-amd64-57r48 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:27:29.930: INFO: nginx-ingress-controller-c54bdfdb5-kwjtf from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 04:27:29.930: INFO: local-volume-provisioner-gbqgz from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:27:29.930: INFO: rook-agent-cp4vx from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:27:29.930: INFO: dex-547c49d486-nk6lm from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.930: INFO: 	Container dex ready: true, restart count 1
Mar  6 04:27:29.930: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-06.local.dev before test
Mar  6 04:27:29.942: INFO: kube-multus-ds-amd64-cxrt5 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:27:29.942: INFO: rook-agent-khk9s from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:27:29.942: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-nr2gk from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 04:27:29.942: INFO: nginx-ingress-controller-c54bdfdb5-fdksz from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 04:27:29.942: INFO: node-feature-discovery-7mh4p from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:27:29.942: INFO: rook-ceph-osd-2wrw6 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:27:29.942: INFO: prometheus-prometheus-node-exporter-4k7zk from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:27:29.942: INFO: kube-proxy-hw2bk from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:27:29.942: INFO: calico-node-m85tv from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:27:29.942: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:27:29.942: INFO: local-volume-provisioner-fhbzm from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:27:29.942: INFO: rook-operator-596d9f8565-4hvxg from rook-system started at 2019-03-06 03:52:02 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container rook-operator ready: true, restart count 0
Mar  6 04:27:29.942: INFO: rook-ceph-mon2-rtjdd from rook started at 2019-03-06 03:53:03 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:27:29.942: INFO: prometheus-prometheus-pushgateway-76f6bd8898-8llhj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  6 04:27:29.942: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-84dpr from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.942: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  6 04:27:29.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 04:27:29.942: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-07.local.dev before test
Mar  6 04:27:29.952: INFO: calico-node-thclf from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:27:29.952: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:27:29.952: INFO: kube-proxy-jp8qs from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:27:29.952: INFO: default-http-backend-c776c779-bgdc2 from ingress-nginx started at 2019-03-06 03:50:31 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container default-http-backend ready: true, restart count 0
Mar  6 04:27:29.952: INFO: node-feature-discovery-9frqx from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container node-feature-discovery ready: true, restart count 1
Mar  6 04:27:29.952: INFO: rook-ceph-osd-ttf2k from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container rook-ceph-osd ready: true, restart count 1
Mar  6 04:27:29.952: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-jkmzt from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 04:27:29.952: INFO: prometheus-prometheus-node-exporter-l2tbq from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:27:29.952: INFO: kube-multus-ds-amd64-fgvjc from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:27:29.952: INFO: tiller-deploy-764968bdfd-tpt2l from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container tiller ready: true, restart count 0
Mar  6 04:27:29.952: INFO: local-volume-provisioner-j7s8k from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:27:29.952: INFO: rook-agent-qfj24 from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:27:29.952: INFO: prometheus-prometheus-alertmanager-7f97c695ff-7ml5d from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Mar  6 04:27:29.952: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Mar  6 04:27:29.952: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-prrwp from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  6 04:27:29.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 04:27:29.952: INFO: rbd-provisioner-rbd-provisioner-74849667b8-997n8 from kube-system started at 2019-03-06 03:51:35 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container rbd-provisioner ready: true, restart count 0
Mar  6 04:27:29.952: INFO: metrics-server-5bddfb6979-6qqvx from kube-system started at 2019-03-06 03:53:07 +0000 UTC (1 container statuses recorded)
Mar  6 04:27:29.952: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-29dbaeae-3fc8-11e9-8de6-d63fb0ed442e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-29dbaeae-3fc8-11e9-8de6-d63fb0ed442e off the node vmw3-k8s-06.local.dev
STEP: verifying the node doesn't have the label kubernetes.io/e2e-29dbaeae-3fc8-11e9-8de6-d63fb0ed442e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:27:36.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-nrns5" for this suite.
Mar  6 04:27:50.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:27:50.195: INFO: namespace: e2e-tests-sched-pred-nrns5, resource: bindings, ignored listing per whitelist
Mar  6 04:27:50.223: INFO: namespace e2e-tests-sched-pred-nrns5 deletion completed in 14.207977103s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:20.395 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:27:50.223: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0306 04:28:00.399286      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 04:28:00.399: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:00.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mf9sn" for this suite.
Mar  6 04:28:06.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:06.443: INFO: namespace: e2e-tests-gc-mf9sn, resource: bindings, ignored listing per whitelist
Mar  6 04:28:06.538: INFO: namespace e2e-tests-gc-mf9sn deletion completed in 6.134008146s

• [SLOW TEST:16.314 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:06.538: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar  6 04:28:06.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 api-versions'
Mar  6 04:28:06.669: INFO: stderr: ""
Mar  6 04:28:06.669: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ndex.coreos.com/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:06.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8hfqf" for this suite.
Mar  6 04:28:12.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:12.768: INFO: namespace: e2e-tests-kubectl-8hfqf, resource: bindings, ignored listing per whitelist
Mar  6 04:28:12.813: INFO: namespace e2e-tests-kubectl-8hfqf deletion completed in 6.139715761s

• [SLOW TEST:6.275 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:28:12.903: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"410ddca4-3fc8-11e9-91c9-005056979acf", Controller:(*bool)(0xc001d2a92e), BlockOwnerDeletion:(*bool)(0xc001d2a92f)}}
Mar  6 04:28:12.909: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"410bee5c-3fc8-11e9-91c9-005056979acf", Controller:(*bool)(0xc0022b3176), BlockOwnerDeletion:(*bool)(0xc0022b3177)}}
Mar  6 04:28:12.914: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"410cf69d-3fc8-11e9-91c9-005056979acf", Controller:(*bool)(0xc001d2ac06), BlockOwnerDeletion:(*bool)(0xc001d2ac07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:17.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-m2mc9" for this suite.
Mar  6 04:28:23.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:24.055: INFO: namespace: e2e-tests-gc-m2mc9, resource: bindings, ignored listing per whitelist
Mar  6 04:28:24.057: INFO: namespace e2e-tests-gc-m2mc9 deletion completed in 6.130514764s

• [SLOW TEST:11.244 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:28:24.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-5bmwn" to be "success or failure"
Mar  6 04:28:24.137: INFO: Pod "downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3654ms
Mar  6 04:28:26.141: INFO: Pod "downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005672247s
STEP: Saw pod success
Mar  6 04:28:26.141: INFO: Pod "downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:28:26.143: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:28:26.169: INFO: Waiting for pod downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:28:26.171: INFO: Pod downwardapi-volume-47bfae2e-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:26.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5bmwn" for this suite.
Mar  6 04:28:32.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:32.283: INFO: namespace: e2e-tests-downward-api-5bmwn, resource: bindings, ignored listing per whitelist
Mar  6 04:28:32.317: INFO: namespace e2e-tests-downward-api-5bmwn deletion completed in 6.141634973s

• [SLOW TEST:8.260 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:32.318: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 04:28:32.395: INFO: Waiting up to 5m0s for pod "pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-nqq92" to be "success or failure"
Mar  6 04:28:32.397: INFO: Pod "pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534587ms
Mar  6 04:28:34.401: INFO: Pod "pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006119829s
STEP: Saw pod success
Mar  6 04:28:34.401: INFO: Pod "pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:28:34.403: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:28:34.427: INFO: Waiting for pod pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:28:34.429: INFO: Pod pod-4cac001a-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:34.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nqq92" for this suite.
Mar  6 04:28:40.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:40.508: INFO: namespace: e2e-tests-emptydir-nqq92, resource: bindings, ignored listing per whitelist
Mar  6 04:28:40.578: INFO: namespace e2e-tests-emptydir-nqq92 deletion completed in 6.144648167s

• [SLOW TEST:8.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:40.578: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-51997381-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:28:40.666: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-gm25q" to be "success or failure"
Mar  6 04:28:40.668: INFO: Pod "pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822841ms
Mar  6 04:28:42.671: INFO: Pod "pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00572544s
STEP: Saw pod success
Mar  6 04:28:42.671: INFO: Pod "pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:28:42.674: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:28:42.696: INFO: Waiting for pod pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:28:42.698: INFO: Pod pod-projected-secrets-519a58aa-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:42.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gm25q" for this suite.
Mar  6 04:28:48.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:48.744: INFO: namespace: e2e-tests-projected-gm25q, resource: bindings, ignored listing per whitelist
Mar  6 04:28:48.834: INFO: namespace e2e-tests-projected-gm25q deletion completed in 6.132079424s

• [SLOW TEST:8.256 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:48.834: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-5683cb95-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:28:48.913: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-sb8jp" to be "success or failure"
Mar  6 04:28:48.915: INFO: Pod "pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330981ms
Mar  6 04:28:50.918: INFO: Pod "pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005298258s
STEP: Saw pod success
Mar  6 04:28:50.918: INFO: Pod "pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:28:50.921: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:28:50.939: INFO: Waiting for pod pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:28:50.944: INFO: Pod pod-projected-configmaps-5684ce1d-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:50.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sb8jp" for this suite.
Mar  6 04:28:56.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:28:57.062: INFO: namespace: e2e-tests-projected-sb8jp, resource: bindings, ignored listing per whitelist
Mar  6 04:28:57.084: INFO: namespace e2e-tests-projected-sb8jp deletion completed in 6.134719617s

• [SLOW TEST:8.250 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:28:57.084: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar  6 04:28:57.157: INFO: Waiting up to 5m0s for pod "var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-var-expansion-sbq6h" to be "success or failure"
Mar  6 04:28:57.160: INFO: Pod "var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499126ms
Mar  6 04:28:59.163: INFO: Pod "var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005838223s
STEP: Saw pod success
Mar  6 04:28:59.163: INFO: Pod "var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:28:59.165: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 04:28:59.189: INFO: Waiting for pod var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:28:59.191: INFO: Pod var-expansion-5b6eb77c-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:28:59.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-sbq6h" for this suite.
Mar  6 04:29:05.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:05.314: INFO: namespace: e2e-tests-var-expansion-sbq6h, resource: bindings, ignored listing per whitelist
Mar  6 04:29:05.335: INFO: namespace e2e-tests-var-expansion-sbq6h deletion completed in 6.139325313s

• [SLOW TEST:8.251 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:29:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:29:11.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-x8vlq" for this suite.
Mar  6 04:29:17.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:17.545: INFO: namespace: e2e-tests-namespaces-x8vlq, resource: bindings, ignored listing per whitelist
Mar  6 04:29:17.615: INFO: namespace e2e-tests-namespaces-x8vlq deletion completed in 6.140364807s
STEP: Destroying namespace "e2e-tests-nsdeletetest-pbvfl" for this suite.
Mar  6 04:29:17.618: INFO: Namespace e2e-tests-nsdeletetest-pbvfl was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-vshzb" for this suite.
Mar  6 04:29:23.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:23.730: INFO: namespace: e2e-tests-nsdeletetest-vshzb, resource: bindings, ignored listing per whitelist
Mar  6 04:29:23.756: INFO: namespace e2e-tests-nsdeletetest-vshzb deletion completed in 6.138521387s

• [SLOW TEST:18.421 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:29:23.757: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 04:29:23.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fs4k2'
Mar  6 04:29:23.991: INFO: stderr: ""
Mar  6 04:29:23.991: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Mar  6 04:29:23.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-fs4k2'
Mar  6 04:29:27.235: INFO: stderr: ""
Mar  6 04:29:27.235: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:29:27.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fs4k2" for this suite.
Mar  6 04:29:33.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:33.310: INFO: namespace: e2e-tests-kubectl-fs4k2, resource: bindings, ignored listing per whitelist
Mar  6 04:29:33.378: INFO: namespace e2e-tests-kubectl-fs4k2 deletion completed in 6.139328256s

• [SLOW TEST:9.622 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:29:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-vdr5x/configmap-test-710fa9d2-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:29:33.448: INFO: Waiting up to 5m0s for pod "pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-vdr5x" to be "success or failure"
Mar  6 04:29:33.451: INFO: Pod "pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148352ms
Mar  6 04:29:35.454: INFO: Pod "pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0061164s
STEP: Saw pod success
Mar  6 04:29:35.454: INFO: Pod "pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:29:35.457: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e container env-test: <nil>
STEP: delete the pod
Mar  6 04:29:35.478: INFO: Waiting for pod pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:29:35.481: INFO: Pod pod-configmaps-71107840-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:29:35.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vdr5x" for this suite.
Mar  6 04:29:41.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:41.525: INFO: namespace: e2e-tests-configmap-vdr5x, resource: bindings, ignored listing per whitelist
Mar  6 04:29:41.624: INFO: namespace e2e-tests-configmap-vdr5x deletion completed in 6.139435418s

• [SLOW TEST:8.246 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:29:41.625: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:29:41.705: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-qwqvm" to be "success or failure"
Mar  6 04:29:41.708: INFO: Pod "downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664495ms
Mar  6 04:29:43.711: INFO: Pod "downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005954088s
STEP: Saw pod success
Mar  6 04:29:43.711: INFO: Pod "downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:29:43.713: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:29:43.738: INFO: Waiting for pod downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:29:43.740: INFO: Pod downwardapi-volume-75fbfafe-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:29:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qwqvm" for this suite.
Mar  6 04:29:49.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:29:49.816: INFO: namespace: e2e-tests-downward-api-qwqvm, resource: bindings, ignored listing per whitelist
Mar  6 04:29:49.888: INFO: namespace e2e-tests-downward-api-qwqvm deletion completed in 6.14212755s

• [SLOW TEST:8.263 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:29:49.888: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 04:29:49.957: INFO: Waiting up to 5m0s for pod "downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-4cjgh" to be "success or failure"
Mar  6 04:29:49.960: INFO: Pod "downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151708ms
Mar  6 04:29:51.964: INFO: Pod "downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006725571s
Mar  6 04:29:53.967: INFO: Pod "downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010297411s
STEP: Saw pod success
Mar  6 04:29:53.967: INFO: Pod "downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:29:53.970: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 04:29:53.991: INFO: Waiting for pod downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:29:53.993: INFO: Pod downward-api-7ae73658-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:29:53.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4cjgh" for this suite.
Mar  6 04:30:00.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:30:00.054: INFO: namespace: e2e-tests-downward-api-4cjgh, resource: bindings, ignored listing per whitelist
Mar  6 04:30:00.129: INFO: namespace e2e-tests-downward-api-4cjgh deletion completed in 6.132082299s

• [SLOW TEST:10.241 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:30:00.129: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-81024fe6-3fc8-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:30:00.204: INFO: Waiting up to 5m0s for pod "pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-n9tb2" to be "success or failure"
Mar  6 04:30:00.206: INFO: Pod "pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.654065ms
Mar  6 04:30:02.210: INFO: Pod "pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006264673s
Mar  6 04:30:04.213: INFO: Pod "pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009673214s
STEP: Saw pod success
Mar  6 04:30:04.214: INFO: Pod "pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:30:04.216: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:30:04.233: INFO: Waiting for pod pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:30:04.236: INFO: Pod pod-secrets-81031a88-3fc8-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:30:04.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n9tb2" for this suite.
Mar  6 04:30:10.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:30:10.358: INFO: namespace: e2e-tests-secrets-n9tb2, resource: bindings, ignored listing per whitelist
Mar  6 04:30:10.401: INFO: namespace e2e-tests-secrets-n9tb2 deletion completed in 6.159300545s

• [SLOW TEST:10.272 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:30:10.401: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-2w2kw
Mar  6 04:30:12.492: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-2w2kw
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 04:30:12.494: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:34:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2w2kw" for this suite.
Mar  6 04:34:18.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:34:19.126: INFO: namespace: e2e-tests-container-probe-2w2kw, resource: bindings, ignored listing per whitelist
Mar  6 04:34:19.153: INFO: namespace e2e-tests-container-probe-2w2kw deletion completed in 6.168287945s

• [SLOW TEST:248.752 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:34:19.153: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:34:21.249: INFO: Waiting up to 5m0s for pod "client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-pods-pthpt" to be "success or failure"
Mar  6 04:34:21.252: INFO: Pod "client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.69871ms
Mar  6 04:34:23.256: INFO: Pod "client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007693073s
STEP: Saw pod success
Mar  6 04:34:23.256: INFO: Pod "client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:34:23.259: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e container env3cont: <nil>
STEP: delete the pod
Mar  6 04:34:23.279: INFO: Waiting for pod client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:34:23.282: INFO: Pod client-envvars-1c9b9671-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:34:23.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-pthpt" for this suite.
Mar  6 04:35:01.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:35:01.335: INFO: namespace: e2e-tests-pods-pthpt, resource: bindings, ignored listing per whitelist
Mar  6 04:35:01.437: INFO: namespace e2e-tests-pods-pthpt deletion completed in 38.151024947s

• [SLOW TEST:42.284 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:35:01.437: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar  6 04:35:01.509: INFO: Waiting up to 5m0s for pod "client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-containers-k6jdz" to be "success or failure"
Mar  6 04:35:01.512: INFO: Pod "client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306322ms
Mar  6 04:35:03.515: INFO: Pod "client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00557267s
Mar  6 04:35:05.519: INFO: Pod "client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009608061s
STEP: Saw pod success
Mar  6 04:35:05.519: INFO: Pod "client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:35:05.521: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:35:05.545: INFO: Waiting for pod client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:35:05.547: INFO: Pod client-containers-349a53e3-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:35:05.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-k6jdz" for this suite.
Mar  6 04:35:11.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:35:11.659: INFO: namespace: e2e-tests-containers-k6jdz, resource: bindings, ignored listing per whitelist
Mar  6 04:35:11.701: INFO: namespace e2e-tests-containers-k6jdz deletion completed in 6.14937798s

• [SLOW TEST:10.264 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:35:11.701: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-3ab961a1-3fc9-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:35:11.782: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-tgrz2" to be "success or failure"
Mar  6 04:35:11.785: INFO: Pod "pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283441ms
Mar  6 04:35:13.789: INFO: Pod "pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006069243s
STEP: Saw pod success
Mar  6 04:35:13.789: INFO: Pod "pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:35:13.792: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:35:13.818: INFO: Waiting for pod pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:35:13.820: INFO: Pod pod-projected-secrets-3aba3111-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:35:13.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tgrz2" for this suite.
Mar  6 04:35:19.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:35:19.961: INFO: namespace: e2e-tests-projected-tgrz2, resource: bindings, ignored listing per whitelist
Mar  6 04:35:19.977: INFO: namespace e2e-tests-projected-tgrz2 deletion completed in 6.153169188s

• [SLOW TEST:8.276 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:35:19.978: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 04:35:22.617: INFO: Successfully updated pod "pod-update-3faa6d34-3fc9-11e9-8de6-d63fb0ed442e"
STEP: verifying the updated pod is in kubernetes
Mar  6 04:35:22.623: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:35:22.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-c7hhc" for this suite.
Mar  6 04:35:44.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:35:44.697: INFO: namespace: e2e-tests-pods-c7hhc, resource: bindings, ignored listing per whitelist
Mar  6 04:35:44.776: INFO: namespace e2e-tests-pods-c7hhc deletion completed in 22.149240601s

• [SLOW TEST:24.798 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:35:44.776: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:36:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-dhmfd" for this suite.
Mar  6 04:36:13.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:36:13.141: INFO: namespace: e2e-tests-container-runtime-dhmfd, resource: bindings, ignored listing per whitelist
Mar  6 04:36:13.180: INFO: namespace e2e-tests-container-runtime-dhmfd deletion completed in 6.138654132s

• [SLOW TEST:28.405 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:36:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  6 04:36:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:13.465: INFO: stderr: ""
Mar  6 04:36:13.465: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:36:13.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:13.556: INFO: stderr: ""
Mar  6 04:36:13.556: INFO: stdout: "update-demo-nautilus-lbw29 update-demo-nautilus-tcjlm "
Mar  6 04:36:13.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-lbw29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:13.634: INFO: stderr: ""
Mar  6 04:36:13.634: INFO: stdout: ""
Mar  6 04:36:13.634: INFO: update-demo-nautilus-lbw29 is created but not running
Mar  6 04:36:18.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:18.720: INFO: stderr: ""
Mar  6 04:36:18.720: INFO: stdout: "update-demo-nautilus-lbw29 update-demo-nautilus-tcjlm "
Mar  6 04:36:18.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-lbw29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:18.795: INFO: stderr: ""
Mar  6 04:36:18.795: INFO: stdout: "true"
Mar  6 04:36:18.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-lbw29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:18.876: INFO: stderr: ""
Mar  6 04:36:18.876: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:36:18.876: INFO: validating pod update-demo-nautilus-lbw29
Mar  6 04:36:18.881: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:36:18.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:36:18.881: INFO: update-demo-nautilus-lbw29 is verified up and running
Mar  6 04:36:18.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-tcjlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:18.964: INFO: stderr: ""
Mar  6 04:36:18.964: INFO: stdout: "true"
Mar  6 04:36:18.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-tcjlm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:19.036: INFO: stderr: ""
Mar  6 04:36:19.036: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:36:19.036: INFO: validating pod update-demo-nautilus-tcjlm
Mar  6 04:36:19.041: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:36:19.041: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:36:19.041: INFO: update-demo-nautilus-tcjlm is verified up and running
STEP: using delete to clean up resources
Mar  6 04:36:19.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:19.114: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 04:36:19.114: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 04:36:19.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-xw7hd'
Mar  6 04:36:19.189: INFO: stderr: "No resources found.\n"
Mar  6 04:36:19.189: INFO: stdout: ""
Mar  6 04:36:19.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -l name=update-demo --namespace=e2e-tests-kubectl-xw7hd -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 04:36:19.265: INFO: stderr: ""
Mar  6 04:36:19.265: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:36:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xw7hd" for this suite.
Mar  6 04:36:41.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:36:41.384: INFO: namespace: e2e-tests-kubectl-xw7hd, resource: bindings, ignored listing per whitelist
Mar  6 04:36:41.412: INFO: namespace e2e-tests-kubectl-xw7hd deletion completed in 22.141507326s

• [SLOW TEST:28.231 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:36:41.412: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 04:36:41.491: INFO: Waiting up to 5m0s for pod "pod-70324141-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-pcrxh" to be "success or failure"
Mar  6 04:36:41.494: INFO: Pod "pod-70324141-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.562057ms
Mar  6 04:36:43.498: INFO: Pod "pod-70324141-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006176568s
STEP: Saw pod success
Mar  6 04:36:43.498: INFO: Pod "pod-70324141-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:36:43.500: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-70324141-3fc9-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:36:43.519: INFO: Waiting for pod pod-70324141-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:36:43.521: INFO: Pod pod-70324141-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:36:43.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pcrxh" for this suite.
Mar  6 04:36:49.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:36:49.638: INFO: namespace: e2e-tests-emptydir-pcrxh, resource: bindings, ignored listing per whitelist
Mar  6 04:36:49.666: INFO: namespace e2e-tests-emptydir-pcrxh deletion completed in 6.14084057s

• [SLOW TEST:8.254 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:36:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:36:49.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-hkjj7" to be "success or failure"
Mar  6 04:36:49.748: INFO: Pod "downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.123151ms
Mar  6 04:36:51.751: INFO: Pod "downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006102836s
STEP: Saw pod success
Mar  6 04:36:51.751: INFO: Pod "downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:36:51.754: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:36:51.774: INFO: Waiting for pod downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:36:51.776: INFO: Pod downwardapi-volume-751dc069-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:36:51.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hkjj7" for this suite.
Mar  6 04:36:57.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:36:57.850: INFO: namespace: e2e-tests-projected-hkjj7, resource: bindings, ignored listing per whitelist
Mar  6 04:36:57.920: INFO: namespace e2e-tests-projected-hkjj7 deletion completed in 6.139745998s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:36:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 04:37:00.519: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e"
Mar  6 04:37:00.519: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-pods-4mnln" to be "terminated due to deadline exceeded"
Mar  6 04:37:00.521: INFO: Pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.421792ms
Mar  6 04:37:02.525: INFO: Pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005847426s
Mar  6 04:37:04.530: INFO: Pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010594285s
Mar  6 04:37:04.530: INFO: Pod "pod-update-activedeadlineseconds-7a091e3f-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:37:04.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4mnln" for this suite.
Mar  6 04:37:10.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:37:10.640: INFO: namespace: e2e-tests-pods-4mnln, resource: bindings, ignored listing per whitelist
Mar  6 04:37:10.686: INFO: namespace e2e-tests-pods-4mnln deletion completed in 6.151584725s

• [SLOW TEST:12.766 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:37:10.686: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar  6 04:37:10.769: INFO: Waiting up to 5m0s for pod "client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-containers-qkwgq" to be "success or failure"
Mar  6 04:37:10.773: INFO: Pod "client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.608356ms
Mar  6 04:37:12.776: INFO: Pod "client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007804564s
STEP: Saw pod success
Mar  6 04:37:12.777: INFO: Pod "client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:37:12.780: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:37:12.800: INFO: Waiting for pod client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:37:12.803: INFO: Pod client-containers-81a5a463-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:37:12.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-qkwgq" for this suite.
Mar  6 04:37:18.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:37:18.859: INFO: namespace: e2e-tests-containers-qkwgq, resource: bindings, ignored listing per whitelist
Mar  6 04:37:18.949: INFO: namespace e2e-tests-containers-qkwgq deletion completed in 6.14283482s

• [SLOW TEST:8.263 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:37:18.949: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 04:37:19.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-85ps6" to be "success or failure"
Mar  6 04:37:19.019: INFO: Pod "downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455219ms
Mar  6 04:37:21.023: INFO: Pod "downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006295299s
STEP: Saw pod success
Mar  6 04:37:21.023: INFO: Pod "downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:37:21.025: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 04:37:21.043: INFO: Waiting for pod downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:37:21.046: INFO: Pod downwardapi-volume-869021da-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:37:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-85ps6" for this suite.
Mar  6 04:37:27.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:37:27.174: INFO: namespace: e2e-tests-projected-85ps6, resource: bindings, ignored listing per whitelist
Mar  6 04:37:27.193: INFO: namespace e2e-tests-projected-85ps6 deletion completed in 6.142221587s

• [SLOW TEST:8.244 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:37:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8b7b4307-3fc9-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:37:27.272: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-rpr7z" to be "success or failure"
Mar  6 04:37:27.274: INFO: Pod "pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.89026ms
Mar  6 04:37:29.278: INFO: Pod "pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006295827s
STEP: Saw pod success
Mar  6 04:37:29.278: INFO: Pod "pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:37:29.280: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:37:29.301: INFO: Waiting for pod pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:37:29.304: INFO: Pod pod-projected-configmaps-8b7c283c-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:37:29.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rpr7z" for this suite.
Mar  6 04:37:35.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:37:35.360: INFO: namespace: e2e-tests-projected-rpr7z, resource: bindings, ignored listing per whitelist
Mar  6 04:37:35.447: INFO: namespace e2e-tests-projected-rpr7z deletion completed in 6.138660985s

• [SLOW TEST:8.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:37:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:37:35.552: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  6 04:37:35.560: INFO: Number of nodes with available pods: 0
Mar  6 04:37:35.560: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  6 04:37:35.575: INFO: Number of nodes with available pods: 0
Mar  6 04:37:35.575: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:36.579: INFO: Number of nodes with available pods: 0
Mar  6 04:37:36.579: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:37.579: INFO: Number of nodes with available pods: 1
Mar  6 04:37:37.579: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  6 04:37:37.596: INFO: Number of nodes with available pods: 1
Mar  6 04:37:37.597: INFO: Number of running nodes: 0, number of available pods: 1
Mar  6 04:37:38.601: INFO: Number of nodes with available pods: 0
Mar  6 04:37:38.601: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  6 04:37:38.609: INFO: Number of nodes with available pods: 0
Mar  6 04:37:38.609: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:39.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:39.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:40.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:40.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:41.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:41.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:42.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:42.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:43.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:43.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:44.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:44.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:45.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:45.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:46.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:46.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:47.612: INFO: Number of nodes with available pods: 0
Mar  6 04:37:47.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:48.612: INFO: Number of nodes with available pods: 0
Mar  6 04:37:48.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:49.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:49.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:50.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:50.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:51.612: INFO: Number of nodes with available pods: 0
Mar  6 04:37:51.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:52.614: INFO: Number of nodes with available pods: 0
Mar  6 04:37:52.614: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:53.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:53.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:54.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:54.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:55.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:55.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:56.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:56.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:57.612: INFO: Number of nodes with available pods: 0
Mar  6 04:37:57.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:58.613: INFO: Number of nodes with available pods: 0
Mar  6 04:37:58.614: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:37:59.614: INFO: Number of nodes with available pods: 0
Mar  6 04:37:59.614: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:00.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:00.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:01.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:01.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:02.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:02.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:03.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:03.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:04.614: INFO: Number of nodes with available pods: 0
Mar  6 04:38:04.614: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:05.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:05.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:06.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:06.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:07.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:07.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:08.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:08.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:09.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:09.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:10.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:10.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:11.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:11.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:12.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:12.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:13.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:13.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:14.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:14.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:15.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:15.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:16.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:16.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:17.613: INFO: Number of nodes with available pods: 0
Mar  6 04:38:17.613: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:18.612: INFO: Number of nodes with available pods: 0
Mar  6 04:38:18.612: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:38:19.613: INFO: Number of nodes with available pods: 1
Mar  6 04:38:19.613: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-s8qb5, will wait for the garbage collector to delete the pods
Mar  6 04:38:19.683: INFO: Deleting DaemonSet.extensions daemon-set took: 8.55385ms
Mar  6 04:38:19.783: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.231858ms
Mar  6 04:38:57.387: INFO: Number of nodes with available pods: 0
Mar  6 04:38:57.387: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 04:38:57.389: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-s8qb5/daemonsets","resourceVersion":"16298"},"items":null}

Mar  6 04:38:57.392: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-s8qb5/pods","resourceVersion":"16298"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:38:57.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-s8qb5" for this suite.
Mar  6 04:39:03.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:39:03.548: INFO: namespace: e2e-tests-daemonsets-s8qb5, resource: bindings, ignored listing per whitelist
Mar  6 04:39:03.561: INFO: namespace e2e-tests-daemonsets-s8qb5 deletion completed in 6.138155215s

• [SLOW TEST:88.115 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:39:03.562: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 04:39:03.627: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:39:05.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xl85q" for this suite.
Mar  6 04:39:11.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:39:11.879: INFO: namespace: e2e-tests-init-container-xl85q, resource: bindings, ignored listing per whitelist
Mar  6 04:39:11.991: INFO: namespace e2e-tests-init-container-xl85q deletion completed in 6.13929914s

• [SLOW TEST:8.429 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:39:11.991: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c9f218bf-3fc9-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:39:12.069: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-26bhw" to be "success or failure"
Mar  6 04:39:12.071: INFO: Pod "pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915951ms
Mar  6 04:39:14.075: INFO: Pod "pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006486613s
STEP: Saw pod success
Mar  6 04:39:14.075: INFO: Pod "pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:39:14.078: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:39:14.101: INFO: Waiting for pod pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:39:14.104: INFO: Pod pod-projected-configmaps-c9f2e731-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:39:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-26bhw" for this suite.
Mar  6 04:39:20.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:39:20.166: INFO: namespace: e2e-tests-projected-26bhw, resource: bindings, ignored listing per whitelist
Mar  6 04:39:20.251: INFO: namespace e2e-tests-projected-26bhw deletion completed in 6.142290452s

• [SLOW TEST:8.260 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:39:20.251: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 04:39:24.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:24.377: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:26.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:26.380: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:28.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:28.380: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:30.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:30.381: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:32.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:32.381: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:34.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:34.381: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:36.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:36.380: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:38.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:38.381: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:40.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:40.381: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:42.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:42.380: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:44.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:44.382: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:46.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:46.382: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 04:39:48.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 04:39:48.380: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:39:48.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-p929f" for this suite.
Mar  6 04:40:10.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:40:10.531: INFO: namespace: e2e-tests-container-lifecycle-hook-p929f, resource: bindings, ignored listing per whitelist
Mar  6 04:40:10.547: INFO: namespace e2e-tests-container-lifecycle-hook-p929f deletion completed in 22.162179355s

• [SLOW TEST:50.296 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:40:10.547: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-ecdab6d2-3fc9-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:40:10.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-wj7t7" to be "success or failure"
Mar  6 04:40:10.639: INFO: Pod "pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.58524ms
Mar  6 04:40:12.643: INFO: Pod "pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005997s
Mar  6 04:40:14.646: INFO: Pod "pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009557441s
STEP: Saw pod success
Mar  6 04:40:14.646: INFO: Pod "pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:40:14.649: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:40:14.669: INFO: Waiting for pod pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:40:14.672: INFO: Pod pod-projected-configmaps-ecdbb1d3-3fc9-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:40:14.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wj7t7" for this suite.
Mar  6 04:40:20.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:40:20.802: INFO: namespace: e2e-tests-projected-wj7t7, resource: bindings, ignored listing per whitelist
Mar  6 04:40:20.817: INFO: namespace e2e-tests-projected-wj7t7 deletion completed in 6.141093274s

• [SLOW TEST:10.270 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:40:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar  6 04:40:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:21.163: INFO: stderr: ""
Mar  6 04:40:21.163: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:40:21.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:21.249: INFO: stderr: ""
Mar  6 04:40:21.249: INFO: stdout: "update-demo-nautilus-2t68f update-demo-nautilus-dxvlj "
Mar  6 04:40:21.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-2t68f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:21.323: INFO: stderr: ""
Mar  6 04:40:21.323: INFO: stdout: ""
Mar  6 04:40:21.323: INFO: update-demo-nautilus-2t68f is created but not running
Mar  6 04:40:26.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:26.406: INFO: stderr: ""
Mar  6 04:40:26.406: INFO: stdout: "update-demo-nautilus-2t68f update-demo-nautilus-dxvlj "
Mar  6 04:40:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-2t68f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:26.495: INFO: stderr: ""
Mar  6 04:40:26.495: INFO: stdout: "true"
Mar  6 04:40:26.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-2t68f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:26.588: INFO: stderr: ""
Mar  6 04:40:26.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:40:26.588: INFO: validating pod update-demo-nautilus-2t68f
Mar  6 04:40:26.593: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:40:26.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:40:26.593: INFO: update-demo-nautilus-2t68f is verified up and running
Mar  6 04:40:26.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-dxvlj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:26.684: INFO: stderr: ""
Mar  6 04:40:26.684: INFO: stdout: "true"
Mar  6 04:40:26.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-dxvlj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:26.764: INFO: stderr: ""
Mar  6 04:40:26.764: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:40:26.764: INFO: validating pod update-demo-nautilus-dxvlj
Mar  6 04:40:26.769: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:40:26.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:40:26.769: INFO: update-demo-nautilus-dxvlj is verified up and running
STEP: rolling-update to new replication controller
Mar  6 04:40:26.770: INFO: scanned /root for discovery docs: <nil>
Mar  6 04:40:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.118: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 04:40:49.118: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:40:49.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.197: INFO: stderr: ""
Mar  6 04:40:49.197: INFO: stdout: "update-demo-kitten-grzpq update-demo-kitten-kpgkq "
Mar  6 04:40:49.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-kitten-grzpq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.274: INFO: stderr: ""
Mar  6 04:40:49.274: INFO: stdout: "true"
Mar  6 04:40:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-kitten-grzpq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.360: INFO: stderr: ""
Mar  6 04:40:49.360: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  6 04:40:49.360: INFO: validating pod update-demo-kitten-grzpq
Mar  6 04:40:49.369: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 04:40:49.369: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 04:40:49.369: INFO: update-demo-kitten-grzpq is verified up and running
Mar  6 04:40:49.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-kitten-kpgkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.464: INFO: stderr: ""
Mar  6 04:40:49.464: INFO: stdout: "true"
Mar  6 04:40:49.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-kitten-kpgkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h852g'
Mar  6 04:40:49.544: INFO: stderr: ""
Mar  6 04:40:49.544: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  6 04:40:49.544: INFO: validating pod update-demo-kitten-kpgkq
Mar  6 04:40:49.551: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 04:40:49.551: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 04:40:49.551: INFO: update-demo-kitten-kpgkq is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:40:49.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h852g" for this suite.
Mar  6 04:41:11.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:41:11.613: INFO: namespace: e2e-tests-kubectl-h852g, resource: bindings, ignored listing per whitelist
Mar  6 04:41:11.692: INFO: namespace e2e-tests-kubectl-h852g deletion completed in 22.135739489s

• [SLOW TEST:50.875 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:41:11.692: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  6 04:41:11.934: INFO: Pod name wrapped-volume-race-11638824-3fca-11e9-8de6-d63fb0ed442e: Found 0 pods out of 5
Mar  6 04:41:16.941: INFO: Pod name wrapped-volume-race-11638824-3fca-11e9-8de6-d63fb0ed442e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-11638824-3fca-11e9-8de6-d63fb0ed442e in namespace e2e-tests-emptydir-wrapper-lzzdd, will wait for the garbage collector to delete the pods
Mar  6 04:41:27.020: INFO: Deleting ReplicationController wrapped-volume-race-11638824-3fca-11e9-8de6-d63fb0ed442e took: 8.006351ms
Mar  6 04:41:27.121: INFO: Terminating ReplicationController wrapped-volume-race-11638824-3fca-11e9-8de6-d63fb0ed442e pods took: 100.298592ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 04:42:03.537: INFO: Pod name wrapped-volume-race-30257329-3fca-11e9-8de6-d63fb0ed442e: Found 0 pods out of 5
Mar  6 04:42:08.542: INFO: Pod name wrapped-volume-race-30257329-3fca-11e9-8de6-d63fb0ed442e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-30257329-3fca-11e9-8de6-d63fb0ed442e in namespace e2e-tests-emptydir-wrapper-lzzdd, will wait for the garbage collector to delete the pods
Mar  6 04:42:18.620: INFO: Deleting ReplicationController wrapped-volume-race-30257329-3fca-11e9-8de6-d63fb0ed442e took: 7.12591ms
Mar  6 04:42:18.720: INFO: Terminating ReplicationController wrapped-volume-race-30257329-3fca-11e9-8de6-d63fb0ed442e pods took: 100.278007ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 04:42:57.336: INFO: Pod name wrapped-volume-race-50369d6e-3fca-11e9-8de6-d63fb0ed442e: Found 0 pods out of 5
Mar  6 04:43:02.341: INFO: Pod name wrapped-volume-race-50369d6e-3fca-11e9-8de6-d63fb0ed442e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-50369d6e-3fca-11e9-8de6-d63fb0ed442e in namespace e2e-tests-emptydir-wrapper-lzzdd, will wait for the garbage collector to delete the pods
Mar  6 04:43:12.418: INFO: Deleting ReplicationController wrapped-volume-race-50369d6e-3fca-11e9-8de6-d63fb0ed442e took: 6.406056ms
Mar  6 04:43:12.518: INFO: Terminating ReplicationController wrapped-volume-race-50369d6e-3fca-11e9-8de6-d63fb0ed442e pods took: 100.225911ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:43:50.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-lzzdd" for this suite.
Mar  6 04:43:58.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:43:58.087: INFO: namespace: e2e-tests-emptydir-wrapper-lzzdd, resource: bindings, ignored listing per whitelist
Mar  6 04:43:58.159: INFO: namespace e2e-tests-emptydir-wrapper-lzzdd deletion completed in 8.1391504s

• [SLOW TEST:166.467 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:43:58.159: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 04:43:58.233: INFO: Waiting up to 5m0s for pod "pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-f2mlh" to be "success or failure"
Mar  6 04:43:58.236: INFO: Pod "pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692875ms
Mar  6 04:44:00.240: INFO: Pod "pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006425443s
STEP: Saw pod success
Mar  6 04:44:00.240: INFO: Pod "pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:44:00.243: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:44:00.266: INFO: Waiting for pod pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:44:00.269: INFO: Pod pod-7483b41a-3fca-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:44:00.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-f2mlh" for this suite.
Mar  6 04:44:06.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:44:06.363: INFO: namespace: e2e-tests-emptydir-f2mlh, resource: bindings, ignored listing per whitelist
Mar  6 04:44:06.416: INFO: namespace e2e-tests-emptydir-f2mlh deletion completed in 6.143229272s

• [SLOW TEST:8.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:44:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0306 04:44:16.513067      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 04:44:16.513: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:44:16.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-btrxj" for this suite.
Mar  6 04:44:22.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:44:22.583: INFO: namespace: e2e-tests-gc-btrxj, resource: bindings, ignored listing per whitelist
Mar  6 04:44:22.661: INFO: namespace e2e-tests-gc-btrxj deletion completed in 6.141845377s

• [SLOW TEST:16.244 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:44:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-831d8a1d-3fca-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 04:44:22.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-c7csp" to be "success or failure"
Mar  6 04:44:22.737: INFO: Pod "pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796477ms
Mar  6 04:44:24.741: INFO: Pod "pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007604841s
STEP: Saw pod success
Mar  6 04:44:24.741: INFO: Pod "pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:44:24.743: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 04:44:24.764: INFO: Waiting for pod pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:44:24.767: INFO: Pod pod-configmaps-831e7e5b-3fca-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:44:24.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c7csp" for this suite.
Mar  6 04:44:30.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:44:30.854: INFO: namespace: e2e-tests-configmap-c7csp, resource: bindings, ignored listing per whitelist
Mar  6 04:44:30.910: INFO: namespace e2e-tests-configmap-c7csp deletion completed in 6.139018384s

• [SLOW TEST:8.249 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:44:30.910: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-674n
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 04:44:30.986: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-674n" in namespace "e2e-tests-subpath-7m6k5" to be "success or failure"
Mar  6 04:44:30.990: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Pending", Reason="", readiness=false. Elapsed: 3.380591ms
Mar  6 04:44:32.993: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007037616s
Mar  6 04:44:34.997: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 4.010703645s
Mar  6 04:44:37.001: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 6.014558472s
Mar  6 04:44:39.005: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 8.018427705s
Mar  6 04:44:41.009: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 10.0231097s
Mar  6 04:44:43.013: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 12.02613227s
Mar  6 04:44:45.017: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 14.030198325s
Mar  6 04:44:47.021: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 16.034364628s
Mar  6 04:44:49.024: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 18.0378369s
Mar  6 04:44:51.028: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Running", Reason="", readiness=false. Elapsed: 20.042002014s
Mar  6 04:44:53.032: INFO: Pod "pod-subpath-test-downwardapi-674n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045991535s
STEP: Saw pod success
Mar  6 04:44:53.032: INFO: Pod "pod-subpath-test-downwardapi-674n" satisfied condition "success or failure"
Mar  6 04:44:53.036: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-subpath-test-downwardapi-674n container test-container-subpath-downwardapi-674n: <nil>
STEP: delete the pod
Mar  6 04:44:53.061: INFO: Waiting for pod pod-subpath-test-downwardapi-674n to disappear
Mar  6 04:44:53.064: INFO: Pod pod-subpath-test-downwardapi-674n no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-674n
Mar  6 04:44:53.064: INFO: Deleting pod "pod-subpath-test-downwardapi-674n" in namespace "e2e-tests-subpath-7m6k5"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:44:53.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7m6k5" for this suite.
Mar  6 04:44:59.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:44:59.123: INFO: namespace: e2e-tests-subpath-7m6k5, resource: bindings, ignored listing per whitelist
Mar  6 04:44:59.223: INFO: namespace e2e-tests-subpath-7m6k5 deletion completed in 6.150735772s

• [SLOW TEST:28.312 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:44:59.223: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 04:44:59.296: INFO: PodSpec: initContainers in spec.initContainers
Mar  6 04:45:44.199: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-98eaa15b-3fca-11e9-8de6-d63fb0ed442e", GenerateName:"", Namespace:"e2e-tests-init-container-bdnpt", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-bdnpt/pods/pod-init-98eaa15b-3fca-11e9-8de6-d63fb0ed442e", UID:"98eb0582-3fca-11e9-91c9-005056979acf", ResourceVersion:"19006", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687444299, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"296806866"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.232.120\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zh7s8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001533340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zh7s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zh7s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zh7s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00148c948), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"vmw3-k8s-07.local.dev", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0018f0c00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00148c9c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00148c9e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00148c9e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00148c9ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687444299, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687444299, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687444299, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687444299, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.5.1.25", PodIP:"192.168.232.120", StartTime:(*v1.Time)(0xc000fb6ae0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002c71f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002c7340)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://84e3a4dfa6d70b02941791700268e4341cfe00c0ee65a04467a83cfe37145c7c"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fb6b60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fb6b20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:45:44.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-bdnpt" for this suite.
Mar  6 04:46:06.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:46:06.347: INFO: namespace: e2e-tests-init-container-bdnpt, resource: bindings, ignored listing per whitelist
Mar  6 04:46:06.351: INFO: namespace e2e-tests-init-container-bdnpt deletion completed in 22.145836839s

• [SLOW TEST:67.128 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:46:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e
Mar  6 04:46:06.457: INFO: Pod name my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e: Found 0 pods out of 1
Mar  6 04:46:11.461: INFO: Pod name my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e: Found 1 pods out of 1
Mar  6 04:46:11.461: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e" are running
Mar  6 04:46:11.465: INFO: Pod "my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e-7mzzd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 04:46:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 04:46:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 04:46:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 04:46:06 +0000 UTC Reason: Message:}])
Mar  6 04:46:11.465: INFO: Trying to dial the pod
Mar  6 04:46:16.479: INFO: Controller my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e: Got expected result from replica 1 [my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e-7mzzd]: "my-hostname-basic-c0f13434-3fca-11e9-8de6-d63fb0ed442e-7mzzd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:46:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-tl48h" for this suite.
Mar  6 04:46:22.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:46:22.603: INFO: namespace: e2e-tests-replication-controller-tl48h, resource: bindings, ignored listing per whitelist
Mar  6 04:46:22.637: INFO: namespace e2e-tests-replication-controller-tl48h deletion completed in 6.152625938s

• [SLOW TEST:16.285 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:46:22.637: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Mar  6 04:46:22.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:22.910: INFO: stderr: ""
Mar  6 04:46:22.910: INFO: stdout: "pod/pause created\n"
Mar  6 04:46:22.910: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  6 04:46:22.910: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-jlccw" to be "running and ready"
Mar  6 04:46:22.913: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880142ms
Mar  6 04:46:24.917: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006482521s
Mar  6 04:46:24.917: INFO: Pod "pause" satisfied condition "running and ready"
Mar  6 04:46:24.917: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  6 04:46:24.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.005: INFO: stderr: ""
Mar  6 04:46:25.005: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  6 04:46:25.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pod pause -L testing-label --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.089: INFO: stderr: ""
Mar  6 04:46:25.089: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  6 04:46:25.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 label pods pause testing-label- --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.178: INFO: stderr: ""
Mar  6 04:46:25.178: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  6 04:46:25.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pod pause -L testing-label --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.263: INFO: stderr: ""
Mar  6 04:46:25.263: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Mar  6 04:46:25.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.344: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 04:46:25.344: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  6 04:46:25.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-jlccw'
Mar  6 04:46:25.434: INFO: stderr: "No resources found.\n"
Mar  6 04:46:25.434: INFO: stdout: ""
Mar  6 04:46:25.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -l name=pause --namespace=e2e-tests-kubectl-jlccw -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 04:46:25.516: INFO: stderr: ""
Mar  6 04:46:25.516: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:46:25.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jlccw" for this suite.
Mar  6 04:46:31.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:46:31.542: INFO: namespace: e2e-tests-kubectl-jlccw, resource: bindings, ignored listing per whitelist
Mar  6 04:46:31.659: INFO: namespace e2e-tests-kubectl-jlccw deletion completed in 6.138669398s

• [SLOW TEST:9.022 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:46:31.659: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 04:46:31.737: INFO: Waiting up to 5m0s for pod "pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-wf659" to be "success or failure"
Mar  6 04:46:31.741: INFO: Pod "pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404572ms
Mar  6 04:46:33.744: INFO: Pod "pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006824336s
STEP: Saw pod success
Mar  6 04:46:33.744: INFO: Pod "pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:46:33.747: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 04:46:33.769: INFO: Waiting for pod pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:46:33.772: INFO: Pod pod-d002ae15-3fca-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:46:33.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wf659" for this suite.
Mar  6 04:46:39.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:46:39.892: INFO: namespace: e2e-tests-emptydir-wf659, resource: bindings, ignored listing per whitelist
Mar  6 04:46:39.917: INFO: namespace e2e-tests-emptydir-wf659 deletion completed in 6.140812553s

• [SLOW TEST:8.258 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:46:39.918: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:46:39.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 version --client'
Mar  6 04:46:40.051: INFO: stderr: ""
Mar  6 04:46:40.051: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar  6 04:46:40.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-sxx4f'
Mar  6 04:46:40.198: INFO: stderr: ""
Mar  6 04:46:40.198: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  6 04:46:40.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-sxx4f'
Mar  6 04:46:40.345: INFO: stderr: ""
Mar  6 04:46:40.345: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 04:46:41.351: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 04:46:41.351: INFO: Found 1 / 1
Mar  6 04:46:41.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 04:46:41.354: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 04:46:41.354: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 04:46:41.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 describe pod redis-master-755zl --namespace=e2e-tests-kubectl-sxx4f'
Mar  6 04:46:41.448: INFO: stderr: ""
Mar  6 04:46:41.448: INFO: stdout: "Name:               redis-master-755zl\nNamespace:          e2e-tests-kubectl-sxx4f\nPriority:           0\nPriorityClassName:  <none>\nNode:               vmw3-k8s-04.local.dev/10.5.1.20\nStart Time:         Wed, 06 Mar 2019 04:46:40 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        k8s.v1.cni.cncf.io/networks-status:\n                      [{\n                          \"name\": \"k8s-pod-network\",\n                          \"ips\": [\n                              \"192.168.40.152\"\n                          ],\n                          \"default\": true,\n                          \"dns\": {}\n                      }]\nStatus:             Running\nIP:                 192.168.40.152\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://1797da25457d3050ecfc083ffbf15b1235bbc8b8d6202f4ed4636cb86b248b55\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 Mar 2019 04:46:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7dmkz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7dmkz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7dmkz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                            Message\n  ----    ------     ----  ----                            -------\n  Normal  Scheduled  1s    default-scheduler               Successfully assigned e2e-tests-kubectl-sxx4f/redis-master-755zl to vmw3-k8s-04.local.dev\n  Normal  Pulled     1s    kubelet, vmw3-k8s-04.local.dev  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, vmw3-k8s-04.local.dev  Created container\n  Normal  Started    0s    kubelet, vmw3-k8s-04.local.dev  Started container\n"
Mar  6 04:46:41.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 describe rc redis-master --namespace=e2e-tests-kubectl-sxx4f'
Mar  6 04:46:41.544: INFO: stderr: ""
Mar  6 04:46:41.544: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-sxx4f\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: redis-master-755zl\n"
Mar  6 04:46:41.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 describe service redis-master --namespace=e2e-tests-kubectl-sxx4f'
Mar  6 04:46:41.629: INFO: stderr: ""
Mar  6 04:46:41.629: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-sxx4f\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.108.147.5\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.40.152:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  6 04:46:41.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 describe node vmw3-k8s-01.local.dev'
Mar  6 04:46:41.741: INFO: stderr: ""
Mar  6 04:46:41.741: INFO: stdout: "Name:               vmw3-k8s-01.local.dev\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    erikube/version=1.7.0\n                    kubernetes.io/hostname=vmw3-k8s-01.local.dev\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Mar 2019 03:46:52 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 06 Mar 2019 04:46:38 +0000   Wed, 06 Mar 2019 03:46:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 06 Mar 2019 04:46:38 +0000   Wed, 06 Mar 2019 03:46:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 06 Mar 2019 04:46:38 +0000   Wed, 06 Mar 2019 03:46:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 06 Mar 2019 04:46:38 +0000   Wed, 06 Mar 2019 03:53:23 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.5.1.10\n  Hostname:    vmw3-k8s-01.local.dev\nCapacity:\n cpu:                4\n ephemeral-storage:  20470Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8043052Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  19317915617\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7940652Ki\n pods:               110\nSystem Info:\n Machine ID:                 8a8a064352ad48de87125094e434a656\n System UUID:                28ED1742-422A-21E2-EBD0-F0C1CAF8B4E4\n Boot ID:                    5258b03e-1173-4c5f-a36c-8f34ed3f50f4\n Kernel Version:             3.10.0-957.el7.x86_64\n OS Image:                   Red Hat Enterprise Linux Server 7.6 (Maipo)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-cvgv8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                calico-kube-controllers-5ccb78cdb4-djnfr                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                calico-node-j8xhb                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                coredns-554655b549-8chqk                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     59m\n  kube-system                coredns-554655b549-pfhwn                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     59m\n  kube-system                kube-apiserver-vmw3-k8s-01.local.dev                       250m (6%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                kube-controller-manager-vmw3-k8s-01.local.dev              200m (5%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                kube-multus-ds-amd64-5klfc                                 100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      58m\n  kube-system                kube-proxy-mpc7q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                kube-scheduler-vmw3-k8s-01.local.dev                       100m (2%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                kubernetes-dashboard-56d89b4fdb-9skqp                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1100m (27%)  100m (2%)\n  memory             190Mi (2%)   390Mi (5%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                               Message\n  ----    ------                   ----               ----                               -------\n  Normal  NodeHasSufficientMemory  60m (x8 over 60m)  kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    60m (x8 over 60m)  kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     60m (x7 over 60m)  kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientPID\n  Normal  Starting                 59m                kube-proxy, vmw3-k8s-01.local.dev  Starting kube-proxy.\n  Normal  Starting                 59m                kubelet, vmw3-k8s-01.local.dev     Starting kubelet.\n  Normal  NodeHasSufficientMemory  59m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    59m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     59m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  59m                kubelet, vmw3-k8s-01.local.dev     Updated Node Allocatable limit across pods\n  Normal  NodeReady                58m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeReady\n  Normal  Starting                 53m                kubelet, vmw3-k8s-01.local.dev     Starting kubelet.\n  Normal  NodeHasSufficientMemory  53m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    53m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     53m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             53m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  53m                kubelet, vmw3-k8s-01.local.dev     Updated Node Allocatable limit across pods\n  Normal  NodeReady                53m                kubelet, vmw3-k8s-01.local.dev     Node vmw3-k8s-01.local.dev status is now: NodeReady\n"
Mar  6 04:46:41.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 describe namespace e2e-tests-kubectl-sxx4f'
Mar  6 04:46:41.826: INFO: stderr: ""
Mar  6 04:46:41.826: INFO: stdout: "Name:         e2e-tests-kubectl-sxx4f\nLabels:       e2e-framework=kubectl\n              e2e-run=be3acd62-3fc3-11e9-8de6-d63fb0ed442e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:46:41.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sxx4f" for this suite.
Mar  6 04:47:03.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:47:03.909: INFO: namespace: e2e-tests-kubectl-sxx4f, resource: bindings, ignored listing per whitelist
Mar  6 04:47:03.969: INFO: namespace e2e-tests-kubectl-sxx4f deletion completed in 22.138944203s

• [SLOW TEST:24.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:47:03.969: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar  6 04:47:04.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 --namespace=e2e-tests-kubectl-j8jzg run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  6 04:47:05.402: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  6 04:47:05.402: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:47:07.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j8jzg" for this suite.
Mar  6 04:47:19.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:47:19.449: INFO: namespace: e2e-tests-kubectl-j8jzg, resource: bindings, ignored listing per whitelist
Mar  6 04:47:19.550: INFO: namespace e2e-tests-kubectl-j8jzg deletion completed in 12.137085971s

• [SLOW TEST:15.581 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:47:19.550: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  6 04:47:19.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19488,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 04:47:19.642: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19489,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 04:47:19.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19490,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  6 04:47:29.670: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19519,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 04:47:29.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19520,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  6 04:47:29.670: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-65v75,SelfLink:/api/v1/namespaces/e2e-tests-watch-65v75/configmaps/e2e-watch-test-label-changed,UID:ec8f26d0-3fca-11e9-91c9-005056979acf,ResourceVersion:19521,Generation:0,CreationTimestamp:2019-03-06 04:47:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:47:29.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-65v75" for this suite.
Mar  6 04:47:35.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:47:35.784: INFO: namespace: e2e-tests-watch-65v75, resource: bindings, ignored listing per whitelist
Mar  6 04:47:35.824: INFO: namespace e2e-tests-watch-65v75 deletion completed in 6.144953082s

• [SLOW TEST:16.274 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:47:35.824: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f641d4d0-3fca-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:47:35.908: INFO: Waiting up to 5m0s for pod "pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-7srdp" to be "success or failure"
Mar  6 04:47:35.911: INFO: Pod "pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.88436ms
Mar  6 04:47:37.916: INFO: Pod "pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007414097s
STEP: Saw pod success
Mar  6 04:47:37.916: INFO: Pod "pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:47:37.918: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:47:37.954: INFO: Waiting for pod pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:47:37.957: INFO: Pod pod-secrets-f642b4ba-3fca-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:47:37.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7srdp" for this suite.
Mar  6 04:47:43.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:47:44.071: INFO: namespace: e2e-tests-secrets-7srdp, resource: bindings, ignored listing per whitelist
Mar  6 04:47:44.133: INFO: namespace e2e-tests-secrets-7srdp deletion completed in 6.168220518s

• [SLOW TEST:8.308 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:47:44.133: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-7k974
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-7k974
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-7k974
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-7k974
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-7k974
Mar  6 04:47:48.233: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-7k974, name: ss-0, uid: fd600f17-3fca-11e9-a5d3-00505697ee14, status phase: Pending. Waiting for statefulset controller to delete.
Mar  6 04:47:48.426: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-7k974, name: ss-0, uid: fd600f17-3fca-11e9-a5d3-00505697ee14, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 04:47:48.431: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-7k974, name: ss-0, uid: fd600f17-3fca-11e9-a5d3-00505697ee14, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 04:47:48.434: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-7k974
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-7k974
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-7k974 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 04:47:52.454: INFO: Deleting all statefulset in ns e2e-tests-statefulset-7k974
Mar  6 04:47:52.457: INFO: Scaling statefulset ss to 0
Mar  6 04:48:12.470: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 04:48:12.473: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:48:12.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-7k974" for this suite.
Mar  6 04:48:18.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:48:18.524: INFO: namespace: e2e-tests-statefulset-7k974, resource: bindings, ignored listing per whitelist
Mar  6 04:48:18.638: INFO: namespace e2e-tests-statefulset-7k974 deletion completed in 6.142171462s

• [SLOW TEST:34.505 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:48:18.638: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-7wczn
Mar  6 04:48:20.722: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-7wczn
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 04:48:20.725: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:52:21.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-7wczn" for this suite.
Mar  6 04:52:27.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:52:27.379: INFO: namespace: e2e-tests-container-probe-7wczn, resource: bindings, ignored listing per whitelist
Mar  6 04:52:27.381: INFO: namespace e2e-tests-container-probe-7wczn deletion completed in 6.151567029s

• [SLOW TEST:248.743 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:52:27.381: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:52:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2q269" for this suite.
Mar  6 04:52:37.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:52:37.530: INFO: namespace: e2e-tests-kubelet-test-2q269, resource: bindings, ignored listing per whitelist
Mar  6 04:52:37.637: INFO: namespace e2e-tests-kubelet-test-2q269 deletion completed in 6.16530868s

• [SLOW TEST:10.256 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:52:37.637: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  6 04:52:37.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:38.024: INFO: stderr: ""
Mar  6 04:52:38.024: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:52:38.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:38.113: INFO: stderr: ""
Mar  6 04:52:38.113: INFO: stdout: "update-demo-nautilus-rx6wg update-demo-nautilus-vcm4t "
Mar  6 04:52:38.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:38.194: INFO: stderr: ""
Mar  6 04:52:38.194: INFO: stdout: ""
Mar  6 04:52:38.194: INFO: update-demo-nautilus-rx6wg is created but not running
Mar  6 04:52:43.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:43.269: INFO: stderr: ""
Mar  6 04:52:43.269: INFO: stdout: "update-demo-nautilus-rx6wg update-demo-nautilus-vcm4t "
Mar  6 04:52:43.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:43.344: INFO: stderr: ""
Mar  6 04:52:43.344: INFO: stdout: "true"
Mar  6 04:52:43.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:43.426: INFO: stderr: ""
Mar  6 04:52:43.426: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:43.426: INFO: validating pod update-demo-nautilus-rx6wg
Mar  6 04:52:43.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:43.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:43.432: INFO: update-demo-nautilus-rx6wg is verified up and running
Mar  6 04:52:43.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-vcm4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:43.511: INFO: stderr: ""
Mar  6 04:52:43.511: INFO: stdout: "true"
Mar  6 04:52:43.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-vcm4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:43.605: INFO: stderr: ""
Mar  6 04:52:43.605: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:43.605: INFO: validating pod update-demo-nautilus-vcm4t
Mar  6 04:52:43.610: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:43.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:43.610: INFO: update-demo-nautilus-vcm4t is verified up and running
STEP: scaling down the replication controller
Mar  6 04:52:43.613: INFO: scanned /root for discovery docs: <nil>
Mar  6 04:52:43.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:44.736: INFO: stderr: ""
Mar  6 04:52:44.736: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:52:44.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:44.816: INFO: stderr: ""
Mar  6 04:52:44.816: INFO: stdout: "update-demo-nautilus-rx6wg update-demo-nautilus-vcm4t "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  6 04:52:49.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:49.909: INFO: stderr: ""
Mar  6 04:52:49.909: INFO: stdout: "update-demo-nautilus-rx6wg "
Mar  6 04:52:49.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:50.031: INFO: stderr: ""
Mar  6 04:52:50.031: INFO: stdout: "true"
Mar  6 04:52:50.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:50.125: INFO: stderr: ""
Mar  6 04:52:50.125: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:50.125: INFO: validating pod update-demo-nautilus-rx6wg
Mar  6 04:52:50.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:50.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:50.130: INFO: update-demo-nautilus-rx6wg is verified up and running
STEP: scaling up the replication controller
Mar  6 04:52:50.132: INFO: scanned /root for discovery docs: <nil>
Mar  6 04:52:50.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:51.244: INFO: stderr: ""
Mar  6 04:52:51.244: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 04:52:51.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:51.325: INFO: stderr: ""
Mar  6 04:52:51.325: INFO: stdout: "update-demo-nautilus-rx6wg update-demo-nautilus-w6fq6 "
Mar  6 04:52:51.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:51.403: INFO: stderr: ""
Mar  6 04:52:51.403: INFO: stdout: "true"
Mar  6 04:52:51.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:51.478: INFO: stderr: ""
Mar  6 04:52:51.478: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:51.478: INFO: validating pod update-demo-nautilus-rx6wg
Mar  6 04:52:51.482: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:51.482: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:51.482: INFO: update-demo-nautilus-rx6wg is verified up and running
Mar  6 04:52:51.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-w6fq6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:51.553: INFO: stderr: ""
Mar  6 04:52:51.553: INFO: stdout: ""
Mar  6 04:52:51.553: INFO: update-demo-nautilus-w6fq6 is created but not running
Mar  6 04:52:56.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:56.673: INFO: stderr: ""
Mar  6 04:52:56.673: INFO: stdout: "update-demo-nautilus-rx6wg update-demo-nautilus-w6fq6 "
Mar  6 04:52:56.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:56.777: INFO: stderr: ""
Mar  6 04:52:56.777: INFO: stdout: "true"
Mar  6 04:52:56.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-rx6wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:56.870: INFO: stderr: ""
Mar  6 04:52:56.870: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:56.870: INFO: validating pod update-demo-nautilus-rx6wg
Mar  6 04:52:56.874: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:56.874: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:56.874: INFO: update-demo-nautilus-rx6wg is verified up and running
Mar  6 04:52:56.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-w6fq6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:56.966: INFO: stderr: ""
Mar  6 04:52:56.966: INFO: stdout: "true"
Mar  6 04:52:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods update-demo-nautilus-w6fq6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:57.064: INFO: stderr: ""
Mar  6 04:52:57.064: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  6 04:52:57.064: INFO: validating pod update-demo-nautilus-w6fq6
Mar  6 04:52:57.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 04:52:57.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 04:52:57.069: INFO: update-demo-nautilus-w6fq6 is verified up and running
STEP: using delete to clean up resources
Mar  6 04:52:57.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:57.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 04:52:57.163: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 04:52:57.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-hnq2f'
Mar  6 04:52:57.261: INFO: stderr: "No resources found.\n"
Mar  6 04:52:57.261: INFO: stdout: ""
Mar  6 04:52:57.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pods -l name=update-demo --namespace=e2e-tests-kubectl-hnq2f -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 04:52:57.351: INFO: stderr: ""
Mar  6 04:52:57.351: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:52:57.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hnq2f" for this suite.
Mar  6 04:53:19.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:53:19.427: INFO: namespace: e2e-tests-kubectl-hnq2f, resource: bindings, ignored listing per whitelist
Mar  6 04:53:19.540: INFO: namespace e2e-tests-kubectl-hnq2f deletion completed in 22.183675999s

• [SLOW TEST:41.902 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:53:19.540: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cghkg
Mar  6 04:53:21.643: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cghkg
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 04:53:21.645: INFO: Initial restart count of pod liveness-exec is 0
Mar  6 04:54:11.745: INFO: Restart count of pod e2e-tests-container-probe-cghkg/liveness-exec is now 1 (50.100098029s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:54:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cghkg" for this suite.
Mar  6 04:54:17.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:54:17.904: INFO: namespace: e2e-tests-container-probe-cghkg, resource: bindings, ignored listing per whitelist
Mar  6 04:54:17.915: INFO: namespace e2e-tests-container-probe-cghkg deletion completed in 6.152134655s

• [SLOW TEST:58.375 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:54:17.915: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 04:54:20.527: INFO: Successfully updated pod "labelsupdatee5ebbe8b-3fcb-11e9-8de6-d63fb0ed442e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:54:24.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cfk5v" for this suite.
Mar  6 04:54:46.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:54:46.704: INFO: namespace: e2e-tests-projected-cfk5v, resource: bindings, ignored listing per whitelist
Mar  6 04:54:46.767: INFO: namespace e2e-tests-projected-cfk5v deletion completed in 22.204626458s

• [SLOW TEST:28.852 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:54:46.767: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:54:46.843: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  6 04:54:46.854: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  6 04:54:51.858: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 04:54:51.858: INFO: Creating deployment "test-rolling-update-deployment"
Mar  6 04:54:51.862: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  6 04:54:51.868: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  6 04:54:53.875: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  6 04:54:53.877: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 04:54:53.885: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-9j444,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9j444/deployments/test-rolling-update-deployment,UID:fa1c72ef-3fcb-11e9-91c9-005056979acf,ResourceVersion:21334,Generation:1,CreationTimestamp:2019-03-06 04:54:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-06 04:54:51 +0000 UTC 2019-03-06 04:54:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-06 04:54:53 +0000 UTC 2019-03-06 04:54:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 04:54:53.888: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-9j444,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9j444/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:fa1f8277-3fcb-11e9-a5d3-00505697ee14,ResourceVersion:21325,Generation:1,CreationTimestamp:2019-03-06 04:54:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment fa1c72ef-3fcb-11e9-91c9-005056979acf 0xc00225fb37 0xc00225fb38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 04:54:53.888: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  6 04:54:53.888: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-9j444,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9j444/replicasets/test-rolling-update-controller,UID:f71f3c06-3fcb-11e9-91c9-005056979acf,ResourceVersion:21333,Generation:2,CreationTimestamp:2019-03-06 04:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment fa1c72ef-3fcb-11e9-91c9-005056979acf 0xc00225fa77 0xc00225fa78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 04:54:53.891: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-rhvrd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-rhvrd,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-9j444,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9j444/pods/test-rolling-update-deployment-68b55d7bc6-rhvrd,UID:fa204c74-3fcb-11e9-a5d3-00505697ee14,ResourceVersion:21324,Generation:0,CreationTimestamp:2019-03-06 04:54:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.232.125"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 fa1f8277-3fcb-11e9-a5d3-00505697ee14 0xc001d25447 0xc001d25448}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7lpnn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7lpnn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7lpnn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d25a30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d25a50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:54:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:54:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:54:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:54:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.25,PodIP:192.168.232.125,StartTime:2019-03-06 04:54:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-06 04:54:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://17e1124a8e452b18806142cca8f8e975021a6b001979cc749da2c74daac63e42}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:54:53.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9j444" for this suite.
Mar  6 04:54:59.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:54:59.963: INFO: namespace: e2e-tests-deployment-9j444, resource: bindings, ignored listing per whitelist
Mar  6 04:55:00.038: INFO: namespace e2e-tests-deployment-9j444 deletion completed in 6.142657134s

• [SLOW TEST:13.271 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:55:00.038: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar  6 04:55:00.634: INFO: created pod pod-service-account-defaultsa
Mar  6 04:55:00.634: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  6 04:55:00.637: INFO: created pod pod-service-account-mountsa
Mar  6 04:55:00.637: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  6 04:55:00.641: INFO: created pod pod-service-account-nomountsa
Mar  6 04:55:00.641: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  6 04:55:00.647: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  6 04:55:00.647: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  6 04:55:00.651: INFO: created pod pod-service-account-mountsa-mountspec
Mar  6 04:55:00.651: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  6 04:55:00.655: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  6 04:55:00.655: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  6 04:55:00.660: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  6 04:55:00.660: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  6 04:55:00.664: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  6 04:55:00.664: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  6 04:55:00.670: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  6 04:55:00.670: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:55:00.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-894jw" for this suite.
Mar  6 04:55:06.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:55:06.788: INFO: namespace: e2e-tests-svcaccounts-894jw, resource: bindings, ignored listing per whitelist
Mar  6 04:55:06.822: INFO: namespace e2e-tests-svcaccounts-894jw deletion completed in 6.148340738s

• [SLOW TEST:6.784 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:55:06.822: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 04:55:10.937: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 04:55:10.939: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 04:55:12.939: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 04:55:12.943: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 04:55:14.939: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 04:55:14.943: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 04:55:16.940: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 04:55:16.943: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 04:55:18.940: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 04:55:18.943: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:55:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dhnmt" for this suite.
Mar  6 04:55:40.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:55:41.016: INFO: namespace: e2e-tests-container-lifecycle-hook-dhnmt, resource: bindings, ignored listing per whitelist
Mar  6 04:55:41.097: INFO: namespace e2e-tests-container-lifecycle-hook-dhnmt deletion completed in 22.1494711s

• [SLOW TEST:34.275 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:55:41.098: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-1782769e-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating configMap with name cm-test-opt-upd-178276ec-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1782769e-3fcc-11e9-8de6-d63fb0ed442e
STEP: Updating configmap cm-test-opt-upd-178276ec-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating configMap with name cm-test-opt-create-1782770a-3fcc-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:55:45.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5rz24" for this suite.
Mar  6 04:56:07.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:56:07.403: INFO: namespace: e2e-tests-configmap-5rz24, resource: bindings, ignored listing per whitelist
Mar  6 04:56:07.461: INFO: namespace e2e-tests-configmap-5rz24 deletion completed in 22.170044477s

• [SLOW TEST:26.363 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:56:07.461: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 04:56:07.537: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 04:56:07.548: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 04:56:07.552: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-04.local.dev before test
Mar  6 04:56:07.565: INFO: rook-ceph-mon0-wgbcm from rook started at 2019-03-06 03:52:44 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:56:07.565: INFO: rook-ceph-osd-6cjzd from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:56:07.565: INFO: calico-node-grzxg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:56:07.565: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:56:07.565: INFO: kube-multus-ds-amd64-zjcth from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:56:07.565: INFO: rook-ceph-mgr0-c69f5fd99-snnnj from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Mar  6 04:56:07.565: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 03:55:26 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 04:56:07.565: INFO: node-feature-discovery-chnnf from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:56:07.565: INFO: prometheus-prometheus-node-exporter-dwxpj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:56:07.565: INFO: kube-proxy-vcgjg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:56:07.565: INFO: nfs-provisioner-54db5878bf-7rzqj from default started at 2019-03-06 03:50:45 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container nfs-provisioner ready: true, restart count 0
Mar  6 04:56:07.565: INFO: local-volume-provisioner-ltngn from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:56:07.565: INFO: rook-agent-pqt8r from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:56:07.565: INFO: dex-547c49d486-94z28 from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container dex ready: true, restart count 0
Mar  6 04:56:07.565: INFO: prometheus-prometheus-kube-state-metrics-5b55c99749-mczll from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Mar  6 04:56:07.565: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-vcqv2 from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.565: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 04:56:07.565: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 04:56:07.565: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-05.local.dev before test
Mar  6 04:56:07.578: INFO: prometheus-prometheus-node-exporter-8px6c from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:56:07.578: INFO: node-feature-discovery-jf2bq from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:56:07.578: INFO: rook-api-8596f5cffd-h55tg from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container rook-api ready: true, restart count 0
Mar  6 04:56:07.578: INFO: rook-ceph-rgw-erikube-rook-rgw-568fb7598d-psm2f from rook started at 2019-03-06 03:53:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container rook-ceph-rgw-erikube-rook-rgw ready: true, restart count 0
Mar  6 04:56:07.578: INFO: kube-multus-ds-amd64-57r48 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:56:07.578: INFO: nginx-ingress-controller-c54bdfdb5-kwjtf from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 04:56:07.578: INFO: local-volume-provisioner-gbqgz from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:56:07.578: INFO: rook-agent-cp4vx from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:56:07.578: INFO: dex-547c49d486-nk6lm from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container dex ready: true, restart count 1
Mar  6 04:56:07.578: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-2dkkl from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 04:56:07.578: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 04:56:07.578: INFO: kube-proxy-pzh8g from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:56:07.578: INFO: calico-node-dlgtm from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:56:07.578: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:56:07.578: INFO: rook-ceph-mon1-d2vgn from rook started at 2019-03-06 03:52:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:56:07.578: INFO: rook-ceph-osd-czks7 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:56:07.578: INFO: prometheus-prometheus-server-86566f7789-44rqb from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.578: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  6 04:56:07.578: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  6 04:56:07.578: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-06.local.dev before test
Mar  6 04:56:07.587: INFO: kube-multus-ds-amd64-cxrt5 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:56:07.587: INFO: rook-agent-khk9s from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:56:07.587: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-nr2gk from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 04:56:07.587: INFO: prometheus-prometheus-node-exporter-4k7zk from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:56:07.587: INFO: nginx-ingress-controller-c54bdfdb5-fdksz from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 04:56:07.587: INFO: node-feature-discovery-7mh4p from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 04:56:07.587: INFO: rook-ceph-osd-2wrw6 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 04:56:07.587: INFO: rook-operator-596d9f8565-4hvxg from rook-system started at 2019-03-06 03:52:02 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container rook-operator ready: true, restart count 0
Mar  6 04:56:07.587: INFO: rook-ceph-mon2-rtjdd from rook started at 2019-03-06 03:53:03 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 04:56:07.587: INFO: prometheus-prometheus-pushgateway-76f6bd8898-8llhj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  6 04:56:07.587: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-84dpr from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 04:56:07.587: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 04:56:07.587: INFO: kube-proxy-hw2bk from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:56:07.587: INFO: calico-node-m85tv from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:56:07.587: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:56:07.587: INFO: local-volume-provisioner-fhbzm from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.587: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 04:56:07.587: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-07.local.dev before test
Mar  6 04:56:07.599: INFO: rbd-provisioner-rbd-provisioner-74849667b8-997n8 from kube-system started at 2019-03-06 03:51:35 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container rbd-provisioner ready: true, restart count 0
Mar  6 04:56:07.599: INFO: metrics-server-5bddfb6979-6qqvx from kube-system started at 2019-03-06 03:53:07 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container metrics-server ready: true, restart count 0
Mar  6 04:56:07.599: INFO: default-http-backend-c776c779-bgdc2 from ingress-nginx started at 2019-03-06 03:50:31 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container default-http-backend ready: true, restart count 0
Mar  6 04:56:07.599: INFO: node-feature-discovery-9frqx from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container node-feature-discovery ready: true, restart count 2
Mar  6 04:56:07.599: INFO: rook-ceph-osd-ttf2k from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container rook-ceph-osd ready: true, restart count 1
Mar  6 04:56:07.599: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-jkmzt from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 04:56:07.599: INFO: prometheus-prometheus-node-exporter-l2tbq from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 04:56:07.599: INFO: calico-node-thclf from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 04:56:07.599: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 04:56:07.599: INFO: kube-proxy-jp8qs from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 04:56:07.599: INFO: kube-multus-ds-amd64-fgvjc from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 04:56:07.599: INFO: rook-agent-qfj24 from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 04:56:07.599: INFO: prometheus-prometheus-alertmanager-7f97c695ff-7ml5d from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Mar  6 04:56:07.599: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Mar  6 04:56:07.599: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-prrwp from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 04:56:07.599: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 04:56:07.599: INFO: tiller-deploy-764968bdfd-tpt2l from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container tiller ready: true, restart count 0
Mar  6 04:56:07.599: INFO: local-volume-provisioner-j7s8k from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 04:56:07.599: INFO: 	Container provisioner ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node vmw3-k8s-04.local.dev
STEP: verifying the node has the label node vmw3-k8s-05.local.dev
STEP: verifying the node has the label node vmw3-k8s-06.local.dev
STEP: verifying the node has the label node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod local-volume-provisioner-fhbzm requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod local-volume-provisioner-gbqgz requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod local-volume-provisioner-j7s8k requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod local-volume-provisioner-ltngn requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod nfs-provisioner-54db5878bf-7rzqj requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod sonobuoy requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-2dkkl requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-84dpr requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-prrwp requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-vcqv2 requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod default-http-backend-c776c779-bgdc2 requesting resource cpu=10m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod nginx-ingress-controller-c54bdfdb5-fdksz requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod nginx-ingress-controller-c54bdfdb5-kwjtf requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod calico-node-dlgtm requesting resource cpu=250m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod calico-node-grzxg requesting resource cpu=250m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod calico-node-m85tv requesting resource cpu=250m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod calico-node-thclf requesting resource cpu=250m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod dex-547c49d486-94z28 requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod dex-547c49d486-nk6lm requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-multus-ds-amd64-57r48 requesting resource cpu=100m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-multus-ds-amd64-cxrt5 requesting resource cpu=100m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-multus-ds-amd64-fgvjc requesting resource cpu=100m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-multus-ds-amd64-zjcth requesting resource cpu=100m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-proxy-hw2bk requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-proxy-jp8qs requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-proxy-pzh8g requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod kube-proxy-vcgjg requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod metrics-server-5bddfb6979-6qqvx requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod node-feature-discovery-7mh4p requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod node-feature-discovery-9frqx requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod node-feature-discovery-chnnf requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod node-feature-discovery-jf2bq requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod rbd-provisioner-rbd-provisioner-74849667b8-997n8 requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod tiller-deploy-764968bdfd-tpt2l requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-alertmanager-7f97c695ff-7ml5d requesting resource cpu=10m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-kube-state-metrics-5b55c99749-mczll requesting resource cpu=100m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-node-exporter-4k7zk requesting resource cpu=100m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-node-exporter-8px6c requesting resource cpu=100m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-node-exporter-dwxpj requesting resource cpu=100m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-node-exporter-l2tbq requesting resource cpu=100m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-pushgateway-76f6bd8898-8llhj requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod prometheus-prometheus-server-86566f7789-44rqb requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-agent-cp4vx requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-agent-khk9s requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-agent-pqt8r requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-agent-qfj24 requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-operator-596d9f8565-4hvxg requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-api-8596f5cffd-h55tg requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-jkmzt requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-nr2gk requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mgr0-c69f5fd99-snnnj requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mon0-wgbcm requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mon1-d2vgn requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-mon2-rtjdd requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-osd-2wrw6 requesting resource cpu=0m on Node vmw3-k8s-06.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-osd-6cjzd requesting resource cpu=0m on Node vmw3-k8s-04.local.dev
Mar  6 04:56:07.677: INFO: Pod rook-ceph-osd-czks7 requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
Mar  6 04:56:07.678: INFO: Pod rook-ceph-osd-ttf2k requesting resource cpu=0m on Node vmw3-k8s-07.local.dev
Mar  6 04:56:07.678: INFO: Pod rook-ceph-rgw-erikube-rook-rgw-568fb7598d-psm2f requesting resource cpu=0m on Node vmw3-k8s-05.local.dev
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e.158945afbca7da46], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-79vsh/filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e to vmw3-k8s-05.local.dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e.158945afeb40d959], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e.158945b02b6e08b8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e.158945b02cb191dd], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274d83b4-3fcc-11e9-8de6-d63fb0ed442e.158945b033acce45], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274f1c00-3fcc-11e9-8de6-d63fb0ed442e.158945afbd2632be], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-79vsh/filler-pod-274f1c00-3fcc-11e9-8de6-d63fb0ed442e to vmw3-k8s-06.local.dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274f1c00-3fcc-11e9-8de6-d63fb0ed442e.158945afedbac84e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274f1c00-3fcc-11e9-8de6-d63fb0ed442e.158945afef2a9240], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-274f1c00-3fcc-11e9-8de6-d63fb0ed442e.158945aff6a4351d], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27504a50-3fcc-11e9-8de6-d63fb0ed442e.158945afbd74601e], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-79vsh/filler-pod-27504a50-3fcc-11e9-8de6-d63fb0ed442e to vmw3-k8s-07.local.dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27504a50-3fcc-11e9-8de6-d63fb0ed442e.158945afec417101], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27504a50-3fcc-11e9-8de6-d63fb0ed442e.158945afed9c0196], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27504a50-3fcc-11e9-8de6-d63fb0ed442e.158945aff5fa4213], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2751268c-3fcc-11e9-8de6-d63fb0ed442e.158945afbdf1bc05], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-79vsh/filler-pod-2751268c-3fcc-11e9-8de6-d63fb0ed442e to vmw3-k8s-04.local.dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2751268c-3fcc-11e9-8de6-d63fb0ed442e.158945afea46ea76], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2751268c-3fcc-11e9-8de6-d63fb0ed442e.158945afebb255d1], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2751268c-3fcc-11e9-8de6-d63fb0ed442e.158945aff3029d08], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158945b0ad673e3a], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taints that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node vmw3-k8s-04.local.dev
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vmw3-k8s-05.local.dev
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vmw3-k8s-06.local.dev
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vmw3-k8s-07.local.dev
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:56:12.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-79vsh" for this suite.
Mar  6 04:56:18.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:56:18.811: INFO: namespace: e2e-tests-sched-pred-79vsh, resource: bindings, ignored listing per whitelist
Mar  6 04:56:18.919: INFO: namespace e2e-tests-sched-pred-79vsh deletion completed in 6.13058578s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.457 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:56:18.919: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-tljtx
I0306 04:56:19.014365      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-tljtx, replica count: 1
I0306 04:56:20.064880      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 04:56:20.176: INFO: Created: latency-svc-dfnzh
Mar  6 04:56:20.188: INFO: Got endpoints: latency-svc-dfnzh [23.676656ms]
Mar  6 04:56:20.203: INFO: Created: latency-svc-4q26g
Mar  6 04:56:20.212: INFO: Got endpoints: latency-svc-4q26g [23.256707ms]
Mar  6 04:56:20.215: INFO: Created: latency-svc-6tcdf
Mar  6 04:56:20.221: INFO: Got endpoints: latency-svc-6tcdf [32.871001ms]
Mar  6 04:56:20.226: INFO: Created: latency-svc-lxpvq
Mar  6 04:56:20.233: INFO: Got endpoints: latency-svc-lxpvq [44.62883ms]
Mar  6 04:56:20.236: INFO: Created: latency-svc-gtrsq
Mar  6 04:56:20.244: INFO: Got endpoints: latency-svc-gtrsq [55.344342ms]
Mar  6 04:56:20.248: INFO: Created: latency-svc-v925d
Mar  6 04:56:20.255: INFO: Got endpoints: latency-svc-v925d [66.04314ms]
Mar  6 04:56:20.258: INFO: Created: latency-svc-7bf6p
Mar  6 04:56:20.269: INFO: Got endpoints: latency-svc-7bf6p [80.190043ms]
Mar  6 04:56:20.272: INFO: Created: latency-svc-f6wvc
Mar  6 04:56:20.276: INFO: Got endpoints: latency-svc-f6wvc [87.745205ms]
Mar  6 04:56:20.283: INFO: Created: latency-svc-g4nqm
Mar  6 04:56:20.291: INFO: Got endpoints: latency-svc-g4nqm [101.786223ms]
Mar  6 04:56:20.296: INFO: Created: latency-svc-8wxzc
Mar  6 04:56:20.306: INFO: Got endpoints: latency-svc-8wxzc [116.757062ms]
Mar  6 04:56:20.309: INFO: Created: latency-svc-89d5v
Mar  6 04:56:20.317: INFO: Got endpoints: latency-svc-89d5v [128.069148ms]
Mar  6 04:56:20.320: INFO: Created: latency-svc-86fjm
Mar  6 04:56:20.329: INFO: Got endpoints: latency-svc-86fjm [139.664082ms]
Mar  6 04:56:20.332: INFO: Created: latency-svc-qnc2m
Mar  6 04:56:20.340: INFO: Got endpoints: latency-svc-qnc2m [150.617613ms]
Mar  6 04:56:20.344: INFO: Created: latency-svc-x9h2z
Mar  6 04:56:20.353: INFO: Got endpoints: latency-svc-x9h2z [164.547881ms]
Mar  6 04:56:20.361: INFO: Created: latency-svc-s2zqp
Mar  6 04:56:20.372: INFO: Created: latency-svc-fnwpq
Mar  6 04:56:20.372: INFO: Got endpoints: latency-svc-s2zqp [182.904334ms]
Mar  6 04:56:20.378: INFO: Got endpoints: latency-svc-fnwpq [189.071658ms]
Mar  6 04:56:20.381: INFO: Created: latency-svc-t2b7m
Mar  6 04:56:20.390: INFO: Got endpoints: latency-svc-t2b7m [178.728364ms]
Mar  6 04:56:20.394: INFO: Created: latency-svc-2jr6f
Mar  6 04:56:20.402: INFO: Got endpoints: latency-svc-2jr6f [180.091735ms]
Mar  6 04:56:20.414: INFO: Created: latency-svc-p2n4r
Mar  6 04:56:20.424: INFO: Got endpoints: latency-svc-p2n4r [190.774483ms]
Mar  6 04:56:20.430: INFO: Created: latency-svc-twcj6
Mar  6 04:56:20.436: INFO: Got endpoints: latency-svc-twcj6 [191.693981ms]
Mar  6 04:56:20.453: INFO: Created: latency-svc-s4hsb
Mar  6 04:56:20.462: INFO: Got endpoints: latency-svc-s4hsb [206.924604ms]
Mar  6 04:56:20.466: INFO: Created: latency-svc-z8656
Mar  6 04:56:20.472: INFO: Got endpoints: latency-svc-z8656 [203.502421ms]
Mar  6 04:56:20.476: INFO: Created: latency-svc-kcd6l
Mar  6 04:56:20.482: INFO: Got endpoints: latency-svc-kcd6l [205.882691ms]
Mar  6 04:56:20.488: INFO: Created: latency-svc-2cvqx
Mar  6 04:56:20.494: INFO: Got endpoints: latency-svc-2cvqx [203.225563ms]
Mar  6 04:56:20.499: INFO: Created: latency-svc-6gqgt
Mar  6 04:56:20.504: INFO: Got endpoints: latency-svc-6gqgt [198.068639ms]
Mar  6 04:56:20.507: INFO: Created: latency-svc-snq5h
Mar  6 04:56:20.527: INFO: Got endpoints: latency-svc-snq5h [209.823593ms]
Mar  6 04:56:20.531: INFO: Created: latency-svc-mxhs9
Mar  6 04:56:20.537: INFO: Got endpoints: latency-svc-mxhs9 [208.290509ms]
Mar  6 04:56:20.540: INFO: Created: latency-svc-zww2b
Mar  6 04:56:20.547: INFO: Got endpoints: latency-svc-zww2b [207.761333ms]
Mar  6 04:56:20.552: INFO: Created: latency-svc-9rj4w
Mar  6 04:56:20.558: INFO: Got endpoints: latency-svc-9rj4w [204.169704ms]
Mar  6 04:56:20.561: INFO: Created: latency-svc-6p85l
Mar  6 04:56:20.567: INFO: Got endpoints: latency-svc-6p85l [194.661508ms]
Mar  6 04:56:20.572: INFO: Created: latency-svc-8zpf5
Mar  6 04:56:20.578: INFO: Got endpoints: latency-svc-8zpf5 [199.606454ms]
Mar  6 04:56:20.581: INFO: Created: latency-svc-nshtc
Mar  6 04:56:20.588: INFO: Got endpoints: latency-svc-nshtc [197.891829ms]
Mar  6 04:56:20.591: INFO: Created: latency-svc-jd2lr
Mar  6 04:56:20.596: INFO: Got endpoints: latency-svc-jd2lr [194.438807ms]
Mar  6 04:56:20.603: INFO: Created: latency-svc-t2bmc
Mar  6 04:56:20.608: INFO: Got endpoints: latency-svc-t2bmc [184.238912ms]
Mar  6 04:56:20.612: INFO: Created: latency-svc-t6gj2
Mar  6 04:56:20.617: INFO: Got endpoints: latency-svc-t6gj2 [180.81421ms]
Mar  6 04:56:20.621: INFO: Created: latency-svc-j2b6m
Mar  6 04:56:20.630: INFO: Got endpoints: latency-svc-j2b6m [167.907652ms]
Mar  6 04:56:20.633: INFO: Created: latency-svc-6clzw
Mar  6 04:56:20.639: INFO: Got endpoints: latency-svc-6clzw [165.974311ms]
Mar  6 04:56:20.642: INFO: Created: latency-svc-n7zwg
Mar  6 04:56:20.649: INFO: Got endpoints: latency-svc-n7zwg [166.097632ms]
Mar  6 04:56:20.655: INFO: Created: latency-svc-k98tm
Mar  6 04:56:20.659: INFO: Got endpoints: latency-svc-k98tm [164.61134ms]
Mar  6 04:56:20.663: INFO: Created: latency-svc-q2p8n
Mar  6 04:56:20.671: INFO: Created: latency-svc-tjbxv
Mar  6 04:56:20.681: INFO: Created: latency-svc-5zmr5
Mar  6 04:56:20.684: INFO: Got endpoints: latency-svc-q2p8n [179.855412ms]
Mar  6 04:56:20.695: INFO: Created: latency-svc-8v6bj
Mar  6 04:56:20.709: INFO: Created: latency-svc-g8hrk
Mar  6 04:56:20.726: INFO: Created: latency-svc-tdvgz
Mar  6 04:56:20.736: INFO: Got endpoints: latency-svc-tjbxv [209.46536ms]
Mar  6 04:56:20.737: INFO: Created: latency-svc-dzrr2
Mar  6 04:56:20.756: INFO: Created: latency-svc-lh7n2
Mar  6 04:56:20.773: INFO: Created: latency-svc-w8zf5
Mar  6 04:56:20.783: INFO: Created: latency-svc-4nndc
Mar  6 04:56:20.783: INFO: Got endpoints: latency-svc-5zmr5 [246.255428ms]
Mar  6 04:56:20.795: INFO: Created: latency-svc-j97j6
Mar  6 04:56:20.808: INFO: Created: latency-svc-fkk6s
Mar  6 04:56:20.821: INFO: Created: latency-svc-h2wcx
Mar  6 04:56:20.833: INFO: Got endpoints: latency-svc-8v6bj [285.213068ms]
Mar  6 04:56:20.841: INFO: Created: latency-svc-br6fv
Mar  6 04:56:20.859: INFO: Created: latency-svc-7px9p
Mar  6 04:56:20.877: INFO: Created: latency-svc-h8hvw
Mar  6 04:56:20.883: INFO: Got endpoints: latency-svc-g8hrk [325.319064ms]
Mar  6 04:56:20.901: INFO: Created: latency-svc-ljlnt
Mar  6 04:56:20.918: INFO: Created: latency-svc-v7qgh
Mar  6 04:56:20.934: INFO: Got endpoints: latency-svc-tdvgz [367.333424ms]
Mar  6 04:56:20.936: INFO: Created: latency-svc-bxdnk
Mar  6 04:56:20.955: INFO: Created: latency-svc-9qk67
Mar  6 04:56:20.981: INFO: Created: latency-svc-g67gz
Mar  6 04:56:20.983: INFO: Got endpoints: latency-svc-dzrr2 [404.938493ms]
Mar  6 04:56:21.009: INFO: Created: latency-svc-fghkb
Mar  6 04:56:21.038: INFO: Got endpoints: latency-svc-lh7n2 [449.273131ms]
Mar  6 04:56:21.073: INFO: Created: latency-svc-kk88x
Mar  6 04:56:21.088: INFO: Got endpoints: latency-svc-w8zf5 [491.746308ms]
Mar  6 04:56:21.119: INFO: Created: latency-svc-l96kq
Mar  6 04:56:21.141: INFO: Got endpoints: latency-svc-4nndc [532.193739ms]
Mar  6 04:56:21.171: INFO: Created: latency-svc-5bgct
Mar  6 04:56:21.184: INFO: Got endpoints: latency-svc-j97j6 [567.742311ms]
Mar  6 04:56:21.210: INFO: Created: latency-svc-lkt9t
Mar  6 04:56:21.237: INFO: Got endpoints: latency-svc-fkk6s [607.260899ms]
Mar  6 04:56:21.255: INFO: Created: latency-svc-4nf2l
Mar  6 04:56:21.284: INFO: Got endpoints: latency-svc-h2wcx [645.075308ms]
Mar  6 04:56:21.311: INFO: Created: latency-svc-np6lc
Mar  6 04:56:21.336: INFO: Got endpoints: latency-svc-br6fv [687.807308ms]
Mar  6 04:56:21.369: INFO: Created: latency-svc-xp4mz
Mar  6 04:56:21.384: INFO: Got endpoints: latency-svc-7px9p [725.218512ms]
Mar  6 04:56:21.415: INFO: Created: latency-svc-lppqk
Mar  6 04:56:21.435: INFO: Got endpoints: latency-svc-h8hvw [751.55195ms]
Mar  6 04:56:21.463: INFO: Created: latency-svc-p78kt
Mar  6 04:56:21.485: INFO: Got endpoints: latency-svc-ljlnt [748.346282ms]
Mar  6 04:56:21.515: INFO: Created: latency-svc-ftqlv
Mar  6 04:56:21.535: INFO: Got endpoints: latency-svc-v7qgh [752.287527ms]
Mar  6 04:56:21.566: INFO: Created: latency-svc-qn27c
Mar  6 04:56:21.585: INFO: Got endpoints: latency-svc-bxdnk [752.776422ms]
Mar  6 04:56:21.614: INFO: Created: latency-svc-z4j8k
Mar  6 04:56:21.638: INFO: Got endpoints: latency-svc-9qk67 [755.314218ms]
Mar  6 04:56:21.671: INFO: Created: latency-svc-wqpgx
Mar  6 04:56:21.686: INFO: Got endpoints: latency-svc-g67gz [752.007445ms]
Mar  6 04:56:21.719: INFO: Created: latency-svc-p8qr2
Mar  6 04:56:21.738: INFO: Got endpoints: latency-svc-fghkb [754.864142ms]
Mar  6 04:56:21.775: INFO: Created: latency-svc-7qcgz
Mar  6 04:56:21.786: INFO: Got endpoints: latency-svc-kk88x [747.820441ms]
Mar  6 04:56:21.817: INFO: Created: latency-svc-px6fd
Mar  6 04:56:21.839: INFO: Got endpoints: latency-svc-l96kq [751.577343ms]
Mar  6 04:56:21.869: INFO: Created: latency-svc-pqc59
Mar  6 04:56:21.886: INFO: Got endpoints: latency-svc-5bgct [745.530194ms]
Mar  6 04:56:21.907: INFO: Created: latency-svc-8rftx
Mar  6 04:56:21.938: INFO: Got endpoints: latency-svc-lkt9t [753.78175ms]
Mar  6 04:56:21.961: INFO: Created: latency-svc-4hf2m
Mar  6 04:56:21.985: INFO: Got endpoints: latency-svc-4nf2l [748.19726ms]
Mar  6 04:56:22.013: INFO: Created: latency-svc-kdg99
Mar  6 04:56:22.041: INFO: Got endpoints: latency-svc-np6lc [757.302236ms]
Mar  6 04:56:22.067: INFO: Created: latency-svc-xsjgl
Mar  6 04:56:22.088: INFO: Got endpoints: latency-svc-xp4mz [751.90503ms]
Mar  6 04:56:22.136: INFO: Created: latency-svc-r4tpj
Mar  6 04:56:22.137: INFO: Got endpoints: latency-svc-lppqk [752.700571ms]
Mar  6 04:56:22.178: INFO: Created: latency-svc-xsfq2
Mar  6 04:56:22.185: INFO: Got endpoints: latency-svc-p78kt [750.233552ms]
Mar  6 04:56:22.206: INFO: Created: latency-svc-jst8l
Mar  6 04:56:22.239: INFO: Got endpoints: latency-svc-ftqlv [754.23556ms]
Mar  6 04:56:22.262: INFO: Created: latency-svc-pf547
Mar  6 04:56:22.290: INFO: Got endpoints: latency-svc-qn27c [754.048277ms]
Mar  6 04:56:22.315: INFO: Created: latency-svc-f6wjg
Mar  6 04:56:22.336: INFO: Got endpoints: latency-svc-z4j8k [750.860112ms]
Mar  6 04:56:22.363: INFO: Created: latency-svc-g8k96
Mar  6 04:56:22.389: INFO: Got endpoints: latency-svc-wqpgx [750.348716ms]
Mar  6 04:56:22.423: INFO: Created: latency-svc-754xm
Mar  6 04:56:22.440: INFO: Got endpoints: latency-svc-p8qr2 [754.31852ms]
Mar  6 04:56:22.465: INFO: Created: latency-svc-whxsm
Mar  6 04:56:22.487: INFO: Got endpoints: latency-svc-7qcgz [749.637552ms]
Mar  6 04:56:22.509: INFO: Created: latency-svc-6s5bm
Mar  6 04:56:22.546: INFO: Got endpoints: latency-svc-px6fd [759.85219ms]
Mar  6 04:56:22.571: INFO: Created: latency-svc-k8zhj
Mar  6 04:56:22.588: INFO: Got endpoints: latency-svc-pqc59 [748.890279ms]
Mar  6 04:56:22.603: INFO: Created: latency-svc-qxmc6
Mar  6 04:56:22.635: INFO: Got endpoints: latency-svc-8rftx [748.527746ms]
Mar  6 04:56:22.655: INFO: Created: latency-svc-88xsb
Mar  6 04:56:22.684: INFO: Got endpoints: latency-svc-4hf2m [745.872988ms]
Mar  6 04:56:22.700: INFO: Created: latency-svc-wpsdj
Mar  6 04:56:22.737: INFO: Got endpoints: latency-svc-kdg99 [752.023801ms]
Mar  6 04:56:22.755: INFO: Created: latency-svc-9frnk
Mar  6 04:56:22.783: INFO: Got endpoints: latency-svc-xsjgl [741.918206ms]
Mar  6 04:56:22.799: INFO: Created: latency-svc-x64zn
Mar  6 04:56:22.836: INFO: Got endpoints: latency-svc-r4tpj [748.11581ms]
Mar  6 04:56:22.850: INFO: Created: latency-svc-llj57
Mar  6 04:56:22.883: INFO: Got endpoints: latency-svc-xsfq2 [746.556466ms]
Mar  6 04:56:22.898: INFO: Created: latency-svc-k8wlm
Mar  6 04:56:22.936: INFO: Got endpoints: latency-svc-jst8l [750.440709ms]
Mar  6 04:56:22.949: INFO: Created: latency-svc-7qwx5
Mar  6 04:56:22.986: INFO: Got endpoints: latency-svc-pf547 [746.776135ms]
Mar  6 04:56:23.001: INFO: Created: latency-svc-m5s9s
Mar  6 04:56:23.035: INFO: Got endpoints: latency-svc-f6wjg [745.372232ms]
Mar  6 04:56:23.056: INFO: Created: latency-svc-vkqck
Mar  6 04:56:23.085: INFO: Got endpoints: latency-svc-g8k96 [749.093512ms]
Mar  6 04:56:23.099: INFO: Created: latency-svc-rmbm7
Mar  6 04:56:23.132: INFO: Got endpoints: latency-svc-754xm [743.34055ms]
Mar  6 04:56:23.150: INFO: Created: latency-svc-9b2sn
Mar  6 04:56:23.183: INFO: Got endpoints: latency-svc-whxsm [742.858944ms]
Mar  6 04:56:23.198: INFO: Created: latency-svc-682m2
Mar  6 04:56:23.232: INFO: Got endpoints: latency-svc-6s5bm [744.682279ms]
Mar  6 04:56:23.247: INFO: Created: latency-svc-hb5bv
Mar  6 04:56:23.286: INFO: Got endpoints: latency-svc-k8zhj [740.713193ms]
Mar  6 04:56:23.314: INFO: Created: latency-svc-8gd7x
Mar  6 04:56:23.332: INFO: Got endpoints: latency-svc-qxmc6 [743.962849ms]
Mar  6 04:56:23.348: INFO: Created: latency-svc-dtqvs
Mar  6 04:56:23.383: INFO: Got endpoints: latency-svc-88xsb [748.682301ms]
Mar  6 04:56:23.402: INFO: Created: latency-svc-25dwz
Mar  6 04:56:23.433: INFO: Got endpoints: latency-svc-wpsdj [749.214031ms]
Mar  6 04:56:23.448: INFO: Created: latency-svc-4x48z
Mar  6 04:56:23.484: INFO: Got endpoints: latency-svc-9frnk [747.192568ms]
Mar  6 04:56:23.498: INFO: Created: latency-svc-6mwtk
Mar  6 04:56:23.535: INFO: Got endpoints: latency-svc-x64zn [751.525221ms]
Mar  6 04:56:23.547: INFO: Created: latency-svc-f9n9h
Mar  6 04:56:23.587: INFO: Got endpoints: latency-svc-llj57 [750.22562ms]
Mar  6 04:56:23.601: INFO: Created: latency-svc-cv6pz
Mar  6 04:56:23.632: INFO: Got endpoints: latency-svc-k8wlm [749.041198ms]
Mar  6 04:56:23.646: INFO: Created: latency-svc-j2d74
Mar  6 04:56:23.686: INFO: Got endpoints: latency-svc-7qwx5 [750.306286ms]
Mar  6 04:56:23.703: INFO: Created: latency-svc-7j8wm
Mar  6 04:56:23.733: INFO: Got endpoints: latency-svc-m5s9s [746.993832ms]
Mar  6 04:56:23.748: INFO: Created: latency-svc-755z6
Mar  6 04:56:23.785: INFO: Got endpoints: latency-svc-vkqck [749.995275ms]
Mar  6 04:56:23.802: INFO: Created: latency-svc-8s5bn
Mar  6 04:56:23.834: INFO: Got endpoints: latency-svc-rmbm7 [748.133361ms]
Mar  6 04:56:23.850: INFO: Created: latency-svc-rzzg5
Mar  6 04:56:23.886: INFO: Got endpoints: latency-svc-9b2sn [753.316387ms]
Mar  6 04:56:23.901: INFO: Created: latency-svc-fhm84
Mar  6 04:56:23.934: INFO: Got endpoints: latency-svc-682m2 [751.053823ms]
Mar  6 04:56:23.950: INFO: Created: latency-svc-94cn8
Mar  6 04:56:23.984: INFO: Got endpoints: latency-svc-hb5bv [751.827303ms]
Mar  6 04:56:23.998: INFO: Created: latency-svc-c84q2
Mar  6 04:56:24.035: INFO: Got endpoints: latency-svc-8gd7x [748.352445ms]
Mar  6 04:56:24.051: INFO: Created: latency-svc-xv7l5
Mar  6 04:56:24.086: INFO: Got endpoints: latency-svc-dtqvs [753.919612ms]
Mar  6 04:56:24.103: INFO: Created: latency-svc-hl7m8
Mar  6 04:56:24.134: INFO: Got endpoints: latency-svc-25dwz [750.938329ms]
Mar  6 04:56:24.150: INFO: Created: latency-svc-6d4zx
Mar  6 04:56:24.184: INFO: Got endpoints: latency-svc-4x48z [750.413124ms]
Mar  6 04:56:24.201: INFO: Created: latency-svc-d6wxs
Mar  6 04:56:24.234: INFO: Got endpoints: latency-svc-6mwtk [749.144066ms]
Mar  6 04:56:24.251: INFO: Created: latency-svc-6bcw4
Mar  6 04:56:24.288: INFO: Got endpoints: latency-svc-f9n9h [753.720181ms]
Mar  6 04:56:24.305: INFO: Created: latency-svc-b99rc
Mar  6 04:56:24.336: INFO: Got endpoints: latency-svc-cv6pz [749.336312ms]
Mar  6 04:56:24.353: INFO: Created: latency-svc-hxkfs
Mar  6 04:56:24.386: INFO: Got endpoints: latency-svc-j2d74 [753.989974ms]
Mar  6 04:56:24.400: INFO: Created: latency-svc-5c5gc
Mar  6 04:56:24.434: INFO: Got endpoints: latency-svc-7j8wm [747.708326ms]
Mar  6 04:56:24.448: INFO: Created: latency-svc-lhbzd
Mar  6 04:56:24.486: INFO: Got endpoints: latency-svc-755z6 [752.893044ms]
Mar  6 04:56:24.507: INFO: Created: latency-svc-95mtv
Mar  6 04:56:24.536: INFO: Got endpoints: latency-svc-8s5bn [750.828889ms]
Mar  6 04:56:24.555: INFO: Created: latency-svc-c6wvh
Mar  6 04:56:24.586: INFO: Got endpoints: latency-svc-rzzg5 [751.895908ms]
Mar  6 04:56:24.598: INFO: Created: latency-svc-bshzr
Mar  6 04:56:24.634: INFO: Got endpoints: latency-svc-fhm84 [748.885918ms]
Mar  6 04:56:24.650: INFO: Created: latency-svc-6cbk8
Mar  6 04:56:24.686: INFO: Got endpoints: latency-svc-94cn8 [752.105683ms]
Mar  6 04:56:24.701: INFO: Created: latency-svc-gfbnj
Mar  6 04:56:24.734: INFO: Got endpoints: latency-svc-c84q2 [750.5599ms]
Mar  6 04:56:24.747: INFO: Created: latency-svc-whqkg
Mar  6 04:56:24.787: INFO: Got endpoints: latency-svc-xv7l5 [752.246317ms]
Mar  6 04:56:24.801: INFO: Created: latency-svc-zp7kq
Mar  6 04:56:24.834: INFO: Got endpoints: latency-svc-hl7m8 [747.207326ms]
Mar  6 04:56:24.849: INFO: Created: latency-svc-sqkfw
Mar  6 04:56:24.884: INFO: Got endpoints: latency-svc-6d4zx [749.054249ms]
Mar  6 04:56:24.902: INFO: Created: latency-svc-wdqr7
Mar  6 04:56:24.935: INFO: Got endpoints: latency-svc-d6wxs [750.869668ms]
Mar  6 04:56:24.948: INFO: Created: latency-svc-qrvj5
Mar  6 04:56:24.985: INFO: Got endpoints: latency-svc-6bcw4 [751.386138ms]
Mar  6 04:56:24.999: INFO: Created: latency-svc-xb4xp
Mar  6 04:56:25.035: INFO: Got endpoints: latency-svc-b99rc [746.789807ms]
Mar  6 04:56:25.047: INFO: Created: latency-svc-bdrzf
Mar  6 04:56:25.084: INFO: Got endpoints: latency-svc-hxkfs [747.421755ms]
Mar  6 04:56:25.100: INFO: Created: latency-svc-7r8rt
Mar  6 04:56:25.132: INFO: Got endpoints: latency-svc-5c5gc [745.894955ms]
Mar  6 04:56:25.147: INFO: Created: latency-svc-s6skn
Mar  6 04:56:25.186: INFO: Got endpoints: latency-svc-lhbzd [751.67743ms]
Mar  6 04:56:25.203: INFO: Created: latency-svc-8v5bt
Mar  6 04:56:25.235: INFO: Got endpoints: latency-svc-95mtv [748.92244ms]
Mar  6 04:56:25.250: INFO: Created: latency-svc-hgn77
Mar  6 04:56:25.284: INFO: Got endpoints: latency-svc-c6wvh [747.986379ms]
Mar  6 04:56:25.303: INFO: Created: latency-svc-x672c
Mar  6 04:56:25.336: INFO: Got endpoints: latency-svc-bshzr [750.178564ms]
Mar  6 04:56:25.357: INFO: Created: latency-svc-78q58
Mar  6 04:56:25.386: INFO: Got endpoints: latency-svc-6cbk8 [751.906734ms]
Mar  6 04:56:25.405: INFO: Created: latency-svc-v44tx
Mar  6 04:56:25.436: INFO: Got endpoints: latency-svc-gfbnj [749.965691ms]
Mar  6 04:56:25.453: INFO: Created: latency-svc-s5lgj
Mar  6 04:56:25.484: INFO: Got endpoints: latency-svc-whqkg [749.942386ms]
Mar  6 04:56:25.500: INFO: Created: latency-svc-k5gv6
Mar  6 04:56:25.535: INFO: Got endpoints: latency-svc-zp7kq [747.816349ms]
Mar  6 04:56:25.560: INFO: Created: latency-svc-lfcmh
Mar  6 04:56:25.584: INFO: Got endpoints: latency-svc-sqkfw [750.001245ms]
Mar  6 04:56:25.596: INFO: Created: latency-svc-gg8bj
Mar  6 04:56:25.635: INFO: Got endpoints: latency-svc-wdqr7 [751.848865ms]
Mar  6 04:56:25.649: INFO: Created: latency-svc-7zfjc
Mar  6 04:56:25.684: INFO: Got endpoints: latency-svc-qrvj5 [749.546516ms]
Mar  6 04:56:25.706: INFO: Created: latency-svc-c8j4z
Mar  6 04:56:25.735: INFO: Got endpoints: latency-svc-xb4xp [749.947738ms]
Mar  6 04:56:25.750: INFO: Created: latency-svc-dksdl
Mar  6 04:56:25.785: INFO: Got endpoints: latency-svc-bdrzf [749.458871ms]
Mar  6 04:56:25.798: INFO: Created: latency-svc-4wf77
Mar  6 04:56:25.834: INFO: Got endpoints: latency-svc-7r8rt [749.943877ms]
Mar  6 04:56:25.852: INFO: Created: latency-svc-95z7m
Mar  6 04:56:25.885: INFO: Got endpoints: latency-svc-s6skn [752.283845ms]
Mar  6 04:56:25.903: INFO: Created: latency-svc-pbsvd
Mar  6 04:56:25.937: INFO: Got endpoints: latency-svc-8v5bt [751.490574ms]
Mar  6 04:56:25.952: INFO: Created: latency-svc-bzwmf
Mar  6 04:56:25.984: INFO: Got endpoints: latency-svc-hgn77 [748.874291ms]
Mar  6 04:56:25.997: INFO: Created: latency-svc-9rg9x
Mar  6 04:56:26.036: INFO: Got endpoints: latency-svc-x672c [752.131015ms]
Mar  6 04:56:26.086: INFO: Got endpoints: latency-svc-78q58 [749.606794ms]
Mar  6 04:56:26.086: INFO: Created: latency-svc-m8gqv
Mar  6 04:56:26.103: INFO: Created: latency-svc-hdpld
Mar  6 04:56:26.134: INFO: Got endpoints: latency-svc-v44tx [747.361534ms]
Mar  6 04:56:26.150: INFO: Created: latency-svc-s8t55
Mar  6 04:56:26.185: INFO: Got endpoints: latency-svc-s5lgj [748.950328ms]
Mar  6 04:56:26.200: INFO: Created: latency-svc-pbcpt
Mar  6 04:56:26.234: INFO: Got endpoints: latency-svc-k5gv6 [749.727616ms]
Mar  6 04:56:26.249: INFO: Created: latency-svc-lvqms
Mar  6 04:56:26.285: INFO: Got endpoints: latency-svc-lfcmh [750.432199ms]
Mar  6 04:56:26.303: INFO: Created: latency-svc-8ckhg
Mar  6 04:56:26.334: INFO: Got endpoints: latency-svc-gg8bj [749.981239ms]
Mar  6 04:56:26.346: INFO: Created: latency-svc-c6hwq
Mar  6 04:56:26.385: INFO: Got endpoints: latency-svc-7zfjc [749.604503ms]
Mar  6 04:56:26.398: INFO: Created: latency-svc-vdjxx
Mar  6 04:56:26.434: INFO: Got endpoints: latency-svc-c8j4z [750.135572ms]
Mar  6 04:56:26.447: INFO: Created: latency-svc-qwxg2
Mar  6 04:56:26.486: INFO: Got endpoints: latency-svc-dksdl [750.60752ms]
Mar  6 04:56:26.501: INFO: Created: latency-svc-98zcl
Mar  6 04:56:26.532: INFO: Got endpoints: latency-svc-4wf77 [747.043842ms]
Mar  6 04:56:26.547: INFO: Created: latency-svc-d4ftt
Mar  6 04:56:26.583: INFO: Got endpoints: latency-svc-95z7m [749.800661ms]
Mar  6 04:56:26.598: INFO: Created: latency-svc-x6dt4
Mar  6 04:56:26.635: INFO: Got endpoints: latency-svc-pbsvd [750.056707ms]
Mar  6 04:56:26.649: INFO: Created: latency-svc-bpwsl
Mar  6 04:56:26.685: INFO: Got endpoints: latency-svc-bzwmf [748.084269ms]
Mar  6 04:56:26.698: INFO: Created: latency-svc-m8b7q
Mar  6 04:56:26.735: INFO: Got endpoints: latency-svc-9rg9x [751.401797ms]
Mar  6 04:56:26.748: INFO: Created: latency-svc-ssxhc
Mar  6 04:56:26.787: INFO: Got endpoints: latency-svc-m8gqv [751.166532ms]
Mar  6 04:56:26.805: INFO: Created: latency-svc-f845h
Mar  6 04:56:26.834: INFO: Got endpoints: latency-svc-hdpld [748.178595ms]
Mar  6 04:56:26.847: INFO: Created: latency-svc-gc6wt
Mar  6 04:56:26.883: INFO: Got endpoints: latency-svc-s8t55 [749.371359ms]
Mar  6 04:56:26.900: INFO: Created: latency-svc-8fpcj
Mar  6 04:56:26.934: INFO: Got endpoints: latency-svc-pbcpt [748.213381ms]
Mar  6 04:56:26.958: INFO: Created: latency-svc-p4c4q
Mar  6 04:56:26.985: INFO: Got endpoints: latency-svc-lvqms [750.801565ms]
Mar  6 04:56:26.998: INFO: Created: latency-svc-95z5h
Mar  6 04:56:27.033: INFO: Got endpoints: latency-svc-8ckhg [747.653922ms]
Mar  6 04:56:27.050: INFO: Created: latency-svc-7pvkp
Mar  6 04:56:27.084: INFO: Got endpoints: latency-svc-c6hwq [750.740509ms]
Mar  6 04:56:27.097: INFO: Created: latency-svc-9zsrd
Mar  6 04:56:27.134: INFO: Got endpoints: latency-svc-vdjxx [748.752957ms]
Mar  6 04:56:27.151: INFO: Created: latency-svc-2lbk9
Mar  6 04:56:27.186: INFO: Got endpoints: latency-svc-qwxg2 [751.756533ms]
Mar  6 04:56:27.200: INFO: Created: latency-svc-jwx8h
Mar  6 04:56:27.234: INFO: Got endpoints: latency-svc-98zcl [748.138706ms]
Mar  6 04:56:27.250: INFO: Created: latency-svc-whjd8
Mar  6 04:56:27.287: INFO: Got endpoints: latency-svc-d4ftt [755.690602ms]
Mar  6 04:56:27.301: INFO: Created: latency-svc-bft4l
Mar  6 04:56:27.336: INFO: Got endpoints: latency-svc-x6dt4 [752.997626ms]
Mar  6 04:56:27.355: INFO: Created: latency-svc-w82ww
Mar  6 04:56:27.385: INFO: Got endpoints: latency-svc-bpwsl [749.951659ms]
Mar  6 04:56:27.401: INFO: Created: latency-svc-f52zm
Mar  6 04:56:27.433: INFO: Got endpoints: latency-svc-m8b7q [747.740583ms]
Mar  6 04:56:27.458: INFO: Created: latency-svc-5tt6c
Mar  6 04:56:27.482: INFO: Got endpoints: latency-svc-ssxhc [747.310682ms]
Mar  6 04:56:27.499: INFO: Created: latency-svc-qdmsw
Mar  6 04:56:27.536: INFO: Got endpoints: latency-svc-f845h [748.099483ms]
Mar  6 04:56:27.549: INFO: Created: latency-svc-trv7g
Mar  6 04:56:27.585: INFO: Got endpoints: latency-svc-gc6wt [751.356089ms]
Mar  6 04:56:27.603: INFO: Created: latency-svc-m6j9n
Mar  6 04:56:27.635: INFO: Got endpoints: latency-svc-8fpcj [751.556146ms]
Mar  6 04:56:27.649: INFO: Created: latency-svc-kdtrk
Mar  6 04:56:27.683: INFO: Got endpoints: latency-svc-p4c4q [748.732495ms]
Mar  6 04:56:27.696: INFO: Created: latency-svc-tjdn4
Mar  6 04:56:27.737: INFO: Got endpoints: latency-svc-95z5h [751.690007ms]
Mar  6 04:56:27.753: INFO: Created: latency-svc-b4rqp
Mar  6 04:56:27.785: INFO: Got endpoints: latency-svc-7pvkp [751.572142ms]
Mar  6 04:56:27.799: INFO: Created: latency-svc-mpksb
Mar  6 04:56:27.835: INFO: Got endpoints: latency-svc-9zsrd [751.058071ms]
Mar  6 04:56:27.853: INFO: Created: latency-svc-lf82p
Mar  6 04:56:27.886: INFO: Got endpoints: latency-svc-2lbk9 [752.393322ms]
Mar  6 04:56:27.899: INFO: Created: latency-svc-2cbmn
Mar  6 04:56:27.934: INFO: Got endpoints: latency-svc-jwx8h [747.334739ms]
Mar  6 04:56:27.947: INFO: Created: latency-svc-vkps2
Mar  6 04:56:27.983: INFO: Got endpoints: latency-svc-whjd8 [748.768096ms]
Mar  6 04:56:27.997: INFO: Created: latency-svc-w2l45
Mar  6 04:56:28.036: INFO: Got endpoints: latency-svc-bft4l [748.041798ms]
Mar  6 04:56:28.083: INFO: Got endpoints: latency-svc-w82ww [746.797008ms]
Mar  6 04:56:28.135: INFO: Got endpoints: latency-svc-f52zm [750.052154ms]
Mar  6 04:56:28.184: INFO: Got endpoints: latency-svc-5tt6c [750.628102ms]
Mar  6 04:56:28.234: INFO: Got endpoints: latency-svc-qdmsw [751.81261ms]
Mar  6 04:56:28.287: INFO: Got endpoints: latency-svc-trv7g [751.428107ms]
Mar  6 04:56:28.333: INFO: Got endpoints: latency-svc-m6j9n [748.135574ms]
Mar  6 04:56:28.382: INFO: Got endpoints: latency-svc-kdtrk [747.123141ms]
Mar  6 04:56:28.435: INFO: Got endpoints: latency-svc-tjdn4 [752.086845ms]
Mar  6 04:56:28.485: INFO: Got endpoints: latency-svc-b4rqp [747.847004ms]
Mar  6 04:56:28.534: INFO: Got endpoints: latency-svc-mpksb [749.491523ms]
Mar  6 04:56:28.586: INFO: Got endpoints: latency-svc-lf82p [750.637563ms]
Mar  6 04:56:28.638: INFO: Got endpoints: latency-svc-2cbmn [751.855723ms]
Mar  6 04:56:28.687: INFO: Got endpoints: latency-svc-vkps2 [753.175186ms]
Mar  6 04:56:28.733: INFO: Got endpoints: latency-svc-w2l45 [749.775103ms]
Mar  6 04:56:28.733: INFO: Latencies: [23.256707ms 32.871001ms 44.62883ms 55.344342ms 66.04314ms 80.190043ms 87.745205ms 101.786223ms 116.757062ms 128.069148ms 139.664082ms 150.617613ms 164.547881ms 164.61134ms 165.974311ms 166.097632ms 167.907652ms 178.728364ms 179.855412ms 180.091735ms 180.81421ms 182.904334ms 184.238912ms 189.071658ms 190.774483ms 191.693981ms 194.438807ms 194.661508ms 197.891829ms 198.068639ms 199.606454ms 203.225563ms 203.502421ms 204.169704ms 205.882691ms 206.924604ms 207.761333ms 208.290509ms 209.46536ms 209.823593ms 246.255428ms 285.213068ms 325.319064ms 367.333424ms 404.938493ms 449.273131ms 491.746308ms 532.193739ms 567.742311ms 607.260899ms 645.075308ms 687.807308ms 725.218512ms 740.713193ms 741.918206ms 742.858944ms 743.34055ms 743.962849ms 744.682279ms 745.372232ms 745.530194ms 745.872988ms 745.894955ms 746.556466ms 746.776135ms 746.789807ms 746.797008ms 746.993832ms 747.043842ms 747.123141ms 747.192568ms 747.207326ms 747.310682ms 747.334739ms 747.361534ms 747.421755ms 747.653922ms 747.708326ms 747.740583ms 747.816349ms 747.820441ms 747.847004ms 747.986379ms 748.041798ms 748.084269ms 748.099483ms 748.11581ms 748.133361ms 748.135574ms 748.138706ms 748.178595ms 748.19726ms 748.213381ms 748.346282ms 748.352445ms 748.527746ms 748.682301ms 748.732495ms 748.752957ms 748.768096ms 748.874291ms 748.885918ms 748.890279ms 748.92244ms 748.950328ms 749.041198ms 749.054249ms 749.093512ms 749.144066ms 749.214031ms 749.336312ms 749.371359ms 749.458871ms 749.491523ms 749.546516ms 749.604503ms 749.606794ms 749.637552ms 749.727616ms 749.775103ms 749.800661ms 749.942386ms 749.943877ms 749.947738ms 749.951659ms 749.965691ms 749.981239ms 749.995275ms 750.001245ms 750.052154ms 750.056707ms 750.135572ms 750.178564ms 750.22562ms 750.233552ms 750.306286ms 750.348716ms 750.413124ms 750.432199ms 750.440709ms 750.5599ms 750.60752ms 750.628102ms 750.637563ms 750.740509ms 750.801565ms 750.828889ms 750.860112ms 750.869668ms 750.938329ms 751.053823ms 751.058071ms 751.166532ms 751.356089ms 751.386138ms 751.401797ms 751.428107ms 751.490574ms 751.525221ms 751.55195ms 751.556146ms 751.572142ms 751.577343ms 751.67743ms 751.690007ms 751.756533ms 751.81261ms 751.827303ms 751.848865ms 751.855723ms 751.895908ms 751.90503ms 751.906734ms 752.007445ms 752.023801ms 752.086845ms 752.105683ms 752.131015ms 752.246317ms 752.283845ms 752.287527ms 752.393322ms 752.700571ms 752.776422ms 752.893044ms 752.997626ms 753.175186ms 753.316387ms 753.720181ms 753.78175ms 753.919612ms 753.989974ms 754.048277ms 754.23556ms 754.31852ms 754.864142ms 755.314218ms 755.690602ms 757.302236ms 759.85219ms]
Mar  6 04:56:28.733: INFO: 50 %ile: 748.874291ms
Mar  6 04:56:28.733: INFO: 90 %ile: 752.287527ms
Mar  6 04:56:28.733: INFO: 99 %ile: 757.302236ms
Mar  6 04:56:28.733: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:56:28.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-tljtx" for this suite.
Mar  6 04:56:48.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:56:48.840: INFO: namespace: e2e-tests-svc-latency-tljtx, resource: bindings, ignored listing per whitelist
Mar  6 04:56:48.890: INFO: namespace e2e-tests-svc-latency-tljtx deletion completed in 20.145489428s

• [SLOW TEST:29.971 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:56:48.890: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 04:56:48.966: INFO: Creating deployment "nginx-deployment"
Mar  6 04:56:48.975: INFO: Waiting for observed generation 1
Mar  6 04:56:50.981: INFO: Waiting for all required pods to come up
Mar  6 04:56:50.985: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  6 04:56:50.985: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  6 04:56:50.990: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  6 04:56:50.997: INFO: Updating deployment nginx-deployment
Mar  6 04:56:50.997: INFO: Waiting for observed generation 2
Mar  6 04:56:53.002: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  6 04:56:53.005: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  6 04:56:53.007: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 04:56:53.015: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  6 04:56:53.015: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  6 04:56:53.018: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 04:56:53.023: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  6 04:56:53.023: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  6 04:56:53.030: INFO: Updating deployment nginx-deployment
Mar  6 04:56:53.030: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  6 04:56:53.037: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  6 04:56:53.040: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 04:56:53.057: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-57vg4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-57vg4/deployments/nginx-deployment,UID:3fe9b9a3-3fcc-11e9-91c9-005056979acf,ResourceVersion:23705,Generation:3,CreationTimestamp:2019-03-06 04:56:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-03-06 04:56:51 +0000 UTC 2019-03-06 04:56:48 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-03-06 04:56:53 +0000 UTC 2019-03-06 04:56:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  6 04:56:53.070: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-57vg4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-57vg4/replicasets/nginx-deployment-65bbdb5f8,UID:411f9d06-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23700,Generation:3,CreationTimestamp:2019-03-06 04:56:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3fe9b9a3-3fcc-11e9-91c9-005056979acf 0xc001d6c187 0xc001d6c188}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 04:56:53.070: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  6 04:56:53.070: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-57vg4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-57vg4/replicasets/nginx-deployment-555b55d965,UID:3febf5f6-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23698,Generation:3,CreationTimestamp:2019-03-06 04:56:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3fe9b9a3-3fcc-11e9-91c9-005056979acf 0xc001d6c0c7 0xc001d6c0c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  6 04:56:53.083: INFO: Pod "nginx-deployment-555b55d965-4fngp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4fngp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-4fngp,UID:42567de1-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23704,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6d197 0xc001d6d198}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6d200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d6d220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.083: INFO: Pod "nginx-deployment-555b55d965-4jqxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4jqxz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-4jqxz,UID:4258c989-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23720,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6d290 0xc001d6d291}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6d390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d6d3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.083: INFO: Pod "nginx-deployment-555b55d965-4x5wf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4x5wf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-4x5wf,UID:3ff11122-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23602,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.20"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6d420 0xc001d6d421}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6d480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d6d4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:192.168.160.20,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ed526070037cc74c3a920ad610db16287f405fcd682d922ec5bff407ad770d72}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.083: INFO: Pod "nginx-deployment-555b55d965-6j4mv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6j4mv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-6j4mv,UID:3ff10d22-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23592,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.68.154"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6d590 0xc001d6d591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-05.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6d5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d6d610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.21,PodIP:192.168.68.154,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://45dcb30e7ad2095621d8e51351380b0eb781a3aae25e7d06fd0b7aa6dde2c725}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-6svph" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6svph,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-6svph,UID:4258f310-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23721,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6d6d0 0xc001d6d6d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6d8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d6dab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-977cp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-977cp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-977cp,UID:3ff34e7e-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23589,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.68.155"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d6db20 0xc001d6db21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-05.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d6db90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d54000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.21,PodIP:192.168.68.155,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c4995a81792de47d80dd41a9d9f19931e37b7824a54d4975b79fc0232a53cb4a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-d6dwt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-d6dwt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-d6dwt,UID:4258e101-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23722,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d54190 0xc001d54191}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-05.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d541f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d54210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-gb2hl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gb2hl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-gb2hl,UID:3ff36680-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23599,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.24"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d542c0 0xc001d542c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d54320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d54340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:192.168.160.24,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://25a9e4c057aae253b254277d0069401c885d009489acd704bbff89059bd6f573}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-jwh4w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jwh4w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-jwh4w,UID:4257879e-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23711,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d54400 0xc001d54401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-05.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d54470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d54650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.084: INFO: Pod "nginx-deployment-555b55d965-jwwn8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jwwn8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-jwwn8,UID:3ff11fa7-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23605,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.232.78"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d546c0 0xc001d546c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d549c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d549e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.25,PodIP:192.168.232.78,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f54ff95e60344f312acef999e560522219d678ef8ff5332297ed3a30317afb60}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-555b55d965-r65jn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-r65jn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-r65jn,UID:3fee5411-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23596,Generation:0,CreationTimestamp:2019-03-06 04:56:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.19"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d54aa0 0xc001d54aa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d54b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d54b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:192.168.160.19,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://b9431dbf62c3e1637a224478bd0c1342238c0001bce0d10628d56a1f19d8d348}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-555b55d965-sldtn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sldtn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-sldtn,UID:3fef8342-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23608,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.232.77"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d554d0 0xc001d554d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d55530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d555c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.25,PodIP:192.168.232.77,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f074b7caf641145299132b3593901c71a2f62353e740c609be997c7eb5cda90c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-555b55d965-v2dqn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v2dqn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-v2dqn,UID:3ff11701-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23579,Generation:0,CreationTimestamp:2019-03-06 04:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.40.166"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d55680 0xc001d55681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d55b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d55b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:49 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.20,PodIP:192.168.40.166,StartTime:2019-03-06 04:56:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-06 04:56:50 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://41ccdf4b99f6366223107dc8e96c907d720bc3d799b0b20bac79af8ce91ae537}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-555b55d965-vmjzr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vmjzr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-vmjzr,UID:425768f2-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23710,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d55c40 0xc001d55c41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d55ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d55cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-555b55d965-w6bkw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w6bkw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-555b55d965-w6bkw,UID:4258da8d-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23717,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3febf5f6-3fcc-11e9-a5d3-00505697ee14 0xc001d55d90 0xc001d55d91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d55e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d55f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-65bbdb5f8-42kz6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-42kz6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-42kz6,UID:41203eba-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23682,Generation:0,CreationTimestamp:2019-03-06 04:56:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.25"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248007 0xc002248008}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002248070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:,StartTime:2019-03-06 04:56:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-65bbdb5f8-5kmbk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-5kmbk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-5kmbk,UID:425a52c3-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23718,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248150 0xc002248151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022481c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.085: INFO: Pod "nginx-deployment-65bbdb5f8-5mfq5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-5mfq5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-5mfq5,UID:41274ac0-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23679,Generation:0,CreationTimestamp:2019-03-06 04:56:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.232.79"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248267 0xc002248268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-07.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002248330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.25,PodIP:,StartTime:2019-03-06 04:56:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.086: INFO: Pod "nginx-deployment-65bbdb5f8-bggmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bggmq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-bggmq,UID:41213b4e-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23680,Generation:0,CreationTimestamp:2019-03-06 04:56:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.68.157"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248470 0xc002248471}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-05.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002248560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.21,PodIP:,StartTime:2019-03-06 04:56:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.086: INFO: Pod "nginx-deployment-65bbdb5f8-drgtr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-drgtr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-drgtr,UID:41215272-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23681,Generation:0,CreationTimestamp:2019-03-06 04:56:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.40.171"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248740 0xc002248741}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022487d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022487f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.20,PodIP:,StartTime:2019-03-06 04:56:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.086: INFO: Pod "nginx-deployment-65bbdb5f8-kgx24" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kgx24,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-kgx24,UID:4257cf1a-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23715,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc0022488d0 0xc0022488d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-04.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022489b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022489d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.086: INFO: Pod "nginx-deployment-65bbdb5f8-kh6dp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kh6dp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-kh6dp,UID:425a631e-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23719,Generation:0,CreationTimestamp:2019-03-06 04:56:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248a70 0xc002248a71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002248b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 04:56:53.086: INFO: Pod "nginx-deployment-65bbdb5f8-rwlcl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rwlcl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-57vg4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-57vg4/pods/nginx-deployment-65bbdb5f8-rwlcl,UID:4128ae42-3fcc-11e9-a5d3-00505697ee14,ResourceVersion:23690,Generation:0,CreationTimestamp:2019-03-06 04:56:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.26"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 411f9d06-3fcc-11e9-a5d3-00505697ee14 0xc002248bf7 0xc002248bf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm2hp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm2hp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fm2hp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002248ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002248cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 04:56:51 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:,StartTime:2019-03-06 04:56:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:56:53.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-57vg4" for this suite.
Mar  6 04:56:59.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:56:59.150: INFO: namespace: e2e-tests-deployment-57vg4, resource: bindings, ignored listing per whitelist
Mar  6 04:56:59.331: INFO: namespace e2e-tests-deployment-57vg4 deletion completed in 6.232904907s

• [SLOW TEST:10.440 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:56:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0306 04:57:00.441448      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 04:57:00.441: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:57:00.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-b7z7k" for this suite.
Mar  6 04:57:06.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:57:06.481: INFO: namespace: e2e-tests-gc-b7z7k, resource: bindings, ignored listing per whitelist
Mar  6 04:57:06.591: INFO: namespace e2e-tests-gc-b7z7k deletion completed in 6.145858654s

• [SLOW TEST:7.260 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:57:06.591: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4a747040-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 04:57:06.665: INFO: Waiting up to 5m0s for pod "pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-wk865" to be "success or failure"
Mar  6 04:57:06.668: INFO: Pod "pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814638ms
Mar  6 04:57:08.671: INFO: Pod "pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006501034s
STEP: Saw pod success
Mar  6 04:57:08.671: INFO: Pod "pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 04:57:08.674: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 04:57:08.695: INFO: Waiting for pod pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e to disappear
Mar  6 04:57:08.697: INFO: Pod pod-secrets-4a75536d-3fcc-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:57:08.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wk865" for this suite.
Mar  6 04:57:14.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:57:14.735: INFO: namespace: e2e-tests-secrets-wk865, resource: bindings, ignored listing per whitelist
Mar  6 04:57:14.842: INFO: namespace e2e-tests-secrets-wk865 deletion completed in 6.141170232s

• [SLOW TEST:8.251 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:57:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-4f60d279-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating configMap with name cm-test-opt-upd-4f60d2b2-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4f60d279-3fcc-11e9-8de6-d63fb0ed442e
STEP: Updating configmap cm-test-opt-upd-4f60d2b2-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating configMap with name cm-test-opt-create-4f60d2c8-3fcc-11e9-8de6-d63fb0ed442e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 04:58:21.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lwsh6" for this suite.
Mar  6 04:58:43.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 04:58:43.489: INFO: namespace: e2e-tests-projected-lwsh6, resource: bindings, ignored listing per whitelist
Mar  6 04:58:43.604: INFO: namespace e2e-tests-projected-lwsh6 deletion completed in 22.164566515s

• [SLOW TEST:88.762 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 04:58:43.604: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 04:58:43.711: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:43.711: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:43.711: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:43.714: INFO: Number of nodes with available pods: 0
Mar  6 04:58:43.714: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:58:44.720: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:44.720: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:44.720: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:44.722: INFO: Number of nodes with available pods: 0
Mar  6 04:58:44.722: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 04:58:45.719: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.719: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.719: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.723: INFO: Number of nodes with available pods: 4
Mar  6 04:58:45.723: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  6 04:58:45.740: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.740: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.740: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:45.743: INFO: Number of nodes with available pods: 3
Mar  6 04:58:45.743: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:46.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:46.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:46.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:46.751: INFO: Number of nodes with available pods: 3
Mar  6 04:58:46.751: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:47.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:47.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:47.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:47.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:47.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:48.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:48.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:48.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:48.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:48.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:49.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:49.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:49.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:49.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:49.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:50.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:50.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:50.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:50.751: INFO: Number of nodes with available pods: 3
Mar  6 04:58:50.751: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:51.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:51.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:51.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:51.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:51.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:52.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:52.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:52.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:52.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:52.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:53.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:53.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:53.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:53.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:53.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:54.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:54.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:54.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:54.753: INFO: Number of nodes with available pods: 3
Mar  6 04:58:54.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:55.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:55.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:55.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:55.755: INFO: Number of nodes with available pods: 3
Mar  6 04:58:55.755: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:56.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:56.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:56.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:56.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:56.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:57.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:57.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:57.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:57.751: INFO: Number of nodes with available pods: 3
Mar  6 04:58:57.751: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:58.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:58.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:58.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:58.752: INFO: Number of nodes with available pods: 3
Mar  6 04:58:58.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:58:59.752: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:59.752: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:59.752: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:58:59.756: INFO: Number of nodes with available pods: 3
Mar  6 04:58:59.756: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:00.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:00.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:00.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:00.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:00.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:01.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:01.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:01.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:01.755: INFO: Number of nodes with available pods: 3
Mar  6 04:59:01.755: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:02.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:02.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:02.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:02.754: INFO: Number of nodes with available pods: 3
Mar  6 04:59:02.754: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:03.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:03.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:03.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:03.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:03.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:04.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:04.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:04.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:04.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:04.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:05.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:05.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:05.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:05.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:05.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:06.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:06.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:06.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:06.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:06.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:07.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:07.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:07.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:07.755: INFO: Number of nodes with available pods: 3
Mar  6 04:59:07.755: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:08.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:08.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:08.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:08.754: INFO: Number of nodes with available pods: 3
Mar  6 04:59:08.754: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:09.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:09.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:09.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:09.751: INFO: Number of nodes with available pods: 3
Mar  6 04:59:09.751: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:10.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:10.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:10.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:10.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:10.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:11.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:11.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:11.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:11.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:11.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:12.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:12.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:12.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:12.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:12.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:13.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:13.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:13.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:13.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:13.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:14.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:14.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:14.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:14.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:14.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:15.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:15.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:15.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:15.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:15.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:16.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:16.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:16.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:16.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:16.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:17.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:17.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:17.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:17.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:17.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:18.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:18.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:18.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:18.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:18.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:19.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:19.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:19.750: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:19.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:19.754: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:20.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:20.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:20.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:20.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:20.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:21.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:21.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:21.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:21.751: INFO: Number of nodes with available pods: 3
Mar  6 04:59:21.751: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:22.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:22.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:22.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:22.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:22.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:23.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:23.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:23.748: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:23.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:23.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:24.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:24.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:24.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:24.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:24.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:25.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:25.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:25.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:25.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:25.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:26.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:26.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:26.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:26.752: INFO: Number of nodes with available pods: 3
Mar  6 04:59:26.752: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:27.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:27.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:27.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:27.753: INFO: Number of nodes with available pods: 3
Mar  6 04:59:27.753: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:28.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:28.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:28.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:28.754: INFO: Number of nodes with available pods: 3
Mar  6 04:59:28.754: INFO: Node vmw3-k8s-06.local.dev is running more than one daemon pod
Mar  6 04:59:29.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:29.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:29.749: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 04:59:29.753: INFO: Number of nodes with available pods: 4
Mar  6 04:59:29.753: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-qtjqd, will wait for the garbage collector to delete the pods
Mar  6 04:59:29.820: INFO: Deleting DaemonSet.extensions daemon-set took: 9.185431ms
Mar  6 04:59:29.920: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.245392ms
Mar  6 05:00:07.425: INFO: Number of nodes with available pods: 0
Mar  6 05:00:07.425: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 05:00:07.428: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qtjqd/daemonsets","resourceVersion":"24906"},"items":null}

Mar  6 05:00:07.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qtjqd/pods","resourceVersion":"24906"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:00:07.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qtjqd" for this suite.
Mar  6 05:00:13.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:00:13.549: INFO: namespace: e2e-tests-daemonsets-qtjqd, resource: bindings, ignored listing per whitelist
Mar  6 05:00:13.592: INFO: namespace e2e-tests-daemonsets-qtjqd deletion completed in 6.138867586s

• [SLOW TEST:89.988 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:00:13.592: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 05:00:13.668: INFO: Waiting up to 5m0s for pod "downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-84psf" to be "success or failure"
Mar  6 05:00:13.672: INFO: Pod "downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.615924ms
Mar  6 05:00:15.676: INFO: Pod "downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00836549s
Mar  6 05:00:17.680: INFO: Pod "downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012320651s
STEP: Saw pod success
Mar  6 05:00:17.680: INFO: Pod "downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:00:17.683: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 05:00:17.705: INFO: Waiting for pod downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:00:17.708: INFO: Pod downward-api-b9eb443c-3fcc-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:00:17.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-84psf" for this suite.
Mar  6 05:00:23.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:00:23.827: INFO: namespace: e2e-tests-downward-api-84psf, resource: bindings, ignored listing per whitelist
Mar  6 05:00:23.850: INFO: namespace e2e-tests-downward-api-84psf deletion completed in 6.138308402s

• [SLOW TEST:10.258 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:00:23.851: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-c0081309-3fcc-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 05:00:23.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-p7lkv" to be "success or failure"
Mar  6 05:00:23.928: INFO: Pod "pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563682ms
Mar  6 05:00:25.931: INFO: Pod "pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006250388s
STEP: Saw pod success
Mar  6 05:00:25.931: INFO: Pod "pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:00:25.934: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 05:00:25.954: INFO: Waiting for pod pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:00:25.956: INFO: Pod pod-projected-secrets-c008dbfe-3fcc-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:00:25.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p7lkv" for this suite.
Mar  6 05:00:31.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:00:32.011: INFO: namespace: e2e-tests-projected-p7lkv, resource: bindings, ignored listing per whitelist
Mar  6 05:00:32.107: INFO: namespace e2e-tests-projected-p7lkv deletion completed in 6.146295723s

• [SLOW TEST:8.256 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:00:32.107: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 05:00:32.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-x9zmg" to be "success or failure"
Mar  6 05:00:32.182: INFO: Pod "downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.809338ms
Mar  6 05:00:34.187: INFO: Pod "downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007123215s
STEP: Saw pod success
Mar  6 05:00:34.187: INFO: Pod "downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:00:34.194: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 05:00:34.216: INFO: Waiting for pod downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:00:34.219: INFO: Pod downwardapi-volume-c4f3d9ac-3fcc-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:00:34.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x9zmg" for this suite.
Mar  6 05:00:40.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:00:40.307: INFO: namespace: e2e-tests-projected-x9zmg, resource: bindings, ignored listing per whitelist
Mar  6 05:00:40.366: INFO: namespace e2e-tests-projected-x9zmg deletion completed in 6.142921639s

• [SLOW TEST:8.259 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:00:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:00:42.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-gxkmr" for this suite.
Mar  6 05:01:20.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:01:20.516: INFO: namespace: e2e-tests-kubelet-test-gxkmr, resource: bindings, ignored listing per whitelist
Mar  6 05:01:20.622: INFO: namespace e2e-tests-kubelet-test-gxkmr deletion completed in 38.152202328s

• [SLOW TEST:40.256 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:01:20.623: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vnw7d
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 05:01:20.704: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 05:01:42.804: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.160.44:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vnw7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:01:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:01:42.921: INFO: Found all expected endpoints: [netserver-0]
Mar  6 05:01:42.924: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.232.93:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vnw7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:01:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:01:43.023: INFO: Found all expected endpoints: [netserver-1]
Mar  6 05:01:43.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.68.164:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vnw7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:01:43.026: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:01:43.120: INFO: Found all expected endpoints: [netserver-2]
Mar  6 05:01:43.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.40.184:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vnw7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:01:43.123: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:01:43.219: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:01:43.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vnw7d" for this suite.
Mar  6 05:02:05.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:05.278: INFO: namespace: e2e-tests-pod-network-test-vnw7d, resource: bindings, ignored listing per whitelist
Mar  6 05:02:05.379: INFO: namespace e2e-tests-pod-network-test-vnw7d deletion completed in 22.154874244s

• [SLOW TEST:44.756 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:05.379: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  6 05:02:05.461: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 05:02:05.473: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 05:02:05.476: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-04.local.dev before test
Mar  6 05:02:05.487: INFO: local-volume-provisioner-ltngn from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 05:02:05.488: INFO: rook-agent-pqt8r from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 05:02:05.488: INFO: dex-547c49d486-94z28 from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container dex ready: true, restart count 0
Mar  6 05:02:05.488: INFO: prometheus-prometheus-kube-state-metrics-5b55c99749-mczll from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Mar  6 05:02:05.488: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-vcqv2 from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 05:02:05.488: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 05:02:05.488: INFO: kube-proxy-vcgjg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 05:02:05.488: INFO: nfs-provisioner-54db5878bf-7rzqj from default started at 2019-03-06 03:50:45 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container nfs-provisioner ready: true, restart count 0
Mar  6 05:02:05.488: INFO: rook-ceph-mon0-wgbcm from rook started at 2019-03-06 03:52:44 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 05:02:05.488: INFO: rook-ceph-osd-6cjzd from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 05:02:05.488: INFO: rook-ceph-mgr0-c69f5fd99-snnnj from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Mar  6 05:02:05.488: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-06 03:55:26 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 05:02:05.488: INFO: calico-node-grzxg from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 05:02:05.488: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 05:02:05.488: INFO: kube-multus-ds-amd64-zjcth from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 05:02:05.488: INFO: node-feature-discovery-chnnf from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 05:02:05.488: INFO: prometheus-prometheus-node-exporter-dwxpj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.488: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 05:02:05.488: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-05.local.dev before test
Mar  6 05:02:05.497: INFO: kube-multus-ds-amd64-57r48 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 05:02:05.497: INFO: nginx-ingress-controller-c54bdfdb5-kwjtf from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 05:02:05.497: INFO: local-volume-provisioner-gbqgz from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 05:02:05.497: INFO: rook-agent-cp4vx from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 05:02:05.497: INFO: dex-547c49d486-nk6lm from kube-system started at 2019-03-06 03:53:19 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container dex ready: true, restart count 1
Mar  6 05:02:05.497: INFO: kube-proxy-pzh8g from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 05:02:05.497: INFO: calico-node-dlgtm from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 05:02:05.497: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 05:02:05.497: INFO: rook-ceph-mon1-d2vgn from rook started at 2019-03-06 03:52:54 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 05:02:05.497: INFO: rook-ceph-osd-czks7 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 05:02:05.497: INFO: prometheus-prometheus-server-86566f7789-44rqb from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container prometheus-server ready: true, restart count 0
Mar  6 05:02:05.497: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Mar  6 05:02:05.497: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-2dkkl from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 05:02:05.497: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 05:02:05.497: INFO: prometheus-prometheus-node-exporter-8px6c from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 05:02:05.497: INFO: node-feature-discovery-jf2bq from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 05:02:05.497: INFO: rook-api-8596f5cffd-h55tg from rook started at 2019-03-06 03:53:10 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container rook-api ready: true, restart count 0
Mar  6 05:02:05.497: INFO: rook-ceph-rgw-erikube-rook-rgw-568fb7598d-psm2f from rook started at 2019-03-06 03:53:39 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.497: INFO: 	Container rook-ceph-rgw-erikube-rook-rgw ready: true, restart count 0
Mar  6 05:02:05.497: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-06.local.dev before test
Mar  6 05:02:05.508: INFO: kube-multus-ds-amd64-cxrt5 from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container kube-multus ready: true, restart count 0
Mar  6 05:02:05.508: INFO: rook-agent-khk9s from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 05:02:05.508: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-nr2gk from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 05:02:05.508: INFO: nginx-ingress-controller-c54bdfdb5-fdksz from ingress-nginx started at 2019-03-06 03:50:33 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  6 05:02:05.508: INFO: node-feature-discovery-7mh4p from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container node-feature-discovery ready: true, restart count 0
Mar  6 05:02:05.508: INFO: rook-ceph-osd-2wrw6 from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Mar  6 05:02:05.508: INFO: prometheus-prometheus-node-exporter-4k7zk from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 05:02:05.508: INFO: kube-proxy-hw2bk from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 05:02:05.508: INFO: calico-node-m85tv from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 05:02:05.508: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 05:02:05.508: INFO: local-volume-provisioner-fhbzm from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 05:02:05.508: INFO: rook-operator-596d9f8565-4hvxg from rook-system started at 2019-03-06 03:52:02 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container rook-operator ready: true, restart count 0
Mar  6 05:02:05.508: INFO: rook-ceph-mon2-rtjdd from rook started at 2019-03-06 03:53:03 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Mar  6 05:02:05.508: INFO: prometheus-prometheus-pushgateway-76f6bd8898-8llhj from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Mar  6 05:02:05.508: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-84dpr from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.508: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 05:02:05.508: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 05:02:05.508: INFO: 
Logging pods the kubelet thinks is on node vmw3-k8s-07.local.dev before test
Mar  6 05:02:05.519: INFO: tiller-deploy-764968bdfd-tpt2l from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container tiller ready: true, restart count 0
Mar  6 05:02:05.519: INFO: local-volume-provisioner-j7s8k from default started at 2019-03-06 03:51:54 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container provisioner ready: true, restart count 0
Mar  6 05:02:05.519: INFO: rook-agent-qfj24 from rook-system started at 2019-03-06 03:52:17 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container rook-agent ready: true, restart count 0
Mar  6 05:02:05.519: INFO: prometheus-prometheus-alertmanager-7f97c695ff-7ml5d from monitoring started at 2019-03-06 03:54:08 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Mar  6 05:02:05.519: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Mar  6 05:02:05.519: INFO: sonobuoy-systemd-logs-daemon-set-c72204c5f5324eb0-prrwp from heptio-sonobuoy started at 2019-03-06 03:55:31 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar  6 05:02:05.519: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 05:02:05.519: INFO: rbd-provisioner-rbd-provisioner-74849667b8-997n8 from kube-system started at 2019-03-06 03:51:35 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container rbd-provisioner ready: true, restart count 0
Mar  6 05:02:05.519: INFO: metrics-server-5bddfb6979-6qqvx from kube-system started at 2019-03-06 03:53:07 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container metrics-server ready: true, restart count 0
Mar  6 05:02:05.519: INFO: calico-node-thclf from kube-system started at 2019-03-06 03:49:27 +0000 UTC (2 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container calico-node ready: true, restart count 0
Mar  6 05:02:05.519: INFO: 	Container install-cni ready: true, restart count 0
Mar  6 05:02:05.519: INFO: kube-proxy-jp8qs from kube-system started at 2019-03-06 03:49:27 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 05:02:05.519: INFO: default-http-backend-c776c779-bgdc2 from ingress-nginx started at 2019-03-06 03:50:31 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container default-http-backend ready: true, restart count 0
Mar  6 05:02:05.519: INFO: node-feature-discovery-9frqx from kube-system started at 2019-03-06 03:50:39 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container node-feature-discovery ready: true, restart count 2
Mar  6 05:02:05.519: INFO: rook-ceph-osd-ttf2k from rook started at 2019-03-06 03:53:12 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container rook-ceph-osd ready: true, restart count 1
Mar  6 05:02:05.519: INFO: rook-ceph-mds-erikube-rook-cephfs-f67f6c7c7-jkmzt from rook started at 2019-03-06 03:53:23 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container rook-ceph-mds-erikube-rook-cephfs ready: true, restart count 0
Mar  6 05:02:05.519: INFO: prometheus-prometheus-node-exporter-l2tbq from monitoring started at 2019-03-06 03:54:08 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Mar  6 05:02:05.519: INFO: kube-multus-ds-amd64-fgvjc from kube-system started at 2019-03-06 03:49:47 +0000 UTC (1 container statuses recorded)
Mar  6 05:02:05.519: INFO: 	Container kube-multus ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158946030e49b0e8], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:02:06.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-gphxn" for this suite.
Mar  6 05:02:12.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:12.609: INFO: namespace: e2e-tests-sched-pred-gphxn, resource: bindings, ignored listing per whitelist
Mar  6 05:02:12.697: INFO: namespace e2e-tests-sched-pred-gphxn deletion completed in 6.143876253s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.318 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:12.697: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-00e83a89-3fcd-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:02:12.770: INFO: Waiting up to 5m0s for pod "pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-z8b74" to be "success or failure"
Mar  6 05:02:12.772: INFO: Pod "pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253807ms
Mar  6 05:02:14.776: INFO: Pod "pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006100333s
STEP: Saw pod success
Mar  6 05:02:14.776: INFO: Pod "pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:02:14.778: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 05:02:14.800: INFO: Waiting for pod pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:02:14.804: INFO: Pod pod-configmaps-00e91fff-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:02:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-z8b74" for this suite.
Mar  6 05:02:20.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:20.864: INFO: namespace: e2e-tests-configmap-z8b74, resource: bindings, ignored listing per whitelist
Mar  6 05:02:20.951: INFO: namespace e2e-tests-configmap-z8b74 deletion completed in 6.142411315s

• [SLOW TEST:8.254 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:20.952: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 05:02:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-trxlk'
Mar  6 05:02:21.111: INFO: stderr: ""
Mar  6 05:02:21.111: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  6 05:02:26.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-trxlk -o json'
Mar  6 05:02:26.248: INFO: stderr: ""
Mar  6 05:02:26.248: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.232.95\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2019-03-06T05:02:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-trxlk\",\n        \"resourceVersion\": \"25642\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-trxlk/pods/e2e-test-nginx-pod\",\n        \"uid\": \"05dfbf3b-3fcd-11e9-91c9-005056979acf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-b6q7d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"vmw3-k8s-07.local.dev\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-b6q7d\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-b6q7d\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T05:02:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T05:02:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T05:02:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-06T05:02:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://afdad4ad3142de37313e1cd4f96e252980c386134e3b1725b1342c0ab0e82b19\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-06T05:02:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.5.1.25\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.232.95\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-06T05:02:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  6 05:02:26.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 replace -f - --namespace=e2e-tests-kubectl-trxlk'
Mar  6 05:02:26.483: INFO: stderr: ""
Mar  6 05:02:26.483: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Mar  6 05:02:26.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-trxlk'
Mar  6 05:02:27.848: INFO: stderr: ""
Mar  6 05:02:27.848: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:02:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-trxlk" for this suite.
Mar  6 05:02:33.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:33.998: INFO: namespace: e2e-tests-kubectl-trxlk, resource: bindings, ignored listing per whitelist
Mar  6 05:02:33.998: INFO: namespace e2e-tests-kubectl-trxlk deletion completed in 6.144472879s

• [SLOW TEST:13.047 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:33.999: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:02:36.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-b254p" for this suite.
Mar  6 05:02:42.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:42.256: INFO: namespace: e2e-tests-emptydir-wrapper-b254p, resource: bindings, ignored listing per whitelist
Mar  6 05:02:42.262: INFO: namespace e2e-tests-emptydir-wrapper-b254p deletion completed in 6.139514925s

• [SLOW TEST:8.263 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:42.262: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 05:02:42.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-f2jhj'
Mar  6 05:02:42.501: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 05:02:42.501: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  6 05:02:42.510: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-9c6b2]
Mar  6 05:02:42.510: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-9c6b2" in namespace "e2e-tests-kubectl-f2jhj" to be "running and ready"
Mar  6 05:02:42.516: INFO: Pod "e2e-test-nginx-rc-9c6b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.590346ms
Mar  6 05:02:44.520: INFO: Pod "e2e-test-nginx-rc-9c6b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010219641s
Mar  6 05:02:44.520: INFO: Pod "e2e-test-nginx-rc-9c6b2" satisfied condition "running and ready"
Mar  6 05:02:44.520: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-9c6b2]
Mar  6 05:02:44.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-f2jhj'
Mar  6 05:02:44.628: INFO: stderr: ""
Mar  6 05:02:44.628: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Mar  6 05:02:44.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-f2jhj'
Mar  6 05:02:44.718: INFO: stderr: ""
Mar  6 05:02:44.718: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:02:44.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f2jhj" for this suite.
Mar  6 05:02:50.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:02:50.748: INFO: namespace: e2e-tests-kubectl-f2jhj, resource: bindings, ignored listing per whitelist
Mar  6 05:02:50.859: INFO: namespace e2e-tests-kubectl-f2jhj deletion completed in 6.135560646s

• [SLOW TEST:8.597 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:02:50.859: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-zdpgj
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar  6 05:02:50.942: INFO: Found 0 stateful pods, waiting for 3
Mar  6 05:03:00.946: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:03:00.947: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:03:00.947: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:03:00.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-zdpgj ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:03:01.137: INFO: stderr: ""
Mar  6 05:03:01.137: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:03:01.137: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  6 05:03:11.173: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  6 05:03:21.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-zdpgj ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:03:21.404: INFO: stderr: ""
Mar  6 05:03:21.404: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:03:21.404: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:03:51.423: INFO: Waiting for StatefulSet e2e-tests-statefulset-zdpgj/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  6 05:04:01.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-zdpgj ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:04:01.633: INFO: stderr: ""
Mar  6 05:04:01.633: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:04:01.634: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 05:04:11.668: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  6 05:04:21.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-zdpgj ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:04:21.865: INFO: stderr: ""
Mar  6 05:04:21.865: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:04:21.865: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:04:41.887: INFO: Waiting for StatefulSet e2e-tests-statefulset-zdpgj/ss2 to complete update
Mar  6 05:04:41.887: INFO: Waiting for Pod e2e-tests-statefulset-zdpgj/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 05:04:51.894: INFO: Deleting all statefulset in ns e2e-tests-statefulset-zdpgj
Mar  6 05:04:51.897: INFO: Scaling statefulset ss2 to 0
Mar  6 05:05:11.910: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 05:05:11.913: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:05:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-zdpgj" for this suite.
Mar  6 05:05:17.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:05:18.070: INFO: namespace: e2e-tests-statefulset-zdpgj, resource: bindings, ignored listing per whitelist
Mar  6 05:05:18.072: INFO: namespace e2e-tests-statefulset-zdpgj deletion completed in 6.140616949s

• [SLOW TEST:147.213 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:05:18.072: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:05:18.158: INFO: (0) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.992498ms)
Mar  6 05:05:18.162: INFO: (1) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.737839ms)
Mar  6 05:05:18.165: INFO: (2) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.486598ms)
Mar  6 05:05:18.169: INFO: (3) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.690636ms)
Mar  6 05:05:18.173: INFO: (4) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.67329ms)
Mar  6 05:05:18.176: INFO: (5) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.428094ms)
Mar  6 05:05:18.180: INFO: (6) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.518603ms)
Mar  6 05:05:18.183: INFO: (7) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.022248ms)
Mar  6 05:05:18.186: INFO: (8) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.657978ms)
Mar  6 05:05:18.190: INFO: (9) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.330399ms)
Mar  6 05:05:18.193: INFO: (10) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.631081ms)
Mar  6 05:05:18.197: INFO: (11) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.470634ms)
Mar  6 05:05:18.201: INFO: (12) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.684512ms)
Mar  6 05:05:18.204: INFO: (13) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.622766ms)
Mar  6 05:05:18.208: INFO: (14) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.78072ms)
Mar  6 05:05:18.212: INFO: (15) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.061178ms)
Mar  6 05:05:18.216: INFO: (16) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.420702ms)
Mar  6 05:05:18.219: INFO: (17) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.56543ms)
Mar  6 05:05:18.222: INFO: (18) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.200897ms)
Mar  6 05:05:18.226: INFO: (19) /api/v1/nodes/vmw3-k8s-04.local.dev/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.635876ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:05:18.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-22cm9" for this suite.
Mar  6 05:05:24.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:05:24.293: INFO: namespace: e2e-tests-proxy-22cm9, resource: bindings, ignored listing per whitelist
Mar  6 05:05:24.366: INFO: namespace e2e-tests-proxy-22cm9 deletion completed in 6.136092993s

• [SLOW TEST:6.293 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:05:24.366: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar  6 05:05:26.491: INFO: Pod pod-hostip-732cdbda-3fcd-11e9-8de6-d63fb0ed442e has hostIP: 10.5.1.25
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:05:26.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6xltw" for this suite.
Mar  6 05:05:48.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:05:48.641: INFO: namespace: e2e-tests-pods-6xltw, resource: bindings, ignored listing per whitelist
Mar  6 05:05:48.641: INFO: namespace e2e-tests-pods-6xltw deletion completed in 22.146060663s

• [SLOW TEST:24.275 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:05:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 05:05:51.254: INFO: Successfully updated pod "annotationupdate819fc8d5-3fcd-11e9-8de6-d63fb0ed442e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:05:55.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wj2zj" for this suite.
Mar  6 05:06:17.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:06:17.440: INFO: namespace: e2e-tests-projected-wj2zj, resource: bindings, ignored listing per whitelist
Mar  6 05:06:17.450: INFO: namespace e2e-tests-projected-wj2zj deletion completed in 22.144058483s

• [SLOW TEST:28.808 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:06:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-9tnj
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 05:06:17.548: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9tnj" in namespace "e2e-tests-subpath-6rqb9" to be "success or failure"
Mar  6 05:06:17.553: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170185ms
Mar  6 05:06:19.558: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009802128s
Mar  6 05:06:21.561: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 4.013686391s
Mar  6 05:06:23.565: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 6.017503278s
Mar  6 05:06:25.570: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 8.022327275s
Mar  6 05:06:27.574: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 10.026583797s
Mar  6 05:06:29.578: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 12.030770948s
Mar  6 05:06:31.583: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 14.035351104s
Mar  6 05:06:33.587: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 16.039176351s
Mar  6 05:06:35.591: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 18.043291317s
Mar  6 05:06:37.595: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Running", Reason="", readiness=false. Elapsed: 20.046989108s
Mar  6 05:06:39.600: INFO: Pod "pod-subpath-test-projected-9tnj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.051803469s
STEP: Saw pod success
Mar  6 05:06:39.600: INFO: Pod "pod-subpath-test-projected-9tnj" satisfied condition "success or failure"
Mar  6 05:06:39.603: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-subpath-test-projected-9tnj container test-container-subpath-projected-9tnj: <nil>
STEP: delete the pod
Mar  6 05:06:39.629: INFO: Waiting for pod pod-subpath-test-projected-9tnj to disappear
Mar  6 05:06:39.632: INFO: Pod pod-subpath-test-projected-9tnj no longer exists
STEP: Deleting pod pod-subpath-test-projected-9tnj
Mar  6 05:06:39.632: INFO: Deleting pod "pod-subpath-test-projected-9tnj" in namespace "e2e-tests-subpath-6rqb9"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:06:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6rqb9" for this suite.
Mar  6 05:06:45.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:06:45.706: INFO: namespace: e2e-tests-subpath-6rqb9, resource: bindings, ignored listing per whitelist
Mar  6 05:06:45.774: INFO: namespace e2e-tests-subpath-6rqb9 deletion completed in 6.134105798s

• [SLOW TEST:28.323 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:06:45.774: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a3adee87-3fcd-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 05:06:45.880: INFO: Waiting up to 5m0s for pod "pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-s89vm" to be "success or failure"
Mar  6 05:06:45.883: INFO: Pod "pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999784ms
Mar  6 05:06:47.888: INFO: Pod "pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008049459s
STEP: Saw pod success
Mar  6 05:06:47.889: INFO: Pod "pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:06:47.891: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 05:06:47.909: INFO: Waiting for pod pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:06:47.911: INFO: Pod pod-secrets-a3b2b3fd-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:06:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-s89vm" for this suite.
Mar  6 05:06:53.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:06:53.965: INFO: namespace: e2e-tests-secrets-s89vm, resource: bindings, ignored listing per whitelist
Mar  6 05:06:54.053: INFO: namespace e2e-tests-secrets-s89vm deletion completed in 6.137706871s
STEP: Destroying namespace "e2e-tests-secret-namespace-q65wv" for this suite.
Mar  6 05:07:00.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:07:00.128: INFO: namespace: e2e-tests-secret-namespace-q65wv, resource: bindings, ignored listing per whitelist
Mar  6 05:07:00.190: INFO: namespace e2e-tests-secret-namespace-q65wv deletion completed in 6.137532356s

• [SLOW TEST:14.416 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:07:00.190: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 05:07:00.269: INFO: Waiting up to 5m0s for pod "pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-jclfm" to be "success or failure"
Mar  6 05:07:00.271: INFO: Pod "pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461364ms
Mar  6 05:07:02.275: INFO: Pod "pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006422285s
Mar  6 05:07:04.279: INFO: Pod "pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01024088s
STEP: Saw pod success
Mar  6 05:07:04.279: INFO: Pod "pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:07:04.282: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 05:07:04.303: INFO: Waiting for pod pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:07:04.305: INFO: Pod pod-ac45c6de-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:07:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jclfm" for this suite.
Mar  6 05:07:10.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:07:10.425: INFO: namespace: e2e-tests-emptydir-jclfm, resource: bindings, ignored listing per whitelist
Mar  6 05:07:10.464: INFO: namespace e2e-tests-emptydir-jclfm deletion completed in 6.15449479s

• [SLOW TEST:10.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:07:10.464: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-q6tsz/configmap-test-b266093b-3fcd-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:07:10.550: INFO: Waiting up to 5m0s for pod "pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-q6tsz" to be "success or failure"
Mar  6 05:07:10.554: INFO: Pod "pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994976ms
Mar  6 05:07:12.559: INFO: Pod "pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008462025s
STEP: Saw pod success
Mar  6 05:07:12.559: INFO: Pod "pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:07:12.567: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e container env-test: <nil>
STEP: delete the pod
Mar  6 05:07:12.589: INFO: Waiting for pod pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:07:12.591: INFO: Pod pod-configmaps-b266d336-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:07:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q6tsz" for this suite.
Mar  6 05:07:18.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:07:18.668: INFO: namespace: e2e-tests-configmap-q6tsz, resource: bindings, ignored listing per whitelist
Mar  6 05:07:18.741: INFO: namespace e2e-tests-configmap-q6tsz deletion completed in 6.144956029s

• [SLOW TEST:8.277 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:07:18.741: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  6 05:07:18.802: INFO: namespace e2e-tests-kubectl-fwkb6
Mar  6 05:07:18.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-fwkb6'
Mar  6 05:07:19.002: INFO: stderr: ""
Mar  6 05:07:19.002: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 05:07:20.006: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:07:20.006: INFO: Found 0 / 1
Mar  6 05:07:21.006: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:07:21.006: INFO: Found 1 / 1
Mar  6 05:07:21.006: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 05:07:21.009: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:07:21.009: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 05:07:21.009: INFO: wait on redis-master startup in e2e-tests-kubectl-fwkb6 
Mar  6 05:07:21.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 logs redis-master-zbzq7 redis-master --namespace=e2e-tests-kubectl-fwkb6'
Mar  6 05:07:21.107: INFO: stderr: ""
Mar  6 05:07:21.107: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 05:07:19.960 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 05:07:19.960 # Server started, Redis version 3.2.12\n1:M 06 Mar 05:07:19.960 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 05:07:19.960 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  6 05:07:21.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-fwkb6'
Mar  6 05:07:21.207: INFO: stderr: ""
Mar  6 05:07:21.207: INFO: stdout: "service/rm2 exposed\n"
Mar  6 05:07:21.212: INFO: Service rm2 in namespace e2e-tests-kubectl-fwkb6 found.
STEP: exposing service
Mar  6 05:07:23.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-fwkb6'
Mar  6 05:07:23.326: INFO: stderr: ""
Mar  6 05:07:23.326: INFO: stdout: "service/rm3 exposed\n"
Mar  6 05:07:23.331: INFO: Service rm3 in namespace e2e-tests-kubectl-fwkb6 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:07:25.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fwkb6" for this suite.
Mar  6 05:07:47.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:07:47.396: INFO: namespace: e2e-tests-kubectl-fwkb6, resource: bindings, ignored listing per whitelist
Mar  6 05:07:47.489: INFO: namespace e2e-tests-kubectl-fwkb6 deletion completed in 22.148504208s

• [SLOW TEST:28.748 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:07:47.489: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-xbzf2
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 05:07:47.559: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 05:08:05.661: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.40.191 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbzf2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:05.661: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:06.756: INFO: Found all expected endpoints: [netserver-0]
Mar  6 05:08:06.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.232.106 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbzf2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:06.759: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:07.854: INFO: Found all expected endpoints: [netserver-1]
Mar  6 05:08:07.857: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.68.165 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbzf2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:07.857: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:08.959: INFO: Found all expected endpoints: [netserver-2]
Mar  6 05:08:08.963: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.160.55 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbzf2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:08.963: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:10.047: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:08:10.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-xbzf2" for this suite.
Mar  6 05:08:32.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:08:32.169: INFO: namespace: e2e-tests-pod-network-test-xbzf2, resource: bindings, ignored listing per whitelist
Mar  6 05:08:32.189: INFO: namespace e2e-tests-pod-network-test-xbzf2 deletion completed in 22.137229157s

• [SLOW TEST:44.700 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:08:32.189: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar  6 05:08:32.254: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-610812775 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:08:32.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f6g79" for this suite.
Mar  6 05:08:38.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:08:38.403: INFO: namespace: e2e-tests-kubectl-f6g79, resource: bindings, ignored listing per whitelist
Mar  6 05:08:38.477: INFO: namespace e2e-tests-kubectl-f6g79 deletion completed in 6.144253606s

• [SLOW TEST:6.288 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:08:38.477: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar  6 05:08:38.552: INFO: Waiting up to 5m0s for pod "var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-var-expansion-hgxd2" to be "success or failure"
Mar  6 05:08:38.555: INFO: Pod "var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.178011ms
Mar  6 05:08:40.559: INFO: Pod "var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006538029s
STEP: Saw pod success
Mar  6 05:08:40.559: INFO: Pod "var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:08:40.561: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 05:08:40.581: INFO: Waiting for pod var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:08:40.583: INFO: Pod var-expansion-e6da373c-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:08:40.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-hgxd2" for this suite.
Mar  6 05:08:46.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:08:46.636: INFO: namespace: e2e-tests-var-expansion-hgxd2, resource: bindings, ignored listing per whitelist
Mar  6 05:08:46.736: INFO: namespace e2e-tests-var-expansion-hgxd2 deletion completed in 6.148491235s

• [SLOW TEST:8.259 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:08:46.736: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 05:08:46.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-mnmpn" to be "success or failure"
Mar  6 05:08:46.815: INFO: Pod "downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368362ms
Mar  6 05:08:48.820: INFO: Pod "downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007218436s
STEP: Saw pod success
Mar  6 05:08:48.820: INFO: Pod "downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:08:48.823: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 05:08:48.846: INFO: Waiting for pod downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:08:48.849: INFO: Pod downwardapi-volume-ebc700e0-3fcd-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:08:48.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mnmpn" for this suite.
Mar  6 05:08:54.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:08:54.878: INFO: namespace: e2e-tests-downward-api-mnmpn, resource: bindings, ignored listing per whitelist
Mar  6 05:08:54.992: INFO: namespace e2e-tests-downward-api-mnmpn deletion completed in 6.138970791s

• [SLOW TEST:8.256 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:08:54.992: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  6 05:08:59.087: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.087: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.184: INFO: Exec stderr: ""
Mar  6 05:08:59.184: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.184: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.287: INFO: Exec stderr: ""
Mar  6 05:08:59.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.287: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.386: INFO: Exec stderr: ""
Mar  6 05:08:59.387: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.387: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.479: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  6 05:08:59.479: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.479: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.557: INFO: Exec stderr: ""
Mar  6 05:08:59.557: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.557: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.645: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  6 05:08:59.645: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.645: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.738: INFO: Exec stderr: ""
Mar  6 05:08:59.738: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.738: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.819: INFO: Exec stderr: ""
Mar  6 05:08:59.819: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.819: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.897: INFO: Exec stderr: ""
Mar  6 05:08:59.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mhss6 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 05:08:59.897: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
Mar  6 05:08:59.975: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:08:59.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-mhss6" for this suite.
Mar  6 05:09:51.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:09:52.082: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-mhss6, resource: bindings, ignored listing per whitelist
Mar  6 05:09:52.113: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-mhss6 deletion completed in 52.133224859s

• [SLOW TEST:57.120 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:09:52.113: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-12be9864-3fce-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:09:52.194: INFO: Waiting up to 5m0s for pod "pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-ftddz" to be "success or failure"
Mar  6 05:09:52.197: INFO: Pod "pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589117ms
Mar  6 05:09:54.201: INFO: Pod "pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006651095s
STEP: Saw pod success
Mar  6 05:09:54.201: INFO: Pod "pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:09:54.203: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 05:09:54.225: INFO: Waiting for pod pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:09:54.227: INFO: Pod pod-configmaps-12bfa061-3fce-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:09:54.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ftddz" for this suite.
Mar  6 05:10:00.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:10:00.271: INFO: namespace: e2e-tests-configmap-ftddz, resource: bindings, ignored listing per whitelist
Mar  6 05:10:00.383: INFO: namespace e2e-tests-configmap-ftddz deletion completed in 6.150845819s

• [SLOW TEST:8.270 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:10:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:10:00.443: INFO: Creating ReplicaSet my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e
Mar  6 05:10:00.453: INFO: Pod name my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e: Found 0 pods out of 1
Mar  6 05:10:05.457: INFO: Pod name my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e: Found 1 pods out of 1
Mar  6 05:10:05.457: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e" is running
Mar  6 05:10:05.460: INFO: Pod "my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e-w6nhq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 05:10:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 05:10:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 05:10:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-06 05:10:00 +0000 UTC Reason: Message:}])
Mar  6 05:10:05.460: INFO: Trying to dial the pod
Mar  6 05:10:10.472: INFO: Controller my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e: Got expected result from replica 1 [my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e-w6nhq]: "my-hostname-basic-17ab7bf4-3fce-11e9-8de6-d63fb0ed442e-w6nhq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:10:10.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-5dvx8" for this suite.
Mar  6 05:10:16.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:10:16.575: INFO: namespace: e2e-tests-replicaset-5dvx8, resource: bindings, ignored listing per whitelist
Mar  6 05:10:16.644: INFO: namespace e2e-tests-replicaset-5dvx8 deletion completed in 6.166845856s

• [SLOW TEST:16.261 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:10:16.644: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  6 05:10:16.723: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  6 05:10:21.727: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:10:21.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-qwwxg" for this suite.
Mar  6 05:10:27.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:10:27.798: INFO: namespace: e2e-tests-replication-controller-qwwxg, resource: bindings, ignored listing per whitelist
Mar  6 05:10:27.910: INFO: namespace e2e-tests-replication-controller-qwwxg deletion completed in 6.128991993s

• [SLOW TEST:11.266 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:10:27.910: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar  6 05:10:27.976: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-610812775 proxy --unix-socket=/tmp/kubectl-proxy-unix274001306/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:10:28.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wcsr8" for this suite.
Mar  6 05:10:34.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:10:34.153: INFO: namespace: e2e-tests-kubectl-wcsr8, resource: bindings, ignored listing per whitelist
Mar  6 05:10:34.176: INFO: namespace e2e-tests-kubectl-wcsr8 deletion completed in 6.139525702s

• [SLOW TEST:6.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:10:34.176: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-gscb4
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar  6 05:10:34.252: INFO: Found 0 stateful pods, waiting for 3
Mar  6 05:10:44.256: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:10:44.256: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:10:44.256: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  6 05:10:44.288: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  6 05:10:54.323: INFO: Updating stateful set ss2
Mar  6 05:10:54.331: INFO: Waiting for Pod e2e-tests-statefulset-gscb4/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  6 05:11:04.337: INFO: Waiting for Pod e2e-tests-statefulset-gscb4/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar  6 05:11:14.382: INFO: Found 2 stateful pods, waiting for 3
Mar  6 05:11:24.386: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:11:24.386: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:11:24.386: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  6 05:11:24.409: INFO: Updating stateful set ss2
Mar  6 05:11:24.417: INFO: Waiting for Pod e2e-tests-statefulset-gscb4/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  6 05:11:34.444: INFO: Updating stateful set ss2
Mar  6 05:11:34.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-gscb4/ss2 to complete update
Mar  6 05:11:34.450: INFO: Waiting for Pod e2e-tests-statefulset-gscb4/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  6 05:11:44.458: INFO: Waiting for StatefulSet e2e-tests-statefulset-gscb4/ss2 to complete update
Mar  6 05:11:44.458: INFO: Waiting for Pod e2e-tests-statefulset-gscb4/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 05:11:54.458: INFO: Deleting all statefulset in ns e2e-tests-statefulset-gscb4
Mar  6 05:11:54.461: INFO: Scaling statefulset ss2 to 0
Mar  6 05:12:24.477: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 05:12:24.480: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:12:24.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-gscb4" for this suite.
Mar  6 05:12:30.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:12:30.553: INFO: namespace: e2e-tests-statefulset-gscb4, resource: bindings, ignored listing per whitelist
Mar  6 05:12:30.635: INFO: namespace e2e-tests-statefulset-gscb4 deletion completed in 6.138629181s

• [SLOW TEST:116.459 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:12:30.635: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cp4mf in namespace e2e-tests-proxy-8zsmp
I0306 05:12:30.718986      18 runners.go:184] Created replication controller with name: proxy-service-cp4mf, namespace: e2e-tests-proxy-8zsmp, replica count: 1
I0306 05:12:31.770012      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 05:12:32.770202      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 05:12:33.770502      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 05:12:34.770743      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:35.770917      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:36.771168      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:37.771339      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:38.771575      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:39.771737      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:40.771948      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:41.772238      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 05:12:42.772478      18 runners.go:184] proxy-service-cp4mf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 05:12:42.775: INFO: setup took 12.074705s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  6 05:12:42.781: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.861156ms)
Mar  6 05:12:42.782: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.274653ms)
Mar  6 05:12:42.783: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 7.143165ms)
Mar  6 05:12:42.792: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 17.187797ms)
Mar  6 05:12:42.792: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 16.813459ms)
Mar  6 05:12:42.796: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 20.636062ms)
Mar  6 05:12:42.799: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 23.180465ms)
Mar  6 05:12:42.799: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 23.394662ms)
Mar  6 05:12:42.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 24.289881ms)
Mar  6 05:12:42.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 24.364575ms)
Mar  6 05:12:42.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 24.641799ms)
Mar  6 05:12:42.801: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 25.049063ms)
Mar  6 05:12:42.801: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 25.527518ms)
Mar  6 05:12:42.802: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 26.513341ms)
Mar  6 05:12:42.802: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 26.364331ms)
Mar  6 05:12:42.805: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 29.836748ms)
Mar  6 05:12:42.812: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 5.075488ms)
Mar  6 05:12:42.812: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 5.956374ms)
Mar  6 05:12:42.812: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.946761ms)
Mar  6 05:12:42.812: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.060668ms)
Mar  6 05:12:42.812: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 5.955825ms)
Mar  6 05:12:42.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.742666ms)
Mar  6 05:12:42.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 6.368338ms)
Mar  6 05:12:42.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 7.392299ms)
Mar  6 05:12:42.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.499917ms)
Mar  6 05:12:42.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 7.317851ms)
Mar  6 05:12:42.814: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.502623ms)
Mar  6 05:12:42.814: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.950728ms)
Mar  6 05:12:42.814: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 8.598769ms)
Mar  6 05:12:42.815: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.85109ms)
Mar  6 05:12:42.815: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.133398ms)
Mar  6 05:12:42.815: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.209435ms)
Mar  6 05:12:42.819: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 4.072895ms)
Mar  6 05:12:42.820: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 4.630365ms)
Mar  6 05:12:42.821: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 6.028003ms)
Mar  6 05:12:42.821: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.575209ms)
Mar  6 05:12:42.821: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.203447ms)
Mar  6 05:12:42.821: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.020559ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 6.0586ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.462142ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.351736ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 7.273386ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 6.313362ms)
Mar  6 05:12:42.822: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.965601ms)
Mar  6 05:12:42.823: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 7.183325ms)
Mar  6 05:12:42.824: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.868666ms)
Mar  6 05:12:42.825: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.515772ms)
Mar  6 05:12:42.825: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.33306ms)
Mar  6 05:12:42.831: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.901179ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.602191ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.390535ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 7.302208ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.741904ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 7.166261ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 7.550526ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 7.152651ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.475633ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.364833ms)
Mar  6 05:12:42.833: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 7.510901ms)
Mar  6 05:12:42.835: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 9.678404ms)
Mar  6 05:12:42.835: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 10.159585ms)
Mar  6 05:12:42.836: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 10.346893ms)
Mar  6 05:12:42.836: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 10.408724ms)
Mar  6 05:12:42.836: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 10.52066ms)
Mar  6 05:12:42.841: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.003662ms)
Mar  6 05:12:42.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 4.967455ms)
Mar  6 05:12:42.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 5.571853ms)
Mar  6 05:12:42.842: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.388711ms)
Mar  6 05:12:42.843: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 7.006316ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.916686ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.710286ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.308387ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.405527ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 7.188295ms)
Mar  6 05:12:42.844: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 7.649967ms)
Mar  6 05:12:42.845: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.048514ms)
Mar  6 05:12:42.847: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 10.1833ms)
Mar  6 05:12:42.847: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 10.791264ms)
Mar  6 05:12:42.847: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 10.126507ms)
Mar  6 05:12:42.847: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 10.369753ms)
Mar  6 05:12:42.851: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 3.895119ms)
Mar  6 05:12:42.852: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 4.99638ms)
Mar  6 05:12:42.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 5.359196ms)
Mar  6 05:12:42.855: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.196971ms)
Mar  6 05:12:42.855: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.598746ms)
Mar  6 05:12:42.855: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.780295ms)
Mar  6 05:12:42.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 7.574291ms)
Mar  6 05:12:42.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 8.02892ms)
Mar  6 05:12:42.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 8.132351ms)
Mar  6 05:12:42.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 8.117951ms)
Mar  6 05:12:42.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.898866ms)
Mar  6 05:12:42.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 10.633205ms)
Mar  6 05:12:42.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 10.314663ms)
Mar  6 05:12:42.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 10.522328ms)
Mar  6 05:12:42.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 10.542928ms)
Mar  6 05:12:42.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 10.835066ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.809649ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 6.482008ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.279737ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.162943ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.759325ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 6.460141ms)
Mar  6 05:12:42.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 7.784354ms)
Mar  6 05:12:42.867: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.74287ms)
Mar  6 05:12:42.867: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.869451ms)
Mar  6 05:12:42.867: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 7.547515ms)
Mar  6 05:12:42.867: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 7.109075ms)
Mar  6 05:12:42.868: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 8.207902ms)
Mar  6 05:12:42.869: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 10.033439ms)
Mar  6 05:12:42.869: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 10.743831ms)
Mar  6 05:12:42.870: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 9.866208ms)
Mar  6 05:12:42.870: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 11.001884ms)
Mar  6 05:12:42.874: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 4.679943ms)
Mar  6 05:12:42.877: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.171569ms)
Mar  6 05:12:42.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 8.052973ms)
Mar  6 05:12:42.878: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 7.773006ms)
Mar  6 05:12:42.879: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 8.013444ms)
Mar  6 05:12:42.879: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 9.016833ms)
Mar  6 05:12:42.879: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 8.563106ms)
Mar  6 05:12:42.879: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 9.04001ms)
Mar  6 05:12:42.879: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 9.156846ms)
Mar  6 05:12:42.880: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 10.16379ms)
Mar  6 05:12:42.881: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 10.421172ms)
Mar  6 05:12:42.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 12.409637ms)
Mar  6 05:12:42.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 12.693273ms)
Mar  6 05:12:42.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 13.47756ms)
Mar  6 05:12:42.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 12.592692ms)
Mar  6 05:12:42.884: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 13.27916ms)
Mar  6 05:12:42.888: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 3.619884ms)
Mar  6 05:12:42.890: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.313261ms)
Mar  6 05:12:42.890: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 5.815333ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.139909ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.793765ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.933503ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.738329ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.2968ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 6.50383ms)
Mar  6 05:12:42.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.139547ms)
Mar  6 05:12:42.893: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.356854ms)
Mar  6 05:12:42.894: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 10.009857ms)
Mar  6 05:12:42.894: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 9.595407ms)
Mar  6 05:12:42.894: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 10.557119ms)
Mar  6 05:12:42.894: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 9.933642ms)
Mar  6 05:12:42.895: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 10.648381ms)
Mar  6 05:12:42.899: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 4.512856ms)
Mar  6 05:12:42.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 4.405312ms)
Mar  6 05:12:42.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 4.679488ms)
Mar  6 05:12:42.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.394723ms)
Mar  6 05:12:42.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.450429ms)
Mar  6 05:12:42.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.451174ms)
Mar  6 05:12:42.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.799429ms)
Mar  6 05:12:42.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 5.589846ms)
Mar  6 05:12:42.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.482834ms)
Mar  6 05:12:42.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.085291ms)
Mar  6 05:12:42.903: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.767856ms)
Mar  6 05:12:42.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.288291ms)
Mar  6 05:12:42.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 9.069534ms)
Mar  6 05:12:42.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 9.248163ms)
Mar  6 05:12:42.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.313599ms)
Mar  6 05:12:42.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 9.45967ms)
Mar  6 05:12:42.908: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 3.915209ms)
Mar  6 05:12:42.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 5.199679ms)
Mar  6 05:12:42.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.552364ms)
Mar  6 05:12:42.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.463886ms)
Mar  6 05:12:42.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.155198ms)
Mar  6 05:12:42.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 5.354093ms)
Mar  6 05:12:42.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.995407ms)
Mar  6 05:12:42.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.924645ms)
Mar  6 05:12:42.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.539451ms)
Mar  6 05:12:42.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.845909ms)
Mar  6 05:12:42.912: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 6.624268ms)
Mar  6 05:12:42.912: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.649225ms)
Mar  6 05:12:42.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 7.134802ms)
Mar  6 05:12:42.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.507144ms)
Mar  6 05:12:42.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 7.897556ms)
Mar  6 05:12:42.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.199194ms)
Mar  6 05:12:42.917: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.223955ms)
Mar  6 05:12:42.919: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.141989ms)
Mar  6 05:12:42.919: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 5.411641ms)
Mar  6 05:12:42.919: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.055976ms)
Mar  6 05:12:42.919: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.479664ms)
Mar  6 05:12:42.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 5.924419ms)
Mar  6 05:12:42.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.103325ms)
Mar  6 05:12:42.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.214514ms)
Mar  6 05:12:42.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.878832ms)
Mar  6 05:12:42.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.080111ms)
Mar  6 05:12:42.921: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 7.193912ms)
Mar  6 05:12:42.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.946091ms)
Mar  6 05:12:42.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 8.400857ms)
Mar  6 05:12:42.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.742861ms)
Mar  6 05:12:42.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 8.968178ms)
Mar  6 05:12:42.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 9.137951ms)
Mar  6 05:12:42.926: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 3.685317ms)
Mar  6 05:12:42.927: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 4.293988ms)
Mar  6 05:12:42.929: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.521329ms)
Mar  6 05:12:42.930: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 7.381235ms)
Mar  6 05:12:42.930: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 7.42281ms)
Mar  6 05:12:42.930: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 7.387474ms)
Mar  6 05:12:42.930: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 7.596347ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 7.844386ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.943023ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 8.035469ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.574762ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.80031ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 8.664656ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 8.747977ms)
Mar  6 05:12:42.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 8.79367ms)
Mar  6 05:12:42.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 9.33172ms)
Mar  6 05:12:42.935: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 2.616096ms)
Mar  6 05:12:42.936: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 3.153924ms)
Mar  6 05:12:42.936: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 3.311439ms)
Mar  6 05:12:42.937: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 4.160453ms)
Mar  6 05:12:42.937: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.499249ms)
Mar  6 05:12:42.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 4.735919ms)
Mar  6 05:12:42.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 4.938409ms)
Mar  6 05:12:42.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.141209ms)
Mar  6 05:12:42.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 4.908004ms)
Mar  6 05:12:42.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 5.466945ms)
Mar  6 05:12:42.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 6.329095ms)
Mar  6 05:12:42.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 6.496771ms)
Mar  6 05:12:42.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 5.716961ms)
Mar  6 05:12:42.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 6.670254ms)
Mar  6 05:12:42.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 6.316349ms)
Mar  6 05:12:42.940: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 6.846119ms)
Mar  6 05:12:42.943: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 2.940331ms)
Mar  6 05:12:42.944: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 3.432788ms)
Mar  6 05:12:42.944: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.134318ms)
Mar  6 05:12:42.944: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 4.014843ms)
Mar  6 05:12:42.945: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 4.212686ms)
Mar  6 05:12:42.945: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 4.146988ms)
Mar  6 05:12:42.945: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.989722ms)
Mar  6 05:12:42.946: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.036583ms)
Mar  6 05:12:42.946: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 5.255164ms)
Mar  6 05:12:42.946: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 6.226511ms)
Mar  6 05:12:42.946: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 5.632538ms)
Mar  6 05:12:42.947: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 6.786581ms)
Mar  6 05:12:42.947: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 6.410129ms)
Mar  6 05:12:42.947: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 6.709678ms)
Mar  6 05:12:42.948: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.022996ms)
Mar  6 05:12:42.948: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 6.991494ms)
Mar  6 05:12:42.952: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 4.093909ms)
Mar  6 05:12:42.954: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 5.725269ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.038641ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 6.208409ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.359173ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.769556ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.407158ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 6.952671ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 7.088671ms)
Mar  6 05:12:42.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 7.327996ms)
Mar  6 05:12:42.956: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.143931ms)
Mar  6 05:12:42.956: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 7.627521ms)
Mar  6 05:12:42.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 8.331442ms)
Mar  6 05:12:42.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 8.678268ms)
Mar  6 05:12:42.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 9.081688ms)
Mar  6 05:12:42.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 9.126757ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.024152ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 6.310532ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.17711ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 7.142565ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 7.66727ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 7.78239ms)
Mar  6 05:12:42.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 7.649758ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.039514ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 7.06339ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 7.297632ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 8.197585ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 8.161654ms)
Mar  6 05:12:42.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 8.412704ms)
Mar  6 05:12:42.967: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 8.494469ms)
Mar  6 05:12:42.967: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 9.182356ms)
Mar  6 05:12:42.967: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 8.776574ms)
Mar  6 05:12:42.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 4.046544ms)
Mar  6 05:12:42.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 4.203291ms)
Mar  6 05:12:42.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 4.577778ms)
Mar  6 05:12:42.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.67971ms)
Mar  6 05:12:42.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 4.78778ms)
Mar  6 05:12:42.973: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 6.041549ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.492707ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 6.301423ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 6.652936ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 6.410898ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 6.598899ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.196612ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 7.174514ms)
Mar  6 05:12:42.974: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 7.062018ms)
Mar  6 05:12:42.975: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 7.004959ms)
Mar  6 05:12:42.975: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 6.966884ms)
Mar  6 05:12:42.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 3.743269ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 4.991391ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.079904ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.483011ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 5.777856ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 5.331911ms)
Mar  6 05:12:42.981: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 5.513277ms)
Mar  6 05:12:42.982: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 5.708127ms)
Mar  6 05:12:42.982: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 5.962472ms)
Mar  6 05:12:42.982: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 6.754609ms)
Mar  6 05:12:42.982: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 6.719372ms)
Mar  6 05:12:42.984: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 7.565012ms)
Mar  6 05:12:42.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 9.345408ms)
Mar  6 05:12:42.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 9.198281ms)
Mar  6 05:12:42.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 9.484506ms)
Mar  6 05:12:42.985: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 10.069779ms)
Mar  6 05:12:42.989: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:462/proxy/: tls qux (200; 3.715227ms)
Mar  6 05:12:42.990: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 3.957456ms)
Mar  6 05:12:42.990: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:460/proxy/: tls baz (200; 4.066795ms)
Mar  6 05:12:42.990: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:162/proxy/: bar (200; 3.980966ms)
Mar  6 05:12:42.991: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.773331ms)
Mar  6 05:12:42.991: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:160/proxy/: foo (200; 4.980761ms)
Mar  6 05:12:42.991: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/http:proxy-service-cp4mf-4fx9q:1080/proxy/... (200; 5.206567ms)
Mar  6 05:12:42.992: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q:1080/proxy/rewri... (200; 5.297899ms)
Mar  6 05:12:42.992: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname2/proxy/: tls qux (200; 6.08169ms)
Mar  6 05:12:42.992: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname1/proxy/: foo (200; 6.317534ms)
Mar  6 05:12:42.992: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/https:proxy-service-cp4mf-4fx9q:443/proxy/... (200; 5.618132ms)
Mar  6 05:12:42.992: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/https:proxy-service-cp4mf:tlsportname1/proxy/: tls baz (200; 6.406745ms)
Mar  6 05:12:42.993: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8zsmp/pods/proxy-service-cp4mf-4fx9q/proxy/rewriteme"... (200; 6.252163ms)
Mar  6 05:12:42.993: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/proxy-service-cp4mf:portname2/proxy/: bar (200; 6.969416ms)
Mar  6 05:12:42.993: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname1/proxy/: foo (200; 7.546021ms)
Mar  6 05:12:42.994: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8zsmp/services/http:proxy-service-cp4mf:portname2/proxy/: bar (200; 8.46166ms)
STEP: deleting ReplicationController proxy-service-cp4mf in namespace e2e-tests-proxy-8zsmp, will wait for the garbage collector to delete the pods
Mar  6 05:12:43.053: INFO: Deleting ReplicationController proxy-service-cp4mf took: 6.309226ms
Mar  6 05:12:43.154: INFO: Terminating ReplicationController proxy-service-cp4mf pods took: 100.246103ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:12:45.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-8zsmp" for this suite.
Mar  6 05:12:51.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:12:51.490: INFO: namespace: e2e-tests-proxy-8zsmp, resource: bindings, ignored listing per whitelist
Mar  6 05:12:51.602: INFO: namespace e2e-tests-proxy-8zsmp deletion completed in 6.141808642s

• [SLOW TEST:20.967 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:12:51.602: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-7db9db0b-3fce-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 05:12:51.677: INFO: Waiting up to 5m0s for pod "pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-6gnvw" to be "success or failure"
Mar  6 05:12:51.680: INFO: Pod "pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227228ms
Mar  6 05:12:53.685: INFO: Pod "pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007699841s
STEP: Saw pod success
Mar  6 05:12:53.685: INFO: Pod "pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:12:53.688: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e container secret-env-test: <nil>
STEP: delete the pod
Mar  6 05:12:53.711: INFO: Waiting for pod pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:12:53.713: INFO: Pod pod-secrets-7dbab129-3fce-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:12:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6gnvw" for this suite.
Mar  6 05:12:59.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:12:59.850: INFO: namespace: e2e-tests-secrets-6gnvw, resource: bindings, ignored listing per whitelist
Mar  6 05:12:59.868: INFO: namespace e2e-tests-secrets-6gnvw deletion completed in 6.15064187s

• [SLOW TEST:8.266 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:12:59.868: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  6 05:12:59.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-rtltv'
Mar  6 05:13:00.119: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 05:13:00.119: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Mar  6 05:13:02.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-rtltv'
Mar  6 05:13:02.220: INFO: stderr: ""
Mar  6 05:13:02.220: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:13:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rtltv" for this suite.
Mar  6 05:13:08.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:13:08.358: INFO: namespace: e2e-tests-kubectl-rtltv, resource: bindings, ignored listing per whitelist
Mar  6 05:13:08.369: INFO: namespace e2e-tests-kubectl-rtltv deletion completed in 6.143386924s

• [SLOW TEST:8.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:13:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:13:10.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-g9sq4" for this suite.
Mar  6 05:13:48.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:13:48.501: INFO: namespace: e2e-tests-kubelet-test-g9sq4, resource: bindings, ignored listing per whitelist
Mar  6 05:13:48.607: INFO: namespace e2e-tests-kubelet-test-g9sq4 deletion completed in 38.132283831s

• [SLOW TEST:40.238 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:13:48.608: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  6 05:13:55.705: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:13:56.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-cq9zq" for this suite.
Mar  6 05:14:18.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:14:18.787: INFO: namespace: e2e-tests-replicaset-cq9zq, resource: bindings, ignored listing per whitelist
Mar  6 05:14:18.871: INFO: namespace e2e-tests-replicaset-cq9zq deletion completed in 22.147672517s

• [SLOW TEST:30.264 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:14:18.872: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar  6 05:14:19.454: INFO: Waiting up to 5m0s for pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc" in namespace "e2e-tests-svcaccounts-qj9r9" to be "success or failure"
Mar  6 05:14:19.456: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.64302ms
Mar  6 05:14:21.460: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006304329s
STEP: Saw pod success
Mar  6 05:14:21.460: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc" satisfied condition "success or failure"
Mar  6 05:14:21.463: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc container token-test: <nil>
STEP: delete the pod
Mar  6 05:14:21.494: INFO: Waiting for pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc to disappear
Mar  6 05:14:21.497: INFO: Pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-jsczc no longer exists
STEP: Creating a pod to test consume service account root CA
Mar  6 05:14:21.501: INFO: Waiting up to 5m0s for pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7" in namespace "e2e-tests-svcaccounts-qj9r9" to be "success or failure"
Mar  6 05:14:21.503: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476799ms
Mar  6 05:14:23.508: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006899757s
Mar  6 05:14:25.512: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011752642s
STEP: Saw pod success
Mar  6 05:14:25.513: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7" satisfied condition "success or failure"
Mar  6 05:14:25.517: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7 container root-ca-test: <nil>
STEP: delete the pod
Mar  6 05:14:25.538: INFO: Waiting for pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7 to disappear
Mar  6 05:14:25.541: INFO: Pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-kmkp7 no longer exists
STEP: Creating a pod to test consume service account namespace
Mar  6 05:14:25.545: INFO: Waiting up to 5m0s for pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj" in namespace "e2e-tests-svcaccounts-qj9r9" to be "success or failure"
Mar  6 05:14:25.547: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411099ms
Mar  6 05:14:27.551: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006487614s
STEP: Saw pod success
Mar  6 05:14:27.551: INFO: Pod "pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj" satisfied condition "success or failure"
Mar  6 05:14:27.554: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj container namespace-test: <nil>
STEP: delete the pod
Mar  6 05:14:27.572: INFO: Waiting for pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj to disappear
Mar  6 05:14:27.575: INFO: Pod pod-service-account-b20bf3c1-3fce-11e9-8de6-d63fb0ed442e-ngwxj no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:14:27.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-qj9r9" for this suite.
Mar  6 05:14:33.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:14:33.707: INFO: namespace: e2e-tests-svcaccounts-qj9r9, resource: bindings, ignored listing per whitelist
Mar  6 05:14:33.719: INFO: namespace e2e-tests-svcaccounts-qj9r9 deletion completed in 6.140914416s

• [SLOW TEST:14.848 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:14:33.720: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ba989b20-3fce-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:14:33.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-t9d57" to be "success or failure"
Mar  6 05:14:33.803: INFO: Pod "pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648587ms
Mar  6 05:14:35.807: INFO: Pod "pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006514565s
STEP: Saw pod success
Mar  6 05:14:35.807: INFO: Pod "pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:14:35.810: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 05:14:35.830: INFO: Waiting for pod pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:14:35.833: INFO: Pod pod-configmaps-ba998741-3fce-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:14:35.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-t9d57" for this suite.
Mar  6 05:14:41.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:14:41.958: INFO: namespace: e2e-tests-configmap-t9d57, resource: bindings, ignored listing per whitelist
Mar  6 05:14:41.969: INFO: namespace e2e-tests-configmap-t9d57 deletion completed in 6.131881712s

• [SLOW TEST:8.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:14:41.969: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0306 05:15:12.569582      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  6 05:15:12.569: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:15:12.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4mkfw" for this suite.
Mar  6 05:15:18.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:15:18.682: INFO: namespace: e2e-tests-gc-4mkfw, resource: bindings, ignored listing per whitelist
Mar  6 05:15:18.712: INFO: namespace e2e-tests-gc-4mkfw deletion completed in 6.138950109s

• [SLOW TEST:36.743 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:15:18.713: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar  6 05:15:18.774: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  6 05:15:18.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:18.984: INFO: stderr: ""
Mar  6 05:15:18.984: INFO: stdout: "service/redis-slave created\n"
Mar  6 05:15:18.984: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  6 05:15:18.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:19.258: INFO: stderr: ""
Mar  6 05:15:19.258: INFO: stdout: "service/redis-master created\n"
Mar  6 05:15:19.258: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  6 05:15:19.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:19.430: INFO: stderr: ""
Mar  6 05:15:19.430: INFO: stdout: "service/frontend created\n"
Mar  6 05:15:19.430: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  6 05:15:19.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:19.570: INFO: stderr: ""
Mar  6 05:15:19.570: INFO: stdout: "deployment.extensions/frontend created\n"
Mar  6 05:15:19.571: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  6 05:15:19.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:19.728: INFO: stderr: ""
Mar  6 05:15:19.728: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar  6 05:15:19.728: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  6 05:15:19.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:19.862: INFO: stderr: ""
Mar  6 05:15:19.862: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar  6 05:15:19.862: INFO: Waiting for all frontend pods to be Running.
Mar  6 05:15:34.913: INFO: Waiting for frontend to serve content.
Mar  6 05:15:34.929: INFO: Trying to add a new entry to the guestbook.
Mar  6 05:15:34.941: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  6 05:15:34.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.062: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.062: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 05:15:35.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.178: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.178: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 05:15:35.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.300: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.300: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 05:15:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.399: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.399: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 05:15:35.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.485: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 05:15:35.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jkrbm'
Mar  6 05:15:35.566: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 05:15:35.566: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:15:35.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jkrbm" for this suite.
Mar  6 05:16:19.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:16:19.697: INFO: namespace: e2e-tests-kubectl-jkrbm, resource: bindings, ignored listing per whitelist
Mar  6 05:16:19.711: INFO: namespace e2e-tests-kubectl-jkrbm deletion completed in 44.138611104s

• [SLOW TEST:60.999 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:16:19.712: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:16:19.786: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  6 05:16:24.790: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 05:16:24.790: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 05:16:24.808: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-92dxw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-92dxw/deployments/test-cleanup-deployment,UID:fcc2da42-3fce-11e9-91c9-005056979acf,ResourceVersion:30342,Generation:1,CreationTimestamp:2019-03-06 05:16:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 05:16:24.811: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:16:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-92dxw" for this suite.
Mar  6 05:16:30.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:16:30.845: INFO: namespace: e2e-tests-deployment-92dxw, resource: bindings, ignored listing per whitelist
Mar  6 05:16:30.956: INFO: namespace e2e-tests-deployment-92dxw deletion completed in 6.13764925s

• [SLOW TEST:11.244 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:16:30.956: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:16:31.021: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:16:33.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dprpq" for this suite.
Mar  6 05:17:19.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:17:19.243: INFO: namespace: e2e-tests-pods-dprpq, resource: bindings, ignored listing per whitelist
Mar  6 05:17:19.297: INFO: namespace e2e-tests-pods-dprpq deletion completed in 46.127432738s

• [SLOW TEST:48.340 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:17:19.297: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  6 05:17:19.362: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:17:23.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8682d" for this suite.
Mar  6 05:17:45.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:17:45.116: INFO: namespace: e2e-tests-init-container-8682d, resource: bindings, ignored listing per whitelist
Mar  6 05:17:45.190: INFO: namespace e2e-tests-init-container-8682d deletion completed in 22.138131338s

• [SLOW TEST:25.893 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:17:45.190: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-dx7d7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-dx7d7 to expose endpoints map[]
Mar  6 05:17:45.279: INFO: Get endpoints failed (3.787579ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  6 05:17:46.283: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dx7d7 exposes endpoints map[] (1.007402111s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-dx7d7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-dx7d7 to expose endpoints map[pod1:[100]]
Mar  6 05:17:48.305: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dx7d7 exposes endpoints map[pod1:[100]] (2.015920469s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-dx7d7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-dx7d7 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  6 05:17:50.338: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dx7d7 exposes endpoints map[pod2:[101] pod1:[100]] (2.028025738s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-dx7d7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-dx7d7 to expose endpoints map[pod2:[101]]
Mar  6 05:17:51.356: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dx7d7 exposes endpoints map[pod2:[101]] (1.013461409s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-dx7d7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-dx7d7 to expose endpoints map[]
Mar  6 05:17:51.366: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-dx7d7 exposes endpoints map[] (3.717506ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:17:51.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-dx7d7" for this suite.
Mar  6 05:17:57.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:17:57.492: INFO: namespace: e2e-tests-services-dx7d7, resource: bindings, ignored listing per whitelist
Mar  6 05:17:57.534: INFO: namespace e2e-tests-services-dx7d7 deletion completed in 6.139622199s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:12.344 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:17:57.534: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 05:17:57.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-6rjqn" to be "success or failure"
Mar  6 05:17:57.615: INFO: Pod "downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893448ms
Mar  6 05:17:59.619: INFO: Pod "downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007111644s
STEP: Saw pod success
Mar  6 05:17:59.619: INFO: Pod "downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:17:59.622: INFO: Trying to get logs from node vmw3-k8s-05.local.dev pod downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 05:17:59.644: INFO: Waiting for pod downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:17:59.646: INFO: Pod downwardapi-volume-34143959-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:17:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6rjqn" for this suite.
Mar  6 05:18:05.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:18:05.670: INFO: namespace: e2e-tests-projected-6rjqn, resource: bindings, ignored listing per whitelist
Mar  6 05:18:05.804: INFO: namespace e2e-tests-projected-6rjqn deletion completed in 6.153460368s

• [SLOW TEST:8.269 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:18:05.804: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-75wdw/secret-test-390268a7-3fcf-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume secrets
Mar  6 05:18:05.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-secrets-75wdw" to be "success or failure"
Mar  6 05:18:05.890: INFO: Pod "pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641638ms
Mar  6 05:18:07.894: INFO: Pod "pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006616388s
STEP: Saw pod success
Mar  6 05:18:07.894: INFO: Pod "pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:18:07.897: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e container env-test: <nil>
STEP: delete the pod
Mar  6 05:18:07.920: INFO: Waiting for pod pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:18:07.923: INFO: Pod pod-configmaps-39035a80-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:18:07.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-75wdw" for this suite.
Mar  6 05:18:13.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:18:14.071: INFO: namespace: e2e-tests-secrets-75wdw, resource: bindings, ignored listing per whitelist
Mar  6 05:18:14.084: INFO: namespace e2e-tests-secrets-75wdw deletion completed in 6.156509287s

• [SLOW TEST:8.280 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:18:14.084: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-rblr9;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-rblr9.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-rblr9.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.41.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.41.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.41.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.41.36_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-rblr9;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-rblr9;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rblr9.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-rblr9.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rblr9.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-rblr9.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-rblr9.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.41.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.41.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.41.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.41.36_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 05:18:28.196: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.199: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.206: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.216: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.241: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.245: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.248: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.251: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.254: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.257: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.260: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.263: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:28.283: INFO: Lookups using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rblr9 jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc]

Mar  6 05:18:33.290: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.294: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.301: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.343: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.347: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.350: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.353: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.357: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.360: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.368: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:33.390: INFO: Lookups using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rblr9 jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc]

Mar  6 05:18:38.288: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.292: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.299: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.342: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.346: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.350: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.353: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.357: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.361: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.365: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.368: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:38.390: INFO: Lookups using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rblr9 jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc]

Mar  6 05:18:43.290: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.294: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.301: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.346: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.349: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.352: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.355: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.358: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.361: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:43.387: INFO: Lookups using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-rblr9 wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rblr9 jessie_tcp@dns-test-service.e2e-tests-dns-rblr9 jessie_udp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@dns-test-service.e2e-tests-dns-rblr9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc]

Mar  6 05:18:48.352: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc from pod e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e: the server could not find the requested resource (get pods dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e)
Mar  6 05:18:48.373: INFO: Lookups using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e failed for: [jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rblr9.svc]

Mar  6 05:18:53.385: INFO: DNS probes using e2e-tests-dns-rblr9/dns-test-3df4b997-3fcf-11e9-8de6-d63fb0ed442e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:18:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-rblr9" for this suite.
Mar  6 05:18:59.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:18:59.486: INFO: namespace: e2e-tests-dns-rblr9, resource: bindings, ignored listing per whitelist
Mar  6 05:18:59.596: INFO: namespace e2e-tests-dns-rblr9 deletion completed in 6.14762297s

• [SLOW TEST:45.511 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:18:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  6 05:18:59.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31155,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 05:18:59.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31155,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  6 05:19:09.683: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31183,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 05:19:09.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31183,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  6 05:19:19.691: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31210,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 05:19:19.691: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31210,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  6 05:19:29.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31237,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 05:19:29.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-a,UID:591300ab-3fcf-11e9-91c9-005056979acf,ResourceVersion:31237,Generation:0,CreationTimestamp:2019-03-06 05:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  6 05:19:39.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-b,UID:70ef4cec-3fcf-11e9-91c9-005056979acf,ResourceVersion:31265,Generation:0,CreationTimestamp:2019-03-06 05:19:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 05:19:39.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-b,UID:70ef4cec-3fcf-11e9-91c9-005056979acf,ResourceVersion:31265,Generation:0,CreationTimestamp:2019-03-06 05:19:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  6 05:19:49.718: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-b,UID:70ef4cec-3fcf-11e9-91c9-005056979acf,ResourceVersion:31292,Generation:0,CreationTimestamp:2019-03-06 05:19:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 05:19:49.718: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hzd8n,SelfLink:/api/v1/namespaces/e2e-tests-watch-hzd8n/configmaps/e2e-watch-test-configmap-b,UID:70ef4cec-3fcf-11e9-91c9-005056979acf,ResourceVersion:31292,Generation:0,CreationTimestamp:2019-03-06 05:19:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:19:59.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hzd8n" for this suite.
Mar  6 05:20:05.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:20:05.785: INFO: namespace: e2e-tests-watch-hzd8n, resource: bindings, ignored listing per whitelist
Mar  6 05:20:05.874: INFO: namespace e2e-tests-watch-hzd8n deletion completed in 6.149423434s

• [SLOW TEST:66.278 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:20:05.874: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 05:20:05.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-7zfv8" to be "success or failure"
Mar  6 05:20:05.952: INFO: Pod "downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267733ms
Mar  6 05:20:07.956: INFO: Pod "downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006380584s
Mar  6 05:20:09.960: INFO: Pod "downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009708761s
STEP: Saw pod success
Mar  6 05:20:09.960: INFO: Pod "downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:20:09.963: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 05:20:09.983: INFO: Waiting for pod downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:20:09.985: INFO: Pod downwardapi-volume-809303a5-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:20:09.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7zfv8" for this suite.
Mar  6 05:20:16.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:20:16.106: INFO: namespace: e2e-tests-downward-api-7zfv8, resource: bindings, ignored listing per whitelist
Mar  6 05:20:16.135: INFO: namespace e2e-tests-downward-api-7zfv8 deletion completed in 6.145487031s

• [SLOW TEST:10.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:20:16.135: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  6 05:20:16.216: INFO: Waiting up to 5m0s for pod "pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-bkpk8" to be "success or failure"
Mar  6 05:20:16.218: INFO: Pod "pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411415ms
Mar  6 05:20:18.222: INFO: Pod "pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006277726s
Mar  6 05:20:20.226: INFO: Pod "pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010708758s
STEP: Saw pod success
Mar  6 05:20:20.226: INFO: Pod "pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:20:20.229: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 05:20:20.277: INFO: Waiting for pod pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:20:20.279: INFO: Pod pod-86b18918-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:20:20.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bkpk8" for this suite.
Mar  6 05:20:26.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:20:26.355: INFO: namespace: e2e-tests-emptydir-bkpk8, resource: bindings, ignored listing per whitelist
Mar  6 05:20:26.436: INFO: namespace e2e-tests-emptydir-bkpk8 deletion completed in 6.151818824s

• [SLOW TEST:10.301 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:20:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:20:26.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 version'
Mar  6 05:20:26.579: INFO: stderr: ""
Mar  6 05:20:26.579: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-10T08:44:14Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:20:26.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b847c" for this suite.
Mar  6 05:20:32.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:20:32.660: INFO: namespace: e2e-tests-kubectl-b847c, resource: bindings, ignored listing per whitelist
Mar  6 05:20:32.722: INFO: namespace e2e-tests-kubectl-b847c deletion completed in 6.138032929s

• [SLOW TEST:6.286 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:20:32.722: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 05:20:32.798: INFO: Waiting up to 5m0s for pod "pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-xt28l" to be "success or failure"
Mar  6 05:20:32.800: INFO: Pod "pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329132ms
Mar  6 05:20:34.804: INFO: Pod "pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00588409s
Mar  6 05:20:36.809: INFO: Pod "pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010794906s
STEP: Saw pod success
Mar  6 05:20:36.809: INFO: Pod "pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:20:36.811: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 05:20:36.832: INFO: Waiting for pod pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:20:36.834: INFO: Pod pod-9093c8b9-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:20:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xt28l" for this suite.
Mar  6 05:20:42.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:20:42.968: INFO: namespace: e2e-tests-emptydir-xt28l, resource: bindings, ignored listing per whitelist
Mar  6 05:20:42.968: INFO: namespace e2e-tests-emptydir-xt28l deletion completed in 6.130086734s

• [SLOW TEST:10.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:20:42.968: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-k55xq
Mar  6 05:20:47.046: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-k55xq
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 05:20:47.048: INFO: Initial restart count of pod liveness-http is 0
Mar  6 05:21:01.080: INFO: Restart count of pod e2e-tests-container-probe-k55xq/liveness-http is now 1 (14.031529781s elapsed)
Mar  6 05:21:21.120: INFO: Restart count of pod e2e-tests-container-probe-k55xq/liveness-http is now 2 (34.071937407s elapsed)
Mar  6 05:21:41.162: INFO: Restart count of pod e2e-tests-container-probe-k55xq/liveness-http is now 3 (54.113669859s elapsed)
Mar  6 05:22:25.250: INFO: Restart count of pod e2e-tests-container-probe-k55xq/liveness-http is now 4 (1m38.201982579s elapsed)
Mar  6 05:22:41.288: INFO: Restart count of pod e2e-tests-container-probe-k55xq/liveness-http is now 5 (1m54.239335393s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:22:41.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-k55xq" for this suite.
Mar  6 05:22:47.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:22:47.342: INFO: namespace: e2e-tests-container-probe-k55xq, resource: bindings, ignored listing per whitelist
Mar  6 05:22:47.444: INFO: namespace e2e-tests-container-probe-k55xq deletion completed in 6.138639903s

• [SLOW TEST:124.476 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:22:47.445: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 05:22:47.529: INFO: Waiting up to 5m0s for pod "pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-pwkr9" to be "success or failure"
Mar  6 05:22:47.533: INFO: Pod "pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685253ms
Mar  6 05:22:49.541: INFO: Pod "pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011783579s
STEP: Saw pod success
Mar  6 05:22:49.541: INFO: Pod "pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:22:49.543: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 05:22:49.570: INFO: Waiting for pod pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:22:49.573: INFO: Pod pod-e0e23e4f-3fcf-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:22:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pwkr9" for this suite.
Mar  6 05:22:55.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:22:55.653: INFO: namespace: e2e-tests-emptydir-pwkr9, resource: bindings, ignored listing per whitelist
Mar  6 05:22:55.734: INFO: namespace e2e-tests-emptydir-pwkr9 deletion completed in 6.156980465s

• [SLOW TEST:8.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:22:55.734: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-t8zr
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 05:22:55.812: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t8zr" in namespace "e2e-tests-subpath-whbhr" to be "success or failure"
Mar  6 05:22:55.816: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572617ms
Mar  6 05:22:57.820: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007418176s
Mar  6 05:22:59.824: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 4.011074304s
Mar  6 05:23:01.828: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 6.015035137s
Mar  6 05:23:03.832: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 8.01903352s
Mar  6 05:23:05.836: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 10.023968678s
Mar  6 05:23:07.841: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 12.028945525s
Mar  6 05:23:09.845: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 14.032957906s
Mar  6 05:23:11.850: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 16.037261536s
Mar  6 05:23:13.854: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 18.041775552s
Mar  6 05:23:15.858: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Running", Reason="", readiness=false. Elapsed: 20.04598101s
Mar  6 05:23:17.863: INFO: Pod "pod-subpath-test-secret-t8zr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05012505s
STEP: Saw pod success
Mar  6 05:23:17.863: INFO: Pod "pod-subpath-test-secret-t8zr" satisfied condition "success or failure"
Mar  6 05:23:17.866: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-subpath-test-secret-t8zr container test-container-subpath-secret-t8zr: <nil>
STEP: delete the pod
Mar  6 05:23:17.892: INFO: Waiting for pod pod-subpath-test-secret-t8zr to disappear
Mar  6 05:23:17.895: INFO: Pod pod-subpath-test-secret-t8zr no longer exists
STEP: Deleting pod pod-subpath-test-secret-t8zr
Mar  6 05:23:17.895: INFO: Deleting pod "pod-subpath-test-secret-t8zr" in namespace "e2e-tests-subpath-whbhr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:23:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-whbhr" for this suite.
Mar  6 05:23:23.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:23:23.930: INFO: namespace: e2e-tests-subpath-whbhr, resource: bindings, ignored listing per whitelist
Mar  6 05:23:24.045: INFO: namespace e2e-tests-subpath-whbhr deletion completed in 6.141496884s

• [SLOW TEST:28.311 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:23:24.045: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  6 05:23:24.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 create -f - --namespace=e2e-tests-kubectl-tbqjd'
Mar  6 05:23:24.362: INFO: stderr: ""
Mar  6 05:23:24.362: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 05:23:25.367: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:23:25.367: INFO: Found 1 / 1
Mar  6 05:23:25.367: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  6 05:23:25.374: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:23:25.374: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 05:23:25.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 patch pod redis-master-df878 --namespace=e2e-tests-kubectl-tbqjd -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  6 05:23:25.453: INFO: stderr: ""
Mar  6 05:23:25.453: INFO: stdout: "pod/redis-master-df878 patched\n"
STEP: checking annotations
Mar  6 05:23:25.456: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 05:23:25.456: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:23:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tbqjd" for this suite.
Mar  6 05:23:47.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:23:47.567: INFO: namespace: e2e-tests-kubectl-tbqjd, resource: bindings, ignored listing per whitelist
Mar  6 05:23:47.609: INFO: namespace e2e-tests-kubectl-tbqjd deletion completed in 22.147487413s

• [SLOW TEST:23.564 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:23:47.609: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 05:23:47.693: INFO: Waiting up to 5m0s for pod "pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-emptydir-sk74g" to be "success or failure"
Mar  6 05:23:47.697: INFO: Pod "pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650987ms
Mar  6 05:23:49.701: INFO: Pod "pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007552638s
STEP: Saw pod success
Mar  6 05:23:49.701: INFO: Pod "pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:23:49.703: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e container test-container: <nil>
STEP: delete the pod
Mar  6 05:23:49.727: INFO: Waiting for pod pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:23:49.733: INFO: Pod pod-04be9d82-3fd0-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:23:49.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sk74g" for this suite.
Mar  6 05:23:55.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:23:55.881: INFO: namespace: e2e-tests-emptydir-sk74g, resource: bindings, ignored listing per whitelist
Mar  6 05:23:55.882: INFO: namespace e2e-tests-emptydir-sk74g deletion completed in 6.144282603s

• [SLOW TEST:8.272 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:23:55.882: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 05:23:55.984: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:55.984: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:55.984: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:55.987: INFO: Number of nodes with available pods: 0
Mar  6 05:23:55.987: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 05:23:56.992: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:56.992: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:56.992: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:56.995: INFO: Number of nodes with available pods: 0
Mar  6 05:23:56.995: INFO: Node vmw3-k8s-04.local.dev is running more than one daemon pod
Mar  6 05:23:57.995: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:57.995: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:57.995: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:57.999: INFO: Number of nodes with available pods: 4
Mar  6 05:23:57.999: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  6 05:23:58.013: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:58.013: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:58.013: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:58.016: INFO: Number of nodes with available pods: 3
Mar  6 05:23:58.016: INFO: Node vmw3-k8s-07.local.dev is running more than one daemon pod
Mar  6 05:23:59.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:59.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:59.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:23:59.025: INFO: Number of nodes with available pods: 3
Mar  6 05:23:59.025: INFO: Node vmw3-k8s-07.local.dev is running more than one daemon pod
Mar  6 05:24:00.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-01.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:24:00.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-02.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:24:00.021: INFO: DaemonSet pods can't tolerate node vmw3-k8s-03.local.dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 05:24:00.024: INFO: Number of nodes with available pods: 4
Mar  6 05:24:00.024: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-bt4px, will wait for the garbage collector to delete the pods
Mar  6 05:24:00.088: INFO: Deleting DaemonSet.extensions daemon-set took: 6.33586ms
Mar  6 05:24:00.188: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.223232ms
Mar  6 05:24:37.391: INFO: Number of nodes with available pods: 0
Mar  6 05:24:37.391: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 05:24:37.394: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-bt4px/daemonsets","resourceVersion":"32594"},"items":null}

Mar  6 05:24:37.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-bt4px/pods","resourceVersion":"32594"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:24:37.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-bt4px" for this suite.
Mar  6 05:24:43.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:24:43.528: INFO: namespace: e2e-tests-daemonsets-bt4px, resource: bindings, ignored listing per whitelist
Mar  6 05:24:43.552: INFO: namespace e2e-tests-daemonsets-bt4px deletion completed in 6.138450849s

• [SLOW TEST:47.670 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:24:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-261503c7-3fd0-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:24:43.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-configmap-9lwn8" to be "success or failure"
Mar  6 05:24:43.632: INFO: Pod "pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.969801ms
Mar  6 05:24:45.636: INFO: Pod "pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007525688s
STEP: Saw pod success
Mar  6 05:24:45.636: INFO: Pod "pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:24:45.639: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 05:24:45.658: INFO: Waiting for pod pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:24:45.660: INFO: Pod pod-configmaps-2615db8f-3fd0-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:24:45.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9lwn8" for this suite.
Mar  6 05:24:51.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:24:51.811: INFO: namespace: e2e-tests-configmap-9lwn8, resource: bindings, ignored listing per whitelist
Mar  6 05:24:51.833: INFO: namespace e2e-tests-configmap-9lwn8 deletion completed in 6.169166055s

• [SLOW TEST:8.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:24:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  6 05:24:51.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-gfpcb" to be "success or failure"
Mar  6 05:24:51.904: INFO: Pod "downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.17145ms
Mar  6 05:24:53.909: INFO: Pod "downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690083s
Mar  6 05:24:55.912: INFO: Pod "downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012056117s
STEP: Saw pod success
Mar  6 05:24:55.912: INFO: Pod "downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:24:55.915: INFO: Trying to get logs from node vmw3-k8s-04.local.dev pod downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e container client-container: <nil>
STEP: delete the pod
Mar  6 05:24:55.934: INFO: Waiting for pod downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:24:55.937: INFO: Pod downwardapi-volume-2b03bdbf-3fd0-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:24:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gfpcb" for this suite.
Mar  6 05:25:01.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:25:02.061: INFO: namespace: e2e-tests-downward-api-gfpcb, resource: bindings, ignored listing per whitelist
Mar  6 05:25:02.102: INFO: namespace e2e-tests-downward-api-gfpcb deletion completed in 6.160512689s

• [SLOW TEST:10.269 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:25:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-3124f620-3fd0-11e9-8de6-d63fb0ed442e
STEP: Creating a pod to test consume configMaps
Mar  6 05:25:02.188: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-projected-d6kt4" to be "success or failure"
Mar  6 05:25:02.191: INFO: Pod "pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.972769ms
Mar  6 05:25:04.194: INFO: Pod "pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006403238s
STEP: Saw pod success
Mar  6 05:25:04.194: INFO: Pod "pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:25:04.197: INFO: Trying to get logs from node vmw3-k8s-06.local.dev pod pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 05:25:04.215: INFO: Waiting for pod pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:25:04.217: INFO: Pod pod-projected-configmaps-3125d1ff-3fd0-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:25:04.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d6kt4" for this suite.
Mar  6 05:25:10.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:25:10.294: INFO: namespace: e2e-tests-projected-d6kt4, resource: bindings, ignored listing per whitelist
Mar  6 05:25:10.366: INFO: namespace e2e-tests-projected-d6kt4 deletion completed in 6.144252675s

• [SLOW TEST:8.263 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:25:10.366: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  6 05:25:12.962: INFO: Successfully updated pod "labelsupdate360fe9d7-3fd0-11e9-8de6-d63fb0ed442e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:25:14.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8kr76" for this suite.
Mar  6 05:25:37.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:25:37.017: INFO: namespace: e2e-tests-downward-api-8kr76, resource: bindings, ignored listing per whitelist
Mar  6 05:25:37.131: INFO: namespace e2e-tests-downward-api-8kr76 deletion completed in 22.140491457s

• [SLOW TEST:26.765 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:25:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-485c2
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-485c2
STEP: Deleting pre-stop pod
Mar  6 05:25:50.235: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:25:50.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-485c2" for this suite.
Mar  6 05:26:28.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:26:28.370: INFO: namespace: e2e-tests-prestop-485c2, resource: bindings, ignored listing per whitelist
Mar  6 05:26:28.393: INFO: namespace e2e-tests-prestop-485c2 deletion completed in 38.146116412s

• [SLOW TEST:51.263 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:26:28.394: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  6 05:26:28.468: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  6 05:26:33.472: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 05:26:33.472: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  6 05:26:35.477: INFO: Creating deployment "test-rollover-deployment"
Mar  6 05:26:35.485: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  6 05:26:37.491: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  6 05:26:37.498: INFO: Ensure that both replica sets have 1 created replica
Mar  6 05:26:37.505: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  6 05:26:37.513: INFO: Updating deployment test-rollover-deployment
Mar  6 05:26:37.513: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  6 05:26:39.520: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  6 05:26:39.525: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  6 05:26:39.531: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 05:26:39.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446798, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 05:26:41.538: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 05:26:41.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446798, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 05:26:43.539: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 05:26:43.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446798, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 05:26:45.538: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 05:26:45.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446798, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 05:26:47.537: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 05:26:47.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446798, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687446795, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 05:26:49.539: INFO: 
Mar  6 05:26:49.539: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  6 05:26:49.549: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-q2hsz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q2hsz/deployments/test-rollover-deployment,UID:68c1b794-3fd0-11e9-91c9-005056979acf,ResourceVersion:33284,Generation:2,CreationTimestamp:2019-03-06 05:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-06 05:26:35 +0000 UTC 2019-03-06 05:26:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-06 05:26:48 +0000 UTC 2019-03-06 05:26:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 05:26:49.553: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-q2hsz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q2hsz/replicasets/test-rollover-deployment-6b7f9d6597,UID:69f88b76-3fd0-11e9-a5d3-00505697ee14,ResourceVersion:33275,Generation:2,CreationTimestamp:2019-03-06 05:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 68c1b794-3fd0-11e9-91c9-005056979acf 0xc000d64b27 0xc000d64b28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 05:26:49.553: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  6 05:26:49.553: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-q2hsz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q2hsz/replicasets/test-rollover-controller,UID:6492fbd2-3fd0-11e9-91c9-005056979acf,ResourceVersion:33283,Generation:2,CreationTimestamp:2019-03-06 05:26:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 68c1b794-3fd0-11e9-91c9-005056979acf 0xc000d647b7 0xc000d647b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 05:26:49.553: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-q2hsz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-q2hsz/replicasets/test-rollover-deployment-6586df867b,UID:68c49404-3fd0-11e9-a5d3-00505697ee14,ResourceVersion:33221,Generation:2,CreationTimestamp:2019-03-06 05:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 68c1b794-3fd0-11e9-91c9-005056979acf 0xc000d64937 0xc000d64938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 05:26:49.557: INFO: Pod "test-rollover-deployment-6b7f9d6597-kjkhr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-kjkhr,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-q2hsz,SelfLink:/api/v1/namespaces/e2e-tests-deployment-q2hsz/pods/test-rollover-deployment-6b7f9d6597-kjkhr,UID:69fca67f-3fd0-11e9-a5d3-00505697ee14,ResourceVersion:33244,Generation:0,CreationTimestamp:2019-03-06 05:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.160.60"
    ],
    "default": true,
    "dns": {}
}],},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 69f88b76-3fd0-11e9-a5d3-00505697ee14 0xc00148f627 0xc00148f628}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-sfvgm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sfvgm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sfvgm true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vmw3-k8s-06.local.dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00148f690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00148f6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 05:26:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 05:26:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 05:26:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-06 05:26:37 +0000 UTC  }],Message:,Reason:,HostIP:10.5.1.22,PodIP:192.168.160.60,StartTime:2019-03-06 05:26:37 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-06 05:26:38 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://eb06f36a54cf0042f007a7b11b773d30471f0d35f47bd551c999a1ec8ede510e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:26:49.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-q2hsz" for this suite.
Mar  6 05:26:55.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:26:55.635: INFO: namespace: e2e-tests-deployment-q2hsz, resource: bindings, ignored listing per whitelist
Mar  6 05:26:55.714: INFO: namespace e2e-tests-deployment-q2hsz deletion completed in 6.151956068s

• [SLOW TEST:27.321 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:26:55.714: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-tth9q
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-tth9q
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-tth9q
Mar  6 05:26:55.806: INFO: Found 0 stateful pods, waiting for 1
Mar  6 05:27:05.811: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  6 05:27:05.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:27:06.021: INFO: stderr: ""
Mar  6 05:27:06.021: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:27:06.021: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 05:27:06.025: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 05:27:16.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 05:27:16.030: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 05:27:16.043: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999717s
Mar  6 05:27:17.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996833086s
Mar  6 05:27:18.053: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992021294s
Mar  6 05:27:19.056: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987419096s
Mar  6 05:27:20.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983485918s
Mar  6 05:27:21.066: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978496052s
Mar  6 05:27:22.071: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973637465s
Mar  6 05:27:23.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968940674s
Mar  6 05:27:24.081: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963770491s
Mar  6 05:27:25.085: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.417554ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-tth9q
Mar  6 05:27:26.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:27:26.284: INFO: stderr: ""
Mar  6 05:27:26.284: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:27:26.284: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:27:26.288: INFO: Found 1 stateful pods, waiting for 3
Mar  6 05:27:36.292: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:27:36.293: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 05:27:36.293: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  6 05:27:36.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:27:36.516: INFO: stderr: ""
Mar  6 05:27:36.517: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:27:36.517: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 05:27:36.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:27:36.713: INFO: stderr: ""
Mar  6 05:27:36.713: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:27:36.713: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 05:27:36.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 05:27:36.897: INFO: stderr: ""
Mar  6 05:27:36.897: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 05:27:36.897: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 05:27:36.897: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 05:27:36.900: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  6 05:27:46.909: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 05:27:46.909: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 05:27:46.909: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 05:27:46.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999949s
Mar  6 05:27:47.924: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994872742s
Mar  6 05:27:48.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99127092s
Mar  6 05:27:49.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987291377s
Mar  6 05:27:50.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982303087s
Mar  6 05:27:51.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977792681s
Mar  6 05:27:52.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973555748s
Mar  6 05:27:53.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967686011s
Mar  6 05:27:54.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963544284s
Mar  6 05:27:55.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.626742ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-tth9q
Mar  6 05:27:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:27:57.232: INFO: stderr: ""
Mar  6 05:27:57.232: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:27:57.232: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:27:57.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:27:57.426: INFO: stderr: ""
Mar  6 05:27:57.426: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:27:57.426: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:27:57.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610812775 exec --namespace=e2e-tests-statefulset-tth9q ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 05:27:57.617: INFO: stderr: ""
Mar  6 05:27:57.617: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 05:27:57.617: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 05:27:57.617: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  6 05:28:17.635: INFO: Deleting all statefulset in ns e2e-tests-statefulset-tth9q
Mar  6 05:28:17.638: INFO: Scaling statefulset ss to 0
Mar  6 05:28:17.648: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 05:28:17.651: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:28:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-tth9q" for this suite.
Mar  6 05:28:23.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:28:23.745: INFO: namespace: e2e-tests-statefulset-tth9q, resource: bindings, ignored listing per whitelist
Mar  6 05:28:23.805: INFO: namespace e2e-tests-statefulset-tth9q deletion completed in 6.136666786s

• [SLOW TEST:88.091 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  6 05:28:23.806: INFO: >>> kubeConfig: /tmp/kubeconfig-610812775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  6 05:28:23.879: INFO: Waiting up to 5m0s for pod "downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e" in namespace "e2e-tests-downward-api-dh8xv" to be "success or failure"
Mar  6 05:28:23.881: INFO: Pod "downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944406ms
Mar  6 05:28:25.885: INFO: Pod "downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006442161s
STEP: Saw pod success
Mar  6 05:28:25.885: INFO: Pod "downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e" satisfied condition "success or failure"
Mar  6 05:28:25.887: INFO: Trying to get logs from node vmw3-k8s-07.local.dev pod downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e container dapi-container: <nil>
STEP: delete the pod
Mar  6 05:28:25.912: INFO: Waiting for pod downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e to disappear
Mar  6 05:28:25.914: INFO: Pod downward-api-a95cf0c1-3fd0-11e9-8de6-d63fb0ed442e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  6 05:28:25.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dh8xv" for this suite.
Mar  6 05:28:31.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 05:28:31.957: INFO: namespace: e2e-tests-downward-api-dh8xv, resource: bindings, ignored listing per whitelist
Mar  6 05:28:32.051: INFO: namespace e2e-tests-downward-api-dh8xv deletion completed in 6.132591664s

• [SLOW TEST:8.246 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSMar  6 05:28:32.051: INFO: Running AfterSuite actions on all nodes
Mar  6 05:28:32.051: INFO: Running AfterSuite actions on node 1
Mar  6 05:28:32.051: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5555.872 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h32m36.715741149s
Test Suite Passed
